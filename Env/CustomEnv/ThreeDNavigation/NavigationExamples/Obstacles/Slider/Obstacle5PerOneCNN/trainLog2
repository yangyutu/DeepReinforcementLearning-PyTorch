eward sum = 0.7700431458051551
running average episode reward sum: 0.6286040913851241
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.93488963,  2.29264087, 44.        ]), 'distance': 2.741509993040039, 'localFrame': array([[-0.10420641, -0.42388239, -0.89970259],
       [ 0.97108601, -0.23872987,  0.        ],
       [-0.21478588, -0.8736886 ,  0.43650344]]), 'currentState': array([ 1.61658481,  2.91311801, 46.6513331 , -0.10420641, -0.42388239,
       -0.89970259]), 'targetState': array([ 1.93488963,  2.29264087, 44.        ]), 'previousTarget': array([ 1.93488963,  2.29264087, 44.        ])}
episode index:18393
target thresh 84.90177946366816
target distance 65.0
model initialize at round 18393
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-14.92581879,  -5.95233805,  31.27766221]), 'distance': 27.499999999999996, 'localFrame': array([[-0.80107135,  0.46231484,  0.38019689],
       [-0.49985085, -0.8661115 ,  0.        ],
       [ 0.3292929 , -0.19004174,  0.92490558]]), 'currentState': array([-7.20596218, -3.47478223,  5.        , -0.80107135,  0.46231484,
        0.38019689]), 'targetState': array([-26.30167456,  -9.60322421,  70.        ]), 'previousTarget': array([-14.92581879,  -5.95233805,  31.27766221])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6286028084654629
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-26.30167456,  -9.60322421,  70.        ]), 'distance': 2.874120739007271, 'localFrame': array([[ 0.48039216, -0.83149931,  0.27898436],
       [ 0.86587846,  0.50025444,  0.        ],
       [-0.13956316,  0.24156654,  0.96029565]]), 'currentState': array([-26.00334511,  -8.90133229,  67.22891406,   0.48039216,
        -0.83149931,   0.27898436]), 'targetState': array([-26.30167456,  -9.60322421,  70.        ]), 'previousTarget': array([-26.30167456,  -9.60322421,  70.        ])}
episode index:18394
target thresh 84.90328921023321
target distance 47.0
model initialize at round 18394
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.12320035, -13.93264706,  51.13377154]), 'distance': 27.5, 'localFrame': array([[-0.90898839, -0.02396759, -0.41613179],
       [ 0.02635817, -0.99965256,  0.        ],
       [-0.41598721, -0.01096847,  0.90930431]]), 'currentState': array([-1.65400847e+01, -4.61272660e+00,  7.55989537e+01, -9.08988389e-01,
       -2.39675937e-02, -4.16131786e-01]), 'targetState': array([ -0.8524386 , -21.98347899,  30.        ]), 'previousTarget': array([ -7.75519788, -13.59705446,  52.28087401])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6286096644849648
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.8524386 , -21.98347899,  30.        ]), 'distance': 4.150058973243938, 'localFrame': array([[ 0.70583169, -0.44858087, -0.54824887],
       [ 0.53637773,  0.84397804,  0.        ],
       [ 0.46271001, -0.29406848,  0.83631524]]), 'currentState': array([ -2.22619143, -19.32464609,  32.87513482,   0.70583169,
        -0.44858087,  -0.54824887]), 'targetState': array([ -0.8524386 , -21.98347899,  30.        ]), 'previousTarget': array([ -0.8524386 , -21.98347899,  30.        ])}
episode index:18395
target thresh 84.90479880583115
target distance 48.432584691948264
model initialize at round 18395
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.96088348, -22.48142653,  88.47836034]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.99516268,  0.09750498,  0.01200111],
       [-0.097512  ,  0.99523435,  0.        ],
       [-0.01194392, -0.00117025,  0.99992798]]), 'currentState': array([-8.01771403e+00, -3.58501632e+01,  8.68803179e+01,  9.95162676e-01,
        9.75049779e-02,  1.20011087e-02]), 'targetState': array([38.79305832, -9.75185243, 90.        ]), 'previousTarget': array([ 14.53215105, -22.76097738,  87.9963153 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285754934877652
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 93, 'trapConfig': [], 'currentTarget': array([ 18.72789305, -17.86306509,  91.11469036]), 'distance': 27.5, 'localFrame': array([[ 0.71021147,  0.37626542, -0.59499916],
       [-0.46815117,  0.88364839,  0.        ],
       [ 0.52577005,  0.27854955,  0.80372632]]), 'currentState': array([ -6.73398937, -28.15586559,  92.5291873 ,   0.71021147,
         0.37626542,  -0.59499916]), 'targetState': array([38.79305832, -9.75185243, 90.        ]), 'previousTarget': array([ 18.72789305, -17.86306509,  91.11469036])}
episode index:18396
target thresh 84.90630825047708
target distance 50.83604972923857
model initialize at round 18396
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([35.99184065, -3.17015891, 35.8175676 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.79552496, -0.386435  , -0.46669907],
       [ 0.43693796,  0.89949164,  0.        ],
       [ 0.41979192, -0.20391854,  0.88441618]]), 'currentState': array([39.33008578, 16.52221142, 54.72038936,  0.79552496, -0.386435  ,
       -0.46669907]), 'targetState': array([ 30.72604746, -34.23317116,   6.        ]), 'previousTarget': array([34.8662919 , -3.10664696, 36.002325  ])}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.6285687701304407
{'scaleFactor': 20, 'timeStep': 69, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([ 30.72604746, -34.23317116,   6.        ]), 'distance': 1.8778593238851062, 'localFrame': array([[ 0.019021  , -0.99354527, -0.11183026],
       [ 0.99981679,  0.01914107,  0.        ],
       [ 0.00214055, -0.11180977,  0.99372732]]), 'currentState': array([ 3.10297818e+01, -3.24523338e+01,  6.51256167e+00,  1.90210033e-02,
       -9.93545265e-01, -1.11830261e-01]), 'targetState': array([ 30.72604746, -34.23317116,   6.        ]), 'previousTarget': array([ 30.72604746, -34.23317116,   6.        ])}
episode index:18397
target thresh 84.90781754418609
target distance 39.0
model initialize at round 18397
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.18329907, -9.46859507, 71.16013916]), 'distance': 27.5, 'localFrame': array([[-0.27047506,  0.27347646,  0.92306764],
       [-0.71099762, -0.70319442,  0.        ],
       [ 0.64909601, -0.65629889,  0.38463766]]), 'currentState': array([ 15.21370744, -11.4762415 ,  50.81881405,  -0.27047506,
         0.27347646,   0.92306764]), 'targetState': array([-18.41352757,  -7.80653587,  88.        ]), 'previousTarget': array([-2.94888636, -9.69581155, 69.86127938])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6285748105480543
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.41352757,  -7.80653587,  88.        ]), 'distance': 3.5428846459865513, 'localFrame': array([[-0.11930182,  0.45653509,  0.88167045],
       [-0.96751071, -0.25283004,  0.        ],
       [ 0.22291277, -0.8530256 ,  0.47186567]]), 'currentState': array([-16.87067498,  -9.91337471,  85.60565926,  -0.11930182,
         0.45653509,   0.88167045]), 'targetState': array([-18.41352757,  -7.80653587,  88.        ]), 'previousTarget': array([-18.41352757,  -7.80653587,  88.        ])}
episode index:18398
target thresh 84.90932668697327
target distance 29.339942451775343
model initialize at round 18398
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.75090231, -32.4641511 ,  46.65062151]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.5507907 , -0.12409576,  0.82536649],
       [ 0.21979516,  0.97554605,  0.        ],
       [-0.80518302,  0.18141156,  0.56459733]]), 'currentState': array([-2.92256808, -6.22220668, 44.13516198,  0.5507907 , -0.12409576,
        0.82536649]), 'targetState': array([-11.8381993 , -36.10896062,  47.        ]), 'previousTarget': array([-11.0787167 , -33.16703576,  46.5989188 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285406470168544
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 87, 'trapConfig': [], 'currentTarget': array([-11.8381993 , -36.10896062,  47.        ]), 'distance': 11.66179343499354, 'localFrame': array([[-0.59383524, -0.78851331, -0.16002022],
       [ 0.79880694, -0.60158746,  0.        ],
       [-0.09626616, -0.12782526,  0.98711374]]), 'currentState': array([-11.34437122, -24.45835565,  46.86975557,  -0.59383524,
        -0.78851331,  -0.16002022]), 'targetState': array([-11.8381993 , -36.10896062,  47.        ]), 'previousTarget': array([-11.8381993 , -36.10896062,  47.        ])}
episode index:18399
target thresh 84.91083567885373
target distance 33.376104328116504
model initialize at round 18399
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.29126456, -16.76445808,  33.64203941]), 'distance': 27.5, 'localFrame': array([[ 0.3864835 ,  0.82029856, -0.42159313],
       [-0.90462289,  0.42621289,  0.        ],
       [ 0.17968843,  0.3813828 ,  0.90678511]]), 'currentState': array([ -2.57982646, -36.68429788,  52.59900948,   0.3864835 ,
         0.82029856,  -0.42159313]), 'targetState': array([-2.11405011, -4.53109172, 22.        ]), 'previousTarget': array([ -2.33294849, -18.05820281,  34.96938525])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6285483373700493
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.11405011, -4.53109172, 22.        ]), 'distance': 1.630168771155178, 'localFrame': array([[-0.07181257,  0.97479766,  0.21121664],
       [-0.99729742, -0.07347011,  0.        ],
       [ 0.01551811, -0.21064581,  0.97743927]]), 'currentState': array([-1.75778662, -6.08928751, 21.67976195, -0.07181257,  0.97479766,
        0.21121664]), 'targetState': array([-2.11405011, -4.53109172, 22.        ]), 'previousTarget': array([-2.11405011, -4.53109172, 22.        ])}
episode index:18400
target thresh 84.91234451984253
target distance 37.653392407701695
model initialize at round 18400
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.22783465, 10.26665858,  4.88365079]), 'distance': 27.499999999999996, 'localFrame': array([[-0.77087168, -0.45395397, -0.44685865],
       [ 0.50743533, -0.86168984,  0.        ],
       [-0.38505356, -0.22675186,  0.89460458]]), 'currentState': array([ 1.80770703, 37.6893767 ,  4.56568671, -0.77087168, -0.45395397,
       -0.44685865]), 'targetState': array([-0.97267875,  0.23215523,  5.        ]), 'previousTarget': array([ 0.10067967, 10.54289416,  5.27383293])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285141789907563
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([-0.97267875,  0.23215523,  5.        ]), 'distance': 15.411775252660682, 'localFrame': array([[ 0.87796664, -0.44323286,  0.18088452],
       [ 0.45066691,  0.89269219,  0.        ],
       [-0.1614742 ,  0.08151867,  0.98350434]]), 'currentState': array([-3.01204177, 15.50507099,  4.68084643,  0.87796664, -0.44323286,
        0.18088452]), 'targetState': array([-0.97267875,  0.23215523,  5.        ]), 'previousTarget': array([-0.97267875,  0.23215523,  5.        ])}
episode index:18401
target thresh 84.9138532099548
target distance 42.0
model initialize at round 18401
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.15022971,  23.29815587,  56.06942024]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.63466519, -0.43855331,  0.63629481],
       [ 0.5684822 ,  0.82269557,  0.        ],
       [-0.52347692,  0.36172227,  0.77144599]]), 'currentState': array([-33.12600647,  24.70995857,  34.48053846,   0.63466519,
        -0.43855331,   0.63629481]), 'targetState': array([-0.47840956, 21.99479766, 76.        ]), 'previousTarget': array([-17.45514911,  23.58161826,  55.24781285])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6285145958643067
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-0.47840956, 21.99479766, 76.        ]), 'distance': 2.8561591541608147, 'localFrame': array([[ 0.39471433, -0.07652698,  0.91561139],
       [ 0.19033514,  0.98171917,  0.        ],
       [-0.89887326,  0.17427302,  0.40206439]]), 'currentState': array([-0.30092067, 22.64975258, 73.22562135,  0.39471433, -0.07652698,
        0.91561139]), 'targetState': array([-0.47840956, 21.99479766, 76.        ]), 'previousTarget': array([-0.47840956, 21.99479766, 76.        ])}
episode index:18402
target thresh 84.91536174920557
target distance 39.6222951999347
model initialize at round 18402
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.53382886,  4.5715859 , 47.73355549]), 'distance': 27.500000000000004, 'localFrame': array([[-0.6124737 , -0.31136558, -0.72658615],
       [ 0.4531753 , -0.89142142,  0.        ],
       [-0.64769446, -0.32927089,  0.68707537]]), 'currentState': array([32.43055033, 18.02001118, 45.65111443, -0.6124737 , -0.31136558,
       -0.72658615]), 'targetState': array([-5.99905378, -3.60712542, 49.        ]), 'previousTarget': array([ 9.42840294,  4.69114626, 48.22127395])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6285198377129233
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-5.99905378, -3.60712542, 49.        ]), 'distance': 3.8980723857144897, 'localFrame': array([[ 0.3757636 , -0.82070775,  0.43039576],
       [ 0.90923013,  0.41629384,  0.        ],
       [-0.1791711 ,  0.39132879,  0.90264029]]), 'currentState': array([-3.18804341, -3.70913925, 46.30133695,  0.3757636 , -0.82070775,
        0.43039576]), 'targetState': array([-5.99905378, -3.60712542, 49.        ]), 'previousTarget': array([-5.99905378, -3.60712542, 49.        ])}
episode index:18403
target thresh 84.91687013760998
target distance 49.016717304065125
model initialize at round 18403
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.4896007 , -14.44100313,  80.39646704]), 'distance': 27.5, 'localFrame': array([[-0.7114746 ,  0.48788273, -0.50574137],
       [-0.56553974, -0.82472105,  0.        ],
       [-0.41709556,  0.28601684,  0.86268515]]), 'currentState': array([  9.43701884, -36.11194098,  80.72813453,  -0.7114746 ,
         0.48788273,  -0.50574137]), 'targetState': array([-27.72325932,  11.46389519,  80.        ]), 'previousTarget': array([ -6.39003046, -15.87517846,  80.55774999])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.628523908994019
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.72325932,  11.46389519,  80.        ]), 'distance': 2.678721055288423, 'localFrame': array([[-0.76751611,  0.62204093,  0.154868  ],
       [-0.6296374 , -0.77688914,  0.        ],
       [ 0.12031527, -0.09751069,  0.98793517]]), 'currentState': array([-25.99132882,   9.97610786,  78.59912452,  -0.76751611,
         0.62204093,   0.154868  ]), 'targetState': array([-27.72325932,  11.46389519,  80.        ]), 'previousTarget': array([-27.72325932,  11.46389519,  80.        ])}
episode index:18404
target thresh 84.91837837518308
target distance 29.086746511651153
model initialize at round 18404
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.64738753, -19.03852102,  13.20999034]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.64856743,  0.28154401, -0.70717272],
       [-0.39820049,  0.91729841,  0.        ],
       [ 0.64868841,  0.28159653,  0.70704084]]), 'currentState': array([-31.62818611, -22.33228147,   4.81956442,   0.64856743,
         0.28154401,  -0.70717272]), 'targetState': array([ -3.20113725, -18.72839343,  14.        ]), 'previousTarget': array([ -6.1131682 , -19.21724575,  13.19907688])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6285355590502206
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.20113725, -18.72839343,  14.        ]), 'distance': 3.458341901390276, 'localFrame': array([[ 0.50513188,  0.6127124 ,  0.60780367],
       [-0.77159321,  0.63611628,  0.        ],
       [-0.38663381, -0.46897719,  0.79408734]]), 'currentState': array([ -4.92127917, -17.71871237,  11.17479476,   0.50513188,
         0.6127124 ,   0.60780367]), 'targetState': array([ -3.20113725, -18.72839343,  14.        ]), 'previousTarget': array([ -3.20113725, -18.72839343,  14.        ])}
episode index:18405
target thresh 84.91988646193997
target distance 47.80369330851771
model initialize at round 18405
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.95509511,  -0.76169542,  90.81827948]), 'distance': 27.5, 'localFrame': array([[ 0.78627362,  0.20298485, -0.58358456],
       [-0.24996522,  0.96825482,  0.        ],
       [ 0.56505856,  0.14587584,  0.81205238]]), 'currentState': array([-35.60646673, -11.00286396,  81.22760236,   0.78627362,
         0.20298485,  -0.58358456]), 'targetState': array([ 10.68775703,   9.04277887, 100.        ]), 'previousTarget': array([-13.31596335,  -1.51104221,  90.96164047])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.628541196766759
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.68775703,   9.04277887, 100.        ]), 'distance': 3.862873800091837, 'localFrame': array([[ 0.9359316 ,  0.32855696,  0.12681628],
       [-0.33123125,  0.94354961,  0.        ],
       [-0.11965745, -0.04200551,  0.99192622]]), 'currentState': array([ 8.06937795,  6.21019544, 99.79419403,  0.9359316 ,  0.32855696,
        0.12681628]), 'targetState': array([ 10.68775703,   9.04277887, 100.        ]), 'previousTarget': array([ 10.68775703,   9.04277887, 100.        ])}
episode index:18406
target thresh 84.92139439789572
target distance 46.0
model initialize at round 18406
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.69941124,  1.93827574, 62.3302329 ]), 'distance': 27.5, 'localFrame': array([[-0.07164897, -0.77613904, -0.62647795],
       [ 0.99576605, -0.09192376,  0.        ],
       [-0.05758821, -0.62382547,  0.77943914]]), 'currentState': array([-3.70749307e-01,  7.19728907e+00,  8.93014682e+01, -7.16489734e-02,
       -7.76139037e-01, -6.26477949e-01]), 'targetState': array([ 1.38703758, -1.44087708, 45.        ]), 'previousTarget': array([ 0.80295632,  2.47371071, 64.07360848])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6285484658229649
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.38703758, -1.44087708, 45.        ]), 'distance': 1.7188872997006823, 'localFrame': array([[ 0.16682113, -0.09532657, -0.98136821],
       [ 0.49613976,  0.86824267,  0.        ],
       [ 0.85206575, -0.48689579,  0.19213652]]), 'currentState': array([ 1.97860749, -1.77481357, 46.57895694,  0.16682113, -0.09532657,
       -0.98136821]), 'targetState': array([ 1.38703758, -1.44087708, 45.        ]), 'previousTarget': array([ 1.38703758, -1.44087708, 45.        ])}
episode index:18407
target thresh 84.92290218306542
target distance 31.0
model initialize at round 18407
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.65303111, 10.65115257, 86.50548848]), 'distance': 27.499999999999996, 'localFrame': array([[-0.74136078, -0.5620612 ,  0.36670342],
       [ 0.60414738, -0.79687261,  0.        ],
       [ 0.29221591,  0.22154291,  0.9303379 ]]), 'currentState': array([ 8.5379513 , -3.01277505, 68.97268578, -0.74136078, -0.5620612 ,
        0.36670342]), 'targetState': array([-19.1913128 ,  20.38856329,  99.        ]), 'previousTarget': array([-6.82101397, 10.90977693, 85.78531326])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6285434527720839
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 29, 'trapConfig': [], 'currentTarget': array([-19.1913128 ,  20.38856329,  99.        ]), 'distance': 2.842072895121556, 'localFrame': array([[-0.98560603,  0.12604177, -0.11266863],
       [-0.12684947, -0.99192198,  0.        ],
       [-0.11175849,  0.01429196,  0.99363262]]), 'currentState': array([-17.10422071,  19.5328452 ,  97.27107793,  -0.98560603,
         0.12604177,  -0.11266863]), 'targetState': array([-19.1913128 ,  20.38856329,  99.        ]), 'previousTarget': array([-19.1913128 ,  20.38856329,  99.        ])}
episode index:18408
target thresh 84.92440981746412
target distance 66.0
model initialize at round 18408
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([30.25341248, -8.88629338, 49.84593859]), 'distance': 27.5, 'localFrame': array([[ 0.35068798,  0.8706755 , -0.34487987],
       [-0.92758582,  0.37361014,  0.        ],
       [ 0.12885062,  0.31990568,  0.93864683]]), 'currentState': array([ 19.34646265, -15.32971072,  74.25435555,   0.35068798,
         0.8706755 ,  -0.34487987]), 'targetState': array([48.95235366,  2.16034051,  8.        ]), 'previousTarget': array([29.57860954, -9.93988399, 49.86010804])}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.6285324280923412
{'scaleFactor': 20, 'timeStep': 86, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([48.95235366,  2.16034051,  8.        ]), 'distance': 2.4989053574938773, 'localFrame': array([[ 0.66731217, -0.40765916, -0.62330448],
       [ 0.52131712,  0.85336303,  0.        ],
       [ 0.531905  , -0.3249393 ,  0.78197924]]), 'currentState': array([47.23131414,  2.27607809,  9.80808069,  0.66731217, -0.40765916,
       -0.62330448]), 'targetState': array([48.95235366,  2.16034051,  8.        ]), 'previousTarget': array([48.95235366,  2.16034051,  8.        ])}
episode index:18409
target thresh 84.92591730110695
target distance 63.7327137246593
model initialize at round 18409
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.95923301, -9.89414858, 73.86913922]), 'distance': 27.499999999999996, 'localFrame': array([[-0.81569344, -0.49478195, -0.29972493],
       [ 0.5186254 , -0.85500157,  0.        ],
       [-0.25626529, -0.15544496,  0.95402566]]), 'currentState': array([ 1.91053335, 17.35105323, 76.02654703, -0.81569344, -0.49478195,
       -0.29972493]), 'targetState': array([  9.01370232, -46.12757495,  71.        ]), 'previousTarget': array([ 6.00637628, -9.71814345, 73.85641622])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6285349917328066
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.01370232, -46.12757495,  71.        ]), 'distance': 3.0262520685975076, 'localFrame': array([[ 0.94426467, -0.32213733,  0.06776269],
       [ 0.32287947,  0.94644009,  0.        ],
       [-0.06413332,  0.02187918,  0.99770147]]), 'currentState': array([ 7.74435578e+00, -4.52692615e+01,  6.83903527e+01,  9.44264667e-01,
       -3.22137326e-01,  6.77626867e-02]), 'targetState': array([  9.01370232, -46.12757495,  71.        ]), 'previousTarget': array([  9.01370232, -46.12757495,  71.        ])}
episode index:18410
target thresh 84.92742463400893
target distance 47.0
model initialize at round 18410
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.52023358, -19.36692673,  35.23795744]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.67884467,  0.24413386, -0.6925089 ],
       [-0.33841239,  0.9409979 ,  0.        ],
       [ 0.65164942,  0.23435359,  0.72140934]]), 'currentState': array([ -9.78332861, -24.97667209,  14.61351823,   0.67884467,
         0.24413386,  -0.6925089 ]), 'targetState': array([ 30.81212651, -11.81578859,  63.        ]), 'previousTarget': array([  7.29743558, -19.91669216,  36.16835233])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6285350617257168
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 30.81212651, -11.81578859,  63.        ]), 'distance': 2.4179326352715744, 'localFrame': array([[ 0.22838538, -0.97134431,  0.06580538],
       [ 0.9734543 ,  0.22888149,  0.        ],
       [-0.01506163,  0.06405853,  0.99783248]]), 'currentState': array([ 30.60797892, -11.82039346,  60.59070533,   0.22838538,
        -0.97134431,   0.06580538]), 'targetState': array([ 30.81212651, -11.81578859,  63.        ]), 'previousTarget': array([ 30.81212651, -11.81578859,  63.        ])}
episode index:18411
target thresh 84.92893181618517
target distance 66.0
model initialize at round 18411
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.19003659, -4.33556408, 70.9191504 ]), 'distance': 27.5, 'localFrame': array([[-0.16899606, -0.70163785, -0.69220276],
       [ 0.97219738, -0.23416286,  0.        ],
       [-0.16208818, -0.67295771,  0.72170309]]), 'currentState': array([-1.10419864, -0.91315838, 97.86533163, -0.16899606, -0.70163785,
       -0.69220276]), 'targetState': array([ 9.23296157, -9.15163486, 33.        ]), 'previousTarget': array([ 3.77454346, -3.74129614, 72.01842249])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628500924474917
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 75, 'trapConfig': [], 'currentTarget': array([10.50082753, -9.48604386, 47.59414298]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.28987558,  0.74939937, -0.59529214],
       [-0.93265794,  0.36076193,  0.        ],
       [ 0.21475874,  0.55520394,  0.80350934]]), 'currentState': array([ 12.88030501, -10.11364858,  74.9838162 ,   0.28987558,
         0.74939937,  -0.59529214]), 'targetState': array([ 9.23296157, -9.15163486, 33.        ]), 'previousTarget': array([10.26973955, -9.74661166, 49.04614371])}
episode index:18412
target thresh 84.93043884765072
target distance 7.804092792892467
model initialize at round 18412
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.47364149, -8.98752823, 26.        ]), 'distance': 10.112719341446494, 'localFrame': array([[-0.54472525, -0.48036399, -0.68740442],
       [ 0.66140799, -0.75002632,  0.        ],
       [-0.51557141, -0.45465477,  0.72627485]]), 'currentState': array([ 0.44182817, -1.21250671, 19.59859015, -0.54472525, -0.48036399,
       -0.68740442]), 'targetState': array([-0.47364149, -8.98752823, 26.        ]), 'previousTarget': array([-0.47364149, -8.98752823, 26.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6285179222061354
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.47364149, -8.98752823, 26.        ]), 'distance': 2.902395974717294, 'localFrame': array([[ 0.16052645, -0.69438911,  0.70146634],
       [ 0.97430418,  0.22523624,  0.        ],
       [-0.15799564,  0.68344159,  0.71270258]]), 'currentState': array([-1.00936704, -7.68921292, 23.46006341,  0.16052645, -0.69438911,
        0.70146634]), 'targetState': array([-0.47364149, -8.98752823, 26.        ]), 'previousTarget': array([-0.47364149, -8.98752823, 26.        ])}
episode index:18413
target thresh 84.93194572842067
target distance 19.0
model initialize at round 18413
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.49613893,  -4.57255825,  31.        ]), 'distance': 22.257551130705004, 'localFrame': array([[-0.61898408, -0.17439423, -0.76579721],
       [ 0.27118499, -0.96252725,  0.        ],
       [-0.73710068, -0.20767271,  0.64308214]]), 'currentState': array([-14.98111678,   7.15507793,  49.09545972,  -0.61898408,
        -0.17439423,  -0.76579721]), 'targetState': array([-20.49613893,  -4.57255825,  31.        ]), 'previousTarget': array([-20.49613893,  -4.57255825,  31.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6285319260591554
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.49613893,  -4.57255825,  31.        ]), 'distance': 2.78130186581149, 'localFrame': array([[-0.20981552,  0.19389627, -0.95832233],
       [-0.67869656, -0.73441881,  0.        ],
       [-0.70380994,  0.65041006,  0.2856892 ]]), 'currentState': array([-18.87107462,  -5.0285469 ,  33.21062896,  -0.20981552,
         0.19389627,  -0.95832233]), 'targetState': array([-20.49613893,  -4.57255825,  31.        ]), 'previousTarget': array([-20.49613893,  -4.57255825,  31.        ])}
episode index:18414
target thresh 84.93345245851006
target distance 50.0
model initialize at round 18414
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.74858081,  1.48922831, 62.34948953]), 'distance': 27.500000000000004, 'localFrame': array([[-0.0561795 ,  0.74295527, -0.66697926],
       [-0.99715329, -0.07540101,  0.        ],
       [-0.05029091,  0.66508056,  0.74507628]]), 'currentState': array([ 7.31614383, -9.83610173, 38.41738181, -0.0561795 ,  0.74295527,
       -0.66697926]), 'targetState': array([23.02525421, 14.10098112, 89.        ]), 'previousTarget': array([15.13027816,  0.63364209, 62.65775355])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6285249374685413
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.02525421, 14.10098112, 89.        ]), 'distance': 4.12841322205656, 'localFrame': array([[-0.67927036,  0.36730331,  0.63535822],
       [-0.47564745, -0.87963601,  0.        ],
       [ 0.55888396, -0.30220652,  0.77221755]]), 'currentState': array([25.92638659, 13.31782001, 86.16911935, -0.67927036,  0.36730331,
        0.63535822]), 'targetState': array([23.02525421, 14.10098112, 89.        ]), 'previousTarget': array([23.02525421, 14.10098112, 89.        ])}
episode index:18415
target thresh 84.93495903793398
target distance 73.0
model initialize at round 18415
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.30673555, -6.6098078 , 28.3135105 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.78002629,  0.20176673, -0.59232523],
       [-0.25042446, -0.96813614,  0.        ],
       [-0.57345146,  0.14833273,  0.80569897]]), 'currentState': array([ 10.06104549, -14.36691857,   2.02598287,  -0.78002629,
         0.20176673,  -0.59232523]), 'targetState': array([16.38049544,  7.46186097, 76.        ]), 'previousTarget': array([13.2752212 , -6.14521201, 29.34630836])}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6285174095219479
{'scaleFactor': 20, 'timeStep': 72, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([16.38049544,  7.46186097, 76.        ]), 'distance': 3.085753553185472, 'localFrame': array([[-0.72323715, -0.56122988,  0.40242893],
       [ 0.61306366, -0.79003351,  0.        ],
       [ 0.31793234,  0.24671455,  0.91545123]]), 'currentState': array([18.27478813,  6.45650225, 73.78126521, -0.72323715, -0.56122988,
        0.40242893]), 'targetState': array([16.38049544,  7.46186097, 76.        ]), 'previousTarget': array([16.38049544,  7.46186097, 76.        ])}
episode index:18416
target thresh 84.93646546670749
target distance 39.163424081996524
model initialize at round 18416
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.0728048 ,  22.06296735,  44.43310538]), 'distance': 27.5, 'localFrame': array([[-0.48104591, -0.60434071, -0.63511191],
       [ 0.78239896, -0.62277754,  0.        ],
       [-0.39553343, -0.4969109 ,  0.77242014]]), 'currentState': array([-9.01276167, 43.98910213, 60.52691362, -0.48104591, -0.60434071,
       -0.63511191]), 'targetState': array([-16.2093492 ,   5.12415832,  32.        ]), 'previousTarget': array([-12.50274253,  22.75445261,  45.50517329])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284832824974856
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([-16.2093492 ,   5.12415832,  32.        ]), 'distance': 21.89432895864366, 'localFrame': array([[-0.87060653, -0.16920851, -0.46196618],
       [ 0.19078699, -0.98163146,  0.        ],
       [-0.45348054, -0.08813714,  0.88689754]]), 'currentState': array([-10.88634902,  21.18715894,  45.89270746,  -0.87060653,
        -0.16920851,  -0.46196618]), 'targetState': array([-16.2093492 ,   5.12415832,  32.        ]), 'previousTarget': array([-16.2093492 ,   5.12415832,  32.        ])}
episode index:18417
target thresh 84.93797174484565
target distance 32.0
model initialize at round 18417
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.55828458, -1.46953311, 57.46436275]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.38275689, -0.90746906, -0.17319661],
       [ 0.92139384,  0.38863016,  0.        ],
       [ 0.06730943, -0.15958229,  0.98488727]]), 'currentState': array([ 4.04137481,  4.07766382, 82.26099993,  0.38275689, -0.90746906,
       -0.17319661]), 'targetState': array([17.72411827, -3.13936801, 50.        ]), 'previousTarget': array([14.07230238, -1.07362047, 57.78901368])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6284922482375849
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([17.72411827, -3.13936801, 50.        ]), 'distance': 2.168433080410354, 'localFrame': array([[ 0.84591242,  0.20312625, -0.49312463],
       [-0.23348954,  0.97235931,  0.        ],
       [ 0.47949433,  0.11513944,  0.86995867]]), 'currentState': array([17.35272848, -3.83359848, 52.02044938,  0.84591242,  0.20312625,
       -0.49312463]), 'targetState': array([17.72411827, -3.13936801, 50.        ]), 'previousTarget': array([17.72411827, -3.13936801, 50.        ])}
episode index:18418
target thresh 84.93947787236354
target distance 38.86501991392051
model initialize at round 18418
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.9249476 , -7.8958963 , 49.35524207]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.54689511, -0.5035634 ,  0.66882706],
       [ 0.67736237,  0.73564952,  0.        ],
       [-0.49202231,  0.45303829,  0.74341803]]), 'currentState': array([ 5.75516916,  3.10761748, 39.43888935,  0.54689511, -0.5035634 ,
        0.66882706]), 'targetState': array([ 44.45057213, -15.26914003,  56.        ]), 'previousTarget': array([28.29856954, -7.16993831, 48.5193388 ])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6285003555784373
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 44.45057213, -15.26914003,  56.        ]), 'distance': 3.453389201645347, 'localFrame': array([[ 0.82055045,  0.2555211 ,  0.51127872],
       [-0.29731982,  0.95477795,  0.        ],
       [-0.48815765, -0.1520133 ,  0.85941496]]), 'currentState': array([ 41.50651601, -13.62598902,  55.25268126,   0.82055045,
         0.2555211 ,   0.51127872]), 'targetState': array([ 44.45057213, -15.26914003,  56.        ]), 'previousTarget': array([ 44.45057213, -15.26914003,  56.        ])}
episode index:18419
target thresh 84.94098384927621
target distance 41.60686039607956
model initialize at round 18419
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.6798511 , 14.08996494, 43.19042809]), 'distance': 27.499999999999996, 'localFrame': array([[-0.30515811, -0.91580746,  0.26110385],
       [ 0.9487178 , -0.31612424,  0.        ],
       [ 0.08254126,  0.24771387,  0.96531072]]), 'currentState': array([36.75912495, 24.72771628, 27.70136718, -0.30515811, -0.91580746,
        0.26110385]), 'targetState': array([-3.8149178 ,  3.23208945, 59.        ]), 'previousTarget': array([17.63083673, 15.08368905, 43.02142377])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6285036644128722
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.8149178 ,  3.23208945, 59.        ]), 'distance': 3.061446139683798, 'localFrame': array([[ 0.16956533, -0.87144218,  0.46025659],
       [ 0.98159043,  0.19099799,  0.        ],
       [-0.08790808,  0.45178347,  0.88778594]]), 'currentState': array([-3.40065168,  4.8827083 , 56.45514373,  0.16956533, -0.87144218,
        0.46025659]), 'targetState': array([-3.8149178 ,  3.23208945, 59.        ]), 'previousTarget': array([-3.8149178 ,  3.23208945, 59.        ])}
episode index:18420
target thresh 84.94248967559871
target distance 17.0
model initialize at round 18420
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.0025468 ,  3.87232487, 66.        ]), 'distance': 15.766866757575023, 'localFrame': array([[-0.09563935,  0.3431292 ,  0.93440648],
       [-0.96328176, -0.26849256,  0.        ],
       [ 0.25088119, -0.90009671,  0.35620856]]), 'currentState': array([-2.23717221,  3.65468888, 50.28305303, -0.09563935,  0.3431292 ,
        0.93440648]), 'targetState': array([-1.0025468 ,  3.87232487, 66.        ]), 'previousTarget': array([-1.0025468 ,  3.87232487, 66.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6285191366230166
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.0025468 ,  3.87232487, 66.        ]), 'distance': 1.7990281759815363, 'localFrame': array([[-0.73248805, -0.3121978 ,  0.6049742 ],
       [ 0.39208758, -0.91992789,  0.        ],
       [ 0.55653265,  0.23720287,  0.79624507]]), 'currentState': array([-0.89420692,  3.99578508, 64.208486  , -0.73248805, -0.3121978 ,
        0.6049742 ]), 'targetState': array([-1.0025468 ,  3.87232487, 66.        ]), 'previousTarget': array([-1.0025468 ,  3.87232487, 66.        ])}
episode index:18421
target thresh 84.94399535134612
target distance 28.0
model initialize at round 18421
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([5.06954844, 1.3275891 , 6.21507809]), 'distance': 27.5, 'localFrame': array([[-0.2811827 ,  0.50129264, -0.81831655],
       [-0.87216549, -0.48921096,  0.        ],
       [-0.40032942,  0.71370746,  0.5747678 ]]), 'currentState': array([-1.35901724, 15.05540667, 29.16002455, -0.2811827 ,  0.50129264,
       -0.81831655]), 'targetState': array([ 5.97032808, -0.59597195,  3.        ]), 'previousTarget': array([4.81976884, 1.92072405, 7.53091373])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6285243728188333
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([ 5.97032808, -0.59597195,  3.        ]), 'distance': 3.197186811206569, 'localFrame': array([[ 0.51165316, -0.64031272, -0.57289673],
       [ 0.781224  ,  0.6242508 ,  0.        ],
       [ 0.35763124, -0.44756067,  0.81962756]]), 'currentState': array([ 6.33165686,  2.2061575 ,  4.4965011 ,  0.51165316, -0.64031272,
       -0.57289673]), 'targetState': array([ 5.97032808, -0.59597195,  3.        ]), 'previousTarget': array([ 5.97032808, -0.59597195,  3.        ])}
episode index:18422
target thresh 84.94550087653346
target distance 61.0
model initialize at round 18422
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.19028428, -12.92350428,  75.51681001]), 'distance': 27.500000000000004, 'localFrame': array([[-0.6005241 , -0.72410746, -0.33917427],
       [ 0.76973459, -0.63836405,  0.        ],
       [-0.21651666, -0.26107417,  0.94072356]]), 'currentState': array([ 4.38531892, -3.00531114, 98.88422455, -0.6005241 , -0.72410746,
       -0.33917427]), 'targetState': array([-23.16960799, -28.84734417,  38.        ]), 'previousTarget': array([ -5.76426376, -11.68210708,  75.89168487])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6285190740710799
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([-23.16960799, -28.84734417,  38.        ]), 'distance': 2.290052822715099, 'localFrame': array([[-0.84353962, -0.52323918, -0.12108541],
       [ 0.52711766, -0.84979231,  0.        ],
       [-0.10289745, -0.06382626,  0.99264209]]), 'currentState': array([-21.06024323, -28.26869913,  38.67830092,  -0.84353962,
        -0.52323918,  -0.12108541]), 'targetState': array([-23.16960799, -28.84734417,  38.        ]), 'previousTarget': array([-23.16960799, -28.84734417,  38.        ])}
episode index:18423
target thresh 84.94700625117582
target distance 24.0
model initialize at round 18423
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.99089628,  0.57240817, 28.        ]), 'distance': 23.903946711014093, 'localFrame': array([[-0.59697646, -0.05540233, -0.80034348],
       [ 0.09240779, -0.99572125,  0.        ],
       [-0.79691901, -0.07395797,  0.59954175]]), 'currentState': array([12.34640392,  0.36851176, 51.22707043, -0.59697646, -0.05540233,
       -0.80034348]), 'targetState': array([17.99089628,  0.57240817, 28.        ]), 'previousTarget': array([17.99089628,  0.57240817, 28.        ])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6285298019016141
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([17.99089628,  0.57240817, 28.        ]), 'distance': 2.746373500170676, 'localFrame': array([[-0.63045312,  0.57188586, -0.52485753],
       [-0.67186621, -0.74067253,  0.        ],
       [-0.38874756,  0.35263404,  0.85119009]]), 'currentState': array([17.50431281,  0.71087626, 30.69937595, -0.63045312,  0.57188586,
       -0.52485753]), 'targetState': array([17.99089628,  0.57240817, 28.        ]), 'previousTarget': array([17.99089628,  0.57240817, 28.        ])}
episode index:18424
target thresh 84.94851147528824
target distance 53.94544107259675
model initialize at round 18424
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.76958915,  17.63695724,   6.64031331]), 'distance': 27.499999999999996, 'localFrame': array([[-0.38903523, -0.77415456, -0.49933587],
       [ 0.89352142, -0.44902056,  0.        ],
       [-0.22421207, -0.44616729,  0.8664085 ]]), 'currentState': array([11.95740935, 28.94903464, 10.74670466, -0.38903523, -0.77415456,
       -0.49933587]), 'targetState': array([-40.71164591,   4.85405888,   2.        ]), 'previousTarget': array([-11.37167199,  18.65570991,   6.89494126])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6285323635975251
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-40.71164591,   4.85405888,   2.        ]), 'distance': 2.3970861460650923, 'localFrame': array([[-0.68147095,  0.72799088, -0.07501076],
       [-0.73004763, -0.68339627,  0.        ],
       [-0.05126207,  0.05476143,  0.99718272]]), 'currentState': array([-38.84840645,   6.0506582 ,   2.91788385,  -0.68147095,
         0.72799088,  -0.07501076]), 'targetState': array([-40.71164591,   4.85405888,   2.        ]), 'previousTarget': array([-40.71164591,   4.85405888,   2.        ])}
episode index:18425
target thresh 84.95001654888577
target distance 73.0
model initialize at round 18425
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.71249454,  -6.76677938,  44.47967942]), 'distance': 27.5, 'localFrame': array([[-0.56577428,  0.29276191,  0.77083716],
       [-0.45957158, -0.88814073,  0.        ],
       [ 0.68461188, -0.35425485,  0.63703223]]), 'currentState': array([-46.91187364,  -7.98944173,  19.86402899,  -0.56577428,
         0.29276191,   0.77083716]), 'targetState': array([-11.16168862,  -4.40643928,  92.        ]), 'previousTarget': array([-33.65322914,  -6.81946286,  43.88668711])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6285307584549477
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.16168862,  -4.40643928,  92.        ]), 'distance': 3.1432770012971627, 'localFrame': array([[ 0.75119256,  0.43625692,  0.49536819],
       [-0.5022049 ,  0.86474866,  0.        ],
       [-0.42836898, -0.24877633,  0.86868312]]), 'currentState': array([-12.00987978,  -6.9995458 ,  90.43905136,   0.75119256,
         0.43625692,   0.49536819]), 'targetState': array([-11.16168862,  -4.40643928,  92.        ]), 'previousTarget': array([-11.16168862,  -4.40643928,  92.        ])}
episode index:18426
target thresh 84.95152147198348
target distance 7.0
model initialize at round 18426
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.96709949,  -5.83538635,  38.        ]), 'distance': 10.27352988642924, 'localFrame': array([[-0.83891509,  0.15832753, -0.52072436],
       [-0.18545499, -0.98265276,  0.        ],
       [-0.51169123,  0.09657093,  0.85372486]]), 'currentState': array([-9.86769473,  0.30911517, 43.53062192, -0.83891509,  0.15832753,
       -0.52072436]), 'targetState': array([-15.96709949,  -5.83538635,  38.        ]), 'previousTarget': array([-15.96709949,  -5.83538635,  38.        ])}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6285487790362438
{'scaleFactor': 20, 'timeStep': 5, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.96709949,  -5.83538635,  38.        ]), 'distance': 3.453003352945621, 'localFrame': array([[-0.89719152, -0.05705775, -0.43794039],
       [ 0.06346773, -0.99798389,  0.        ],
       [-0.43705745, -0.02779508,  0.89900401]]), 'currentState': array([-15.01726485,  -3.85279195,  40.66277407,  -0.89719152,
        -0.05705775,  -0.43794039]), 'targetState': array([-15.96709949,  -5.83538635,  38.        ]), 'previousTarget': array([-15.96709949,  -5.83538635,  38.        ])}
episode index:18427
target thresh 84.9530262445964
target distance 74.71355584303242
model initialize at round 18427
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.06447611, -17.46316288,  60.71996344]), 'distance': 27.5, 'localFrame': array([[-0.60152653,  0.66073008, -0.44900066],
       [-0.73945926, -0.67320131,  0.        ],
       [-0.30226783,  0.33201769,  0.89353142]]), 'currentState': array([-26.03778286, -37.13390982,  79.70604155,  -0.60152653,
         0.66073008,  -0.44900066]), 'targetState': array([-14.80828851,  37.15796807,   8.        ]), 'previousTarget': array([-22.17455587, -17.97510566,  61.86859636])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285146706805331
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 34, 'trapConfig': [], 'currentTarget': array([-14.80828851,  37.15796807,   8.        ]), 'distance': 13.694252562463639, 'localFrame': array([[ 0.21011764,  0.76215776, -0.61234478],
       [-0.96403566,  0.26577292,  0.        ],
       [ 0.16274466,  0.5903222 ,  0.79059084]]), 'currentState': array([-18.79255428,  24.11705469,   9.26204499,   0.21011764,
         0.76215776,  -0.61234478]), 'targetState': array([-14.80828851,  37.15796807,   8.        ]), 'previousTarget': array([-14.80828851,  37.15796807,   8.        ])}
episode index:18428
target thresh 84.95453086673957
target distance 62.0
model initialize at round 18428
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.30493102,  4.3385943 , 60.53208161]), 'distance': 27.5, 'localFrame': array([[ 0.2626107 , -0.86218122, -0.43321954],
       [ 0.95660966,  0.29137254,  0.        ],
       [ 0.12622828, -0.41442199,  0.90128843]]), 'currentState': array([28.01100798, 12.50701412, 86.16346089,  0.2626107 , -0.86218122,
       -0.43321954]), 'targetState': array([14.39475226, -6.98506317, 25.        ]), 'previousTarget': array([21.64164102,  5.06814074, 61.35250148])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6285168659753343
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([14.39475226, -6.98506317, 25.        ]), 'distance': 3.2980304616072558, 'localFrame': array([[-0.7018526 ,  0.36394048, -0.61233181],
       [-0.46033402, -0.88774579,  0.        ],
       [-0.54359499,  0.28187717,  0.79060088]]), 'currentState': array([12.7617538 , -7.09823378, 27.86313   , -0.7018526 ,  0.36394048,
       -0.61233181]), 'targetState': array([14.39475226, -6.98506317, 25.        ]), 'previousTarget': array([14.39475226, -6.98506317, 25.        ])}
episode index:18429
target thresh 84.95603533842807
target distance 23.553733567837764
model initialize at round 18429
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.90639787, -16.89276908,  65.        ]), 'distance': 25.81248863860051, 'localFrame': array([[ 0.16164721, -0.81755293, -0.55270008],
       [ 0.98100831,  0.19396573,  0.        ],
       [ 0.10720488, -0.54220337,  0.83338024]]), 'currentState': array([-11.65174175,   5.50604244,  56.65697388,   0.16164721,
        -0.81755293,  -0.55270008]), 'targetState': array([ -1.90639787, -16.89276908,  65.        ]), 'previousTarget': array([ -1.90639787, -16.89276908,  65.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6285294292682625
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.90639787, -16.89276908,  65.        ]), 'distance': 2.1017002656435535, 'localFrame': array([[ 0.89891638, -0.28175979, -0.33550076],
       [ 0.29909538,  0.95422322,  0.        ],
       [ 0.32014262, -0.10034673,  0.94203993]]), 'currentState': array([ -2.93770848, -15.09353326,  65.34101744,   0.89891638,
        -0.28175979,  -0.33550076]), 'targetState': array([ -1.90639787, -16.89276908,  65.        ]), 'previousTarget': array([ -1.90639787, -16.89276908,  65.        ])}
episode index:18430
target thresh 84.95753965967691
target distance 5.0
model initialize at round 18430
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.55455577, 12.83395746, 98.        ]), 'distance': 4.6059525464727855, 'localFrame': array([[ 0.58264876, -0.09215368,  0.80748258],
       [ 0.15622142,  0.98772206,  0.        ],
       [-0.79756836,  0.12614608,  0.58989141]]), 'currentState': array([-9.42624687e+00,  1.61949708e+01,  9.48532675e+01,  5.82648762e-01,
       -9.21536766e-02,  8.07482582e-01]), 'targetState': array([-9.55455577, 12.83395746, 98.        ]), 'previousTarget': array([-9.55455577, 12.83395746, 98.        ])}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6285490413658553
{'scaleFactor': 20, 'timeStep': 2, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.55455577, 12.83395746, 98.        ]), 'distance': 2.8553888867161876, 'localFrame': array([[-0.55320639, -0.66618275,  0.50016321],
       [ 0.76932531, -0.63885724,  0.        ],
       [ 0.31953289,  0.38478822,  0.86593115]]), 'currentState': array([-9.73585812, 15.12484565, 96.30524148, -0.55320639, -0.66618275,
        0.50016321]), 'targetState': array([-9.55455577, 12.83395746, 98.        ]), 'previousTarget': array([-9.55455577, 12.83395746, 98.        ])}
episode index:18431
target thresh 84.95904383050114
target distance 55.0
model initialize at round 18431
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.25879698, 23.72438212, 61.8861764 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23249575, -0.30839619,  0.92240854],
       [ 0.79850787, -0.60198438,  0.        ],
       [ 0.55527553,  0.73655048,  0.38621559]]), 'currentState': array([25.69572501, 37.53448067, 40.51793569, -0.23249575, -0.30839619,
        0.92240854]), 'targetState': array([-0.42661532,  2.96951164, 94.        ]), 'previousTarget': array([16.12409153, 23.95948271, 60.49657149])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6285519713546599
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.42661532,  2.96951164, 94.        ]), 'distance': 1.8967215288390011, 'localFrame': array([[-0.54487405, -0.6863341 ,  0.48172375],
       [ 0.78319752, -0.62177299,  0.        ],
       [ 0.29952281,  0.37728484,  0.87632313]]), 'currentState': array([ 0.18738973,  2.58545226, 92.24698866, -0.54487405, -0.6863341 ,
        0.48172375]), 'targetState': array([-0.42661532,  2.96951164, 94.        ]), 'previousTarget': array([-0.42661532,  2.96951164, 94.        ])}
episode index:18432
target thresh 84.96054785091583
target distance 52.0
model initialize at round 18432
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.50050294, 12.01389099, 43.32887649]), 'distance': 27.499999999999996, 'localFrame': array([[-0.35836027,  0.5630967 , -0.74464758],
       [-0.84364384, -0.53690323,  0.        ],
       [-0.39980369,  0.62821734,  0.66745784]]), 'currentState': array([ 9.60137026, 22.19343299, 64.63061578, -0.35836027,  0.5630967 ,
       -0.74464758]), 'targetState': array([-23.91638687,  -2.00160915,  14.        ]), 'previousTarget': array([-4.51747729, 11.40087638, 44.29756904])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285178720777459
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 96, 'trapConfig': [], 'currentTarget': array([-7.29784402, 11.00860442, 39.13615019]), 'distance': 27.499999999999996, 'localFrame': array([[-0.98110843, -0.13999842,  0.13351662],
       [ 0.14126321, -0.98997207,  0.        ],
       [ 0.13217772,  0.01886099,  0.99104657]]), 'currentState': array([ 6.6261498 , 21.90932749, 60.19669702, -0.98110843, -0.13999842,
        0.13351662]), 'targetState': array([-23.91638687,  -2.00160915,  14.        ]), 'previousTarget': array([-7.29784402, 11.00860442, 39.13615019])}
episode index:18433
target thresh 84.96205172093597
target distance 58.0
model initialize at round 18433
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.04464236,  6.03197857, 55.65692182]), 'distance': 27.5, 'localFrame': array([[ 0.40294662,  0.73324292,  0.54771237],
       [-0.87638593,  0.48160949,  0.        ],
       [-0.26378348, -0.48000742,  0.83666669]]), 'currentState': array([12.66037623, -5.04521018, 30.48951773,  0.40294662,  0.73324292,
        0.54771237]), 'targetState': array([13.52320119, 19.82733037, 87.        ]), 'previousTarget': array([12.99996981,  5.1210154 , 54.10292629])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6285162684178994
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.52320119, 19.82733037, 87.        ]), 'distance': 2.896698083658751, 'localFrame': array([[ 0.08091178,  0.35867763,  0.92994819],
       [-0.97548769,  0.22005399,  0.        ],
       [-0.20463881, -0.90715302,  0.36769057]]), 'currentState': array([1.27788745e+01, 2.05351267e+01, 8.42915204e+01, 8.09117784e-02,
       3.58677630e-01, 9.29948193e-01]), 'targetState': array([13.52320119, 19.82733037, 87.        ]), 'previousTarget': array([13.52320119, 19.82733037, 87.        ])}
episode index:18434
target thresh 84.96355544057664
target distance 42.0
model initialize at round 18434
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.97472425, -28.45038721,  36.3483155 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.54662977,  0.0934119 , -0.83214789],
       [-0.16844517, -0.98571103,  0.        ],
       [-0.82025735,  0.14017129,  0.55455377]]), 'currentState': array([ 27.1453432 , -16.56865887,  59.39119465,  -0.54662977,
         0.0934119 ,  -0.83214789]), 'targetState': array([ 11.07043118, -37.39579593,  19.        ]), 'previousTarget': array([ 18.60070149, -27.7009678 ,  37.92765997])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6285211078138409
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 11.07043118, -37.39579593,  19.        ]), 'distance': 3.2694645066702357, 'localFrame': array([[-0.75810387, -0.05340354, -0.64994352],
       [ 0.07026943, -0.99752805,  0.        ],
       [-0.64833689, -0.04567116,  0.75998251]]), 'currentState': array([ 13.0450066 , -34.78998083,  19.01332884,  -0.75810387,
        -0.05340354,  -0.64994352]), 'targetState': array([ 11.07043118, -37.39579593,  19.        ]), 'previousTarget': array([ 11.07043118, -37.39579593,  19.        ])}
episode index:18435
target thresh 84.96505900985287
target distance 16.0
model initialize at round 18435
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 47.6525949 , -11.41184471,  32.        ]), 'distance': 19.39027724539162, 'localFrame': array([[ 0.04738794, -0.08598011, -0.99516923],
       [ 0.87579027,  0.48269183,  0.        ],
       [ 0.48036006, -0.87155953,  0.09817432]]), 'currentState': array([ 42.85478015, -23.58125048,  46.31325919,   0.04738794,
        -0.08598011,  -0.99516923]), 'targetState': array([ 47.6525949 , -11.41184471,  32.        ]), 'previousTarget': array([ 47.6525949 , -11.41184471,  32.        ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6285283665254123
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([ 47.6525949 , -11.41184471,  32.        ]), 'distance': 3.1229435632662255, 'localFrame': array([[-0.27884206,  0.81827949, -0.50265872],
       [-0.94655146, -0.32255282,  0.        ],
       [-0.16213399,  0.47579234,  0.86448494]]), 'currentState': array([ 46.79655087, -13.68663091,  33.96094693,  -0.27884206,
         0.81827949,  -0.50265872]), 'targetState': array([ 47.6525949 , -11.41184471,  32.        ]), 'previousTarget': array([ 47.6525949 , -11.41184471,  32.        ])}
episode index:18436
target thresh 84.96656242877968
target distance 41.63940196150219
model initialize at round 18436
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.42998777, 19.0910844 , 65.06331907]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.50174002,  0.62662365,  0.59632186],
       [-0.78060069,  0.62503004,  0.        ],
       [-0.37271908, -0.46548925,  0.80274544]]), 'currentState': array([-6.61258494, -0.74887038, 56.56761957,  0.50174002,  0.62662365,
        0.59632186]), 'targetState': array([28.35717959, 39.96086042, 74.        ]), 'previousTarget': array([ 9.95501609, 18.16311435, 64.05371941])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6285339951527991
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.35717959, 39.96086042, 74.        ]), 'distance': 3.5477895214246282, 'localFrame': array([[ 0.94633929,  0.30441994,  0.10849173],
       [-0.30622749,  0.95195836,  0.        ],
       [-0.10327961, -0.03322315,  0.99409735]]), 'currentState': array([27.30179873, 37.55155361, 71.6191971 ,  0.94633929,  0.30441994,
        0.10849173]), 'targetState': array([28.35717959, 39.96086042, 74.        ]), 'previousTarget': array([28.35717959, 39.96086042, 74.        ])}
episode index:18437
target thresh 84.96806569737213
target distance 42.46174793260092
model initialize at round 18437
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.71132565, -17.25016638,  34.44281697]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.52192062, -0.79028526,  0.32101103],
       [ 0.83444804,  0.55108663,  0.        ],
       [-0.17690488,  0.26786702,  0.94707546]]), 'currentState': array([ -7.26464932, -11.4152187 ,  48.38225597,   0.52192062,
        -0.79028526,   0.32101103]), 'targetState': array([ 34.57219057, -22.04004626,  23.        ]), 'previousTarget': array([ 15.38968998, -16.75092643,  33.84222946])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6285380581585397
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 34.57219057, -22.04004626,  23.        ]), 'distance': 2.228608538874892, 'localFrame': array([[ 0.26788353,  0.80574141, -0.52822267],
       [-0.94892925,  0.31548896,  0.        ],
       [ 0.16664842,  0.50124594,  0.84910589]]), 'currentState': array([ 32.71739736, -22.08068384,  21.76517745,   0.26788353,
         0.80574141,  -0.52822267]), 'targetState': array([ 34.57219057, -22.04004626,  23.        ]), 'previousTarget': array([ 34.57219057, -22.04004626,  23.        ])}
episode index:18438
target thresh 84.96956881564522
target distance 62.0
model initialize at round 18438
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.56628605,  4.46566734, 68.91362963]), 'distance': 27.499999999999996, 'localFrame': array([[-0.09043119, -0.78468188, -0.61326711],
       [ 0.99342464, -0.11448789,  0.        ],
       [-0.07021166, -0.60923466,  0.7898756 ]]), 'currentState': array([ 2.73141176e+01,  1.36543954e+01,  9.43945121e+01, -9.04311936e-02,
       -7.84681882e-01, -6.13267106e-01]), 'targetState': array([15.87452938, -8.48524113, 33.        ]), 'previousTarget': array([23.17338736,  5.51025063, 69.73381599])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6285345529322419
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([15.87452938, -8.48524113, 33.        ]), 'distance': 1.9078306985243636, 'localFrame': array([[-0.77718143, -0.19383245, -0.59868022],
       [ 0.24199167, -0.97027833,  0.        ],
       [-0.58088645, -0.14487563,  0.80098813]]), 'currentState': array([16.50188091, -7.78060357, 34.65823217, -0.77718143, -0.19383245,
       -0.59868022]), 'targetState': array([15.87452938, -8.48524113, 33.        ]), 'previousTarget': array([15.87452938, -8.48524113, 33.        ])}
episode index:18439
target thresh 84.97107178361401
target distance 22.0
model initialize at round 18439
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-33.09984297,  25.85344069,  81.        ]), 'distance': 24.998151928752478, 'localFrame': array([[-0.10596003,  0.9397697 ,  0.32496981],
       [-0.99370357, -0.11204113,  0.        ],
       [ 0.03640998, -0.32292366,  0.94572439]]), 'currentState': array([-30.20232079,  36.74062649,  58.68450873,  -0.10596003,
         0.9397697 ,   0.32496981]), 'targetState': array([-33.09984297,  25.85344069,  81.        ]), 'previousTarget': array([-33.09984297,  25.85344069,  81.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6285471084529419
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-33.09984297,  25.85344069,  81.        ]), 'distance': 3.297942680228073, 'localFrame': array([[ 0.81739117, -0.01480796,  0.5758927 ],
       [ 0.01811315,  0.99983594,  0.        ],
       [-0.57579822,  0.01043123,  0.81752529]]), 'currentState': array([-3.54938296e+01,  2.74391529e+01,  7.93780349e+01,  8.17391168e-01,
       -1.48079559e-02,  5.75892700e-01]), 'targetState': array([-33.09984297,  25.85344069,  81.        ]), 'previousTarget': array([-33.09984297,  25.85344069,  81.        ])}
episode index:18440
target thresh 84.9725746012935
target distance 24.793425377361764
model initialize at round 18440
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.74380222,  -2.26398385,  61.85897918]), 'distance': 27.500000000000004, 'localFrame': array([[-0.68592809,  0.67480498, -0.27228825],
       [-0.70130321, -0.71286311,  0.        ],
       [-0.19410425,  0.19095663,  0.96221573]]), 'currentState': array([ 1.2898449 ,  9.36710241, 75.22137124, -0.68592809,  0.67480498,
       -0.27228825]), 'targetState': array([-22.67000824,  -3.88210336,  60.        ]), 'previousTarget': array([-19.72166853,  -2.50325218,  61.78374286])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6285587345081954
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.67000824,  -3.88210336,  60.        ]), 'distance': 3.156386781721693, 'localFrame': array([[-0.4134079 ,  0.66495184, -0.62203935],
       [-0.84925127, -0.52798889,  0.        ],
       [-0.32842987,  0.52826771,  0.78298598]]), 'currentState': array([-20.54335213,  -2.01496162,  61.39781725,  -0.4134079 ,
         0.66495184,  -0.62203935]), 'targetState': array([-22.67000824,  -3.88210336,  60.        ]), 'previousTarget': array([-22.67000824,  -3.88210336,  60.        ])}
episode index:18441
target thresh 84.97407726869875
target distance 82.0
model initialize at round 18441
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.78583824,  -5.33645214,  58.19050311]), 'distance': 27.500000000000004, 'localFrame': array([[-0.07386177, -0.8151935 , -0.57445974],
       [ 0.99592034, -0.09023678,  0.        ],
       [-0.0518374 , -0.57211614,  0.81853284]]), 'currentState': array([-3.13432251e+01, -4.81061227e-01,  8.46818343e+01, -7.38617652e-02,
       -8.15193502e-01, -5.74459741e-01]), 'targetState': array([-14.41768209, -15.26860973,   4.        ]), 'previousTarget': array([-26.3309055 ,  -4.42275799,  59.58699852])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.628557457387093
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.41768209, -15.26860973,   4.        ]), 'distance': 3.0151662514087785, 'localFrame': array([[ 0.71900876,  0.2428368 , -0.65119636],
       [-0.31998133,  0.94742385,  0.        ],
       [ 0.61695896,  0.20837067,  0.75890929]]), 'currentState': array([-15.450834  , -14.26940374,   6.65054939,   0.71900876,
         0.2428368 ,  -0.65119636]), 'targetState': array([-14.41768209, -15.26860973,   4.        ]), 'previousTarget': array([-14.41768209, -15.26860973,   4.        ])}
episode index:18442
target thresh 84.97557978584479
target distance 41.0
model initialize at round 18442
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.44524543, -10.85080387,  51.03655499]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.29786702,  0.6589478 , -0.6906975 ],
       [-0.9112265 ,  0.41190565,  0.        ],
       [ 0.28450221,  0.62938187,  0.7231438 ]]), 'currentState': array([12.2423331 , -9.59295165, 24.27479709,  0.29786702,  0.6589478 ,
       -0.6906975 ]), 'targetState': array([ 22.14529571, -11.60111537,  67.        ]), 'previousTarget': array([ 18.71288244, -11.12994705,  52.72424332])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6285582194184707
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.14529571, -11.60111537,  67.        ]), 'distance': 2.027141566852966, 'localFrame': array([[-0.26108151,  0.12793692,  0.95680123],
       [-0.4400346 , -0.89798082,  0.        ],
       [ 0.85918915, -0.42102565,  0.29074285]]), 'currentState': array([ 23.09452647, -11.91998078,  65.23744823,  -0.26108151,
         0.12793692,   0.95680123]), 'targetState': array([ 22.14529571, -11.60111537,  67.        ]), 'previousTarget': array([ 22.14529571, -11.60111537,  67.        ])}
episode index:18443
target thresh 84.97708215274659
target distance 23.07043402584306
model initialize at round 18443
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.27917915, -11.10523711,   2.3457411 ]), 'distance': 27.5, 'localFrame': array([[ 0.59376742, -0.08269864, -0.80037565],
       [ 0.1379463 ,  0.99043971,  0.        ],
       [ 0.79272383, -0.11040886,  0.5994988 ]]), 'currentState': array([ 0.8288909 , -1.00652359, 17.70971506,  0.59376742, -0.08269864,
       -0.80037565]), 'targetState': array([ 23.07043403, -11.9897904 ,   1.        ]), 'previousTarget': array([ 20.06265749, -10.42663774,   3.34672558])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6285689334937481
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.07043403, -11.9897904 ,   1.        ]), 'distance': 2.53767017994311, 'localFrame': array([[ 0.85074638, -0.16947617, -0.49750218],
       [ 0.19536998,  0.98072961,  0.        ],
       [ 0.48791512, -0.09719699,  0.86746273]]), 'currentState': array([ 21.20372947, -13.16182419,   2.25758532,   0.85074638,
        -0.16947617,  -0.49750218]), 'targetState': array([ 23.07043403, -11.9897904 ,   1.        ]), 'previousTarget': array([ 23.07043403, -11.9897904 ,   1.        ])}
episode index:18444
target thresh 84.97858436941922
target distance 66.0
model initialize at round 18444
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.53861605, 21.44167616, 71.36857782]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.12330946, -0.69097098, -0.71228778],
       [ 0.98444685,  0.17568264,  0.        ],
       [ 0.1251366 , -0.70120946,  0.70188754]]), 'currentState': array([ 0.89835306, 35.10202093, 94.7803898 ,  0.12330946, -0.69097098,
       -0.71228778]), 'targetState': array([13.73794214, -2.69609823, 30.        ]), 'previousTarget': array([ 4.77999714, 22.31360863, 72.656741  ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6285348554816313
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 63, 'trapConfig': [], 'currentTarget': array([13.73794214, -2.69609823, 30.        ]), 'distance': 26.284400294055278, 'localFrame': array([[ 0.48618937, -0.61068555, -0.62504645],
       [ 0.78234087,  0.62285052,  0.        ],
       [ 0.3893105 , -0.48899938,  0.78058756]]), 'currentState': array([17.3343448 , 17.32228435, 46.64932267,  0.48618937, -0.61068555,
       -0.62504645]), 'targetState': array([13.73794214, -2.69609823, 30.        ]), 'previousTarget': array([13.73794214, -2.69609823, 30.        ])}
episode index:18445
target thresh 84.98008643587772
target distance 54.0
model initialize at round 18445
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.38218729,  15.34959226,  50.60345391]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.62450271, -0.26850735, -0.73341678],
       [ 0.39499198,  0.91868457,  0.        ],
       [ 0.67377868, -0.28969375,  0.67977925]]), 'currentState': array([-28.25187497,  26.47111485,  72.77713764,   0.62450271,
        -0.26850735,  -0.73341678]), 'targetState': array([ 0., -0., 20.]), 'previousTarget': array([-17.06158633,  16.35047045,  51.90217716])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.628532287529698
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-1.11022302e-16,  0.00000000e+00,  2.00000000e+01]), 'distance': 2.752336783673374, 'localFrame': array([[-0.14026068, -0.50203687,  0.8533967 ],
       [ 0.9631181 , -0.26907903,  0.        ],
       [ 0.22963116,  0.82192181,  0.521262  ]]), 'currentState': array([ 0.73460328,  0.92386054, 17.51359748, -0.14026068, -0.50203687,
        0.8533967 ]), 'targetState': array([ 0., -0., 20.]), 'previousTarget': array([ 0.,  0., 20.])}
episode index:18446
target thresh 84.98158835213707
target distance 37.276048615758185
model initialize at round 18446
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.69537689,  15.06236501,  73.91991101]), 'distance': 27.5, 'localFrame': array([[ 0.19084621, -0.70830589,  0.6796179 ],
       [ 0.96556493,  0.26016219,  0.        ],
       [-0.17681088,  0.65621521,  0.73356629]]), 'currentState': array([-21.88476646,  35.11363797,  58.78736286,   0.19084621,
        -0.70830589,   0.6796179 ]), 'targetState': array([-1.76305298, -0.94426914, 86.        ]), 'previousTarget': array([-11.6001349 ,  16.32219035,  73.03025445])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284982151988295
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 62, 'trapConfig': [], 'currentTarget': array([-1.76305298, -0.94426914, 86.        ]), 'distance': 10.656342650314588, 'localFrame': array([[-0.98191058,  0.18565145,  0.03721758],
       [-0.18578016, -0.98259133,  0.        ],
       [ 0.03656967, -0.00691429,  0.99930719]]), 'currentState': array([-4.16844653e+00,  1.39706735e+00,  7.58861548e+01, -9.81910582e-01,
        1.85651451e-01,  3.72175816e-02]), 'targetState': array([-1.76305298, -0.94426914, 86.        ]), 'previousTarget': array([-1.76305298, -0.94426914, 86.        ])}
episode index:18447
target thresh 84.9830901182123
target distance 40.0
model initialize at round 18447
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.78961934,  17.65264117,  24.00775035]), 'distance': 27.499999999999996, 'localFrame': array([[-0.40679977, -0.47850341, -0.77816993],
       [ 0.76188285, -0.64771485,  0.        ],
       [-0.50403222, -0.59287432,  0.62805379]]), 'currentState': array([-25.66963261,  30.62135584,  46.9417389 ,  -0.40679977,
        -0.47850341,  -0.77816993]), 'targetState': array([-12.28942974,   8.60057652,   8.        ]), 'previousTarget': array([-17.96616363,  18.70823689,  25.19718235])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6285038421044266
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-12.28942974,   8.60057652,   8.        ]), 'distance': 3.03397188213116, 'localFrame': array([[-0.18229639, -0.94014079, -0.28792937],
       [ 0.98171479, -0.19035773,  0.        ],
       [-0.05480958, -0.28266452,  0.95765165]]), 'currentState': array([-14.61538428,  10.42087181,   8.69386306,  -0.18229639,
        -0.94014079,  -0.28792937]), 'targetState': array([-12.28942974,   8.60057652,   8.        ]), 'previousTarget': array([-12.28942974,   8.60057652,   8.        ])}
episode index:18448
target thresh 84.98459173411842
target distance 35.10423298580473
model initialize at round 18448
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.12777551, -13.60258221,  15.21499347]), 'distance': 27.5, 'localFrame': array([[-0.58998659,  0.09237399, -0.80211151],
       [-0.15468514, -0.98796382,  0.        ],
       [-0.79245715,  0.12407473,  0.59717429]]), 'currentState': array([ -2.65840757, -34.68984423,  32.68420301,  -0.58998659,
         0.09237399,  -0.80211151]), 'targetState': array([1.64171806, 1.1422617 , 3.        ]), 'previousTarget': array([  0.29578316, -13.39834877,  15.84058606])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6285110966370431
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.64171806, 1.1422617 , 3.        ]), 'distance': 2.7958430355095722, 'localFrame': array([[ 0.45633881,  0.49248054, -0.74109231],
       [-0.73350942,  0.67967928,  0.        ],
       [ 0.50370509,  0.54359819,  0.67140315]]), 'currentState': array([-0.2933706 , -0.54827976,  4.10192545,  0.45633881,  0.49248054,
       -0.74109231]), 'targetState': array([1.64171806, 1.1422617 , 3.        ]), 'previousTarget': array([1.64171806, 1.1422617 , 3.        ])}
episode index:18449
target thresh 84.98609319987048
target distance 57.0
model initialize at round 18449
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.09623515, 11.71345027, 26.51281929]), 'distance': 27.499999999999996, 'localFrame': array([[-0.73866058, -0.25444462,  0.62421028],
       [ 0.32568645, -0.94547783,  0.        ],
       [ 0.59017699,  0.20329683,  0.78125637]]), 'currentState': array([41.2514876 , 19.25918888,  4.84186749, -0.73866058, -0.25444462,
        0.62421028]), 'targetState': array([ 1.97814678, -0.29484792, 61.        ]), 'previousTarget': array([27.07083769, 12.65758592, 25.51748216])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6285122127092236
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.97814678, -0.29484792, 61.        ]), 'distance': 2.0420865677410887, 'localFrame': array([[-0.20933305, -0.73609112,  0.64369989],
       [ 0.96186104, -0.27353855,  0.        ],
       [ 0.17607674,  0.61914984,  0.76527803]]), 'currentState': array([ 2.8454404 ,  0.07816037, 59.18925867, -0.20933305, -0.73609112,
        0.64369989]), 'targetState': array([ 1.97814678, -0.29484792, 61.        ]), 'previousTarget': array([ 1.97814678, -0.29484792, 61.        ])}
episode index:18450
target thresh 84.98759451548345
target distance 19.48344380947684
model initialize at round 18450
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.41901343, 17.44913523, 16.        ]), 'distance': 18.79819784558792, 'localFrame': array([[-0.4027513 , -0.57390618, -0.71303793],
       [ 0.81854991, -0.57443542,  0.        ],
       [-0.40959424, -0.58365713,  0.70112546]]), 'currentState': array([ 7.5150657 , 35.94364807, 17.31897587, -0.4027513 , -0.57390618,
       -0.71303793]), 'targetState': array([ 4.41901343, 17.44913523, 16.        ]), 'previousTarget': array([ 4.41901343, 17.44913523, 16.        ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6285194660018169
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.41901343, 17.44913523, 16.        ]), 'distance': 3.5071653733411408, 'localFrame': array([[-0.27250455, -0.63611767,  0.72186951],
       [ 0.9192064 , -0.39377608,  0.        ],
       [ 0.28425494,  0.66354707,  0.6920292 ]]), 'currentState': array([ 3.51078831, 19.23531687, 13.1216513 , -0.27250455, -0.63611767,
        0.72186951]), 'targetState': array([ 4.41901343, 17.44913523, 16.        ]), 'previousTarget': array([ 4.41901343, 17.44913523, 16.        ])}
episode index:18451
target thresh 84.9890956809724
target distance 56.0
model initialize at round 18451
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.70633371,  -7.84586316,  37.94000134]), 'distance': 27.5, 'localFrame': array([[-0.26186786,  0.4231263 , -0.8674038 ],
       [-0.85032582, -0.52625659,  0.        ],
       [-0.45647697,  0.73757585,  0.49760491]]), 'currentState': array([-39.87108946,  -7.74934756,  60.18727885,  -0.26186786,
         0.4231263 ,  -0.8674038 ]), 'targetState': array([-0.4988954 , -7.98442881,  6.        ]), 'previousTarget': array([-24.25292372,  -8.20022179,  39.55515469])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.628518857432282
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-0.4988954 , -7.98442881,  6.        ]), 'distance': 2.273843395870662, 'localFrame': array([[ 0.75899066,  0.49655861, -0.42114454],
       [-0.54747756,  0.83682037,  0.        ],
       [ 0.35242233,  0.23056718,  0.90699354]]), 'currentState': array([-2.08397449, -9.56670799,  5.60721424,  0.75899066,  0.49655861,
       -0.42114454]), 'targetState': array([-0.4988954 , -7.98442881,  6.        ]), 'previousTarget': array([-0.4988954 , -7.98442881,  6.        ])}
episode index:18452
target thresh 84.99059669635227
target distance 25.814132958989564
model initialize at round 18452
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.694686  ,  25.60298237,  34.01905184]), 'distance': 27.499999999999996, 'localFrame': array([[-0.17944923,  0.81540026,  0.55038204],
       [-0.97662909, -0.21493167,  0.        ],
       [ 0.11829453, -0.53751911,  0.83491294]]), 'currentState': array([-8.48834688,  4.58760276, 44.63902117, -0.17944923,  0.81540026,
        0.55038204]), 'targetState': array([-25.3955728 ,  29.59839323,  32.        ]), 'previousTarget': array([-22.55544103,  25.34324135,  33.81321878])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6285300206525724
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.3955728 ,  29.59839323,  32.        ]), 'distance': 2.6849734187877505, 'localFrame': array([[-0.55662277,  0.70757105,  0.43533241],
       [-0.78595445, -0.6182844 ,  0.        ],
       [ 0.26915924, -0.34215145,  0.90026979]]), 'currentState': array([-23.11893571,  28.17623851,  32.05900647,  -0.55662277,
         0.70757105,   0.43533241]), 'targetState': array([-25.3955728 ,  29.59839323,  32.        ]), 'previousTarget': array([-25.3955728 ,  29.59839323,  32.        ])}
episode index:18453
target thresh 84.99209756163812
target distance 16.39205055393642
model initialize at round 18453
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.40962146, -17.51354685,   1.        ]), 'distance': 20.81308575808036, 'localFrame': array([[ 0.87742381, -0.28857318, -0.38321401],
       [ 0.31242374,  0.94994284,  0.        ],
       [ 0.36403141, -0.11972516,  0.92365958]]), 'currentState': array([  1.05920235, -28.60818732,   9.62891213,   0.87742381,
        -0.28857318,  -0.38321401]), 'targetState': array([ 16.40962146, -17.51354685,   1.        ]), 'previousTarget': array([ 16.40962146, -17.51354685,   1.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284959613689128
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 97, 'trapConfig': [], 'currentTarget': array([ 16.40962146, -17.51354685,   1.        ]), 'distance': 19.267559010577024, 'localFrame': array([[ 0.76206274,  0.06163332,  0.6445632 ],
       [-0.08061374,  0.99674542,  0.        ],
       [-0.64246542, -0.05196065,  0.76455103]]), 'currentState': array([  1.85378571, -25.92350921,  10.4148292 ,   0.76206274,
         0.06163332,   0.6445632 ]), 'targetState': array([ 16.40962146, -17.51354685,   1.        ]), 'previousTarget': array([ 16.40962146, -17.51354685,   1.        ])}
episode index:18454
target thresh 84.99359827684493
target distance 54.41194230385658
model initialize at round 18454
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.06193133,  -9.03022562,  71.90317844]), 'distance': 27.500000000000004, 'localFrame': array([[-0.89594119, -0.44268678, -0.03630155],
       [ 0.44297875, -0.89653211,  0.        ],
       [-0.0325455 , -0.01608081,  0.99934088]]), 'currentState': array([-1.87042737e+01,  1.77100064e+01,  7.27920999e+01, -8.95941190e-01,
       -4.42686776e-01, -3.63015465e-02]), 'targetState': array([-31.52155606, -36.19933015,  71.        ]), 'previousTarget': array([-24.15847719,  -8.36850867,  71.5114837 ])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6285007966206728
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.52155606, -36.19933015,  71.        ]), 'distance': 2.8994035678362464, 'localFrame': array([[-0.07154606, -0.76133655, -0.64439725],
       [ 0.99561345, -0.09356206,  0.        ],
       [-0.06029113, -0.64157057,  0.76469091]]), 'currentState': array([-32.77373019, -33.69102883,  70.26038828,  -0.07154606,
        -0.76133655,  -0.64439725]), 'targetState': array([-31.52155606, -36.19933015,  71.        ]), 'previousTarget': array([-31.52155606, -36.19933015,  71.        ])}
episode index:18455
target thresh 84.99509884198775
target distance 54.0
model initialize at round 18455
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.00263784, -14.08932142,  60.12094699]), 'distance': 27.5, 'localFrame': array([[-0.68641709,  0.05308394, -0.72526801],
       [-0.0771046 , -0.99702301,  0.        ],
       [-0.72310889,  0.0559215 ,  0.68846664]]), 'currentState': array([ 2.96660011e+01, -8.51438393e+00,  8.27075769e+01, -6.86417085e-01,
        5.30839435e-02, -7.25268006e-01]), 'targetState': array([ -4.55204515, -21.52391426,  30.        ]), 'previousTarget': array([ 15.84045004, -14.50280883,  61.34483942])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6284969911017048
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([ -4.55204515, -21.52391426,  30.        ]), 'distance': 2.2578124019881645, 'localFrame': array([[-0.72267534, -0.1008052 , -0.68379724],
       [ 0.13815137, -0.99041113,  0.        ],
       [-0.67724039, -0.09446753,  0.72967208]]), 'currentState': array([ -2.38162575, -20.93921079,  30.21241083,  -0.72267534,
        -0.1008052 ,  -0.68379724]), 'targetState': array([ -4.55204515, -21.52391426,  30.        ]), 'previousTarget': array([ -4.55204515, -21.52391426,  30.        ])}
episode index:18456
target thresh 84.99659925708156
target distance 37.0
model initialize at round 18456
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.14180524,   6.3207376 ,  25.06688842]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.08122653, -0.73311984,  0.67523148],
       [ 0.99391809,  0.11012186,  0.        ],
       [-0.07435775,  0.67112479,  0.73760589]]), 'currentState': array([-36.94586716,   6.39155278,   4.10834924,   0.08122653,
        -0.73311984,   0.67523148]), 'targetState': array([-6.45628166,  6.27028127, 40.        ]), 'previousTarget': array([-18.97810789,   6.85001365,  24.42390704])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284629391435804
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-6.45628166,  6.27028127, 40.        ]), 'distance': 24.323858404426314, 'localFrame': array([[-0.17764971, -0.5841363 ,  0.79197561],
       [ 0.95673358, -0.29096539,  0.        ],
       [ 0.23043749,  0.75770966,  0.61055273]]), 'currentState': array([-13.94555044,  -7.09912411,  21.11032185,  -0.17764971,
        -0.5841363 ,   0.79197561]), 'targetState': array([-6.45628166,  6.27028127, 40.        ]), 'previousTarget': array([-6.45628166,  6.27028127, 40.        ])}
episode index:18457
target thresh 84.99809952214133
target distance 44.0
model initialize at round 18457
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.33089172, 15.4677229 , 25.2519396 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.69186085, -0.14416788,  0.70749147],
       [ 0.20399522, -0.97897188,  0.        ],
       [ 0.69261426,  0.14432488,  0.70672188]]), 'currentState': array([14.20545107, 29.82936816,  2.31219643, -0.69186085, -0.14416788,
        0.70749147]), 'targetState': array([ 5.13454603,  3.10426111, 45.        ]), 'previousTarget': array([ 9.95305698, 15.45814757, 24.19135962])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6284633575283958
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 5.13454603,  3.10426111, 45.        ]), 'distance': 2.336095912458896, 'localFrame': array([[ 0.00251479,  0.10419038,  0.99455419],
       [-0.99970884,  0.02412948,  0.        ],
       [-0.02399808, -0.99426462,  0.10422072]]), 'currentState': array([5.86153739e+00, 4.37946140e+00, 4.31826690e+01, 2.51479202e-03,
       1.04190379e-01, 9.94554192e-01]), 'targetState': array([ 5.13454603,  3.10426111, 45.        ]), 'previousTarget': array([ 5.13454603,  3.10426111, 45.        ])}
episode index:18458
target thresh 84.99959963718211
target distance 71.0
model initialize at round 18458
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.55636851,  -1.43905939,  50.60609537]), 'distance': 27.5, 'localFrame': array([[ 0.7285105 ,  0.01012306,  0.68495984],
       [-0.01389422,  0.99990347,  0.        ],
       [-0.68489372, -0.00951698,  0.72858082]]), 'currentState': array([-2.54408045e+00, -1.30559109e+00,  2.54076490e+01,  7.28510495e-01,
        1.01230595e-02,  6.84959840e-01]), 'targetState': array([-32.95750376,  -1.67420003,  95.        ]), 'previousTarget': array([-13.98750647,  -1.92418893,  49.39095974])}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.6284523670503558
{'scaleFactor': 20, 'timeStep': 86, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-32.95750376,  -1.67420003,  95.        ]), 'distance': 3.2274279586657997, 'localFrame': array([[ 0.44901368, -0.69781229,  0.55807232],
       [ 0.84094813,  0.54111574,  0.        ],
       [-0.30198172,  0.46930987,  0.82979232]]), 'currentState': array([-30.89314948,  -0.84497403,  92.66181334,   0.44901368,
        -0.69781229,   0.55807232]), 'targetState': array([-32.95750376,  -1.67420003,  95.        ]), 'previousTarget': array([-32.95750376,  -1.67420003,  95.        ])}
episode index:18459
target thresh 85.00109960221891
target distance 55.69718111064758
model initialize at round 18459
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.70569656,  -0.08231357,  73.31463   ]), 'distance': 27.5, 'localFrame': array([[-0.86879411,  0.29750451, -0.39583817],
       [-0.32396596, -0.94606874,  0.        ],
       [-0.37449012,  0.12823809,  0.91832028]]), 'currentState': array([12.87767678, -8.3973426 , 79.02438488, -0.86879411,  0.29750451,
       -0.39583817]), 'targetState': array([-40.99929941,   9.11358589,  67.        ]), 'previousTarget': array([-10.98054916,  -0.40233679,  73.46756256])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284183230434733
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 65, 'trapConfig': [], 'currentTarget': array([-40.99929941,   9.11358589,  67.        ]), 'distance': 15.078391990459643, 'localFrame': array([[-0.81365469,  0.38645998, -0.43429797],
       [-0.42903326, -0.90328869,  0.        ],
       [-0.39229645,  0.18632828,  0.90076927]]), 'currentState': array([-30.56527143,   3.13231665,  57.90531015,  -0.81365469,
         0.38645998,  -0.43429797]), 'targetState': array([-40.99929941,   9.11358589,  67.        ]), 'previousTarget': array([-40.99929941,   9.11358589,  67.        ])}
episode index:18460
target thresh 85.00259941726668
target distance 18.91829049237586
model initialize at round 18460
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.93694803,  6.59682525, 83.        ]), 'distance': 22.761216232167538, 'localFrame': array([[ 0.4196046 , -0.71658204, -0.55717336],
       [ 0.86293994,  0.50530651,  0.        ],
       [ 0.28154333, -0.48080715,  0.8303962 ]]), 'currentState': array([ 9.51292289, 24.91335295, 74.40247261,  0.4196046 , -0.71658204,
       -0.55717336]), 'targetState': array([19.93694803,  6.59682525, 83.        ]), 'previousTarget': array([19.93694803,  6.59682525, 83.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6284318164999468
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.93694803,  6.59682525, 83.        ]), 'distance': 1.746334505648878, 'localFrame': array([[ 0.57883493,  0.04428153,  0.81424153],
       [-0.07627825,  0.99708657,  0.        ],
       [-0.8118693 , -0.06210892,  0.58052625]]), 'currentState': array([1.83613879e+01, 7.34915431e+00, 8.30359921e+01, 5.78834926e-01,
       4.42815280e-02, 8.14241534e-01]), 'targetState': array([19.93694803,  6.59682525, 83.        ]), 'previousTarget': array([19.93694803,  6.59682525, 83.        ])}
episode index:18461
target thresh 85.00409908234045
target distance 32.69924403621498
model initialize at round 18461
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 20.71502738, -16.00131615,  70.22065237]), 'distance': 27.5, 'localFrame': array([[ 0.54202477, -0.48046598,  0.68946472],
       [ 0.66333452,  0.748323  ,  0.        ],
       [-0.51594231,  0.45734575,  0.72431927]]), 'currentState': array([ -0.84041201, -18.92275332,  53.39572331,   0.54202477,
        -0.48046598,   0.68946472]), 'targetState': array([ 30.68162252, -14.65053035,  78.        ]), 'previousTarget': array([ 19.397239  , -16.11444211,  69.02749987])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.628430878650474
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([ 30.68162252, -14.65053035,  78.        ]), 'distance': 2.9357853608157494, 'localFrame': array([[ 0.87761622, -0.12960383,  0.46151123],
       [ 0.14609268,  0.98927091,  0.        ],
       [-0.45655963,  0.06742341,  0.88713437]]), 'currentState': array([ 31.7986254 , -16.08743116,  75.69642532,   0.87761622,
        -0.12960383,   0.46151123]), 'targetState': array([ 30.68162252, -14.65053035,  78.        ]), 'previousTarget': array([ 30.68162252, -14.65053035,  78.        ])}
episode index:18462
target thresh 85.00559859745522
target distance 37.0
model initialize at round 18462
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-44.49398332,  -0.67911351,  48.15724033]), 'distance': 27.499999999999996, 'localFrame': array([[-0.89942144, -0.28095785, -0.334819  ],
       [ 0.29816733, -0.95451362,  0.        ],
       [-0.3195893 , -0.09983209,  0.94228246]]), 'currentState': array([-42.49852273, -17.06215213,  26.16032731,  -0.89942144,
        -0.28095785,  -0.334819  ]), 'targetState': array([-45.84045143,  10.37559697,  63.        ]), 'previousTarget': array([-43.88579325,  -0.19725924,  48.23192833])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6283968413391675
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 48, 'trapConfig': [], 'currentTarget': array([-45.84045143,  10.37559697,  63.        ]), 'distance': 19.14060141647782, 'localFrame': array([[-0.94714657, -0.08657762, -0.30889754],
       [ 0.09102939, -0.99584821,  0.        ],
       [-0.30761506, -0.02811875,  0.95109532]]), 'currentState': array([-47.74767373,  10.86423166,  43.96092539,  -0.94714657,
        -0.08657762,  -0.30889754]), 'targetState': array([-45.84045143,  10.37559697,  63.        ]), 'previousTarget': array([-45.84045143,  10.37559697,  63.        ])}
episode index:18463
target thresh 85.00709796262595
target distance 43.31682844099746
model initialize at round 18463
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.40415171, -11.13395024,  64.77072649]), 'distance': 27.5, 'localFrame': array([[ 0.93186107, -0.05666805, -0.35836248],
       [ 0.06069956,  0.99815608,  0.        ],
       [ 0.35770169, -0.02175244,  0.93358252]]), 'currentState': array([-2.03794091e+01,  2.66400629e+00,  7.43280731e+01,  9.31861075e-01,
       -5.66680465e-02, -3.58362484e-01]), 'targetState': array([ 21.39481039, -23.79626207,  56.        ]), 'previousTarget': array([  0.21690324, -11.25727484,  65.28923585])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6284020722476715
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.39481039, -23.79626207,  56.        ]), 'distance': 2.6380830708350786, 'localFrame': array([[ 0.23168472, -0.86468236, -0.44569789],
       [ 0.96592756,  0.25881256,  0.        ],
       [ 0.11535221, -0.43051188,  0.89518344]]), 'currentState': array([ 18.94837441, -23.6089166 ,  55.03080707,   0.23168472,
        -0.86468236,  -0.44569789]), 'targetState': array([ 21.39481039, -23.79626207,  56.        ]), 'previousTarget': array([ 21.39481039, -23.79626207,  56.        ])}
episode index:18464
target thresh 85.00859717786768
target distance 74.08278022457218
model initialize at round 18464
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.73758723,  7.29276648, 19.57194968]), 'distance': 27.500000000000004, 'localFrame': array([[-0.59541001,  0.48685735,  0.63910628],
       [-0.633007  , -0.77414607,  0.        ],
       [ 0.49476162, -0.40455875,  0.76911843]]), 'currentState': array([43.10912782,  3.84055884, 12.58094766, -0.59541001,  0.48685735,
        0.63910628]), 'targetState': array([-30.14365135,  13.42982812,  32.        ]), 'previousTarget': array([17.66388962,  6.25625147, 19.09348088])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6283680401831035
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 51, 'trapConfig': [], 'currentTarget': array([-23.21884728,  11.69203086,  27.01118501]), 'distance': 27.500000000000004, 'localFrame': array([[-0.31085232,  0.51307321,  0.80007919],
       [-0.85527255, -0.51817841,  0.        ],
       [ 0.41458376, -0.68428577,  0.59989439]]), 'currentState': array([-1.35480078,  6.20519258, 11.25973821, -0.31085232,  0.51307321,
        0.80007919]), 'targetState': array([-30.14365135,  13.42982812,  32.        ]), 'previousTarget': array([-23.21884728,  11.69203086,  27.01118501])}
episode index:18465
target thresh 85.01009624319538
target distance 42.0
model initialize at round 18465
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.4882404 , -1.40917527, 76.29037616]), 'distance': 27.500000000000004, 'localFrame': array([[-0.28832171, -0.80150924,  0.52388312],
       [ 0.9409703 , -0.33848913,  0.        ],
       [ 0.17732874,  0.49295846,  0.85179016]]), 'currentState': array([29.42850849,  5.68676327, 49.72170874, -0.28832171, -0.80150924,
        0.52388312]), 'targetState': array([29.5213107 , -5.33780988, 91.        ]), 'previousTarget': array([29.35933419, -0.60488499, 75.32485032])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6283542362320731
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 65, 'trapConfig': [], 'currentTarget': array([29.5213107 , -5.33780988, 91.        ]), 'distance': 2.8156328565502085, 'localFrame': array([[ 0.96740239, -0.02679687,  0.25182243],
       [ 0.0276892 ,  0.99961658,  0.        ],
       [-0.25172588,  0.00697276,  0.96777346]]), 'currentState': array([ 2.75376184e+01, -3.43608005e+00,  9.16133326e+01,  9.67402394e-01,
       -2.67968717e-02,  2.51822431e-01]), 'targetState': array([29.5213107 , -5.33780988, 91.        ]), 'previousTarget': array([29.5213107 , -5.33780988, 91.        ])}
episode index:18466
target thresh 85.01159515862405
target distance 8.0
model initialize at round 18466
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.61523156, -28.63039709,  29.        ]), 'distance': 9.132485727461418, 'localFrame': array([[ 0.06154421,  0.99059117, -0.12223516],
       [-0.99807558,  0.06200921,  0.        ],
       [ 0.00757971,  0.12199993,  0.99250117]]), 'currentState': array([  8.02820004, -29.27746323,  37.4460196 ,   0.06154421,
         0.99059117,  -0.12223516]), 'targetState': array([  4.61523156, -28.63039709,  29.        ]), 'previousTarget': array([  4.61523156, -28.63039709,  29.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6283717071701609
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.61523156, -28.63039709,  29.        ]), 'distance': 3.0435591692394817, 'localFrame': array([[-0.20228825, -0.7328516 , -0.64962142],
       [ 0.96395137, -0.26607848,  0.        ],
       [-0.17285028, -0.62620346,  0.76025786]]), 'currentState': array([  3.22056681, -28.74704951,  31.702694  ,  -0.20228825,
        -0.7328516 ,  -0.64962142]), 'targetState': array([  4.61523156, -28.63039709,  29.        ]), 'previousTarget': array([  4.61523156, -28.63039709,  29.        ])}
episode index:18467
target thresh 85.01309392416864
target distance 54.0
model initialize at round 18467
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.90228374, -12.04061963,  30.56890512]), 'distance': 27.5, 'localFrame': array([[ 0.32903983, -0.64441334,  0.69026389],
       [ 0.89061775,  0.45475271,  0.        ],
       [-0.31389938,  0.61476127,  0.72355771]]), 'currentState': array([-16.93979031, -20.0592575 ,   8.98592936,   0.32903983,
        -0.64441334,   0.69026389]), 'targetState': array([19.9967028, -0.3631489, 62.       ]), 'previousTarget': array([ -1.86763576, -11.16282939,  29.89816731])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6283711071286714
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.9967028, -0.3631489, 62.       ]), 'distance': 2.35089280328125, 'localFrame': array([[-0.93697684, -0.24571128,  0.24839559],
       [ 0.25366136, -0.96729309,  0.        ],
       [ 0.24027133,  0.06300836,  0.96865868]]), 'currentState': array([19.91575063,  0.3366438 , 59.75713712, -0.93697684, -0.24571128,
        0.24839559]), 'targetState': array([19.9967028, -0.3631489, 62.       ]), 'previousTarget': array([19.9967028, -0.3631489, 62.       ])}
episode index:18468
target thresh 85.0145925398442
target distance 42.71524371570419
model initialize at round 18468
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.62699983,  14.98666894,  26.67654559]), 'distance': 27.5, 'localFrame': array([[-0.47554072, -0.79934328,  0.36730279],
       [ 0.85941518, -0.51127835,  0.        ],
       [ 0.18779397,  0.31566559,  0.93010142]]), 'currentState': array([ 0.12018763,  6.2061452 , 42.44698044, -0.47554072, -0.79934328,
        0.36730279]), 'targetState': array([-42.5662438 ,  24.27168904,  10.        ]), 'previousTarget': array([-20.90321371,  16.01880042,  26.22879568])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6283733054421394
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-42.5662438 ,  24.27168904,  10.        ]), 'distance': 2.2389773479405477, 'localFrame': array([[-0.41505825,  0.53201803, -0.73802674],
       [-0.7884418 , -0.61510937,  0.        ],
       [-0.45396716,  0.58189113,  0.67477147]]), 'currentState': array([-40.33344806,  24.11562222,   9.94267713,  -0.41505825,
         0.53201803,  -0.73802674]), 'targetState': array([-42.5662438 ,  24.27168904,  10.        ]), 'previousTarget': array([-42.5662438 ,  24.27168904,  10.        ])}
episode index:18469
target thresh 85.01609100566567
target distance 22.118334942098265
model initialize at round 18469
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-33.14224951,  23.95973297,  47.93994984]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.71897558,  0.04432562,  0.69362046],
       [-0.06153425,  0.99810497,  0.        ],
       [-0.69230603, -0.04268141,  0.72034065]]), 'currentState': array([-23.33620911,   1.36498989,  35.71010287,   0.71897558,
         0.04432562,   0.69362046]), 'targetState': array([-33.19039846,  24.07067614,  48.        ]), 'previousTarget': array([-33.19039846,  24.07067614,  48.        ])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6283853836481845
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-33.19039846,  24.07067614,  48.        ]), 'distance': 2.142465066773748, 'localFrame': array([[-0.75250554,  0.01972417,  0.65829049],
       [-0.02620233, -0.99965666,  0.        ],
       [ 0.65806447, -0.01724874,  0.75276399]]), 'currentState': array([-3.12197309e+01,  2.38789127e+01,  4.71815548e+01, -7.52505539e-01,
        1.97241684e-02,  6.58290492e-01]), 'targetState': array([-33.19039846,  24.07067614,  48.        ]), 'previousTarget': array([-33.19039846,  24.07067614,  48.        ])}
episode index:18470
target thresh 85.01758932164806
target distance 51.0
model initialize at round 18470
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.483582  , -18.10710942,  60.14307494]), 'distance': 27.499999999999996, 'localFrame': array([[-0.88790265,  0.45997724,  0.00705785],
       [-0.4599887 , -0.88792477,  0.        ],
       [ 0.00626684, -0.00324653,  0.99997509]]), 'currentState': array([ 3.00773386e+01, -2.66857819e+01,  4.08268613e+01, -8.87902655e-01,
        4.59977242e-01,  7.05784903e-03]), 'targetState': array([-16.5326111 ,  -3.95888497,  92.        ]), 'previousTarget': array([ 13.37400646, -19.1916066 ,  59.98215918])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6283851205336397
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.5326111 ,  -3.95888497,  92.        ]), 'distance': 2.951517777448221, 'localFrame': array([[-0.41084661,  0.36550582,  0.83523084],
       [-0.66467751, -0.74713038,  0.        ],
       [ 0.62402634, -0.55515915,  0.54989948]]), 'currentState': array([-14.68283083,  -2.54947822,  90.18248996,  -0.41084661,
         0.36550582,   0.83523084]), 'targetState': array([-16.5326111 ,  -3.95888497,  92.        ]), 'previousTarget': array([-16.5326111 ,  -3.95888497,  92.        ])}
episode index:18471
target thresh 85.01908748780633
target distance 38.00463395317614
model initialize at round 18471
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.97993766, -8.13345555, 15.17123367]), 'distance': 27.5, 'localFrame': array([[-0.79778025,  0.01978365, -0.60262367],
       [-0.02479075, -0.99969266,  0.        ],
       [-0.60243846,  0.01493949,  0.79802551]]), 'currentState': array([ 1.79229888e+01, -1.37337303e+01,  2.25146201e+01, -7.97780246e-01,
        1.97836480e-02, -6.02623668e-01]), 'targetState': array([-19.16608911,  -5.71498279,  12.        ]), 'previousTarget': array([-6.93524304, -8.1222552 , 15.8619015 ])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6283940653778964
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.16608911,  -5.71498279,  12.        ]), 'distance': 4.239131381153643, 'localFrame': array([[-0.5624192 ,  0.70706976,  0.42864554],
       [-0.78261327, -0.62250821,  0.        ],
       [ 0.26683537, -0.33546369,  0.90347275]]), 'currentState': array([-16.23958237,  -7.67100421,   9.6378456 ,  -0.5624192 ,
         0.70706976,   0.42864554]), 'targetState': array([-19.16608911,  -5.71498279,  12.        ]), 'previousTarget': array([-19.16608911,  -5.71498279,  12.        ])}
episode index:18472
target thresh 85.0205855041555
target distance 57.0
model initialize at round 18472
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.01886702, -3.59288228, 42.37255707]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.47104378,  0.71663652, -0.51434411],
       [-0.83564596,  0.54926845,  0.        ],
       [ 0.28251299,  0.42980957,  0.8575839 ]]), 'currentState': array([-1.38170182,  1.53405229, 69.38797661,  0.47104378,  0.71663652,
       -0.51434411]), 'targetState': array([-0.63780498, -8.97737182, 14.        ]), 'previousTarget': array([-1.27656101, -3.87718836, 43.89761361])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6283951863957599
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-0.63780498, -8.97737182, 14.        ]), 'distance': 2.344800653330166, 'localFrame': array([[-0.2202534 , -0.64884483, -0.72834664],
       [ 0.94693001, -0.32143981,  0.        ],
       [-0.2341196 , -0.68969329,  0.68520885]]), 'currentState': array([  0.09905365, -10.34664639,  15.75505459,  -0.2202534 ,
        -0.64884483,  -0.72834664]), 'targetState': array([-0.63780498, -8.97737182, 14.        ]), 'previousTarget': array([-0.63780498, -8.97737182, 14.        ])}
episode index:18473
target thresh 85.0220833707105
target distance 53.060057202200454
model initialize at round 18473
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.22290897, -21.22488386,  60.72826473]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.14922939,  0.90716779, -0.39341733],
       [-0.98673837,  0.16231878,  0.        ],
       [ 0.06385902,  0.38819997,  0.91936   ]]), 'currentState': array([ -7.64134876, -41.53962005,  73.03045361,   0.14922939,
         0.90716779,  -0.39341733]), 'targetState': array([27.32919272,  9.70130018, 42.        ]), 'previousTarget': array([  6.07060664, -22.69918704,  60.92977801])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6283945852782189
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([27.32919272,  9.70130018, 42.        ]), 'distance': 3.7724541772751383, 'localFrame': array([[ 0.89155287,  0.13758832, -0.43151237],
       [-0.15251885,  0.98830056,  0.        ],
       [ 0.42646392,  0.06581377,  0.90210702]]), 'currentState': array([24.52274351,  7.18624808, 42.17252897,  0.89155287,  0.13758832,
       -0.43151237]), 'targetState': array([27.32919272,  9.70130018, 42.        ]), 'previousTarget': array([27.32919272,  9.70130018, 42.        ])}
episode index:18474
target thresh 85.02358108748635
target distance 65.67433886129264
model initialize at round 18474
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.33743161, -11.48781923,  39.80905872]), 'distance': 27.500000000000004, 'localFrame': array([[-0.21790565,  0.80516472,  0.55156767],
       [-0.96527469, -0.26123699,  0.        ],
       [ 0.14408988, -0.53241432,  0.83413015]]), 'currentState': array([ -6.32704538, -35.29774991,  27.43115671,  -0.21790565,
         0.80516472,   0.55156767]), 'targetState': array([-22.62719976,  29.27473025,  61.        ]), 'previousTarget': array([-12.41496016, -12.67231751,  38.64504833])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6283943217225821
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.62719976,  29.27473025,  61.        ]), 'distance': 2.843640735237266, 'localFrame': array([[-0.23105964,  0.95066455,  0.20699843],
       [-0.97171057, -0.23617488,  0.        ],
       [ 0.04888783, -0.20114256,  0.97834127]]), 'currentState': array([-21.5573923 ,  26.85947038,  59.94723012,  -0.23105964,
         0.95066455,   0.20699843]), 'targetState': array([-22.62719976,  29.27473025,  61.        ]), 'previousTarget': array([-22.62719976,  29.27473025,  61.        ])}
episode index:18475
target thresh 85.025078654498
target distance 31.0
model initialize at round 18475
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.57404695,  19.66806173,  27.44994945]), 'distance': 27.5, 'localFrame': array([[ 0.34293568, -0.48622581, -0.80372855],
       [ 0.81719152,  0.57636622,  0.        ],
       [ 0.46324199, -0.65680015,  0.59499615]]), 'currentState': array([-28.74050187,  10.10750513,  52.89570373,   0.34293568,
        -0.48622581,  -0.80372855]), 'targetState': array([-23.84541797,  21.34001035,  23.        ]), 'previousTarget': array([-24.64790505,  19.7448964 ,  28.06466844])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6284028345943663
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.84541797,  21.34001035,  23.        ]), 'distance': 4.361417914872575, 'localFrame': array([[-0.37041937, -0.91317295, -0.1700137 ],
       [ 0.9266636 , -0.37589171,  0.        ],
       [-0.06390674, -0.1575455 ,  0.9854417 ]]), 'currentState': array([-26.83532139,  20.16595432,  20.04973963,  -0.37041937,
        -0.91317295,  -0.1700137 ]), 'targetState': array([-23.84541797,  21.34001035,  23.        ]), 'previousTarget': array([-23.84541797,  21.34001035,  23.        ])}
episode index:18476
target thresh 85.02657607176043
target distance 70.87241959404619
model initialize at round 18476
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.86515208, 10.12873322, 54.3166333 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.07262634, -0.86783259,  0.4915201 ],
       [ 0.99651653, -0.08339552,  0.        ],
       [ 0.04099057,  0.4898079 ,  0.87086623]]), 'currentState': array([ 3.24048405, 31.26298832, 38.21995053, -0.07262634, -0.86783259,
        0.4915201 ]), 'targetState': array([-20.05846718, -38.03495622,  91.        ]), 'previousTarget': array([-3.69442466, 11.67672914, 53.12305118])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6283999635609401
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.05846718, -38.03495622,  91.        ]), 'distance': 3.4210980931226844, 'localFrame': array([[ 0.50581259, -0.81360666,  0.28670164],
       [ 0.8492586 ,  0.52797711,  0.        ],
       [-0.1513719 ,  0.24348383,  0.95801992]]), 'currentState': array([-19.3827646 , -35.07407688,  89.4250296 ,   0.50581259,
        -0.81360666,   0.28670164]), 'targetState': array([-20.05846718, -38.03495622,  91.        ]), 'previousTarget': array([-20.05846718, -38.03495622,  91.        ])}
episode index:18477
target thresh 85.02807333928864
target distance 61.6708274029378
model initialize at round 18477
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.95688674, -3.22371898, 21.09592245]), 'distance': 27.499999999999996, 'localFrame': array([[-0.40197282,  0.65954662, -0.63515046],
       [-0.85390536, -0.52042833,  0.        ],
       [-0.33055029,  0.54235838,  0.77238844]]), 'currentState': array([ 26.28105771, -27.76591744,  22.52606389,  -0.40197282,
         0.65954662,  -0.63515046]), 'targetState': array([-4.10461819, 32.7437339 , 19.        ]), 'previousTarget': array([15.06569261, -4.51461591, 21.41659477])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6283996997570289
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.10461819, 32.7437339 , 19.        ]), 'distance': 1.781207344397233, 'localFrame': array([[-0.39914445, -0.23701232,  0.88572505],
       [ 0.51057109, -0.85983554,  0.        ],
       [ 0.76157788,  0.45222561,  0.46421022]]), 'currentState': array([-2.96471371, 32.55601078, 17.64424286, -0.39914445, -0.23701232,
        0.88572505]), 'targetState': array([-4.10461819, 32.7437339 , 19.        ]), 'previousTarget': array([-4.10461819, 32.7437339 , 19.        ])}
episode index:18478
target thresh 85.02957045709756
target distance 45.0
model initialize at round 18478
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.00006159,  15.48150602,  52.05977301]), 'distance': 27.5, 'localFrame': array([[ 0.07499921, -0.74009734,  0.6683046 ],
       [ 0.99490462,  0.1008206 ,  0.        ],
       [-0.06737887,  0.66489934,  0.74388773]]), 'currentState': array([ 3.05264534, 11.51017627, 30.85402552,  0.07499921, -0.74009734,
        0.6683046 ]), 'targetState': array([-32.44755881,  19.77766233,  75.        ]), 'previousTarget': array([-14.50946289,  16.17019735,  51.64695911])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6284041455348045
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.44755881,  19.77766233,  75.        ]), 'distance': 3.0503525397134283, 'localFrame': array([[-0.36980138, -0.41755504,  0.82999682],
       [ 0.74861762, -0.663002  ,  0.        ],
       [ 0.55028955,  0.62135025,  0.55776812]]), 'currentState': array([-31.6484535 ,  21.4603909 ,  72.58452781,  -0.36980138,
        -0.41755504,   0.82999682]), 'targetState': array([-32.44755881,  19.77766233,  75.        ]), 'previousTarget': array([-32.44755881,  19.77766233,  75.        ])}
episode index:18479
target thresh 85.03106742520221
target distance 37.0
model initialize at round 18479
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.41410298,  -5.00791498,  64.6697671 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.1646525 , -0.85346874, -0.49444986],
       [ 0.9818945 ,  0.18942859,  0.        ],
       [ 0.09366294, -0.48549759,  0.86920615]]), 'currentState': array([-22.25106701,   4.74577695,  39.41666288,   0.1646525 ,
        -0.85346874,  -0.49444986]), 'targetState': array([-14.86083838, -10.15654875,  78.        ]), 'previousTarget': array([-17.29521208,  -4.94117557,  65.85186764])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6284130854773429
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.86083838, -10.15654875,  78.        ]), 'distance': 2.439377245080907, 'localFrame': array([[ 0.52969124,  0.1015814 ,  0.84208575],
       [-0.18834262,  0.98210338,  0.        ],
       [-0.82701527, -0.15860063,  0.53934367]]), 'currentState': array([-14.18029031,  -9.82518492,  75.68103177,   0.52969124,
         0.1015814 ,   0.84208575]), 'targetState': array([-14.86083838, -10.15654875,  78.        ]), 'previousTarget': array([-14.86083838, -10.15654875,  78.        ])}
episode index:18480
target thresh 85.03256424361751
target distance 59.21725172259687
model initialize at round 18480
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.46288063, -14.36416553,  77.48999135]), 'distance': 27.500000000000004, 'localFrame': array([[-0.35919462,  0.61866389, -0.69873759],
       [-0.8648069 , -0.5021046 ,  0.        ],
       [-0.35083936,  0.60427309,  0.71537807]]), 'currentState': array([-20.96868045, -35.21952699,  70.49982179,  -0.35919462,
         0.61866389,  -0.69873759]), 'targetState': array([25.0768461 , 22.95978636, 90.        ]), 'previousTarget': array([ -4.73168397, -15.18496007,  78.40531406])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6284128210062327
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.0768461 , 22.95978636, 90.        ]), 'distance': 3.5941565401796707, 'localFrame': array([[ 0.46480888,  0.62890361,  0.62324389],
       [-0.80419626,  0.59436384,  0.        ],
       [-0.37043363, -0.50121041,  0.78202753]]), 'currentState': array([26.23156896, 20.12682008, 88.11350102,  0.46480888,  0.62890361,
        0.62324389]), 'targetState': array([25.0768461 , 22.95978636, 90.        ]), 'previousTarget': array([25.0768461 , 22.95978636, 90.        ])}
episode index:18481
target thresh 85.03406091235848
target distance 19.687020340177174
model initialize at round 18481
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.53137496, -3.69525246, 99.        ]), 'distance': 20.157426309606308, 'localFrame': array([[ 0.61063162, -0.43676079,  0.66058235],
       [ 0.58176316,  0.81335824,  0.        ],
       [-0.5372901 ,  0.38430248,  0.75075359]]), 'currentState': array([-0.1996757 , 14.9540987 , 91.54835586,  0.61063162, -0.43676079,
        0.66058235]), 'targetState': array([ 1.53137496, -3.69525246, 99.        ]), 'previousTarget': array([ 1.53137496, -3.69525246, 99.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6284272634601475
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.53137496, -3.69525246, 99.        ]), 'distance': 2.5091761867725006, 'localFrame': array([[ 0.49229092, -0.85375377,  0.16957049],
       [ 0.86629946,  0.49952501,  0.        ],
       [-0.0847047 ,  0.14689883,  0.98551806]]), 'currentState': array([-0.62628667, -2.43708633, 98.76025136,  0.49229092, -0.85375377,
        0.16957049]), 'targetState': array([ 1.53137496, -3.69525246, 99.        ]), 'previousTarget': array([ 1.53137496, -3.69525246, 99.        ])}
episode index:18482
target thresh 85.03555743144004
target distance 83.0
model initialize at round 18482
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.07858096, 12.32582432, 28.84105478]), 'distance': 27.5, 'localFrame': array([[ 0.67740961,  0.6886482 , -0.25861145],
       [-0.71290004,  0.70126567,  0.        ],
       [ 0.18135533,  0.18436411,  0.96598143]]), 'currentState': array([45.06708932, 10.28268859,  3.71480794,  0.67740961,  0.6886482 ,
       -0.25861145]), 'targetState': array([ 8.20648921, 17.13632209, 88.        ]), 'previousTarget': array([33.15477256, 11.74701748, 30.15930922])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.628424706469976
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.20648921, 17.13632209, 88.        ]), 'distance': 2.169752364459735, 'localFrame': array([[ 0.51043792, -0.68284526,  0.52266192],
       [ 0.80095389,  0.59872603,  0.        ],
       [-0.3129313 ,  0.4186281 ,  0.85254004]]), 'currentState': array([ 9.17694836, 16.7461223 , 86.09900592,  0.51043792, -0.68284526,
        0.52266192]), 'targetState': array([ 8.20648921, 17.13632209, 88.        ]), 'previousTarget': array([ 8.20648921, 17.13632209, 88.        ])}
episode index:18483
target thresh 85.03705380087717
target distance 32.9586216462298
model initialize at round 18483
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.48241092, -25.93038698,  46.30266627]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.30349549, -0.80326752,  0.51249563],
       [ 0.93545713,  0.35344018,  0.        ],
       [-0.18113655,  0.47941769,  0.85868983]]), 'currentState': array([-7.34956624, -4.32155781, 57.46729183,  0.30349549, -0.80326752,
        0.51249563]), 'targetState': array([ 11.57698992, -36.19355335,  41.        ]), 'previousTarget': array([  5.404225  , -25.42567249,  45.90063616])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6284340770544311
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.57698992, -36.19355335,  41.        ]), 'distance': 2.908189882595855, 'localFrame': array([[-0.02403967, -0.70769054, -0.70611345],
       [ 0.99942355, -0.0339496 ,  0.        ],
       [-0.02397227, -0.7057064 ,  0.70809872]]), 'currentState': array([ 1.03238975e+01, -3.36459660e+01,  4.16301798e+01, -2.40396696e-02,
       -7.07690538e-01, -7.06113445e-01]), 'targetState': array([ 11.57698992, -36.19355335,  41.        ]), 'previousTarget': array([ 11.57698992, -36.19355335,  41.        ])}
episode index:18484
target thresh 85.03855002068484
target distance 13.0
model initialize at round 18484
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.91594638, -1.29383435, 43.        ]), 'distance': 15.87631408533279, 'localFrame': array([[ 0.85692022, -0.32315717, -0.4015684 ],
       [ 0.35285753,  0.93567706,  0.        ],
       [ 0.37573834, -0.14169643,  0.91582903]]), 'currentState': array([ 1.83524336,  2.7701367 , 56.04774796,  0.85692022, -0.32315717,
       -0.4015684 ]), 'targetState': array([ 9.91594638, -1.29383435, 43.        ]), 'previousTarget': array([ 9.91594638, -1.29383435, 43.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6284505028737901
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.91594638, -1.29383435, 43.        ]), 'distance': 4.639384904668743, 'localFrame': array([[ 0.9734288 , -0.17595661,  0.14654569],
       [ 0.17787699,  0.98405273,  0.        ],
       [-0.14420869,  0.02606711,  0.9892039 ]]), 'currentState': array([ 7.92987735,  1.66922451, 45.96642956,  0.9734288 , -0.17595661,
        0.14654569]), 'targetState': array([ 9.91594638, -1.29383435, 43.        ]), 'previousTarget': array([ 9.91594638, -1.29383435, 43.        ])}
episode index:18485
target thresh 85.04004609087802
target distance 22.955328527265557
model initialize at round 18485
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.67343315, -16.90232946,  30.50559894]), 'distance': 27.5, 'localFrame': array([[ 0.89814781, -0.17198279,  0.40466336],
       [ 0.18806915,  0.98215579,  0.        ],
       [-0.39744246,  0.07610469,  0.91446573]]), 'currentState': array([ 1.13925326, -0.02237672, 16.43655168,  0.89814781, -0.17198279,
        0.40466336]), 'targetState': array([ 22.95532853, -22.29468305,  35.        ]), 'previousTarget': array([ 16.72866221, -16.24721778,  29.57497085])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6284581623265074
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.95532853, -22.29468305,  35.        ]), 'distance': 2.857209538877655, 'localFrame': array([[-0.29439857, -0.76081072,  0.57835675],
       [ 0.93261299, -0.36087811,  0.        ],
       [ 0.20871629,  0.53938301,  0.81578396]]), 'currentState': array([ 22.37485665, -19.49706276,  34.99558617,  -0.29439857,
        -0.76081072,   0.57835675]), 'targetState': array([ 22.95532853, -22.29468305,  35.        ]), 'previousTarget': array([ 22.95532853, -22.29468305,  35.        ])}
episode index:18486
target thresh 85.04154201147166
target distance 61.0
model initialize at round 18486
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.03477314, 17.82474755, 71.6584704 ]), 'distance': 27.5, 'localFrame': array([[ 0.81806068,  0.35289065, -0.45414195],
       [-0.39609275,  0.91821051,  0.        ],
       [ 0.41699791,  0.17988234,  0.89092934]]), 'currentState': array([12.60677265, 20.41453008, 98.82079429,  0.81806068,  0.35289065,
       -0.45414195]), 'targetState': array([20.28261488, 14.61559214, 38.        ]), 'previousTarget': array([14.99689687, 17.78721608, 71.9425784 ])}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.6284488666909482
{'scaleFactor': 20, 'timeStep': 79, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([20.28261488, 14.61559214, 38.        ]), 'distance': 1.683283297898617, 'localFrame': array([[ 0.40316393, -0.16235019, -0.9006116 ],
       [ 0.37354099,  0.92761367,  0.        ],
       [ 0.83541964, -0.33641535,  0.43462483]]), 'currentState': array([21.22165793, 14.57100286, 39.39629962,  0.40316393, -0.16235019,
       -0.9006116 ]), 'targetState': array([20.28261488, 14.61559214, 38.        ]), 'previousTarget': array([20.28261488, 14.61559214, 38.        ])}
episode index:18487
target thresh 85.04303778248071
target distance 45.0
model initialize at round 18487
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.83611038, -4.28700663, 71.37549269]), 'distance': 27.5, 'localFrame': array([[-0.92652075,  0.14040183, -0.34906537],
       [-0.14982614, -0.98871236,  0.        ],
       [-0.34512524,  0.05229912,  0.93709838]]), 'currentState': array([ 2.44908338, -4.97463772, 98.13877916, -0.92652075,  0.14040183,
       -0.34906537]), 'targetState': array([-8.1514822 , -3.81488374, 53.        ]), 'previousTarget': array([-3.09903752, -4.56580565, 71.49260558])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6284556965492625
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.1514822 , -3.81488374, 53.        ]), 'distance': 2.8700559133343817, 'localFrame': array([[ 0.51542709, -0.61804623, -0.59359394],
       [ 0.76798377,  0.6404693 ,  0.        ],
       [ 0.3801787 , -0.45587052,  0.8047647 ]]), 'currentState': array([-8.86679815, -4.78884231, 55.60325734,  0.51542709, -0.61804623,
       -0.59359394]), 'targetState': array([-8.1514822 , -3.81488374, 53.        ]), 'previousTarget': array([-8.1514822 , -3.81488374, 53.        ])}
episode index:18488
target thresh 85.04453340392016
target distance 35.91608272323767
model initialize at round 18488
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 32.83200815, -15.28437947,  85.11576472]), 'distance': 27.5, 'localFrame': array([[-0.42127575,  0.32353469,  0.8472615 ],
       [-0.6090913 , -0.79310011,  0.        ],
       [ 0.67196319, -0.51605961,  0.53117601]]), 'currentState': array([ 27.88369422, -37.42904686,  69.5794236 ,  -0.42127575,
         0.32353469,   0.8472615 ]), 'targetState': array([35.98013027, -1.1959206 , 95.        ]), 'previousTarget': array([ 33.15544944, -15.40294951,  84.31983247])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6284637751723816
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([35.98013027, -1.1959206 , 95.        ]), 'distance': 3.0983057802440643, 'localFrame': array([[ 0.17008427,  0.84194029,  0.5120624 ],
       [-0.98019908,  0.19801457,  0.        ],
       [-0.10139581, -0.50192309,  0.85894825]]), 'currentState': array([37.22855674, -3.87152082, 94.06080166,  0.17008427,  0.84194029,
        0.5120624 ]), 'targetState': array([35.98013027, -1.1959206 , 95.        ]), 'previousTarget': array([35.98013027, -1.1959206 , 95.        ])}
episode index:18489
target thresh 85.04602887580491
target distance 83.0
model initialize at round 18489
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.99487207,   5.20193455,  60.78904843]), 'distance': 27.500000000000004, 'localFrame': array([[-0.44460659, -0.41915465, -0.7916024 ],
       [ 0.68597315, -0.72762685,  0.        ],
       [-0.57599116, -0.54301799,  0.61103653]]), 'currentState': array([-11.3349003 ,  -2.73981024,  85.97840177,  -0.44460659,
        -0.41915465,  -0.7916024 ]), 'targetState': array([-36.26417241,  23.10648826,   4.        ]), 'previousTarget': array([-18.65686951,   6.16927734,  61.677977  ])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6284590818949772
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-36.26417241,  23.10648826,   4.        ]), 'distance': 2.410904844207353, 'localFrame': array([[-0.69429883, -0.44891597, -0.56251541],
       [ 0.54296461, -0.83975558,  0.        ],
       [-0.47237546, -0.30542596,  0.8267868 ]]), 'currentState': array([-34.33098838,  21.89196148,   4.77471698,  -0.69429883,
        -0.44891597,  -0.56251541]), 'targetState': array([-36.26417241,  23.10648826,   4.        ]), 'previousTarget': array([-36.26417241,  23.10648826,   4.        ])}
episode index:18490
target thresh 85.04752419814997
target distance 19.68816041477121
model initialize at round 18490
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.89490843, -10.96353679,  12.        ]), 'distance': 22.125218314668025, 'localFrame': array([[ 0.62828031,  0.56461488, -0.53523255],
       [-0.66841684,  0.74378688,  0.        ],
       [ 0.39809895,  0.35775845,  0.84470475]]), 'currentState': array([-19.55757022, -13.87042849,  23.52303437,   0.62828031,
         0.56461488,  -0.53523255]), 'targetState': array([ -0.89490843, -10.96353679,  12.        ]), 'previousTarget': array([ -0.89490843, -10.96353679,  12.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6284725512552664
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.89490843, -10.96353679,  12.        ]), 'distance': 3.604963016075603, 'localFrame': array([[ 0.54883327, -0.82979968,  0.10106698],
       [ 0.83407043,  0.55165797,  0.        ],
       [-0.0557544 ,  0.08429698,  0.99487962]]), 'currentState': array([ -3.82702271, -10.94570723,   9.9028242 ,   0.54883327,
        -0.82979968,   0.10106698]), 'targetState': array([ -0.89490843, -10.96353679,  12.        ]), 'previousTarget': array([ -0.89490843, -10.96353679,  12.        ])}
episode index:18491
target thresh 85.04901937097027
target distance 64.31763299402357
model initialize at round 18491
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.05946539, 14.77343764, 39.93528435]), 'distance': 27.499999999999996, 'localFrame': array([[-0.15462473, -0.58285161,  0.79773128],
       [ 0.96656541, -0.25642019,  0.        ],
       [ 0.2045544 ,  0.77105946,  0.60301311]]), 'currentState': array([-22.88891064,  22.24640789,  26.54793415,  -0.15462473,
        -0.58285161,   0.79773128]), 'targetState': array([40.9780887 ,  1.34024128, 64.        ]), 'previousTarget': array([-0.76946406, 15.61312079, 38.6857202 ])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6284706311760508
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([40.9780887 ,  1.34024128, 64.        ]), 'distance': 2.7450326709436226, 'localFrame': array([[ 0.72363694, -0.43845079,  0.53302015],
       [ 0.51820053,  0.85525915,  0.        ],
       [-0.45587036,  0.27621133,  0.84610254]]), 'currentState': array([38.47072998,  2.41452259, 63.69295558,  0.72363694, -0.43845079,
        0.53302015]), 'targetState': array([40.9780887 ,  1.34024128, 64.        ]), 'previousTarget': array([40.9780887 ,  1.34024128, 64.        ])}
episode index:18492
target thresh 85.05051439428077
target distance 66.17775921127456
model initialize at round 18492
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.20033894,  -3.23158766,  48.0019215 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.1630688 , -0.98258136, -0.08912035],
       [ 0.9865068 ,  0.16372027,  0.        ],
       [ 0.01459081, -0.08791783,  0.99602086]]), 'currentState': array([-22.28352708,  23.6045522 ,  53.69379548,   0.1630688 ,
        -0.98258136,  -0.08912035]), 'targetState': array([-26.89508904, -40.95917706,  40.        ]), 'previousTarget': array([-24.46910896,  -1.71618914,  47.70891685])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6284739285563943
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.89508904, -40.95917706,  40.        ]), 'distance': 2.729220739135741, 'localFrame': array([[-0.30901533, -0.59584263, -0.74126992],
       [ 0.88771806, -0.46038749,  0.        ],
       [-0.34127139, -0.65803869,  0.67120706]]), 'currentState': array([-28.47411317, -38.84705706,  39.29695111,  -0.30901533,
        -0.59584263,  -0.74126992]), 'targetState': array([-26.89508904, -40.95917706,  40.        ]), 'previousTarget': array([-26.89508904, -40.95917706,  40.        ])}
episode index:18493
target thresh 85.0520092680964
target distance 22.478207475500994
model initialize at round 18493
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.14798009, 23.6474508 , 88.1868911 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.06708586, -0.43364225, -0.89858438],
       [ 0.98824406, -0.15288455,  0.        ],
       [-0.13737967, -0.88802067,  0.43880077]]), 'currentState': array([ 1.33583757e+01,  4.15529849e+01,  9.48211511e+01, -6.70858583e-02,
       -4.33642254e-01, -8.98584377e-01]), 'targetState': array([36.68840588, 20.44409142, 87.        ]), 'previousTarget': array([33.00704433, 24.08862445, 88.47397224])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6284828579580969
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([36.68840588, 20.44409142, 87.        ]), 'distance': 2.826879579840046, 'localFrame': array([[ 0.74637277,  0.05661335,  0.66311583],
       [-0.07563404,  0.99713564,  0.        ],
       [-0.66121643, -0.05015413,  0.74851679]]), 'currentState': array([3.41702416e+01, 2.16035935e+01, 8.75528578e+01, 7.46372775e-01,
       5.66133485e-02, 6.63115834e-01]), 'targetState': array([36.68840588, 20.44409142, 87.        ]), 'previousTarget': array([36.68840588, 20.44409142, 87.        ])}
episode index:18494
target thresh 85.05350399243213
target distance 13.474159710671046
model initialize at round 18494
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.08126497,  3.99917442, 76.        ]), 'distance': 15.031791638233399, 'localFrame': array([[-0.42188465, -0.76022242, -0.49403969],
       [ 0.87438235, -0.48523758,  0.        ],
       [-0.23972663, -0.43197959,  0.86943935]]), 'currentState': array([12.05548313, 12.25455074, 72.75918723, -0.42188465, -0.76022242,
       -0.49403969]), 'targetState': array([-0.08126497,  3.99917442, 76.        ]), 'previousTarget': array([-0.08126497,  3.99917442, 76.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6284992722587159
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.08126497,  3.99917442, 76.        ]), 'distance': 3.2268695152715337, 'localFrame': array([[-0.78902225,  0.60811569, -0.08740252],
       [-0.61045184, -0.79205337,  0.        ],
       [-0.06922746,  0.05335503,  0.99617308]]), 'currentState': array([ 1.91671332,  6.11605854, 74.60731509, -0.78902225,  0.60811569,
       -0.08740252]), 'targetState': array([-0.08126497,  3.99917442, 76.        ]), 'previousTarget': array([-0.08126497,  3.99917442, 76.        ])}
episode index:18495
target thresh 85.0549985673029
target distance 19.0
model initialize at round 18495
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.56394077, -23.27581895,  80.        ]), 'distance': 21.971217465506708, 'localFrame': array([[ 0.30046681,  0.27759149,  0.91250351],
       [-0.67859354,  0.734514  ,  0.        ],
       [-0.6702466 , -0.61921899,  0.40906887]]), 'currentState': array([  2.93296852, -18.25079755,  62.7390007 ,   0.30046681,
         0.27759149,   0.91250351]), 'targetState': array([ 15.56394077, -23.27581895,  80.        ]), 'previousTarget': array([ 15.56394077, -23.27581895,  80.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6285136991067911
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.56394077, -23.27581895,  80.        ]), 'distance': 4.558725331763544, 'localFrame': array([[ 0.93276486, -0.3578227 , -0.0437336 ],
       [ 0.35816538,  0.93365816,  0.        ],
       [ 0.04083223, -0.01566386,  0.99904323]]), 'currentState': array([ 1.26681940e+01, -2.57843232e+01,  7.75293658e+01,  9.32764862e-01,
       -3.57822700e-01, -4.37335993e-02]), 'targetState': array([ 15.56394077, -23.27581895,  80.        ]), 'previousTarget': array([ 15.56394077, -23.27581895,  80.        ])}
episode index:18496
target thresh 85.05649299272365
target distance 46.19834244593673
model initialize at round 18496
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.05936767, -15.5529693 ,  74.91963463]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.85023803,  0.03934883,  0.52492567],
       [-0.0462303 ,  0.99893081,  0.        ],
       [-0.52436443, -0.02426747,  0.85114807]]), 'currentState': array([-3.45312091e+01, -2.77460235e+01,  6.11907186e+01,  8.50238029e-01,
        3.93488292e-02,  5.24925674e-01]), 'targetState': array([ 9.91883195, -1.27152381, 91.        ]), 'previousTarget': array([-15.57109439, -16.17382235,  74.44750591])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6285056776848019
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([ 9.91883195, -1.27152381, 91.        ]), 'distance': 3.4015899166575045, 'localFrame': array([[ 0.36720547,  0.48048476,  0.7964261 ],
       [-0.79453657,  0.6072163 ,  0.        ],
       [-0.48360291, -0.63278967,  0.60473586]]), 'currentState': array([11.63568691, -3.93427743, 89.76186991,  0.36720547,  0.48048476,
        0.7964261 ]), 'targetState': array([ 9.91883195, -1.27152381, 91.        ]), 'previousTarget': array([ 9.91883195, -1.27152381, 91.        ])}
episode index:18497
target thresh 85.05798726870933
target distance 60.0
model initialize at round 18497
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.31313973,  9.28787605, 43.12435236]), 'distance': 27.5, 'localFrame': array([[-0.04460808,  0.30013312, -0.95285373],
       [-0.9891346 , -0.14701275,  0.        ],
       [-0.14008165,  0.94250059,  0.30343001]]), 'currentState': array([-6.78689478e+00,  9.04320786e+00,  6.87012842e+01, -4.46080821e-02,
        3.00133124e-01, -9.52853728e-01]), 'targetState': array([16.39356332,  9.60474266, 10.        ]), 'previousTarget': array([ 3.50796783,  8.61495894, 44.26893901])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6285089722792545
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.39356332,  9.60474266, 10.        ]), 'distance': 3.435654603155635, 'localFrame': array([[ 0.48007953,  0.65863745, -0.5794138 ],
       [-0.80811087,  0.5890304 ,  0.        ],
       [ 0.34129234,  0.46823059,  0.81503353]]), 'currentState': array([14.27536924,  9.0846704 , 12.65452467,  0.48007953,  0.65863745,
       -0.5794138 ]), 'targetState': array([16.39356332,  9.60474266, 10.        ]), 'previousTarget': array([16.39356332,  9.60474266, 10.        ])}
episode index:18498
target thresh 85.05948139527489
target distance 11.216620836130257
model initialize at round 18498
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.38301143, 13.76476494, 39.        ]), 'distance': 15.019391337508107, 'localFrame': array([[-0.71212519,  0.54113644, -0.44726845],
       [-0.60502748, -0.79620459,  0.        ],
       [-0.35611719,  0.27060971,  0.89439976]]), 'currentState': array([33.78699946,  5.77256077, 47.55989473, -0.71212519,  0.54113644,
       -0.44726845]), 'targetState': array([24.38301143, 13.76476494, 39.        ]), 'previousTarget': array([24.38301143, 13.76476494, 39.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6285253816189824
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.38301143, 13.76476494, 39.        ]), 'distance': 2.854781529062972, 'localFrame': array([[-0.32753169,  0.43278102, -0.83989498],
       [-0.79738712, -0.60346813,  0.        ],
       [-0.50684985,  0.66972144,  0.54274894]]), 'currentState': array([26.5804254 , 12.15622593, 39.85659307, -0.32753169,  0.43278102,
       -0.83989498]), 'targetState': array([24.38301143, 13.76476494, 39.        ]), 'previousTarget': array([24.38301143, 13.76476494, 39.        ])}
episode index:18499
target thresh 85.06097537243525
target distance 69.37238534605265
model initialize at round 18499
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.85856501, 15.49341226, 15.6798242 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.04300622, -0.70461004, -0.7082903 ],
       [ 0.99814252,  0.06092212,  0.        ],
       [ 0.04315055, -0.70697467,  0.70592128]]), 'currentState': array([21.76345408, 38.88950624, 23.87411504,  0.04300622, -0.70461004,
       -0.7082903 ]), 'targetState': array([-12.92151092, -29.27515253,   0.        ]), 'previousTarget': array([10.62431668, 16.78229062, 16.59790236])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6285171013303547
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([-12.92151092, -29.27515253,   0.        ]), 'distance': 1.9852121902346787, 'localFrame': array([[ 0.71643742, -0.68144118, -0.14951703],
       [ 0.68918824,  0.72458234,  0.        ],
       [ 0.1083374 , -0.10304538,  0.98875915]]), 'currentState': array([-14.07320468, -27.85947931,   0.78136948,   0.71643742,
        -0.68144118,  -0.14951703]), 'targetState': array([-12.92151092, -29.27515253,   0.        ]), 'previousTarget': array([-12.92151092, -29.27515253,   0.        ])}
episode index:18500
target thresh 85.06246920020538
target distance 27.233384078868074
model initialize at round 18500
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.85784134,  9.14223171, 13.06943939]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.92681808,  0.17449609,  0.33250467],
       [-0.18502363,  0.98273407,  0.        ],
       [-0.32676367, -0.06152122,  0.94310161]]), 'currentState': array([ 1.64881383, -1.42591198, 16.08096238,  0.92681808,  0.17449609,
        0.33250467]), 'targetState': array([27.43910853,  9.38591089, 13.        ]), 'previousTarget': array([25.8469806 ,  8.77997653, 13.11692472])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6285255960625029
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.43910853,  9.38591089, 13.        ]), 'distance': 1.7068862492584667, 'localFrame': array([[ 0.56793467,  0.08355494,  0.81882158],
       [-0.1455539 ,  0.98935032,  0.        ],
       [-0.81010139, -0.11918268,  0.5740481 ]]), 'currentState': array([26.85650481,  9.39850107, 11.3956699 ,  0.56793467,  0.08355494,
        0.81882158]), 'targetState': array([27.43910853,  9.38591089, 13.        ]), 'previousTarget': array([27.43910853,  9.38591089, 13.        ])}
episode index:18501
target thresh 85.0639628786002
target distance 31.653048070346884
model initialize at round 18501
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.74567648, -18.97782065,  26.7748376 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.85046874,  0.35585777, -0.38738632],
       [-0.38599742,  0.92249986,  0.        ],
       [ 0.35736383,  0.14953012,  0.92191748]]), 'currentState': array([ 2.06364714, -2.07223818, 26.19098432,  0.85046874,  0.35585777,
       -0.38738632]), 'targetState': array([ 32.10732815, -25.49744064,  27.        ]), 'previousTarget': array([ 22.85037668, -18.90793445,  26.70754945])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6285316048603261
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 32.10732815, -25.49744064,  27.        ]), 'distance': 1.773821435180754, 'localFrame': array([[ 0.83309908,  0.44601706,  0.32713103],
       [-0.47198621,  0.88160593,  0.        ],
       [-0.28840065, -0.15440134,  0.94497899]]), 'currentState': array([ 30.38458369, -25.20958681,  27.30940944,   0.83309908,
         0.44601706,   0.32713103]), 'targetState': array([ 32.10732815, -25.49744064,  27.        ]), 'previousTarget': array([ 32.10732815, -25.49744064,  27.        ])}
episode index:18502
target thresh 85.06545640763463
target distance 65.0227343503539
model initialize at round 18502
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.68381868, -2.24674494, 82.39111233]), 'distance': 27.500000000000004, 'localFrame': array([[-0.33591409,  0.42239361, -0.84187016],
       [-0.78267401, -0.62243184,  0.        ],
       [-0.52400679,  0.65890989,  0.53968012]]), 'currentState': array([ 40.76411697, -10.81679439,  84.01046202,  -0.33591409,
         0.42239361,  -0.84187016]), 'targetState': array([-23.82603712,  10.40768732,  80.        ]), 'previousTarget': array([15.30520095, -3.27124369, 83.0090428 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284976356874967
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 88, 'trapConfig': [], 'currentTarget': array([ 3.03981514,  0.85972509, 77.97489311]), 'distance': 27.5, 'localFrame': array([[-0.91549415,  0.28260457, -0.28636536],
       [-0.29495724, -0.95551045,  0.        ],
       [-0.2736251 ,  0.08446554,  0.95812049]]), 'currentState': array([28.88692646, -8.32618273, 76.02657736, -0.91549415,  0.28260457,
       -0.28636536]), 'targetState': array([-23.82603712,  10.40768732,  80.        ]), 'previousTarget': array([ 3.03981514,  0.85972509, 77.97489311])}
episode index:18503
target thresh 85.06694978732364
target distance 27.232429126352635
model initialize at round 18503
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-35.9682024 ,  11.56825864,  83.52868841]), 'distance': 27.5, 'localFrame': array([[ 0.21533717,  0.66348545,  0.7165312 ],
       [-0.95115856,  0.30870276,  0.        ],
       [-0.22119516, -0.68153478,  0.69755505]]), 'currentState': array([-15.90789238,  -2.22456499,  70.73838043,   0.21533717,
         0.66348545,   0.7165312 ]), 'targetState': array([-42.98100434,  16.39003558,  88.        ]), 'previousTarget': array([-35.2690126 ,  10.94841294,  82.619362  ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6285074297986367
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-42.98100434,  16.39003558,  88.        ]), 'distance': 2.703065291185296, 'localFrame': array([[-0.62546137,  0.73023623,  0.2748693 ],
       [-0.75949059, -0.65051829,  0.        ],
       [ 0.1788075 , -0.20876064,  0.9614816 ]]), 'currentState': array([-41.57417789,  16.46414004,  85.69307351,  -0.62546137,
         0.73023623,   0.2748693 ]), 'targetState': array([-42.98100434,  16.39003558,  88.        ]), 'previousTarget': array([-42.98100434,  16.39003558,  88.        ])}
episode index:18504
target thresh 85.06844301768214
target distance 47.67664556452738
model initialize at round 18504
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.50694536, -17.22179004,  83.09008234]), 'distance': 27.5, 'localFrame': array([[-0.55898232, -0.80458592, -0.20045015],
       [ 0.82125418, -0.57056251,  0.        ],
       [-0.11436934, -0.16462053,  0.9797039 ]]), 'currentState': array([-27.98783295,   4.58185986,  69.27112356,  -0.55898232,
        -0.80458592,  -0.20045015]), 'targetState': array([ -7.59149628, -42.32456951,  99.        ]), 'previousTarget': array([-17.65399062, -16.90450631,  83.53788382])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6285088970664441
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([ -7.59149628, -42.32456951,  99.        ]), 'distance': 2.920803449046179, 'localFrame': array([[ 8.92163744e-01, -1.39810700e-03, -4.51709972e-01],
       [ 1.56709493e-03,  9.99998772e-01,  0.00000000e+00],
       [ 4.51709418e-01, -7.07872406e-04,  8.92164840e-01]]), 'currentState': array([-1.03364500e+01, -4.25070314e+01,  9.80186593e+01,  8.92163744e-01,
       -1.39810700e-03, -4.51709972e-01]), 'targetState': array([ -7.59149628, -42.32456951,  99.        ]), 'previousTarget': array([ -7.59149628, -42.32456951,  99.        ])}
episode index:18505
target thresh 85.06993609872508
target distance 27.233225801829608
model initialize at round 18505
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.49961883,  9.64425686, 84.45820737]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.15802714,  0.60184373, -0.78282281],
       [-0.96721393,  0.25396301,  0.        ],
       [ 0.19880804,  0.75715713,  0.62224469]]), 'currentState': array([-5.60171856, -7.73658599, 93.90782173,  0.15802714,  0.60184373,
       -0.78282281]), 'targetState': array([20.48999784, 16.00499886, 81.        ]), 'previousTarget': array([12.29438722,  8.57302517, 85.21318244])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6285178187884033
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.48999784, 16.00499886, 81.        ]), 'distance': 2.256989125800408, 'localFrame': array([[ 0.66240212,  0.11831543, -0.73974651],
       [-0.17583289,  0.98442003,  0.        ],
       [ 0.72822128,  0.13007177,  0.67288566]]), 'currentState': array([18.68695284, 15.28035058, 79.85199587,  0.66240212,  0.11831543,
       -0.73974651]), 'targetState': array([20.48999784, 16.00499886, 81.        ]), 'previousTarget': array([20.48999784, 16.00499886, 81.        ])}
episode index:18506
target thresh 85.07142903046739
target distance 43.0
model initialize at round 18506
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.39956203, -15.92099621,  73.11235904]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83413454,  0.1044193 ,  0.54158673],
       [-0.12421333, -0.99225554,  0.        ],
       [ 0.53739243, -0.06727229,  0.84064488]]), 'currentState': array([ 29.42466889, -19.60819415,  50.37690797,  -0.83413454,
         0.1044193 ,   0.54158673]), 'targetState': array([  1.91734548, -12.85782977,  92.        ]), 'previousTarget': array([ 15.40098396, -15.79426708,  71.73154236])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6285178893461606
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  1.91734548, -12.85782977,  92.        ]), 'distance': 3.363535161704456, 'localFrame': array([[-0.23164148, -0.21963275,  0.94768332],
       [ 0.6880466 , -0.72566651,  0.        ],
       [ 0.68770205,  0.65205028,  0.31921203]]), 'currentState': array([  0.58093547, -13.62101476,  89.00919313,  -0.23164148,
        -0.21963275,   0.94768332]), 'targetState': array([  1.91734548, -12.85782977,  92.        ]), 'previousTarget': array([  1.91734548, -12.85782977,  92.        ])}
episode index:18507
target thresh 85.07292181292397
target distance 54.1998118563732
model initialize at round 18507
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([11.77308945,  3.98469345, 84.30436216]), 'distance': 27.5, 'localFrame': array([[ 0.96547899,  0.07903395,  0.24820145],
       [-0.08158693,  0.99666623,  0.        ],
       [-0.247374  , -0.02024999,  0.96870844]]), 'currentState': array([35.59073639, 10.11432362, 72.        ,  0.96547899,  0.07903395,
        0.24820145]), 'targetState': array([-18.60907547,  -3.83435917, 100.        ]), 'previousTarget': array([11.77308945,  3.98469345, 84.30436216])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6285172827031736
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([-18.60907547,  -3.83435917, 100.        ]), 'distance': 3.7634712179047782, 'localFrame': array([[-0.88902886,  0.45781244,  0.00595397],
       [-0.45782056, -0.88904462,  0.        ],
       [ 0.00529335, -0.00272585,  0.99998227]]), 'currentState': array([-1.77936897e+01, -1.62108086e+00,  9.70673799e+01, -8.89028861e-01,
        4.57812444e-01,  5.95397335e-03]), 'targetState': array([-18.60907547,  -3.83435917, 100.        ]), 'previousTarget': array([-18.60907547,  -3.83435917, 100.        ])}
episode index:18508
target thresh 85.07441444610977
target distance 71.03473122228864
model initialize at round 18508
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.925349  , -14.61670989,  59.17080033]), 'distance': 27.499999999999996, 'localFrame': array([[-0.25351397,  0.95445701,  0.15729745],
       [-0.96648856, -0.25670968,  0.        ],
       [ 0.04037978, -0.15202618,  0.98755127]]), 'currentState': array([  5.49204983, -36.92307174,  73.44169433,  -0.25351397,
         0.95445701,   0.15729745]), 'targetState': array([-17.60683743,  32.54226906,  29.        ]), 'previousTarget': array([ -1.24062331, -16.14514457,  59.84313231])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284833253158105
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 56, 'trapConfig': [], 'currentTarget': array([-17.60683743,  32.54226906,  29.        ]), 'distance': 17.65984024589286, 'localFrame': array([[ 0.65414613,  0.66525895, -0.35989357],
       [-0.71303717,  0.70112624,  0.        ],
       [ 0.25233082,  0.25661749,  0.93299337]]), 'currentState': array([-14.54026415,  28.43990828,  45.90079057,   0.65414613,
         0.66525895,  -0.35989357]), 'targetState': array([-17.60683743,  32.54226906,  29.        ]), 'previousTarget': array([-17.60683743,  32.54226906,  29.        ])}
episode index:18509
target thresh 85.07590693003972
target distance 28.0
model initialize at round 18509
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 18.12487958, -14.3393122 ,  42.77895194]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.32121124,  0.9059468 , -0.27583279],
       [-0.94251092,  0.33417536,  0.        ],
       [ 0.09217652,  0.25997542,  0.96120563]]), 'currentState': array([ 21.42470551, -16.43112755,  70.        ,   0.32121124,
         0.9059468 ,  -0.27583279]), 'targetState': array([ 18.03045244, -14.27945324,  42.        ]), 'previousTarget': array([ 18.12487958, -14.3393122 ,  42.77895194])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6284949114783209
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 18.03045244, -14.27945324,  42.        ]), 'distance': 3.0471870567012904, 'localFrame': array([[-0.22290405, -0.71408761, -0.66362088],
       [ 0.95457444, -0.29797255,  0.        ],
       [-0.19774081, -0.63347553,  0.74806906]]), 'currentState': array([ 18.03392072, -13.06421387,  44.79437474,  -0.22290405,
        -0.71408761,  -0.66362088]), 'targetState': array([ 18.03045244, -14.27945324,  42.        ]), 'previousTarget': array([ 18.03045244, -14.27945324,  42.        ])}
episode index:18510
target thresh 85.07739926472874
target distance 52.0
model initialize at round 18510
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.17415506,   8.4051018 ,  68.40640751]), 'distance': 27.499999999999996, 'localFrame': array([[-0.58540799, -0.75492397, -0.2956134 ],
       [ 0.79024173, -0.61279524,  0.        ],
       [-0.18115048, -0.23360604,  0.95530766]]), 'currentState': array([-16.45071483,  16.51990377,  94.25382713,  -0.58540799,
        -0.75492397,  -0.2956134 ]), 'targetState': array([-25.99974653,   0.11480547,  42.        ]), 'previousTarget': array([-20.75183197,   9.29995131,  68.46450673])}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.6284848932767191
{'scaleFactor': 20, 'timeStep': 82, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([-25.99974653,   0.11480547,  42.        ]), 'distance': 2.9975206763813675, 'localFrame': array([[-0.46421078,  0.14827475, -0.8732256 ],
       [-0.30426806, -0.95258645,  0.        ],
       [-0.83182288,  0.26569466,  0.48731617]]), 'currentState': array([-27.06199261,   0.47969938,  44.77913942,  -0.46421078,
         0.14827475,  -0.8732256 ]), 'targetState': array([-25.99974653,   0.11480547,  42.        ]), 'previousTarget': array([-25.99974653,   0.11480547,  42.        ])}
episode index:18511
target thresh 85.07889145019175
target distance 48.0
model initialize at round 18511
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.14353004, -26.60491926,  74.26897263]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.13622326, -0.82679927, -0.54575287],
       [ 0.98669734,  0.16256803,  0.        ],
       [ 0.08872197, -0.5384929 ,  0.83794619]]), 'currentState': array([ 17.73251714, -11.48624666,  96.95806495,   0.13622326,
        -0.82679927,  -0.54575287]), 'targetState': array([ 10.30463563, -42.77633089,  50.        ]), 'previousTarget': array([ 14.6043416 , -25.43664029,  75.48240092])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284509431420346
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 72, 'trapConfig': [], 'currentTarget': array([ 10.30463563, -42.77633089,  50.        ]), 'distance': 13.788019817210417, 'localFrame': array([[ 0.91324158, -0.34481693, -0.21700486],
       [ 0.35323432,  0.93553488,  0.        ],
       [ 0.20301561, -0.07665356,  0.97617052]]), 'currentState': array([  6.76894636, -40.80958705,  63.18105878,   0.91324158,
        -0.34481693,  -0.21700486]), 'targetState': array([ 10.30463563, -42.77633089,  50.        ]), 'previousTarget': array([ 10.30463563, -42.77633089,  50.        ])}
episode index:18512
target thresh 85.08038348644367
target distance 68.0
model initialize at round 18512
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 25.84083323, -10.36186283,  72.00341756]), 'distance': 27.499999999999996, 'localFrame': array([[-0.02798611, -0.7508694 , -0.6598575 ],
       [ 0.99930614, -0.03724575,  0.        ],
       [-0.02457688, -0.65939965,  0.75139076]]), 'currentState': array([ 3.28164861e+01, -8.15434207e-01,  9.68319489e+01, -2.79861090e-02,
       -7.50869400e-01, -6.59857501e-01]), 'targetState': array([ 14.03984279, -26.51193721,  30.        ]), 'previousTarget': array([25.43803884, -9.47373343, 73.15952832])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284169966750578
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 57, 'trapConfig': [], 'currentTarget': array([ 14.03984279, -26.51193721,  30.        ]), 'distance': 9.903005516024283, 'localFrame': array([[ 0.37002978, -0.68796273, -0.62432783],
       [ 0.880691  ,  0.47369121,  0.        ],
       [ 0.2957386 , -0.54983991,  0.78116244]]), 'currentState': array([ 18.90149876, -19.81274639,  35.43641995,   0.37002978,
        -0.68796273,  -0.62432783]), 'targetState': array([ 14.03984279, -26.51193721,  30.        ]), 'previousTarget': array([ 14.03984279, -26.51193721,  30.        ])}
episode index:18513
target thresh 85.08187537349943
target distance 46.0
model initialize at round 18513
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.61125062,  0.4900359 , 26.37713559]), 'distance': 27.5, 'localFrame': array([[-0.23474367, -0.66219743,  0.71161083],
       [ 0.9425307 , -0.33411956,  0.        ],
       [ 0.2377631 ,  0.67071505,  0.70257386]]), 'currentState': array([41.33030093,  1.85894941,  7.25816253, -0.23474367, -0.66219743,
        0.71161083]), 'targetState': array([-4.81582528, -1.34455454, 52.        ]), 'previousTarget': array([21.54644903,  1.22194212, 25.46358359])}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.6284055839000365
{'scaleFactor': 20, 'timeStep': 88, 'trapCount': 44, 'trapConfig': [], 'currentTarget': array([-4.81582528, -1.34455454, 52.        ]), 'distance': 2.6775065372922064, 'localFrame': array([[-0.5348611 , -0.54829311,  0.64288278],
       [ 0.71582046, -0.69828438,  0.        ],
       [ 0.448915  ,  0.46018865,  0.76596457]]), 'currentState': array([-3.62563187, -0.19623936, 49.89432834, -0.5348611 , -0.54829311,
        0.64288278]), 'targetState': array([-4.81582528, -1.34455454, 52.        ]), 'previousTarget': array([-4.81582528, -1.34455454, 52.        ])}
episode index:18514
target thresh 85.08336711137395
target distance 26.0
model initialize at round 18514
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.2773097 , -4.70883972,  6.80803662]), 'distance': 27.5, 'localFrame': array([[ 0.88694115, -0.39797573, -0.2344157 ],
       [ 0.40938258,  0.91236281,  0.        ],
       [ 0.21387217, -0.0959657 ,  0.97213645]]), 'currentState': array([  6.02249918, -15.28930837,  31.39697907,   0.88694115,
        -0.39797573,  -0.2344157 ]), 'targetState': array([-0.74053764, -3.93085283,  5.        ]), 'previousTarget': array([-0.55801582, -4.28892361,  5.89822371])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6284176310070955
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.74053764, -3.93085283,  5.        ]), 'distance': 4.015492219749505, 'localFrame': array([[ 0.32615617,  0.82599866, -0.4597264 ],
       [-0.93011519,  0.36726792,  0.        ],
       [ 0.16884276,  0.42759851,  0.88806061]]), 'currentState': array([ 1.88772932, -5.05689947,  7.81929238,  0.32615617,  0.82599866,
       -0.4597264 ]), 'targetState': array([-0.74053764, -3.93085283,  5.        ]), 'previousTarget': array([-0.74053764, -3.93085283,  5.        ])}
episode index:18515
target thresh 85.08485870008214
target distance 23.0
model initialize at round 18515
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.78520373, 27.86113135, 53.        ]), 'distance': 24.49762349549072, 'localFrame': array([[ 0.23090436, -0.1837152 , -0.9554747 ],
       [ 0.62260923,  0.7825329 ,  0.        ],
       [ 0.74769039, -0.59488737,  0.29507304]]), 'currentState': array([ 8.46746726, 32.11651765, 74.34016495,  0.23090436, -0.1837152 ,
       -0.9554747 ]), 'targetState': array([-2.78520373, 27.86113135, 53.        ]), 'previousTarget': array([-2.78520373, 27.86113135, 53.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6284310844199272
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.78520373, 27.86113135, 53.        ]), 'distance': 2.27382448471681, 'localFrame': array([[-0.32863081,  0.53258403, -0.77997182],
       [-0.85102521, -0.52512484,  0.        ],
       [-0.40958258,  0.66377568,  0.62581464]]), 'currentState': array([-1.95665057, 29.13866214, 54.68869551, -0.32863081,  0.53258403,
       -0.77997182]), 'targetState': array([-2.78520373, 27.86113135, 53.        ]), 'previousTarget': array([-2.78520373, 27.86113135, 53.        ])}
episode index:18516
target thresh 85.0863501396389
target distance 32.78196239274076
model initialize at round 18516
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.42508916,   7.88293034,  14.79623811]), 'distance': 27.5, 'localFrame': array([[-0.25655908,  0.5884602 ,  0.76674118],
       [-0.91666698, -0.39965191,  0.        ],
       [ 0.30642958, -0.70284632,  0.64195636]]), 'currentState': array([-24.79805262, -17.58262699,   5.91442221,  -0.25655908,
         0.5884602 ,   0.76674118]), 'targetState': array([-18.091946  ,  14.20146084,  17.        ]), 'previousTarget': array([-19.30282023,   6.9333464 ,  14.33947065])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6284408752491005
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.091946  ,  14.20146084,  17.        ]), 'distance': 3.4814551451120717, 'localFrame': array([[-0.15755601,  0.81419956,  0.55879798],
       [-0.98178682, -0.18998588,  0.        ],
       [ 0.10616372, -0.5486205 ,  0.82930381]]), 'currentState': array([-19.10858017,  12.03044591,  14.47538536,  -0.15755601,
         0.81419956,   0.55879798]), 'targetState': array([-18.091946  ,  14.20146084,  17.        ]), 'previousTarget': array([-18.091946  ,  14.20146084,  17.        ])}
episode index:18517
target thresh 85.08784143005917
target distance 42.0
model initialize at round 18517
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.68708652, -14.52741419,  20.71864929]), 'distance': 27.5, 'localFrame': array([[-0.70703905, -0.00406509,  0.70716282],
       [ 0.00574937, -0.99998347,  0.        ],
       [ 0.70715113,  0.00406574,  0.70705074]]), 'currentState': array([ 1.74041611e+01, -2.45416461e+01,  1.31492341e+00, -7.07039054e-01,
       -4.06509316e-03,  7.07162818e-01]), 'targetState': array([-17.64763581,  -3.5441431 ,  42.        ]), 'previousTarget': array([  1.55021541, -14.29827765,  19.61240722])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6284326075726104
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([-17.64763581,  -3.5441431 ,  42.        ]), 'distance': 1.6880372521949232, 'localFrame': array([[ 0.54723021,  0.19082326,  0.81493901],
       [-0.32926294,  0.94423827,  0.        ],
       [-0.7694966 , -0.26832921,  0.57954673]]), 'currentState': array([-16.87161548,  -3.95097768,  40.55717366,   0.54723021,
         0.19082326,   0.81493901]), 'targetState': array([-17.64763581,  -3.5441431 ,  42.        ]), 'previousTarget': array([-17.64763581,  -3.5441431 ,  42.        ])}
episode index:18518
target thresh 85.08933257135786
target distance 42.0
model initialize at round 18518
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.93608793, -11.22578672,  50.67732005]), 'distance': 27.499999999999996, 'localFrame': array([[-0.52422097,  0.72487484,  0.4469327 ],
       [-0.81030752, -0.58600489,  0.        ],
       [ 0.26190475, -0.36215293,  0.89456758]]), 'currentState': array([ -3.78318483, -28.08626113,  29.13971475,  -0.52422097,
         0.72487484,   0.4469327 ]), 'targetState': array([ 1.75040567,  4.68359691, 71.        ]), 'previousTarget': array([ -0.72261895, -12.44015552,  50.15903391])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.628438216447932
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.75040567,  4.68359691, 71.        ]), 'distance': 2.693218227485211, 'localFrame': array([[ 0.29146788,  0.92369582, -0.24866144],
       [-0.9536495 ,  0.30091963,  0.        ],
       [ 0.07482711,  0.23713586,  0.96859046]]), 'currentState': array([ 3.20798846,  3.04436127, 69.43737932,  0.29146788,  0.92369582,
       -0.24866144]), 'targetState': array([ 1.75040567,  4.68359691, 71.        ]), 'previousTarget': array([ 1.75040567,  4.68359691, 71.        ])}
episode index:18519
target thresh 85.09082356354986
target distance 24.4942090446589
model initialize at round 18519
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.46066397,  3.37812676, 11.86002646]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.49380784, -0.4223376 , -0.76012155],
       [ 0.64996923,  0.75996052,  0.        ],
       [ 0.57766237, -0.49405562,  0.6497809 ]]), 'currentState': array([ 14.67659024, -21.9983143 ,   5.93328303,   0.49380784,
        -0.4223376 ,  -0.76012155]), 'targetState': array([23.66811987,  3.97744917, 12.        ]), 'previousTarget': array([23.66811987,  3.97744917, 12.        ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6284484469404346
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.66811987,  3.97744917, 12.        ]), 'distance': 2.345841192257369, 'localFrame': array([[ 3.02650313e-01,  7.08456366e-04,  9.53101404e-01],
       [-2.34083493e-03,  9.99997260e-01,  0.00000000e+00],
       [-9.53098793e-01, -2.23105306e-03,  3.02651142e-01]]), 'currentState': array([2.52418930e+01, 4.35214966e+00, 1.03012332e+01, 3.02650313e-01,
       7.08456366e-04, 9.53101404e-01]), 'targetState': array([23.66811987,  3.97744917, 12.        ]), 'previousTarget': array([23.66811987,  3.97744917, 12.        ])}
episode index:18520
target thresh 85.09231440665013
target distance 55.18543990908806
model initialize at round 18520
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-10.85858448,  -7.46173384,   9.8964234 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.72486553, -0.68688944,  0.0524677 ],
       [ 0.68783685,  0.72586533,  0.        ],
       [-0.03808449,  0.03608922,  0.99862262]]), 'currentState': array([13.96589184, -0.97666025,  0.        ,  0.72486553, -0.68688944,
        0.0524677 ]), 'targetState': array([-41.21954807, -15.39314318,  22.        ]), 'previousTarget': array([-10.85858448,  -7.46173384,   9.8964234 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6284145152711434
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 56, 'trapConfig': [], 'currentTarget': array([-41.21954807, -15.39314318,  22.        ]), 'distance': 18.675385610604597, 'localFrame': array([[-0.87004412,  0.3349231 , -0.36173159],
       [-0.35925073, -0.93324108,  0.        ],
       [-0.33758278,  0.12995234,  0.93228228]]), 'currentState': array([-43.10277212, -11.75066546,   3.78034438,  -0.87004412,
         0.3349231 ,  -0.36173159]), 'targetState': array([-41.21954807, -15.39314318,  22.        ]), 'previousTarget': array([-41.21954807, -15.39314318,  22.        ])}
episode index:18521
target thresh 85.09380510067352
target distance 50.0
model initialize at round 18521
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.54817903, -14.33507407,  27.43950643]), 'distance': 27.5, 'localFrame': array([[-0.96640259,  0.05929364, -0.25010059],
       [-0.06123986, -0.99812308,  0.        ],
       [-0.24963117,  0.01531612,  0.96821986]]), 'currentState': array([ 16.12674763, -21.90369177,   7.77830114,  -0.96640259,
         0.05929364,  -0.25010059]), 'targetState': array([-29.92026512,  -2.18580313,  59.        ]), 'previousTarget': array([ -0.70887678, -14.31602764,  28.18883188])}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6284104265637626
{'scaleFactor': 20, 'timeStep': 60, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.92026512,  -2.18580313,  59.        ]), 'distance': 3.550137160909331, 'localFrame': array([[-0.92687816, -0.21842554,  0.30526572],
       [ 0.22937421, -0.97333831,  0.        ],
       [ 0.29712683,  0.07002008,  0.95226721]]), 'currentState': array([-30.73071041,  -0.38252832,  56.05129651,  -0.92687816,
        -0.21842554,   0.30526572]), 'targetState': array([-29.92026512,  -2.18580313,  59.        ]), 'previousTarget': array([-29.92026512,  -2.18580313,  59.        ])}
episode index:18522
target thresh 85.09529564563496
target distance 75.0
model initialize at round 18522
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.2030099 ,  5.98624987, 66.27744995]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.79281685,  0.32726899, -0.5141366 ],
       [-0.38156211,  0.9243432 ,  0.        ],
       [ 0.47523868,  0.19617505,  0.85770831]]), 'currentState': array([-19.62249521,  -5.28479165,  87.47016184,   0.79281685,
         0.32726899,  -0.5141366 ]), 'targetState': array([27.53292539, 34.32110166, 13.        ]), 'previousTarget': array([-7.64624214,  5.97253479, 66.92118669])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6283765006108087
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 44, 'trapConfig': [], 'currentTarget': array([27.53292539, 34.32110166, 13.        ]), 'distance': 15.610105530873499, 'localFrame': array([[-0.50630025,  0.11829713, -0.8542048 ],
       [-0.22752218, -0.9737729 ,  0.        ],
       [-0.83180148,  0.19435054,  0.51993669]]), 'currentState': array([28.33583115, 28.53127917, 27.4744151 , -0.50630025,  0.11829713,
       -0.8542048 ]), 'targetState': array([27.53292539, 34.32110166, 13.        ]), 'previousTarget': array([27.53292539, 34.32110166, 13.        ])}
episode index:18523
target thresh 85.09678604154935
target distance 69.0
model initialize at round 18523
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.16402754, 21.0828574 , 56.90025083]), 'distance': 27.5, 'localFrame': array([[-0.79812536, -0.11774161, -0.59087462],
       [ 0.14594317, -0.98929297,  0.        ],
       [-0.58454811, -0.08623412,  0.8067634 ]]), 'currentState': array([30.95063158, 18.88256446, 83.45869048, -0.79812536, -0.11774161,
       -0.59087462]), 'targetState': array([13.45705772, 24.55417678, 15.        ]), 'previousTarget': array([25.16090987, 21.61761168, 57.54357401])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6283779734417289
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.45705772, 24.55417678, 15.        ]), 'distance': 2.783351322954994, 'localFrame': array([[-0.85945543,  0.06825221, -0.50663399],
       [-0.07916409, -0.9968616 ,  0.        ],
       [-0.50504397,  0.04010722,  0.86216124]]), 'currentState': array([13.28882752, 27.05281604, 16.21472014, -0.85945543,  0.06825221,
       -0.50663399]), 'targetState': array([13.45705772, 24.55417678, 15.        ]), 'previousTarget': array([13.45705772, 24.55417678, 15.        ])}
episode index:18524
target thresh 85.0982762884316
target distance 65.0
model initialize at round 18524
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.20791224,  7.400816  , 51.98066677]), 'distance': 27.5, 'localFrame': array([[-0.35180318, -0.16604086,  0.92123013],
       [ 0.42682045, -0.90433639,  0.        ],
       [ 0.83310193,  0.39319986,  0.38901805]]), 'currentState': array([37.44544026, 21.77014859, 30.42978843, -0.35180318, -0.16604086,
        0.92123013]), 'targetState': array([ 10.19682417, -20.61612905,  94.        ]), 'previousTarget': array([29.05556438,  7.29208035, 50.70505022])}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6283738873690552
{'scaleFactor': 20, 'timeStep': 60, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.19682417, -20.61612905,  94.        ]), 'distance': 2.0084336850276996, 'localFrame': array([[ 0.59143901, -0.56017687,  0.58000152],
       [ 0.6876581 ,  0.72603467,  0.        ],
       [-0.42110122,  0.39884274,  0.81461539]]), 'currentState': array([  9.86072076, -20.72979629,  92.023154  ,   0.59143901,
        -0.56017687,   0.58000152]), 'targetState': array([ 10.19682417, -20.61612905,  94.        ]), 'previousTarget': array([ 10.19682417, -20.61612905,  94.        ])}
episode index:18525
target thresh 85.09976638629664
target distance 44.25414527917798
model initialize at round 18525
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.5978106,   4.404029 ,  42.5040775]), 'distance': 27.5, 'localFrame': array([[-0.11750929,  0.86182081, -0.49341307],
       [-0.99083197, -0.13509996,  0.        ],
       [-0.06666009,  0.48888944,  0.86979512]]), 'currentState': array([-35.91660306, -19.58214873,  54.85817498,  -0.11750929,
         0.86182081,  -0.49341307]), 'targetState': array([-26.5060333 ,  22.85673203,  33.        ]), 'previousTarget': array([-30.87861189,   2.7725345 ,  42.98442841])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6283794972946886
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-26.5060333 ,  22.85673203,  33.        ]), 'distance': 2.5323715655810046, 'localFrame': array([[ 0.83778968,  0.42785101, -0.3391931 ],
       [-0.45481384,  0.89058653,  0.        ],
       [ 0.30208081,  0.15426972,  0.94071677]]), 'currentState': array([-27.4097009 ,  20.49115914,  32.98114439,   0.83778968,
         0.42785101,  -0.3391931 ]), 'targetState': array([-26.5060333 ,  22.85673203,  33.        ]), 'previousTarget': array([-26.5060333 ,  22.85673203,  33.        ])}
episode index:18526
target thresh 85.10125633515932
target distance 71.0
model initialize at round 18526
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.65861087, 19.31595196, 23.44626394]), 'distance': 27.5, 'localFrame': array([[-0.85860637, -0.49815742,  0.1209723 ],
       [ 0.50184301, -0.86495872,  0.        ],
       [ 0.10463604,  0.0607091 ,  0.99265588]]), 'currentState': array([35.32151831, 27.14281911,  1.53760592, -0.85860637, -0.49815742,
        0.1209723 ]), 'targetState': array([-11.83716945,   1.9701318 ,  72.        ]), 'previousTarget': array([22.11963373, 19.47644448, 22.73190772])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6283697355262677
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-11.83716945,   1.9701318 ,  72.        ]), 'distance': 3.875234401767589, 'localFrame': array([[-0.82850451, -0.55703507,  0.05737778],
       [ 0.55795428, -0.82987169,  0.        ],
       [ 0.0476162 ,  0.03201418,  0.99835254]]), 'currentState': array([-1.22857270e+01,  4.40462123e+00,  6.90184737e+01, -8.28504506e-01,
       -5.57035074e-01,  5.73777809e-02]), 'targetState': array([-11.83716945,   1.9701318 ,  72.        ]), 'previousTarget': array([-11.83716945,   1.9701318 ,  72.        ])}
episode index:18527
target thresh 85.10274613503456
target distance 50.0
model initialize at round 18527
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.17424905, 23.85653605, 32.11496531]), 'distance': 27.5, 'localFrame': array([[ 0.87018383,  0.35681982, -0.33979365],
       [-0.37939375,  0.92523531,  0.        ],
       [ 0.31438908,  0.12891558,  0.94050001]]), 'currentState': array([-0.53595314, 10.54042039, 53.66079166,  0.87018383,  0.35681982,
       -0.33979365]), 'targetState': array([23.65281125, 40.61458507,  5.        ]), 'previousTarget': array([ 9.2748993 , 23.10440774, 33.47945984])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6283712084043468
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([23.65281125, 40.61458507,  5.        ]), 'distance': 3.454776180884569, 'localFrame': array([[-0.41650309,  0.773481  , -0.47775759],
       [-0.88046477, -0.47411158,  0.        ],
       [-0.2265104 ,  0.42064872,  0.87849171]]), 'currentState': array([22.72253863, 38.17300342,  7.2602545 , -0.41650309,  0.773481  ,
       -0.47775759]), 'targetState': array([23.65281125, 40.61458507,  5.        ]), 'previousTarget': array([23.65281125, 40.61458507,  5.        ])}
episode index:18528
target thresh 85.10423578593728
target distance 64.0
model initialize at round 18528
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.89780429,  7.82421857, 48.77492763]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.77776572,  0.4131726 ,  0.47367592],
       [-0.46914156,  0.88312298,  0.        ],
       [-0.41831409, -0.22222106,  0.88069922]]), 'currentState': array([10.11129134, 13.41396723, 22.18073209,  0.77776572,  0.4131726 ,
        0.47367592]), 'targetState': array([-0.,  0., 86.]), 'previousTarget': array([ 5.13544411,  7.0532459 , 48.77444854])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6283730385534417
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 86.]), 'distance': 2.4483733805017835, 'localFrame': array([[-0.70837924, -0.65188832,  0.27062976],
       [ 0.67715746, -0.73583814,  0.        ],
       [ 0.1991397 ,  0.18325896,  0.96268351]]), 'currentState': array([-1.55528869,  1.65855406, 85.09181074, -0.70837924, -0.65188832,
        0.27062976]), 'targetState': array([-0.,  0., 86.]), 'previousTarget': array([ 0.,  0., 86.])}
episode index:18529
target thresh 85.10572528788234
target distance 79.0
model initialize at round 18529
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.08256607, -1.29641587, 34.63944191]), 'distance': 27.5, 'localFrame': array([[-0.80648611, -0.23967865,  0.54049449],
       [ 0.28487473, -0.95856476,  0.        ],
       [ 0.51809897,  0.15397322,  0.84134755]]), 'currentState': array([18.17248416, -1.7497135 ,  9.4787933 , -0.80648611, -0.23967865,
        0.54049449]), 'targetState': array([-15.99610371,  -0.35308101,  87.        ]), 'previousTarget': array([ 7.84136381, -0.81873307, 33.1459296 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6283391274342537
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.99610371,  -0.35308101,  87.        ]), 'distance': 6.708433948639243, 'localFrame': array([[-0.16549267,  0.38559512,  0.90770512],
       [-0.91893987, -0.39439767,  0.        ],
       [ 0.35799678, -0.83412642,  0.41960865]]), 'currentState': array([-17.30061008,  -1.18767201,  80.47276421,  -0.16549267,
         0.38559512,   0.90770512]), 'targetState': array([-15.99610371,  -0.35308101,  87.        ]), 'previousTarget': array([-15.99610371,  -0.35308101,  87.        ])}
episode index:18530
target thresh 85.10721464088466
target distance 32.23854167420656
model initialize at round 18530
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.05955524, -9.55175734, 30.14380067]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.43089889,  0.61169319, -0.66344374],
       [-0.81752444,  0.5758939 ,  0.        ],
       [ 0.3820732 ,  0.54238147,  0.74822618]]), 'currentState': array([ -2.61567692, -33.48112961,  42.55344739,   0.43089889,
         0.61169319,  -0.66344374]), 'targetState': array([-9.87736255, -1.56131642, 26.        ]), 'previousTarget': array([ -8.23310953, -10.1197944 ,  30.77852271])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6283480462813861
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.87736255, -1.56131642, 26.        ]), 'distance': 2.273945460671925, 'localFrame': array([[ 0.68276477,  0.46585243,  0.56286213],
       [-0.56361016,  0.82604091,  0.        ],
       [-0.46494715, -0.31723482,  0.8265508 ]]), 'currentState': array([-9.72207671, -3.59743316, 24.99952868,  0.68276477,  0.46585243,
        0.56286213]), 'targetState': array([-9.87736255, -1.56131642, 26.        ]), 'previousTarget': array([-9.87736255, -1.56131642, 26.        ])}
episode index:18531
target thresh 85.10870384495914
target distance 71.0
model initialize at round 18531
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.58455226,   2.53781993,  64.58361374]), 'distance': 27.5, 'localFrame': array([[ 0.80496115, -0.13665175, -0.57737669],
       [ 0.16736736,  0.9858946 ,  0.        ],
       [ 0.56923256, -0.09663401,  0.8164779 ]]), 'currentState': array([-31.69594883,   4.26763969,  89.67932234,   0.80496115,
        -0.13665175,  -0.57737669]), 'targetState': array([-0.8446748 , -0.53527982, 20.        ]), 'previousTarget': array([-21.39592682,   3.16516708,  65.96278224])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6283382888438446
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([-0.8446748 , -0.53527982, 20.        ]), 'distance': 2.8628704651992636, 'localFrame': array([[-0.08335099,  0.70256776, -0.70671858],
       [-0.99303598, -0.11781146,  0.        ],
       [-0.08325955,  0.70179698,  0.70749477]]), 'currentState': array([ 1.93804232, -0.85248837, 20.59320441, -0.08335099,  0.70256776,
       -0.70671858]), 'targetState': array([-0.8446748 , -0.53527982, 20.        ]), 'previousTarget': array([-0.8446748 , -0.53527982, 20.        ])}
episode index:18532
target thresh 85.11019290012064
target distance 16.310841533447064
model initialize at round 18532
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.17522516, -27.31055829,  10.        ]), 'distance': 17.305771935292867, 'localFrame': array([[-0.28153094, -0.79205215,  0.54165831],
       [ 0.94224758, -0.33491715,  0.        ],
       [ 0.18141066,  0.51037623,  0.84059876]]), 'currentState': array([  0.16932398, -12.14867122,  15.41789802,  -0.28153094,
        -0.79205215,   0.54165831]), 'targetState': array([ -6.17522516, -27.31055829,  10.        ]), 'previousTarget': array([ -6.17522516, -27.31055829,  10.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6283541743672667
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.17522516, -27.31055829,  10.        ]), 'distance': 3.776300269805746, 'localFrame': array([[-0.18582231, -0.59499434, -0.78195383],
       [ 0.95453177, -0.29810922,  0.        ],
       [-0.23310765, -0.74639977,  0.62333635]]), 'currentState': array([ -4.5917804 , -24.41504777,  11.8355286 ,  -0.18582231,
        -0.59499434,  -0.78195383]), 'targetState': array([ -6.17522516, -27.31055829,  10.        ]), 'previousTarget': array([ -6.17522516, -27.31055829,  10.        ])}
episode index:18533
target thresh 85.11168180638407
target distance 36.461021567192034
model initialize at round 18533
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.8492314 , -25.05489929,  93.65206277]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69920989, -0.13926571,  0.70122078],
       [ 0.19533887,  0.98073581,  0.        ],
       [-0.68771233,  0.13697568,  0.71294419]]), 'currentState': array([-16.36262382,   1.38419725,  92.77279691,   0.69920989,
        -0.13926571,   0.70122078]), 'targetState': array([ -5.87608184, -35.51720234,  94.        ]), 'previousTarget': array([ -9.32582001, -25.12135589,  93.42975561])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6283639603656402
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.87608184, -35.51720234,  94.        ]), 'distance': 3.5253531610788302, 'localFrame': array([[ 0.60184457, -0.60918881, -0.51640305],
       [ 0.71138191,  0.70280565,  0.        ],
       [ 0.36293098, -0.36735979,  0.85634566]]), 'currentState': array([ -8.41413707, -33.09645181,  94.35546786,   0.60184457,
        -0.60918881,  -0.51640305]), 'targetState': array([ -5.87608184, -35.51720234,  94.        ]), 'previousTarget': array([ -5.87608184, -35.51720234,  94.        ])}
episode index:18534
target thresh 85.11317056376433
target distance 66.0
model initialize at round 18534
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([38.35910638, -9.16037099, 25.31052948]), 'distance': 27.500000000000004, 'localFrame': array([[-0.54071747, -0.82167905,  0.18018925],
       [ 0.83535212, -0.54971523,  0.        ],
       [ 0.09905277,  0.15052147,  0.98363196]]), 'currentState': array([43.270592  ,  5.38875223,  2.49714914, -0.54071747, -0.82167905,
        0.18018925]), 'targetState': array([ 28.95321375, -37.02311999,  69.        ]), 'previousTarget': array([39.20349856, -8.24538967, 25.50414254])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6283636993154392
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.95321375, -37.02311999,  69.        ]), 'distance': 1.9362063850976734, 'localFrame': array([[ 0.08789712, -0.28650262,  0.95403897],
       [ 0.95602023,  0.29330073,  0.        ],
       [-0.27982033,  0.91208055,  0.29968259]]), 'currentState': array([ 27.67013233, -36.78830195,  67.56910597,   0.08789712,
        -0.28650262,   0.95403897]), 'targetState': array([ 28.95321375, -37.02311999,  69.        ]), 'previousTarget': array([ 28.95321375, -37.02311999,  69.        ])}
episode index:18535
target thresh 85.11465917227628
target distance 35.0
model initialize at round 18535
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.12008596, -2.59625636, 76.20024887]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.93024677,  0.08079817,  0.35792821],
       [-0.08653092,  0.99624917,  0.        ],
       [-0.35658568, -0.03097186,  0.93374911]]), 'currentState': array([-12.54805882,  -7.10581138,  50.76351871,   0.93024677,
         0.08079817,   0.35792821]), 'targetState': array([ 0.51213356, -0.85890583, 86.        ]), 'previousTarget': array([-3.75286515, -2.61880509, 75.9793734 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628329799676935
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 85, 'trapConfig': [], 'currentTarget': array([ 0.51213356, -0.85890583, 86.        ]), 'distance': 15.273140624987123, 'localFrame': array([[ 0.9244924 ,  0.24172746,  0.29475692],
       [-0.25296617,  0.96747512,  0.        ],
       [-0.28516999, -0.07456353,  0.95557227]]), 'currentState': array([-5.9893685 , -3.40213056, 72.41577004,  0.9244924 ,  0.24172746,
        0.29475692]), 'targetState': array([ 0.51213356, -0.85890583, 86.        ]), 'previousTarget': array([ 0.51213356, -0.85890583, 86.        ])}
episode index:18536
target thresh 85.11614763193482
target distance 40.0
model initialize at round 18536
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.82507045,  5.48502023, 48.39727996]), 'distance': 27.500000000000004, 'localFrame': array([[-0.35873665, -0.62010474, -0.69769487],
       [ 0.86559048, -0.50075255,  0.        ],
       [-0.34937249, -0.60391804,  0.71639505]]), 'currentState': array([-3.20062767, 10.03034859, 73.97318197, -0.35873665, -0.62010474,
       -0.69769487]), 'targetState': array([10.5529508 ,  3.10406661, 35.        ]), 'previousTarget': array([ 5.525754  ,  6.16425427, 49.51517658])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6283374445680247
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.5529508 ,  3.10406661, 35.        ]), 'distance': 2.811168233637775, 'localFrame': array([[ 0.36803939, -0.16213316, -0.91556531],
       [ 0.40314653,  0.91513544,  0.        ],
       [ 0.83786627, -0.36910698,  0.40216932]]), 'currentState': array([10.79704093,  4.60977617, 37.36133978,  0.36803939, -0.16213316,
       -0.91556531]), 'targetState': array([10.5529508 ,  3.10406661, 35.        ]), 'previousTarget': array([10.5529508 ,  3.10406661, 35.        ])}
episode index:18537
target thresh 85.11763594275486
target distance 34.0
model initialize at round 18537
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.39096765, -16.36628736,  61.60709259]), 'distance': 27.499999999999996, 'localFrame': array([[-0.40669859, -0.78126545, -0.47351934],
       [ 0.88701198, -0.46174641,  0.        ],
       [-0.21864586, -0.42001732,  0.88078342]]), 'currentState': array([ 0.5107914 , -8.31257057, 86.97938787, -0.40669859, -0.78126545,
       -0.47351934]), 'targetState': array([ -8.73226467, -19.09836521,  53.        ]), 'previousTarget': array([ -5.94776785, -15.79504177,  62.13840293])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6283430528280897
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([ -8.73226467, -19.09836521,  53.        ]), 'distance': 1.996621203195282, 'localFrame': array([[-0.54562524,  0.15814029, -0.82297311],
       [-0.27837663, -0.96047199,  0.        ],
       [-0.79044263,  0.22909648,  0.56808033]]), 'currentState': array([ -8.42095507, -18.08513794,  54.69202632,  -0.54562524,
         0.15814029,  -0.82297311]), 'targetState': array([ -8.73226467, -19.09836521,  53.        ]), 'previousTarget': array([ -8.73226467, -19.09836521,  53.        ])}
episode index:18538
target thresh 85.11912410475124
target distance 62.0
model initialize at round 18538
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-12.22019449,  11.8300124 ,  46.79259341]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.74325927,  0.64437992, -0.17983373],
       [-0.65505934,  0.75557744,  0.        ],
       [ 0.13587831,  0.11780177,  0.98369702]]), 'currentState': array([-19.99341965,  19.60263252,  72.        ,   0.74325927,
         0.64437992,  -0.17983373]), 'targetState': array([-0.87443738,  0.4851384 , 10.        ]), 'previousTarget': array([-12.22019449,  11.8300124 ,  46.79259341])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6283091597889382
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 66, 'trapConfig': [], 'currentTarget': array([-0.87443738,  0.4851384 , 10.        ]), 'distance': 16.482985384552133, 'localFrame': array([[ 0.62100058, -0.40716399, -0.66975799],
       [ 0.54831035,  0.83627493,  0.        ],
       [ 0.56010182, -0.36723523,  0.74257945]]), 'currentState': array([-7.2763521 ,  5.24995807, 24.42223244,  0.62100058, -0.40716399,
       -0.66975799]), 'targetState': array([-0.87443738,  0.4851384 , 10.        ]), 'previousTarget': array([-0.87443738,  0.4851384 , 10.        ])}
episode index:18539
target thresh 85.12061211793886
target distance 59.0
model initialize at round 18539
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([42.38262949,  6.23440368, 69.68193416]), 'distance': 27.5, 'localFrame': array([[ 0.80000075,  0.02044865, -0.59965044],
       [-0.02555245,  0.99967348,  0.        ],
       [ 0.59945464,  0.01532254,  0.80026205]]), 'currentState': array([ 3.98013566e+01,  1.55478105e+01,  9.54277587e+01,  8.00000754e-01,
        2.04486531e-02, -5.99650437e-01]), 'targetState': array([45.65931581, -5.58810166, 37.        ]), 'previousTarget': array([41.48001911,  5.88920455, 70.1920117 ])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6283072534937179
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([45.65931581, -5.58810166, 37.        ]), 'distance': 2.35569923434545, 'localFrame': array([[ 0.32712085,  0.62847552, -0.70569857],
       [-0.88703557,  0.46170109,  0.        ],
       [ 0.32582179,  0.62597973,  0.7085122 ]]), 'currentState': array([45.81520924, -4.68451867, 39.16992025,  0.32712085,  0.62847552,
       -0.70569857]), 'targetState': array([45.65931581, -5.58810166, 37.        ]), 'previousTarget': array([45.65931581, -5.58810166, 37.        ])}
episode index:18540
target thresh 85.12209998233263
target distance 38.314779572465696
model initialize at round 18540
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.79675488, 21.34430208, 28.77636762]), 'distance': 27.500000000000004, 'localFrame': array([[-0.46655267, -0.71744976,  0.51729532],
       [ 0.8383313 , -0.54516111,  0.        ],
       [ 0.28200929,  0.43366486,  0.85580696]]), 'currentState': array([17.54769764, 31.72616628, 26.3013682 , -0.46655267, -0.71744976,
        0.51729532]), 'targetState': array([-20.32697707,  16.2115392 ,  30.        ]), 'previousTarget': array([-6.97237943, 22.22799813, 28.60580196])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6283157412175361
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.32697707,  16.2115392 ,  30.        ]), 'distance': 2.7734548147330393, 'localFrame': array([[-0.88108156, -0.32797076, -0.3407792 ],
       [ 0.34885186, -0.93717788,  0.        ],
       [-0.31937072, -0.11888146,  0.94014336]]), 'currentState': array([-17.90183626,  17.14521393,  29.03098237,  -0.88108156,
        -0.32797076,  -0.3407792 ]), 'targetState': array([-20.32697707,  16.2115392 ,  30.        ]), 'previousTarget': array([-20.32697707,  16.2115392 ,  30.        ])}
episode index:18541
target thresh 85.12358769794737
target distance 72.0
model initialize at round 18541
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.60464341, -16.89232231,  44.41491965]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.65197471, -0.41574386, -0.63410253],
       [ 0.53765847,  0.84316272,  0.        ],
       [ 0.53465162, -0.34093059,  0.77324897]]), 'currentState': array([ 10.06710278, -20.25361243,  71.66951452,   0.65197471,
        -0.41574386,  -0.63410253]), 'targetState': array([  6.22137497, -11.41466134,   0.        ]), 'previousTarget': array([  7.69929307, -16.21825188,  44.67239974])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6283088121531786
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([  6.22137497, -11.41466134,   0.        ]), 'distance': 2.798422640490366, 'localFrame': array([[-0.46340829, -0.66732677, -0.58303322],
       [ 0.82137751, -0.57038495,  0.        ],
       [-0.33255337, -0.47889037,  0.81244832]]), 'currentState': array([  6.32534495, -10.94025754,   2.75595728,  -0.46340829,
        -0.66732677,  -0.58303322]), 'targetState': array([  6.22137497, -11.41466134,   0.        ]), 'previousTarget': array([  6.22137497, -11.41466134,   0.        ])}
episode index:18542
target thresh 85.12507526479797
target distance 51.0
model initialize at round 18542
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.04633471, -13.80075549,  38.79027451]), 'distance': 27.5, 'localFrame': array([[-0.46508615,  0.59889169, -0.65193835],
       [-0.78981119, -0.61335005,  0.        ],
       [-0.39986642,  0.5149082 ,  0.75827197]]), 'currentState': array([ 21.94230163, -24.75364661,  61.5402653 ,  -0.46508615,
         0.59889169,  -0.65193835]), 'targetState': array([-1.78470104, -0.9026861 , 12.        ]), 'previousTarget': array([ 11.94139641, -14.26809973,  40.24215785])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6283110050532712
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.78470104, -0.9026861 , 12.        ]), 'distance': 3.3196980517827206, 'localFrame': array([[-0.89166616,  0.44622039,  0.07628123],
       [-0.44752433, -0.89427176,  0.        ],
       [ 0.06821615, -0.0341377 ,  0.99708634]]), 'currentState': array([ 0.87398256, -0.7787816 , 10.01595254, -0.89166616,  0.44622039,
        0.07628123]), 'targetState': array([-1.78470104, -0.9026861 , 12.        ]), 'previousTarget': array([-1.78470104, -0.9026861 , 12.        ])}
episode index:18543
target thresh 85.12656268289935
target distance 82.90912952902383
model initialize at round 18543
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.66534455,  16.55895728,  76.58163992]), 'distance': 27.5, 'localFrame': array([[ 0.89896835, -0.27562027, -0.34042528],
       [ 0.29312836,  0.9560731 ,  0.        ],
       [ 0.32547145, -0.0997883 ,  0.94027157]]), 'currentState': array([-35.5805273 ,  24.47950858,  91.1843668 ,   0.89896835,
        -0.27562027,  -0.34042528]), 'targetState': array([45.73718205, -4.91021162, 37.        ]), 'previousTarget': array([-15.13688135,  17.31666182,  76.64822019])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282771228808676
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 46, 'trapConfig': [], 'currentTarget': array([45.73718205, -4.91021162, 37.        ]), 'distance': 23.420999876120252, 'localFrame': array([[ 0.39767686, -0.62435121, -0.67233822],
       [ 0.84343964,  0.53722396,  0.        ],
       [ 0.3611962 , -0.56707671,  0.74024409]]), 'currentState': array([23.87797515,  3.04403456, 39.72915309,  0.39767686, -0.62435121,
       -0.67233822]), 'targetState': array([45.73718205, -4.91021162, 37.        ]), 'previousTarget': array([45.73718205, -4.91021162, 37.        ])}
episode index:18544
target thresh 85.12804995226637
target distance 25.175053403430336
model initialize at round 18544
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.75874119, 32.98703675, 99.        ]), 'distance': 27.116625105484914, 'localFrame': array([[ 0.8535632 , -0.27468635,  0.44269319],
       [ 0.30633944,  0.95192234,  0.        ],
       [-0.42140954,  0.13561439,  0.89667315]]), 'currentState': array([-7.27193036, 41.11813847, 89.42276829,  0.8535632 , -0.27468635,
        0.44269319]), 'targetState': array([16.75874119, 32.98703675, 99.        ]), 'previousTarget': array([15.73993139, 33.31730677, 98.55484076])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6282831311445778
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([16.75874119, 32.98703675, 99.        ]), 'distance': 2.650471649999367, 'localFrame': array([[ 0.49601085,  0.8279876 , -0.26155262],
       [-0.85785003,  0.51390012,  0.        ],
       [ 0.13441192,  0.22437292,  0.96518922]]), 'currentState': array([14.35728215, 33.95953643, 99.55878336,  0.49601085,  0.8279876 ,
       -0.26155262]), 'targetState': array([16.75874119, 32.98703675, 99.        ]), 'previousTarget': array([16.75874119, 32.98703675, 99.        ])}
episode index:18545
target thresh 85.12953707291386
target distance 68.5419540737033
model initialize at round 18545
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.25000843, 20.22276072, 81.22027961]), 'distance': 27.499999999999996, 'localFrame': array([[-0.37679734, -0.81529183, -0.43968512],
       [ 0.90774372, -0.41952514,  0.        ],
       [-0.18445896, -0.3991214 ,  0.89815199]]), 'currentState': array([-11.21458175,  44.90780239,  74.32056977,  -0.37679734,
        -0.81529183,  -0.43968512]), 'targetState': array([ 15.7622849 , -21.92145923,  93.        ]), 'previousTarget': array([-1.57478499, 21.78717529, 81.52154958])}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.628276477567399
{'scaleFactor': 20, 'timeStep': 69, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 15.7622849 , -21.92145923,  93.        ]), 'distance': 2.8102658225723958, 'localFrame': array([[ 0.98346294,  0.06632358, -0.16852844],
       [-0.06728599,  0.99773373,  0.        ],
       [ 0.16814651,  0.0113396 ,  0.98569679]]), 'currentState': array([ 1.34604819e+01, -2.04427912e+01,  9.36425245e+01,  9.83462937e-01,
        6.63235842e-02, -1.68528436e-01]), 'targetState': array([ 15.7622849 , -21.92145923,  93.        ]), 'previousTarget': array([ 15.7622849 , -21.92145923,  93.        ])}
episode index:18546
target thresh 85.13102404485673
target distance 41.0
model initialize at round 18546
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.11419307,  3.15760958, 40.88642908]), 'distance': 27.5, 'localFrame': array([[-0.39991735,  0.83733368, -0.37274445],
       [-0.90236336, -0.43097606,  0.        ],
       [-0.16064393,  0.33635094,  0.92793404]]), 'currentState': array([29.21233167,  7.0336877 , 62.84265718, -0.39991735,  0.83733368,
       -0.37274445]), 'targetState': array([ 0., -0., 23.]), 'previousTarget': array([14.12532877,  2.87289854, 42.06436841])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6282762214029154
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 23.]), 'distance': 2.229888702261388, 'localFrame': array([[ 0.42155829,  0.69776765,  0.57914499],
       [-0.85592079,  0.51710696,  0.        ],
       [-0.2994799 , -0.49570224,  0.81522456]]), 'currentState': array([ 0.64325292, -0.21440681, 20.87569799,  0.42155829,  0.69776765,
        0.57914499]), 'targetState': array([ 0., -0., 23.]), 'previousTarget': array([ 0.,  0., 23.])}
episode index:18547
target thresh 85.13251086810985
target distance 24.0
model initialize at round 18547
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.27563907, -30.89053573,  64.45217684]), 'distance': 27.5, 'localFrame': array([[-0.09481656, -0.89510217,  0.43566264],
       [ 0.99443639, -0.10533885,  0.        ],
       [ 0.0458922 ,  0.43323878,  0.90011003]]), 'currentState': array([  3.36587348, -15.94695069,  82.30057666,  -0.09481656,
        -0.89510217,   0.43566264]), 'targetState': array([-17.38885604, -37.12987592,  57.        ]), 'previousTarget': array([-11.57057679, -30.69448146,  63.89295184])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6282855676595542
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.38885604, -37.12987592,  57.        ]), 'distance': 3.060135649328717, 'localFrame': array([[-0.26148132, -0.83173823, -0.48973363],
       [ 0.95396817, -0.29990788,  0.        ],
       [-0.14687498, -0.46719029,  0.87187211]]), 'currentState': array([-17.00945845, -34.86446035,  59.02197428,  -0.26148132,
        -0.83173823,  -0.48973363]), 'targetState': array([-17.38885604, -37.12987592,  57.        ]), 'previousTarget': array([-17.38885604, -37.12987592,  57.        ])}
episode index:18548
target thresh 85.13399754268806
target distance 48.058643753682745
model initialize at round 18548
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.14094034, -29.67754161,  74.44110286]), 'distance': 27.5, 'localFrame': array([[-0.15287916,  0.88908556, -0.43145664],
       [-0.98553638, -0.16946398,  0.        ],
       [-0.07311636,  0.42521622,  0.90213367]]), 'currentState': array([ -3.58123957, -42.87467574,  89.6583835 ,  -0.15287916,
         0.88908556,  -0.43145664]), 'targetState': array([45.21140664, -8.48107949, 50.        ]), 'previousTarget': array([ 15.40025264, -30.45653974,  75.43262187])}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.6282737360032258
{'scaleFactor': 20, 'timeStep': 90, 'trapCount': 38, 'trapConfig': [], 'currentTarget': array([45.21140664, -8.48107949, 50.        ]), 'distance': 4.079241845402493, 'localFrame': array([[ 0.90560285,  0.28402454,  0.31498181],
       [-0.29925743,  0.95417241,  0.        ],
       [-0.30054695, -0.09426065,  0.94909771]]), 'currentState': array([ 42.31187446, -11.30590969,  49.49674909,   0.90560285,
         0.28402454,   0.31498181]), 'targetState': array([45.21140664, -8.48107949, 50.        ]), 'previousTarget': array([45.21140664, -8.48107949, 50.        ])}
episode index:18549
target thresh 85.13548406860627
target distance 34.73323290474952
model initialize at round 18549
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-31.36523237,  23.42094561,  99.        ]), 'distance': 27.5, 'localFrame': array([[-0.62071738, -0.57859318,  0.52909344],
       [ 0.68185013, -0.7314919 ,  0.        ],
       [ 0.38702757,  0.36076243,  0.84856357]]), 'currentState': array([-3.89474988, 24.69475555, 99.        , -0.62071738, -0.57859318,
        0.52909344]), 'targetState': array([-38.62798278,  23.0841709 ,  99.        ]), 'previousTarget': array([-31.36523237,  23.42094561,  99.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6282848540638968
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-38.62798278,  23.0841709 ,  99.        ]), 'distance': 3.890439907953669, 'localFrame': array([[-0.5625192 , -0.57761279,  0.59155355],
       [ 0.71640501, -0.69768464,  0.        ],
       [ 0.41271783,  0.42379193,  0.80626571]]), 'currentState': array([-35.69290852,  24.99525715,  97.30630255,  -0.5625192 ,
        -0.57761279,   0.59155355]), 'targetState': array([-38.62798278,  23.0841709 ,  99.        ]), 'previousTarget': array([-38.62798278,  23.0841709 ,  99.        ])}
episode index:18550
target thresh 85.1369704458793
target distance 48.78124439796794
model initialize at round 18550
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.30344975, -11.98128135,  33.78121689]), 'distance': 27.5, 'localFrame': array([[ 0.04173641, -0.8858828 , -0.46202786],
       [ 0.99889204,  0.0470606 ,  0.        ],
       [ 0.02174331, -0.46151595,  0.88686541]]), 'currentState': array([-6.80308962e+00, -4.77584281e+00,  4.68356986e+01,  4.17364142e-02,
       -8.85882797e-01, -4.62027859e-01]), 'targetState': array([ 42.46632445, -20.13979364,  19.        ]), 'previousTarget': array([ 16.50191479, -11.02769767,  33.90333999])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282509860862102
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 54, 'trapConfig': [], 'currentTarget': array([ 42.46632445, -20.13979364,  19.        ]), 'distance': 6.324275440575659, 'localFrame': array([[ 0.89139494,  0.23208775,  0.38929467],
       [-0.2519644 ,  0.9677365 ,  0.        ],
       [-0.37673466, -0.0980884 ,  0.92111327]]), 'currentState': array([ 36.81139129, -22.56433439,  17.53719689,   0.89139494,
         0.23208775,   0.38929467]), 'targetState': array([ 42.46632445, -20.13979364,  19.        ]), 'previousTarget': array([ 42.46632445, -20.13979364,  19.        ])}
episode index:18551
target thresh 85.13845667452205
target distance 56.43072471566555
model initialize at round 18551
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.86615139,  -4.70293007,  60.5032541 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.11204917,  0.71918885, -0.68572034],
       [-0.98807985,  0.15394222,  0.        ],
       [ 0.10556131,  0.67754645,  0.72786511]]), 'currentState': array([-41.18784083, -19.54223115,  54.35611461,   0.11204917,
         0.71918885,  -0.68572034]), 'targetState': array([15.61873589, 18.22237881, 70.        ]), 'previousTarget': array([-18.7890367 ,  -5.82834631,  60.85397908])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6282517601599489
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.61873589, 18.22237881, 70.        ]), 'distance': 1.7764392953602803, 'localFrame': array([[ 0.47016838,  0.58590164,  0.66004618],
       [-0.77992828,  0.6258689 ,  0.        ],
       [-0.41310238, -0.51478868,  0.75122502]]), 'currentState': array([14.8107228 , 16.77672493, 70.64260118,  0.47016838,  0.58590164,
        0.66004618]), 'targetState': array([15.61873589, 18.22237881, 70.        ]), 'previousTarget': array([15.61873589, 18.22237881, 70.        ])}
episode index:18552
target thresh 85.13994275454935
target distance 66.58793156555048
model initialize at round 18552
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.32559051,  -3.54145045,  50.26237856]), 'distance': 27.5, 'localFrame': array([[ 0.66026224,  0.32630138, -0.67644748],
       [-0.44304877,  0.89649751,  0.        ],
       [ 0.60643348,  0.29969922,  0.73649088]]), 'currentState': array([-39.8158384 , -20.54071629,  47.9290962 ,   0.66026224,
         0.32630138,  -0.67644748]), 'targetState': array([25.30935948, 30.97476913, 55.        ]), 'previousTarget': array([-19.54511655,  -3.56438976,  50.95832383])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6282489090302026
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([25.30935948, 30.97476913, 55.        ]), 'distance': 3.83100906965601, 'localFrame': array([[-0.43228539,  0.87709039,  0.20938433],
       [-0.89697321, -0.4420849 ,  0.        ],
       [ 0.09256565, -0.18781213,  0.97783342]]), 'currentState': array([26.35994635, 28.55779831, 57.78049452, -0.43228539,  0.87709039,
        0.20938433]), 'targetState': array([25.30935948, 30.97476913, 55.        ]), 'previousTarget': array([25.30935948, 30.97476913, 55.        ])}
episode index:18553
target thresh 85.14142868597608
target distance 24.32890206907082
model initialize at round 18553
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.9575336 , 39.89465067, 92.61805844]), 'distance': 27.5, 'localFrame': array([[-0.67468937, -0.05377132,  0.73614054],
       [ 0.07944598, -0.99683917,  0.        ],
       [ 0.73381373,  0.05848341,  0.67682871]]), 'currentState': array([ 2.40116360e+01,  2.24882190e+01,  8.94572185e+01, -6.74689369e-01,
       -5.37713207e-02,  7.36140544e-01]), 'targetState': array([ 0.41345137, 41.99796493, 93.        ]), 'previousTarget': array([ 3.213593  , 39.84820361, 92.42452363])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6282595762563966
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.41345137, 41.99796493, 93.        ]), 'distance': 3.0745710769723376, 'localFrame': array([[ 0.10297107,  0.83287452, -0.54379867],
       [-0.99244391,  0.12269916,  0.        ],
       [ 0.06672364,  0.53968968,  0.83921571]]), 'currentState': array([ 1.97609459, 41.28992298, 95.55143294,  0.10297107,  0.83287452,
       -0.54379867]), 'targetState': array([ 0.41345137, 41.99796493, 93.        ]), 'previousTarget': array([ 0.41345137, 41.99796493, 93.        ])}
episode index:18554
target thresh 85.14291446881711
target distance 45.56413333569294
model initialize at round 18554
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.36476448,  11.74353978,  82.27519575]), 'distance': 27.5, 'localFrame': array([[-0.27713687, -0.92745156, -0.2510553 ],
       [ 0.95813809, -0.28630648,  0.        ],
       [-0.07187876, -0.24054565,  0.96797275]]), 'currentState': array([-4.58654692, 34.46439835, 91.03474722, -0.27713687, -0.92745156,
       -0.2510553 ]), 'targetState': array([-29.43641586,  -9.72097843,  74.        ]), 'previousTarget': array([-16.35322753,  13.13690567,  82.52828752])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6282651835748227
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.43641586,  -9.72097843,  74.        ]), 'distance': 3.210625916162491, 'localFrame': array([[-0.19503558, -0.88758619, -0.41731509],
       [ 0.97669833, -0.21461682,  0.        ],
       [-0.08956284, -0.40759095,  0.90876186]]), 'currentState': array([-29.07351158,  -8.2120041 ,  76.81058993,  -0.19503558,
        -0.88758619,  -0.41731509]), 'targetState': array([-29.43641586,  -9.72097843,  74.        ]), 'previousTarget': array([-29.43641586,  -9.72097843,  74.        ])}
episode index:18555
target thresh 85.14440010308726
target distance 56.486643072331454
model initialize at round 18555
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.7294374 , -2.72420927, 20.43341746]), 'distance': 27.5, 'localFrame': array([[-0.35718692,  0.91624425, -0.18142208],
       [-0.93170561, -0.36321434,  0.        ],
       [-0.0658951 ,  0.16903196,  0.98340532]]), 'currentState': array([46.41692563, -3.05562835, 32.5440775 , -0.35718692,  0.91624425,
       -0.18142208]), 'targetState': array([-9.73146653, -2.30185995,  5.        ]), 'previousTarget': array([21.96344704, -3.69850848, 20.14982339])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6282642594562604
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-9.73146653, -2.30185995,  5.        ]), 'distance': 3.445783383606611, 'localFrame': array([[-9.99254462e-01,  3.52656562e-02,  1.57115547e-02],
       [-3.52700098e-02, -9.99377820e-01,  0.00000000e+00],
       [ 1.57017793e-02, -5.54146687e-04,  9.99876566e-01]]), 'currentState': array([-7.27575676, -4.50361366,  4.00240627, -0.99925446,  0.03526566,
        0.01571155]), 'targetState': array([-9.73146653, -2.30185995,  5.        ]), 'previousTarget': array([-9.73146653, -2.30185995,  5.        ])}
episode index:18556
target thresh 85.14588558880143
target distance 62.0
model initialize at round 18556
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.54550437, -3.18006434, 39.46580814]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55918912,  0.693915  , -0.45364028],
       [-0.77864314,  0.62746702,  0.        ],
       [ 0.28464431,  0.35322389,  0.89118488]]), 'currentState': array([ 25.46077358, -13.71953219,  64.7936853 ,   0.55918912,
         0.693915  ,  -0.45364028]), 'targetState': array([20.78799575, 11.99413326,  3.        ]), 'previousTarget': array([22.61924463, -3.78725706, 39.75701932])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6282650326061568
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.78799575, 11.99413326,  3.        ]), 'distance': 2.4394523347513646, 'localFrame': array([[-0.88519021,  0.02061974, -0.46477211],
       [-0.02328782, -0.9997288 ,  0.        ],
       [-0.46464606,  0.01082353,  0.88543034]]), 'currentState': array([ 2.06981759e+01,  1.35798987e+01,  4.85154204e+00, -8.85190213e-01,
        2.06197413e-02, -4.64772108e-01]), 'targetState': array([20.78799575, 11.99413326,  3.        ]), 'previousTarget': array([20.78799575, 11.99413326,  3.        ])}
episode index:18557
target thresh 85.14737092597446
target distance 46.36142721274322
model initialize at round 18557
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.36052449, -8.23617533, 49.95689923]), 'distance': 27.5, 'localFrame': array([[-0.40019146, -0.64437598,  0.65163364],
       [ 0.84950194, -0.5275855 ,  0.        ],
       [ 0.34379246,  0.55356404,  0.75853385]]), 'currentState': array([ 7.9414073 , 11.63222551, 35.46003328, -0.40019146, -0.64437598,
        0.65163364]), 'targetState': array([-20.52035707, -34.33533087,  69.        ]), 'previousTarget': array([-3.48189296, -7.54491926, 48.77490206])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6282672260928452
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.52035707, -34.33533087,  69.        ]), 'distance': 3.2242601755351656, 'localFrame': array([[-0.64091215, -0.7371572 ,  0.21408148],
       [ 0.75465322, -0.65612385,  0.        ],
       [ 0.14046397,  0.16155728,  0.97681581]]), 'currentState': array([-18.90195042, -33.31520146,  66.4046292 ,  -0.64091215,
        -0.7371572 ,   0.21408148]), 'targetState': array([-20.52035707, -34.33533087,  69.        ]), 'previousTarget': array([-20.52035707, -34.33533087,  69.        ])}
episode index:18558
target thresh 85.1488561146212
target distance 56.018298641897246
model initialize at round 18558
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.34965324, -10.66204232,  76.78475964]), 'distance': 27.499999999999996, 'localFrame': array([[-0.64266425,  0.21591193,  0.73509503],
       [-0.31847104, -0.94793259,  0.        ],
       [ 0.69682053, -0.23410648,  0.67796408]]), 'currentState': array([ -4.65505612, -33.60148506,  65.12085149,  -0.64266425,
         0.21591193,   0.73509503]), 'targetState': array([-27.8271447 ,  21.22851898,  93.        ]), 'previousTarget': array([-13.60823514, -11.96412528,  75.81656697])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6282697834409227
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.8271447 ,  21.22851898,  93.        ]), 'distance': 2.9314768877079014, 'localFrame': array([[-0.91353542,  0.20925141, -0.3488078 ],
       [-0.22327432, -0.97475565,  0.        ],
       [-0.34000238,  0.07787982,  0.93719428]]), 'currentState': array([-25.58108745,  21.15979228,  94.88256745,  -0.91353542,
         0.20925141,  -0.3488078 ]), 'targetState': array([-27.8271447 ,  21.22851898,  93.        ]), 'previousTarget': array([-27.8271447 ,  21.22851898,  93.        ])}
episode index:18559
target thresh 85.15034115475649
target distance 43.62617930193442
model initialize at round 18559
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.76127135, 22.12291214, 17.84497135]), 'distance': 27.5, 'localFrame': array([[-0.81918793,  0.23225443,  0.52439395],
       [-0.27276693, -0.96208014,  0.        ],
       [ 0.50450901, -0.14303733,  0.85147577]]), 'currentState': array([31.33565164, 13.02270976, 22.24808269, -0.81918793,  0.23225443,
        0.52439395]), 'targetState': array([-10.7630392,  28.0028032,  15.       ]), 'previousTarget': array([ 7.32553483, 21.38510285, 17.90238614])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6282746035243901
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.7630392,  28.0028032,  15.       ]), 'distance': 3.250738161466879, 'localFrame': array([[ 0.32101757, -0.78127642, -0.5353082 ],
       [ 0.92496315,  0.38005681,  0.        ],
       [ 0.20344753, -0.49514036,  0.84465681]]), 'currentState': array([-7.99231454, 28.05584356, 16.6992852 ,  0.32101757, -0.78127642,
       -0.5353082 ]), 'targetState': array([-10.7630392,  28.0028032,  15.       ]), 'previousTarget': array([-10.7630392,  28.0028032,  15.       ])}
episode index:18560
target thresh 85.1518260463952
target distance 57.58797832144413
model initialize at round 18560
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.21309732,   3.2386185 ,  63.03016916]), 'distance': 27.499999999999996, 'localFrame': array([[-0.30766654,  0.69321754, -0.65175973],
       [-0.91402192, -0.40566481,  0.        ],
       [-0.26439599,  0.59572268,  0.75842551]]), 'currentState': array([ -3.66346457, -21.29987881,  67.58188679,  -0.30766654,
         0.69321754,  -0.65175973]), 'targetState': array([-30.51418691,  35.74750896,  57.        ]), 'previousTarget': array([-14.41874916,   2.48889371,  63.9303246 ])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6282771601994365
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.51418691,  35.74750896,  57.        ]), 'distance': 3.976336353977567, 'localFrame': array([[-0.02694838,  0.92984947, -0.36695196],
       [-0.9995803 , -0.02896928,  0.        ],
       [-0.01063033,  0.36679795,  0.93023989]]), 'currentState': array([-2.75303123e+01,  3.32579567e+01,  5.61574606e+01, -2.69483764e-02,
        9.29849475e-01, -3.66951958e-01]), 'targetState': array([-30.51418691,  35.74750896,  57.        ]), 'previousTarget': array([-30.51418691,  35.74750896,  57.        ])}
episode index:18561
target thresh 85.15331078955217
target distance 37.63890259420778
model initialize at round 18561
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.13707261, 23.76722821, 47.54378553]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.11021526,  0.78692464,  0.60712618],
       [-0.99033383,  0.13870439,  0.        ],
       [-0.08421106, -0.6012576 ,  0.79460544]]), 'currentState': array([ 1.12333911, -0.27949558, 50.48450233,  0.11021526,  0.78692464,
        0.60712618]), 'targetState': array([20.96888094, 36.39101582, 46.        ]), 'previousTarget': array([14.13355567, 23.13352266, 47.05668542])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6282835654862662
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.96888094, 36.39101582, 46.        ]), 'distance': 1.5116233488382689, 'localFrame': array([[-0.17555316,  0.84641663, -0.5027524 ],
       [-0.97916104, -0.20308534,  0.        ],
       [-0.10210164,  0.49227557,  0.86443046]]), 'currentState': array([21.56814488, 35.13101725, 45.41837168, -0.17555316,  0.84641663,
       -0.5027524 ]), 'targetState': array([20.96888094, 36.39101582, 46.        ]), 'previousTarget': array([20.96888094, 36.39101582, 46.        ])}
episode index:18562
target thresh 85.15479538424225
target distance 32.25977257715753
model initialize at round 18562
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.68016883, 15.71788901, 39.80628517]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.30400261,  0.67425105, -0.67302892],
       [-0.91162293,  0.41102754,  0.        ],
       [ 0.27663342,  0.6135486 ,  0.73961616]]), 'currentState': array([-1.52072064e-02,  6.14992136e-01,  2.51425144e+01,  3.04002614e-01,
        6.74251052e-01, -6.73028922e-01]), 'targetState': array([32.39482667, 28.27676086, 52.        ]), 'previousTarget': array([17.62345205, 14.87546767, 40.09490242])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6282771927114326
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([32.39482667, 28.27676086, 52.        ]), 'distance': 2.8651040958492233, 'localFrame': array([[ 0.04545823,  0.2737044 ,  0.96073901],
       [-0.98648679,  0.16384079,  0.        ],
       [-0.15740824, -0.94775635,  0.27745369]]), 'currentState': array([2.99825624e+01, 2.67314750e+01, 5.20435229e+01, 4.54582304e-02,
       2.73704398e-01, 9.60739013e-01]), 'targetState': array([32.39482667, 28.27676086, 52.        ]), 'previousTarget': array([32.39482667, 28.27676086, 52.        ])}
episode index:18563
target thresh 85.15627983048026
target distance 44.87824769949884
model initialize at round 18563
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.04306018,  -7.27228385,  56.3816664 ]), 'distance': 27.5, 'localFrame': array([[-0.82579993, -0.26385398, -0.4984331 ],
       [ 0.30435507, -0.95255866,  0.        ],
       [-0.47478677, -0.15170064,  0.86692816]]), 'currentState': array([ -3.87692796, -24.56473783,  39.5331057 ,  -0.82579993,
        -0.26385398,  -0.4984331 ]), 'targetState': array([-38.62511923,  21.07368418,  84.        ]), 'previousTarget': array([-16.71553186,  -6.57052489,  57.51275839])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6282734213769053
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-38.62511923,  21.07368418,  84.        ]), 'distance': 2.8292214523197763, 'localFrame': array([[-0.41150507,  0.26388874,  0.87236822],
       [-0.53981608, -0.84178299,  0.        ],
       [ 0.73434473, -0.47091839,  0.48884935]]), 'currentState': array([-37.67486703,  22.57486681,  81.79819041,  -0.41150507,
         0.26388874,   0.87236822]), 'targetState': array([-38.62511923,  21.07368418,  84.        ]), 'previousTarget': array([-38.62511923,  21.07368418,  84.        ])}
episode index:18564
target thresh 85.15776412828109
target distance 41.43702298571967
model initialize at round 18564
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.34664122, 35.23127033, 23.11719082]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.11729374, -0.9389004 , -0.32358649],
       [ 0.99228682,  0.12396313,  0.        ],
       [ 0.0401128 , -0.32109061,  0.9461986 ]]), 'currentState': array([21.61354509, 35.17125368, 34.65995901,  0.11729374, -0.9389004 ,
       -0.32358649]), 'targetState': array([-20.89932805,  35.27347568,  15.        ]), 'previousTarget': array([-4.21757643, 35.82203783, 23.05161685])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282395795551237
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 71, 'trapConfig': [], 'currentTarget': array([-20.89932805,  35.27347568,  15.        ]), 'distance': 9.081751890157589, 'localFrame': array([[ 0.76697942,  0.04177188, -0.64031061],
       [-0.05438224,  0.99852019,  0.        ],
       [ 0.63936307,  0.03482153,  0.76811609]]), 'currentState': array([-20.84516712,  32.40833473,  23.61778692,   0.76697942,
         0.04177188,  -0.64031061]), 'targetState': array([-20.89932805,  35.27347568,  15.        ]), 'previousTarget': array([-20.89932805,  35.27347568,  15.        ])}
episode index:18565
target thresh 85.15924827765954
target distance 63.91927448824815
model initialize at round 18565
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.82672001, -10.34017574,  66.84507882]), 'distance': 27.499999999999996, 'localFrame': array([[-0.22375078, -0.83942378,  0.49528105],
       [ 0.96626232, -0.25755995,  0.        ],
       [ 0.12756456,  0.47857141,  0.8687328 ]]), 'currentState': array([-13.22620599,  17.11644781,  67.49720029,  -0.22375078,
        -0.83942378,   0.49528105]), 'targetState': array([-10.0131377 , -45.92098729,  66.        ]), 'previousTarget': array([-11.51738922,  -9.47831534,  66.        ])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6282400075367304
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.0131377 , -45.92098729,  66.        ]), 'distance': 3.148651722931569, 'localFrame': array([[ 0.25073926, -0.93753246,  0.24116947],
       [ 0.96604726,  0.25836543,  0.        ],
       [-0.06230985,  0.23298111,  0.97048302]]), 'currentState': array([-10.10621116, -43.47075845,  67.97527811,   0.25073926,
        -0.93753246,   0.24116947]), 'targetState': array([-10.0131377 , -45.92098729,  66.        ]), 'previousTarget': array([-10.0131377 , -45.92098729,  66.        ])}
episode index:18566
target thresh 85.1607322786305
target distance 10.825836826390745
model initialize at round 18566
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.88326544, 12.16241436,  6.        ]), 'distance': 13.562638619001124, 'localFrame': array([[ 0.45717763,  0.83568339,  0.30433843],
       [-0.87729897,  0.47994428,  0.        ],
       [-0.14606549, -0.26699579,  0.95256397]]), 'currentState': array([25.78722617,  2.33587829,  1.32683155,  0.45717763,  0.83568339,
        0.30433843]), 'targetState': array([33.88326544, 12.16241436,  6.        ]), 'previousTarget': array([33.88326544, 12.16241436,  6.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.62825637126487
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.88326544, 12.16241436,  6.        ]), 'distance': 1.9195133132087203, 'localFrame': array([[ 0.67480623,  0.67982223, -0.28719033],
       [-0.70972023,  0.70448364,  0.        ],
       [ 0.20232089,  0.20382479,  0.95787354]]), 'currentState': array([33.69526734, 10.26020722,  5.8245119 ,  0.67480623,  0.67982223,
       -0.28719033]), 'targetState': array([33.88326544, 12.16241436,  6.        ]), 'previousTarget': array([33.88326544, 12.16241436,  6.        ])}
episode index:18567
target thresh 85.16221613120878
target distance 55.37101801548914
model initialize at round 18567
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.69705064, 21.71861042, 55.15830333]), 'distance': 27.499999999999996, 'localFrame': array([[-0.95957762,  0.00780749,  0.28133581],
       [-0.00813611, -0.9999669 ,  0.        ],
       [ 0.2813265 , -0.00228898,  0.95960938]]), 'currentState': array([ 1.34623606e+01,  4.64042822e+01,  4.51035436e+01, -9.59577619e-01,
        7.80748516e-03,  2.81335807e-01]), 'targetState': array([-1.94344009, -9.80933436, 68.        ]), 'previousTarget': array([ 7.58322777, 21.13367922, 55.14689854])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6282582037546223
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-1.94344009, -9.80933436, 68.        ]), 'distance': 3.5118018841979355, 'localFrame': array([[-0.24963954, -0.80217519,  0.54239751],
       [ 0.95483181, -0.29714678,  0.        ],
       [ 0.16117167,  0.51789839,  0.84012198]]), 'currentState': array([-2.42257167, -7.61874845, 65.29731267, -0.24963954, -0.80217519,
        0.54239751]), 'targetState': array([-1.94344009, -9.80933436, 68.        ]), 'previousTarget': array([-1.94344009, -9.80933436, 68.        ])}
episode index:18568
target thresh 85.16369983540922
target distance 62.0
model initialize at round 18568
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([11.60933744, 10.51972592, 63.20607198]), 'distance': 27.5, 'localFrame': array([[-0.69949748,  0.63069883, -0.33603907],
       [-0.66963969, -0.74268613,  0.        ],
       [-0.24957156,  0.2250251 ,  0.94184805]]), 'currentState': array([-4.89707851,  7.55106735, 85.        , -0.69949748,  0.63069883,
       -0.33603907]), 'targetState': array([42.06085345, 15.99639356, 23.        ]), 'previousTarget': array([11.60933744, 10.51972592, 63.20607198])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6282582880578939
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([42.06085345, 15.99639356, 23.        ]), 'distance': 3.4395907309174465, 'localFrame': array([[ 0.8515615 ,  0.36538661, -0.37594633],
       [-0.39431283,  0.91897627,  0.        ],
       [ 0.34548575,  0.14824046,  0.92664144]]), 'currentState': array([42.23653377, 13.55964987, 25.42119826,  0.8515615 ,  0.36538661,
       -0.37594633]), 'targetState': array([42.06085345, 15.99639356, 23.        ]), 'previousTarget': array([42.06085345, 15.99639356, 23.        ])}
episode index:18569
target thresh 85.16518339124663
target distance 44.0
model initialize at round 18569
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.99628407,  7.15866962, 62.58166096]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.81074587,  0.42228437, -0.40542206],
       [-0.4619524 ,  0.88690472,  0.        ],
       [ 0.35957073,  0.18728569,  0.91412962]]), 'currentState': array([-5.06564437,  1.62049732, 82.56528898,  0.81074587,  0.42228437,
       -0.40542206]), 'targetState': array([33.406409  , 13.41684902, 40.        ]), 'previousTarget': array([11.99201194,  6.58433801, 63.94864475])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6282612119301046
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.406409  , 13.41684902, 40.        ]), 'distance': 3.637003071575169, 'localFrame': array([[-0.4138517 ,  0.64356191, -0.64385933],
       [-0.8410992 , -0.54088089,  0.        ],
       [-0.3482512 ,  0.54154957,  0.76514389]]), 'currentState': array([32.75511384, 10.80464893, 42.44540725, -0.4138517 ,  0.64356191,
       -0.64385933]), 'targetState': array([33.406409  , 13.41684902, 40.        ]), 'previousTarget': array([33.406409  , 13.41684902, 40.        ])}
episode index:18570
target thresh 85.1666667987359
target distance 52.27243236266273
model initialize at round 18570
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.46986817, -23.47505871,  70.44130295]), 'distance': 27.499999999999996, 'localFrame': array([[-0.74914712, -0.09018793, -0.65623528],
       [ 0.11952445, -0.99283126,  0.        ],
       [-0.65153089, -0.07843616,  0.75455634]]), 'currentState': array([ 38.68090054, -23.86945211,  89.08556814,  -0.74914712,
        -0.09018793,  -0.65623528]), 'targetState': array([-12.3614941 , -22.87342265,  42.        ]), 'previousTarget': array([ 19.65534416, -22.87239925,  71.39997561])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628227381699534
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 79, 'trapConfig': [], 'currentTarget': array([  6.40695464, -20.89375335,  57.20277781]), 'distance': 27.5, 'localFrame': array([[ 0.0690737 ,  0.55043648, -0.83201473],
       [-0.99221805,  0.12451241,  0.        ],
       [ 0.10359616,  0.82554003,  0.55475354]]), 'currentState': array([ 2.77046070e+01, -1.86473073e+01,  7.44542556e+01,  6.90736995e-02,
        5.50436478e-01, -8.32014728e-01]), 'targetState': array([-12.3614941 , -22.87342265,  42.        ]), 'previousTarget': array([  6.40695464, -20.89375335,  57.20277781])}
episode index:18571
target thresh 85.16815005789184
target distance 20.908929169008594
model initialize at round 18571
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.91022099, 15.93955106, 25.        ]), 'distance': 21.856934834063708, 'localFrame': array([[ 0.31110734, -0.03891287, -0.94957781],
       [ 0.12411154,  0.99226827,  0.        ],
       [ 0.94223593, -0.11785356,  0.31353147]]), 'currentState': array([ 3.67832307, 37.67329923, 24.37676476,  0.31110734, -0.03891287,
       -0.94957781]), 'targetState': array([ 5.91022099, 15.93955106, 25.        ]), 'previousTarget': array([ 5.91022099, 15.93955106, 25.        ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6282371545019528
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 5.91022099, 15.93955106, 25.        ]), 'distance': 3.129381535364874, 'localFrame': array([[-0.68377829, -0.27396881, -0.67630491],
       [ 0.37192602, -0.92826237,  0.        ],
       [-0.6277884 , -0.2515354 ,  0.73662179]]), 'currentState': array([ 5.44360768, 17.68402999, 27.55579617, -0.68377829, -0.27396881,
       -0.67630491]), 'targetState': array([ 5.91022099, 15.93955106, 25.        ]), 'previousTarget': array([ 5.91022099, 15.93955106, 25.        ])}
episode index:18572
target thresh 85.16963316872928
target distance 29.817053599700465
model initialize at round 18572
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.34268373, 18.68499926, 67.95216412]), 'distance': 27.499999999999996, 'localFrame': array([[-0.93810172,  0.32220484,  0.1270795 ],
       [-0.32483845, -0.94576952,  0.        ],
       [ 0.12018792, -0.04128031,  0.99189253]]), 'currentState': array([31.79529354, 19.05731223, 66.38194186, -0.93810172,  0.32220484,
        0.1270795 ]), 'targetState': array([ 3.50635643, 18.67365697, 68.        ]), 'previousTarget': array([ 5.84146166, 18.64152473, 67.92168558])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.628249173056661
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.50635643, 18.67365697, 68.        ]), 'distance': 2.3596000363999607, 'localFrame': array([[-0.99311767,  0.04041406, -0.10992726],
       [-0.04066048, -0.99917302,  0.        ],
       [-0.10983635,  0.0044697 ,  0.99393963]]), 'currentState': array([ 5.14046276e+00,  1.99987723e+01,  6.90683999e+01, -9.93117667e-01,
        4.04140634e-02, -1.09927262e-01]), 'targetState': array([ 3.50635643, 18.67365697, 68.        ]), 'previousTarget': array([ 3.50635643, 18.67365697, 68.        ])}
episode index:18573
target thresh 85.17111613126302
target distance 48.33272611435131
model initialize at round 18573
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.51501019, 14.98825736, 77.18993642]), 'distance': 27.5, 'localFrame': array([[ 0.60394747, -0.36405172, -0.70902313],
       [ 0.51624979,  0.85643806,  0.        ],
       [ 0.6072344 , -0.36603304,  0.70518522]]), 'currentState': array([ 0.73368382, 24.8912022 , 67.56547802,  0.60394747, -0.36405172,
       -0.70902313]), 'targetState': array([48.75495326,  4.8943368 , 87.        ]), 'previousTarget': array([24.27386681, 15.582863  , 77.8827912 ])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6282524679803614
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([48.75495326,  4.8943368 , 87.        ]), 'distance': 1.866939190523307, 'localFrame': array([[ 0.63336683,  0.15338692,  0.7584978 ],
       [-0.23537312,  0.97190509,  0.        ],
       [-0.73718787, -0.17852999,  0.6516756 ]]), 'currentState': array([47.29667791,  6.05994762, 87.01569619,  0.63336683,  0.15338692,
        0.7584978 ]), 'targetState': array([48.75495326,  4.8943368 , 87.        ]), 'previousTarget': array([48.75495326,  4.8943368 , 87.        ])}
episode index:18574
target thresh 85.17259894550796
target distance 19.428221189669905
model initialize at round 18574
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.99803301, -0.31981178, 34.        ]), 'distance': 22.84270524810757, 'localFrame': array([[-0.15213533, -0.67440562, -0.72251775],
       [ 0.97548754, -0.22005469,  0.        ],
       [-0.15899342, -0.70480706,  0.69135237]]), 'currentState': array([ 6.67577474, -5.08729548, 45.21207464, -0.15213533, -0.67440562,
       -0.72251775]), 'targetState': array([25.99803301, -0.31981178, 34.        ]), 'previousTarget': array([25.99803301, -0.31981178, 34.        ])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.628255762549292
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([25.99803301, -0.31981178, 34.        ]), 'distance': 3.0474533924159357, 'localFrame': array([[ 0.75232835, -0.0860139 ,  0.65314904],
       [ 0.11359029,  0.99352768,  0.        ],
       [-0.64892165,  0.07419139,  0.75722938]]), 'currentState': array([23.50947219, -1.2182322 , 35.5122427 ,  0.75232835, -0.0860139 ,
        0.65314904]), 'targetState': array([25.99803301, -0.31981178, 34.        ]), 'previousTarget': array([25.99803301, -0.31981178, 34.        ])}
episode index:18575
target thresh 85.17408161147887
target distance 69.9762345281916
model initialize at round 18575
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.24160004, -21.5462657 ,  39.11719691]), 'distance': 27.5, 'localFrame': array([[-0.22249794,  0.97120619, -0.08516574],
       [-0.97474765, -0.22330926,  0.        ],
       [-0.0190183 ,  0.0830151 ,  0.9963668 ]]), 'currentState': array([  8.64255602, -43.48436135,  55.69456683,  -0.22249794,
         0.97120619,  -0.08516574]), 'targetState': array([ 7.39222222, 24.92699442,  4.        ]), 'previousTarget': array([  8.69704065, -22.8307692 ,  38.80675919])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6282526050040685
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.39222222, 24.92699442,  4.        ]), 'distance': 2.203584928690849, 'localFrame': array([[-0.56756952,  0.70835304, -0.41964367],
       [-0.78039184, -0.62529079,  0.        ],
       [-0.26239932,  0.3274865 ,  0.90768893]]), 'currentState': array([ 8.41412762, 23.32885267,  2.87864419, -0.56756952,  0.70835304,
       -0.41964367]), 'targetState': array([ 7.39222222, 24.92699442,  4.        ]), 'previousTarget': array([ 7.39222222, 24.92699442,  4.        ])}
episode index:18576
target thresh 85.1755641291906
target distance 44.04773762892284
model initialize at round 18576
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.67548422, -21.24626745,  72.28670453]), 'distance': 27.500000000000004, 'localFrame': array([[-0.43786787, -0.60006186, -0.66947553],
       [ 0.80780064, -0.58945579,  0.        ],
       [-0.39462623, -0.54080276,  0.74283411]]), 'currentState': array([ 0.4651934 ,  5.77636459, 69.40559207, -0.43786787, -0.60006186,
       -0.66947553]), 'targetState': array([  7.17919497, -37.31566909,  74.        ]), 'previousTarget': array([  5.16637552, -20.46351563,  72.4696464 ])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6282602375895667
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.17919497, -37.31566909,  74.        ]), 'distance': 3.203122825893207, 'localFrame': array([[-0.1000324 , -0.89086596,  0.44311552],
       [ 0.99375484, -0.11158546,  0.        ],
       [ 0.04944525,  0.44034819,  0.89646452]]), 'currentState': array([  6.19579143, -34.34061017,  73.33521601,  -0.1000324 ,
        -0.89086596,   0.44311552]), 'targetState': array([  7.17919497, -37.31566909,  74.        ]), 'previousTarget': array([  7.17919497, -37.31566909,  74.        ])}
episode index:18577
target thresh 85.17704649865799
target distance 23.0
model initialize at round 18577
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.26773953,  -6.21075881,  32.        ]), 'distance': 25.172077648862576, 'localFrame': array([[-0.52792135, -0.69255675, -0.49159353],
       [ 0.79528829, -0.60623143,  0.        ],
       [-0.29801945, -0.39095858,  0.87082478]]), 'currentState': array([ 0.64081725, -2.26401009, 54.33965212, -0.52792135, -0.69255675,
       -0.49159353]), 'targetState': array([-10.26773953,  -6.21075881,  32.        ]), 'previousTarget': array([-10.26773953,  -6.21075881,  32.        ])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6282631600977711
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-10.26773953,  -6.21075881,  32.        ]), 'distance': 3.745404288657078, 'localFrame': array([[-0.36494109,  0.28179638, -0.88736058],
       [-0.61117104, -0.79149856,  0.        ],
       [-0.70234462,  0.54232909,  0.46107613]]), 'currentState': array([-13.02146554,  -7.14957013,  34.35874535,  -0.36494109,
         0.28179638,  -0.88736058]), 'targetState': array([-10.26773953,  -6.21075881,  32.        ]), 'previousTarget': array([-10.26773953,  -6.21075881,  32.        ])}
episode index:18578
target thresh 85.17852871989582
target distance 23.991867777368302
model initialize at round 18578
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.62472438, -23.99186778,  43.        ]), 'distance': 26.734600841795704, 'localFrame': array([[ 0.49737941, -0.73570583, -0.4597289 ],
       [ 0.82844222,  0.56007454,  0.        ],
       [ 0.25748245, -0.38085883,  0.88805931]]), 'currentState': array([ 0.49837287, -1.75126461, 57.83504257,  0.49737941, -0.73570583,
       -0.4597289 ]), 'targetState': array([  0.62472438, -23.99186778,  43.        ]), 'previousTarget': array([  0.60702264, -23.31205157,  43.42502915])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.628273812203037
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([  0.62472438, -23.99186778,  43.        ]), 'distance': 2.6513053627719354, 'localFrame': array([[-0.62898083, -0.54817726, -0.55125748],
       [ 0.65702283, -0.75387068,  0.        ],
       [-0.41557686, -0.36218875,  0.83433518]]), 'currentState': array([  0.62029593, -21.35607791,  43.28637792,  -0.62898083,
        -0.54817726,  -0.55125748]), 'targetState': array([  0.62472438, -23.99186778,  43.        ]), 'previousTarget': array([  0.62472438, -23.99186778,  43.        ])}
episode index:18579
target thresh 85.18001079291895
target distance 82.0
model initialize at round 18579
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.14059737,   0.14890282,  31.98249607]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.80651775,  0.40919254,  0.42672073],
       [-0.4524547 ,  0.89178739,  0.        ],
       [-0.38054417, -0.1930718 ,  0.90438345]]), 'currentState': array([-22.19516768,   0.51581299,   5.40527999,   0.80651775,
         0.40919254,   0.42672073]), 'targetState': array([-0.80236578, -0.5968326 , 86.        ]), 'previousTarget': array([-15.81222982,  -0.44454297,  30.54475079])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282399976813899
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-4.22283114, -2.02980965, 75.2771682 ]), 'distance': 27.5, 'localFrame': array([[ 0.56418579,  0.73848721,  0.36923033],
       [-0.79463798,  0.60708358,  0.        ],
       [-0.22415367, -0.29340445,  0.92933792]]), 'currentState': array([-12.51321073,  -5.50299894,  49.28762103,   0.56418579,
         0.73848721,   0.36923033]), 'targetState': array([-0.80236578, -0.5968326 , 86.        ]), 'previousTarget': array([-4.75078466, -2.49798747, 73.77056499])}
episode index:18580
target thresh 85.18149271774217
target distance 31.115312887475145
model initialize at round 18580
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.91414365, 10.66226396, 58.45553142]), 'distance': 27.5, 'localFrame': array([[-0.84028061, -0.15470001,  0.51961179],
       [ 0.18106222, -0.98347164,  0.        ],
       [ 0.51102346,  0.09408206,  0.85440247]]), 'currentState': array([29.11949713, 32.07574208, 57.15600598, -0.84028061, -0.15470001,
        0.51961179]), 'targetState': array([ 4.70553178,  1.69055335, 59.        ]), 'previousTarget': array([13.15176849, 11.76108686, 58.35269598])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282061867994309
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([ 4.70553178,  1.69055335, 59.        ]), 'distance': 15.362115994895913, 'localFrame': array([[-0.93830207,  0.17588066, -0.29775027],
       [-0.18423695, -0.98288186,  0.        ],
       [-0.29265334,  0.0548566 ,  0.9546438 ]]), 'currentState': array([13.8090004 , 13.99915611, 60.27269996, -0.93830207,  0.17588066,
       -0.29775027]), 'targetState': array([ 4.70553178,  1.69055335, 59.        ]), 'previousTarget': array([ 4.70553178,  1.69055335, 59.        ])}
episode index:18581
target thresh 85.18297449438033
target distance 38.0
model initialize at round 18581
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.98582226, -1.95951448, 31.53239199]), 'distance': 27.499999999999996, 'localFrame': array([[-0.45812494,  0.58170517, -0.67211654],
       [-0.78561523, -0.61871538,  0.        ],
       [-0.41584883,  0.52802498,  0.74044538]]), 'currentState': array([ -1.71842908, -12.74936099,  56.38594771,  -0.45812494,
         0.58170517,  -0.67211654]), 'targetState': array([ 5.16865964,  3.04712283, 20.        ]), 'previousTarget': array([ 3.11872358, -2.40172804, 32.92584247])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6282110045986882
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 5.16865964,  3.04712283, 20.        ]), 'distance': 2.840524322611671, 'localFrame': array([[ 0.45128306,  0.47056121,  0.75823199],
       [-0.72173641,  0.69216801,  0.        ],
       [-0.52482393, -0.54724363,  0.65198485]]), 'currentState': array([ 4.82157419,  2.14699927, 17.32831744,  0.45128306,  0.47056121,
        0.75823199]), 'targetState': array([ 5.16865964,  3.04712283, 20.        ]), 'previousTarget': array([ 5.16865964,  3.04712283, 20.        ])}
episode index:18582
target thresh 85.18445612284825
target distance 9.52308190901751
model initialize at round 18582
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.90685178,  1.42849021, 11.        ]), 'distance': 11.972049844457784, 'localFrame': array([[ 0.63869417, -0.46680294, -0.6116901 ],
       [ 0.59007006,  0.80735205,  0.        ],
       [ 0.49384925, -0.36094001,  0.79109748]]), 'currentState': array([ 2.42687778, -6.89143761, 12.48284173,  0.63869417, -0.46680294,
       -0.6116901 ]), 'targetState': array([10.90685178,  1.42849021, 11.        ]), 'previousTarget': array([10.90685178,  1.42849021, 11.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6282273557983495
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.90685178,  1.42849021, 11.        ]), 'distance': 2.3248498379354356, 'localFrame': array([[ 0.97112003, -0.15808952,  0.17869975],
       [ 0.16067581,  0.98700724,  0.        ],
       [-0.17637795,  0.02871273,  0.98390365]]), 'currentState': array([ 8.99025172,  0.69476381, 12.09234452,  0.97112003, -0.15808952,
        0.17869975]), 'targetState': array([10.90685178,  1.42849021, 11.        ]), 'previousTarget': array([10.90685178,  1.42849021, 11.        ])}
episode index:18583
target thresh 85.1859376031607
target distance 84.0
model initialize at round 18583
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-11.01324329,   8.50380329,  31.72799617]), 'distance': 27.5, 'localFrame': array([[-0.39457447, -0.58282515,  0.71037021],
       [ 0.82807868, -0.56061189,  0.        ],
       [ 0.39824199,  0.58824243,  0.70382822]]), 'currentState': array([-0.84321952, 17.98023796,  8.        , -0.39457447, -0.58282515,
        0.71037021]), 'targetState': array([-36.84634386, -15.56749641,  92.        ]), 'previousTarget': array([-11.01324329,   8.50380329,  31.72799617])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.6282171529435289
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-36.84634386, -15.56749641,  92.        ]), 'distance': 2.8174829458243438, 'localFrame': array([[ 0.23015141,  0.21867162,  0.94826845],
       [-0.68879546,  0.72495574,  0.        ],
       [-0.68745265, -0.653163  ,  0.31746961]]), 'currentState': array([-37.90961922, -14.40393393,  89.66466747,   0.23015141,
         0.21867162,   0.94826845]), 'targetState': array([-36.84634386, -15.56749641,  92.        ]), 'previousTarget': array([-36.84634386, -15.56749641,  92.        ])}
episode index:18584
target thresh 85.18741893533253
target distance 40.0
model initialize at round 18584
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.95407354,  6.17847529, 42.03672542]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.77848185,  0.07165443, -0.62356368],
       [-0.09165636,  0.9957907 ,  0.        ],
       [ 0.62093891,  0.05715358,  0.78177256]]), 'currentState': array([ 1.00025252, -0.61441119, 62.59577807,  0.77848185,  0.07165443,
       -0.62356368]), 'targetState': array([32.82788215, 12.13796332, 24.        ]), 'previousTarget': array([16.98503059,  6.28013947, 43.3041409 ])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6282215831869686
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([32.82788215, 12.13796332, 24.        ]), 'distance': 2.489796041856016, 'localFrame': array([[ 0.38868754,  0.62195783, -0.67977235],
       [-0.84802043,  0.52996354,  0.        ],
       [ 0.36025457,  0.57646084,  0.73342317]]), 'currentState': array([30.93448129, 12.4596056 , 25.58450742,  0.38868754,  0.62195783,
       -0.67977235]), 'targetState': array([32.82788215, 12.13796332, 24.        ]), 'previousTarget': array([32.82788215, 12.13796332, 24.        ])}
episode index:18585
target thresh 85.18890011937856
target distance 67.0
model initialize at round 18585
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.97820342,  3.54215991, 47.1071275 ]), 'distance': 27.5, 'localFrame': array([[-0.91679521,  0.38461552, -0.10750559],
       [-0.38685756, -0.92213948,  0.        ],
       [-0.09913515,  0.04158935,  0.99420448]]), 'currentState': array([ 1.29138006e+00, -1.89734244e-02,  2.22764042e+01, -9.16795206e-01,
        3.84615520e-01, -1.07505592e-01]), 'targetState': array([-29.44540953,   9.69370196,  90.        ]), 'previousTarget': array([-8.88667319,  3.58259753, 47.54553192])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628187782391575
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-29.99982705,  11.4286806 ,  81.58035324]), 'distance': 27.499999999999996, 'localFrame': array([[-0.85916154, -0.05417077, -0.50882902],
       [ 0.06292579, -0.99801821,  0.        ],
       [-0.50782063, -0.03201847,  0.8608676 ]]), 'currentState': array([-3.17697091e+01,  1.69672999e+01,  5.47020867e+01, -8.59161542e-01,
       -5.41707737e-02, -5.08829021e-01]), 'targetState': array([-29.44540953,   9.69370196,  90.        ]), 'previousTarget': array([-29.70686799,  11.0857355 ,  83.0884644 ])}
episode index:18586
target thresh 85.1903811553136
target distance 8.187958878920854
model initialize at round 18586
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.58632721,  1.77151267, 25.        ]), 'distance': 7.901972573281837, 'localFrame': array([[ 0.46163184,  0.42123599, -0.78067681],
       [-0.67404771,  0.73868781,  0.        ],
       [ 0.57667644,  0.52621342,  0.62493497]]), 'currentState': array([-2.81523243, -5.28632197, 21.53105871,  0.46163184,  0.42123599,
       -0.78067681]), 'targetState': array([-3.58632721,  1.77151267, 25.        ]), 'previousTarget': array([-3.58632721,  1.77151267, 25.        ])}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6282056663011681
{'scaleFactor': 20, 'timeStep': 5, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.58632721,  1.77151267, 25.        ]), 'distance': 3.425958498136831, 'localFrame': array([[-0.12206445,  0.55494401,  0.8228836 ],
       [-0.97665304, -0.21482279,  0.        ],
       [ 0.17677415, -0.80367177,  0.56820999]]), 'currentState': array([-5.93710063, -0.68723171, 24.59302083, -0.12206445,  0.55494401,
        0.8228836 ]), 'targetState': array([-3.58632721,  1.77151267, 25.        ]), 'previousTarget': array([-3.58632721,  1.77151267, 25.        ])}
episode index:18587
target thresh 85.19186204315244
target distance 55.0
model initialize at round 18587
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.75813042, -29.94349094,  70.07815475]), 'distance': 27.499999999999996, 'localFrame': array([[-0.67816786, -0.6998187 , -0.22437056],
       [ 0.71812817, -0.69591087,  0.        ],
       [-0.15614191, -0.16112682,  0.9745039 ]]), 'currentState': array([-10.47895447, -25.35910561,  95.39097741,  -0.67816786,
        -0.6998187 ,  -0.22437056]), 'targetState': array([ 10.79271296, -35.39092182,  40.        ]), 'previousTarget': array([  0.17474462, -29.48682263,  69.56398392])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6281993062882538
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([ 10.79271296, -35.39092182,  40.        ]), 'distance': 4.39361167117159, 'localFrame': array([[-0.10817412,  0.81292518, -0.57223336],
       [-0.99126236, -0.13190505,  0.        ],
       [-0.07548047,  0.56723339,  0.82009084]]), 'currentState': array([  9.31606398, -38.37242911,  42.86948525,  -0.10817412,
         0.81292518,  -0.57223336]), 'targetState': array([ 10.79271296, -35.39092182,  40.        ]), 'previousTarget': array([ 10.79271296, -35.39092182,  40.        ])}
episode index:18588
target thresh 85.19334278290991
target distance 31.127977554593045
model initialize at round 18588
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.03128204, -12.81262377,  26.44429002]), 'distance': 27.5, 'localFrame': array([[-0.73941401,  0.14099227, -0.65832218],
       [-0.18730633, -0.98230155,  0.        ],
       [-0.6466709 ,  0.12330791,  0.75273628]]), 'currentState': array([-1.22977933, -3.13976782, 19.5470934 , -0.73941401,  0.14099227,
       -0.65832218]), 'targetState': array([-31.62543101, -14.99440275,  28.        ]), 'previousTarget': array([-25.85599144, -12.95088094,  26.70257948])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6281974109275628
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([-31.62543101, -14.99440275,  28.        ]), 'distance': 2.4036399581704275, 'localFrame': array([[-0.76640826, -0.61804501,  0.17503927],
       [ 0.62773634, -0.77842603,  0.        ],
       [ 0.13625512,  0.10987851,  0.98456145]]), 'currentState': array([-29.29303404, -14.62291166,  27.55345344,  -0.76640826,
        -0.61804501,   0.17503927]), 'targetState': array([-31.62543101, -14.99440275,  28.        ]), 'previousTarget': array([-31.62543101, -14.99440275,  28.        ])}
episode index:18589
target thresh 85.1948233746008
target distance 32.0
model initialize at round 18589
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.96363252,   1.90405019,  60.52088356]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83873974, -0.02992658, -0.54370952],
       [ 0.03565772, -0.99936406,  0.        ],
       [-0.54336376, -0.01938744,  0.83927347]]), 'currentState': array([-1.97328379e+00,  3.00247757e-01,  8.35201719e+01, -8.38739742e-01,
       -2.99265806e-02, -5.43709522e-01]), 'targetState': array([-21.86555235,   2.4285017 ,  53.        ]), 'previousTarget': array([-15.98289451,   1.7084965 ,  62.01834852])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.628207175879542
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.86555235,   2.4285017 ,  53.        ]), 'distance': 3.128768435712189, 'localFrame': array([[-0.87780557,  0.45862625,  0.13827276],
       [-0.46307445, -0.88631938,  0.        ],
       [ 0.12255382, -0.06403058,  0.99039419]]), 'currentState': array([-19.62822082,   2.21858448,  55.17703338,  -0.87780557,
         0.45862625,   0.13827276]), 'targetState': array([-21.86555235,   2.4285017 ,  53.        ]), 'previousTarget': array([-21.86555235,   2.4285017 ,  53.        ])}
episode index:18590
target thresh 85.19630381823993
target distance 26.34323598574432
model initialize at round 18590
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.57882235,   3.34104087,  47.56267628]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.67185195, -0.69116007, -0.2662944 ],
       [ 0.71705155,  0.69702014,  0.        ],
       [ 0.18561256, -0.19094682,  0.96389174]]), 'currentState': array([-39.60297687,  20.13323763,  63.32804685,   0.67185195,
        -0.69116007,  -0.2662944 ]), 'targetState': array([-17.37170873,  -4.71420576,  40.        ]), 'previousTarget': array([-25.38343941,   4.37275092,  47.93372552])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6282152235468821
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.37170873,  -4.71420576,  40.        ]), 'distance': 1.4680568593277386, 'localFrame': array([[ 0.88559986,  0.14267035,  0.44199328],
       [-0.15904953,  0.98727061,  0.        ],
       [-0.43636698, -0.07029882,  0.89701836]]), 'currentState': array([-18.57903167,  -4.51982506,  40.81226744,   0.88559986,
         0.14267035,   0.44199328]), 'targetState': array([-17.37170873,  -4.71420576,  40.        ]), 'previousTarget': array([-17.37170873,  -4.71420576,  40.        ])}
episode index:18591
target thresh 85.19778411384208
target distance 52.10057008001431
model initialize at round 18591
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.35944241,  -6.45632937,  48.18147522]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.1841163 ,  0.8318032 ,  0.52364551],
       [-0.97636789,  0.21611511,  0.        ],
       [-0.11316771, -0.51127066,  0.85193626]]), 'currentState': array([-17.25465327, -31.82030514,  38.29481424,   0.1841163 ,
         0.8318032 ,   0.52364551]), 'targetState': array([-9.49107629, 18.73284471, 58.        ]), 'previousTarget': array([-13.95055656,  -8.00012428,  47.73793609])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6282204282108457
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.49107629, 18.73284471, 58.        ]), 'distance': 3.223172743764786, 'localFrame': array([[ 0.17864379,  0.76686671,  0.6164429 ],
       [-0.97392315,  0.22687817,  0.        ],
       [-0.13985744, -0.600368  ,  0.78739962]]), 'currentState': array([-7.70113108, 16.30356054, 59.13292414,  0.17864379,  0.76686671,
        0.6164429 ]), 'targetState': array([-9.49107629, 18.73284471, 58.        ]), 'previousTarget': array([-9.49107629, 18.73284471, 58.        ])}
episode index:18592
target thresh 85.19926426142209
target distance 48.0
model initialize at round 18592
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.75761399, -29.3987308 ,  32.43521531]), 'distance': 27.500000000000004, 'localFrame': array([[-0.40670626,  0.88740741, -0.21702098],
       [-0.90907341, -0.41663597,  0.        ],
       [-0.09041875,  0.19728801,  0.97616694]]), 'currentState': array([-17.03441917, -36.83445976,  10.13874096,  -0.40670626,
         0.88740741,  -0.21702098]), 'targetState': array([ 14.25226165, -20.53954814,  59.        ]), 'previousTarget': array([ -1.80899511, -29.81098762,  33.3001849 ])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6282233505023962
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 14.25226165, -20.53954814,  59.        ]), 'distance': 3.3624828110954317, 'localFrame': array([[ 0.53893418, -0.53976089,  0.64669014],
       [ 0.7076485 ,  0.70656465,  0.        ],
       [-0.45692839,  0.45762931,  0.76275282]]), 'currentState': array([ 13.25745832, -21.90853741,  56.09439759,   0.53893418,
        -0.53976089,   0.64669014]), 'targetState': array([ 14.25226165, -20.53954814,  59.        ]), 'previousTarget': array([ 14.25226165, -20.53954814,  59.        ])}
episode index:18593
target thresh 85.20074426099472
target distance 30.031686331723023
model initialize at round 18593
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.30482623, -7.97328966, 69.82083356]), 'distance': 27.5, 'localFrame': array([[-0.54903024,  0.60525398, -0.57639692],
       [-0.74067095, -0.67186795,  0.        ],
       [-0.38726262,  0.42692045,  0.81716987]]), 'currentState': array([  3.861803  , -34.18751487,  76.10079333,  -0.54903024,
         0.60525398,  -0.57639692]), 'targetState': array([10.01626647, -4.54691169, 69.        ]), 'previousTarget': array([ 9.44247575, -8.29182419, 69.99758967])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6282335518354656
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([10.01626647, -4.54691169, 69.        ]), 'distance': 2.7011433642964864, 'localFrame': array([[-0.60834336,  0.70948825,  0.35573133],
       [-0.75914526, -0.65092125,  0.        ],
       [ 0.23155308, -0.27005175,  0.93458826]]), 'currentState': array([ 8.577203  , -6.41647326, 67.68470105, -0.60834336,  0.70948825,
        0.35573133]), 'targetState': array([10.01626647, -4.54691169, 69.        ]), 'previousTarget': array([10.01626647, -4.54691169, 69.        ])}
episode index:18594
target thresh 85.20222411257483
target distance 18.55902303178864
model initialize at round 18594
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.66314641, -12.20796382,  36.        ]), 'distance': 26.51806975713353, 'localFrame': array([[-0.72035789,  0.48517424, -0.49567173],
       [-0.5586283 , -0.82941812,  0.        ],
       [-0.41111912,  0.27689626,  0.86850995]]), 'currentState': array([ -4.75945242, -29.25088408,  48.64197018,  -0.72035789,
         0.48517424,  -0.49567173]), 'targetState': array([-20.66314641, -12.20796382,  36.        ]), 'previousTarget': array([-20.21578401, -12.70013351,  36.34474907])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6282460188858988
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.66314641, -12.20796382,  36.        ]), 'distance': 3.1333146675660903, 'localFrame': array([[-0.90327027,  0.00664891,  0.42902053],
       [-0.00736073, -0.99997291,  0.        ],
       [ 0.42900891, -0.0031579 ,  0.90329474]]), 'currentState': array([-1.83307613e+01, -1.09646274e+01,  3.76827819e+01, -9.03270267e-01,
        6.64890903e-03,  4.29020532e-01]), 'targetState': array([-20.66314641, -12.20796382,  36.        ]), 'previousTarget': array([-20.66314641, -12.20796382,  36.        ])}
episode index:18595
target thresh 85.20370381617714
target distance 40.884371792746364
model initialize at round 18595
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.87574857,  4.32813085, 37.80859912]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.35045004,  0.83093115, -0.43213215],
       [-0.92140349,  0.38860727,  0.        ],
       [ 0.1679297 ,  0.39816807,  0.90181029]]), 'currentState': array([ 14.38126847, -18.66597334,  51.85176198,   0.35045004,
         0.83093115,  -0.43213215]), 'targetState': array([ 5.03035861, 20.38861183, 28.        ]), 'previousTarget': array([ 9.02974527,  2.77785479, 38.33789075])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6282482089127694
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 5.03035861, 20.38861183, 28.        ]), 'distance': 2.3410592277904487, 'localFrame': array([[-0.5165506 ,  0.6181418 , -0.59251683],
       [-0.76734605, -0.64123322,  0.        ],
       [-0.37994147,  0.45466545,  0.80555807]]), 'currentState': array([ 4.52887789, 18.59775316, 29.42193552, -0.5165506 ,  0.6181418 ,
       -0.59251683]), 'targetState': array([ 5.03035861, 20.38861183, 28.        ]), 'previousTarget': array([ 5.03035861, 20.38861183, 28.        ])}
episode index:18596
target thresh 85.20518337181652
target distance 33.13050275954049
model initialize at round 18596
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.53695144, 20.85483101,  2.51356359]), 'distance': 27.499999999999996, 'localFrame': array([[-0.05423689, -0.69397334, -0.71795499],
       [ 0.99695988, -0.07791654,  0.        ],
       [-0.05594057, -0.71577232,  0.69608953]]), 'currentState': array([-9.32530756, 46.61038446,  8.19185343, -0.05423689, -0.69397334,
       -0.71795499]), 'targetState': array([ 0.53905605, 13.98961824,  1.        ]), 'previousTarget': array([-1.68018722, 21.57925662,  3.06174793])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6282486357169396
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([ 0.53905605, 13.98961824,  1.        ]), 'distance': 1.8877244643339204, 'localFrame': array([[ 0.88324629, -0.3164858 ,  0.34599527],
       [ 0.33731994,  0.94139007,  0.        ],
       [-0.32571651,  0.11671111,  0.93823626]]), 'currentState': array([-0.80185568, 14.32344534,  2.28608665,  0.88324629, -0.3164858 ,
        0.34599527]), 'targetState': array([ 0.53905605, 13.98961824,  1.        ]), 'previousTarget': array([ 0.53905605, 13.98961824,  1.        ])}
episode index:18597
target thresh 85.20666277950771
target distance 77.0
model initialize at round 18597
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.16672677,  26.59333663,  64.64765434]), 'distance': 27.5, 'localFrame': array([[ 0.69379135,  0.25599387, -0.67314241],
       [-0.34616554,  0.93817345,  0.        ],
       [ 0.63152434,  0.2330187 ,  0.73951288]]), 'currentState': array([-27.32939694,  29.69678135,  90.72426161,   0.69379135,
         0.25599387,  -0.67314241]), 'targetState': array([-3.62569327, 20.6846404 , 15.        ]), 'previousTarget': array([-19.73813518,  25.89672239,  65.90595889])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282148552762623
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 63, 'trapConfig': [], 'currentTarget': array([-4.04799586, 20.59715274, 19.78420849]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.38486747,  0.46617326, -0.79659244],
       [-0.77115031,  0.63665312,  0.        ],
       [ 0.50715307,  0.61429251,  0.60451673]]), 'currentState': array([-6.46562084, 20.09629782, 47.17315213,  0.38486747,  0.46617326,
       -0.79659244]), 'targetState': array([-3.62569327, 20.6846404 , 15.        ]), 'previousTarget': array([-4.04799586, 20.59715274, 19.78420849])}
episode index:18598
target thresh 85.20814203926554
target distance 49.08416308070652
model initialize at round 18598
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.10235145,   8.62947404,  61.72525234]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.64615949,  0.51105975,  0.56682965],
       [-0.62034234,  0.78433117,  0.        ],
       [-0.44458216, -0.35162843,  0.82383503]]), 'currentState': array([-38.82661398,  18.33749602,  80.376119  ,   0.64615949,
         0.51105975,   0.56682965]), 'targetState': array([ 9.0467409 , -7.88393804, 30.        ]), 'previousTarget': array([-21.82519094,   8.55224483,  60.81899671])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.628208775501121
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 9.0467409 , -7.88393804, 30.        ]), 'distance': 2.649464270950248, 'localFrame': array([[ 0.03272033, -0.14205442,  0.98931791],
       [ 0.97448349,  0.2244592 ,  0.        ],
       [-0.22206151,  0.96407396,  0.14577407]]), 'currentState': array([ 9.21184641, -7.55139002, 27.37667903,  0.03272033, -0.14205442,
        0.98931791]), 'targetState': array([ 9.0467409 , -7.88393804, 30.        ]), 'previousTarget': array([ 9.0467409 , -7.88393804, 30.        ])}
episode index:18599
target thresh 85.2096211511048
target distance 33.0
model initialize at round 18599
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.19962527, 20.93824166, 75.23673892]), 'distance': 27.500000000000004, 'localFrame': array([[-0.9583603 ,  0.01346998, -0.28524391],
       [-0.01405385, -0.99990124,  0.        ],
       [-0.28521574,  0.00400878,  0.95845496]]), 'currentState': array([ 5.06996280e+00,  3.14391216e+01,  9.95910940e+01, -9.58360303e-01,
        1.34699835e-02, -2.85243912e-01]), 'targetState': array([-4.65822862, 17.38680264, 67.        ]), 'previousTarget': array([-1.54683737, 21.49875734, 76.08423445])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6282172415960299
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.65822862, 17.38680264, 67.        ]), 'distance': 2.0664728715790623, 'localFrame': array([[-0.54281931, -0.81637534, -0.19717633],
       [ 0.83272333, -0.55368932,  0.        ],
       [-0.10917443, -0.16419333,  0.98036804]]), 'currentState': array([-5.20217357, 17.15038324, 68.97953022, -0.54281931, -0.81637534,
       -0.19717633]), 'targetState': array([-4.65822862, 17.38680264, 67.        ]), 'previousTarget': array([-4.65822862, 17.38680264, 67.        ])}
episode index:18600
target thresh 85.21110011504025
target distance 59.09817223324649
model initialize at round 18600
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.57609932,   7.81838768,  70.50297246]), 'distance': 27.5, 'localFrame': array([[ 0.86073322, -0.28792532, -0.41980632],
       [ 0.31723333,  0.94834752,  0.        ],
       [ 0.39812228, -0.13317655,  0.90761372]]), 'currentState': array([-33.37445068,  11.74035976,  85.3725155 ,   0.86073322,
        -0.28792532,  -0.41980632]), 'targetState': array([23.92601495,  1.88303182, 48.        ]), 'previousTarget': array([-12.2542791 ,   7.76179755,  71.2638527 ])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.628218015444774
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([23.92601495,  1.88303182, 48.        ]), 'distance': 3.6657028528062527, 'localFrame': array([[ 0.71832206,  0.56604473, -0.40448335],
       [-0.61893566,  0.78544169,  0.        ],
       [ 0.31769809,  0.25034917,  0.91454536]]), 'currentState': array([21.85089385,  1.10929478, 50.92105817,  0.71832206,  0.56604473,
       -0.40448335]), 'targetState': array([23.92601495,  1.88303182, 48.        ]), 'previousTarget': array([23.92601495,  1.88303182, 48.        ])}
episode index:18601
target thresh 85.21257893108671
target distance 24.0
model initialize at round 18601
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.29872297, -1.52095978, 53.        ]), 'distance': 27.23156264193426, 'localFrame': array([[-0.67228985, -0.65952662, -0.33623057],
       [ 0.70029819, -0.71385044,  0.        ],
       [-0.24001834, -0.23546166,  0.9417797 ]]), 'currentState': array([14.26638845, -0.26154676, 76.9125811 , -0.67228985, -0.65952662,
       -0.33623057]), 'targetState': array([ 1.29872297, -1.52095978, 53.        ]), 'previousTarget': array([ 1.42680257, -1.49294481, 53.22552502])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6282304786390109
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.29872297, -1.52095978, 53.        ]), 'distance': 2.4134088616196743, 'localFrame': array([[ 0.31202365, -0.17029386, -0.93468778],
       [ 0.47906706,  0.87777831,  0.        ],
       [ 0.82044866, -0.44777812,  0.35546977]]), 'currentState': array([ 0.54093533, -0.0958893 , 54.79428937,  0.31202365, -0.17029386,
       -0.93468778]), 'targetState': array([ 1.29872297, -1.52095978, 53.        ]), 'previousTarget': array([ 1.29872297, -1.52095978, 53.        ])}
episode index:18602
target thresh 85.21405759925898
target distance 18.0
model initialize at round 18602
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 38.05172179, -15.26651463,  79.        ]), 'distance': 21.565408345389066, 'localFrame': array([[ 0.01897861, -0.58067048, -0.81391744],
       [ 0.99946631,  0.03266651,  0.        ],
       [ 0.02658784, -0.81348306,  0.58098055]]), 'currentState': array([ 3.33237392e+01, -2.90885430e+00,  9.60294230e+01,  1.89786058e-02,
       -5.80670485e-01, -8.13917441e-01]), 'targetState': array([ 38.05172179, -15.26651463,  79.        ]), 'previousTarget': array([ 38.05172179, -15.26651463,  79.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6282438791950696
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 38.05172179, -15.26651463,  79.        ]), 'distance': 2.2276754134755143, 'localFrame': array([[ 0.5941532 ,  0.80424615,  0.01304223],
       [-0.80431456,  0.59420374,  0.        ],
       [-0.00774974, -0.01049006,  0.99991495]]), 'currentState': array([ 3.63576176e+01, -1.44955160e+01,  8.02239730e+01,  5.94153202e-01,
        8.04246152e-01,  1.30422305e-02]), 'targetState': array([ 38.05172179, -15.26651463,  79.        ]), 'previousTarget': array([ 38.05172179, -15.26651463,  79.        ])}
episode index:18603
target thresh 85.2155361195718
target distance 43.557184330018956
model initialize at round 18603
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.81430302, -14.00090803,  60.47417306]), 'distance': 27.5, 'localFrame': array([[-0.6488024 , -0.04438227, -0.75966155],
       [ 0.06824695, -0.99766846,  0.        ],
       [-0.75789036, -0.05184459,  0.65031864]]), 'currentState': array([ 1.34284330e+01, -4.29492252e+00,  6.54589622e+01, -6.48802398e-01,
       -4.43822661e-02, -7.59661545e-01]), 'targetState': array([-29.40735065, -20.76554183,  57.        ]), 'previousTarget': array([-11.1223194 , -14.13810368,  61.19793693])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6282519192660331
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.40735065, -20.76554183,  57.        ]), 'distance': 3.124032083102466, 'localFrame': array([[-0.19464203, -0.75699543, -0.62375669],
       [ 0.96849723, -0.24902431,  0.        ],
       [-0.15533058, -0.60410663,  0.78161857]]), 'currentState': array([-26.86612538, -18.95162004,  57.10695013,  -0.19464203,
        -0.75699543,  -0.62375669]), 'targetState': array([-29.40735065, -20.76554183,  57.        ]), 'previousTarget': array([-29.40735065, -20.76554183,  57.        ])}
episode index:18604
target thresh 85.21701449203998
target distance 32.499704831916496
model initialize at round 18604
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.26277279, 20.17468106, 57.48531537]), 'distance': 27.5, 'localFrame': array([[-0.18492382,  0.87154384,  0.45410849],
       [-0.97822253, -0.20755886,  0.        ],
       [ 0.09425424, -0.44421916,  0.89094639]]), 'currentState': array([-3.19577466, -1.81792226, 56.18469442, -0.18492382,  0.87154384,
        0.45410849]), 'targetState': array([19.77578608, 28.87764334, 58.        ]), 'previousTarget': array([12.52110776, 18.72483134, 57.37520589])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6282555828671501
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([19.77578608, 28.87764334, 58.        ]), 'distance': 3.948206698217693, 'localFrame': array([[ 0.7971705 ,  0.37279303, -0.47491532],
       [-0.42361328,  0.90584314,  0.        ],
       [ 0.43019878,  0.20118043,  0.8800315 ]]), 'currentState': array([21.52679698, 26.83142841, 60.88709221,  0.7971705 ,  0.37279303,
       -0.47491532]), 'targetState': array([19.77578608, 28.87764334, 58.        ]), 'previousTarget': array([19.77578608, 28.87764334, 58.        ])}
episode index:18605
target thresh 85.21849271667831
target distance 85.0
model initialize at round 18605
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.30561821, 11.95924834, 27.85987377]), 'distance': 27.5, 'localFrame': array([[-0.72095639,  0.16046492,  0.67414605],
       [-0.21725608, -0.97611464,  0.        ],
       [ 0.65804383, -0.14646233,  0.73859807]]), 'currentState': array([36.35293781,  6.66804237,  4.8180433 , -0.72095639,  0.16046492,
        0.67414605]), 'targetState': array([-14.96810855,  25.99914857,  89.        ]), 'previousTarget': array([23.4163977 , 11.09620464, 26.91748611])}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.6282489522268148
{'scaleFactor': 20, 'timeStep': 69, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-14.96810855,  25.99914857,  89.        ]), 'distance': 3.084269852235241, 'localFrame': array([[-0.10196359,  0.7430055 ,  0.6614728 ],
       [-0.99071473, -0.13595704,  0.        ],
       [ 0.08993188, -0.65533085,  0.74996916]]), 'currentState': array([-12.20764932,  26.64204163,  87.78374597,  -0.10196359,
         0.7430055 ,   0.6614728 ]), 'targetState': array([-14.96810855,  25.99914857,  89.        ]), 'previousTarget': array([-14.96810855,  25.99914857,  89.        ])}
episode index:18606
target thresh 85.21997079350157
target distance 33.0
model initialize at round 18606
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.18896908,   0.7686496 ,  28.63296152]), 'distance': 27.500000000000004, 'localFrame': array([[-0.73867843, -0.36779684,  0.56487137],
       [ 0.44571766, -0.8951736 ,  0.        ],
       [ 0.50565794,  0.25177314,  0.82517897]]), 'currentState': array([ 7.65596827,  3.89087984,  8.84988757, -0.73867843, -0.36779684,
        0.56487137]), 'targetState': array([-22.96954807,  -1.18315736,  41.        ]), 'previousTarget': array([-10.30073858,   1.4254568 ,  27.70835146])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.628242050957275
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([-22.96954807,  -1.18315736,  41.        ]), 'distance': 3.5989888513094503, 'localFrame': array([[-0.31893849,  0.24811405,  0.91472272],
       [-0.61401891, -0.78929132,  0.        ],
       [ 0.7219827 , -0.56165705,  0.4040821 ]]), 'currentState': array([-21.58999522,  -3.57323431,  38.6897864 ,  -0.31893849,
         0.24811405,   0.91472272]), 'targetState': array([-22.96954807,  -1.18315736,  41.        ]), 'previousTarget': array([-22.96954807,  -1.18315736,  41.        ])}
episode index:18607
target thresh 85.22144872252454
target distance 41.4781830508784
model initialize at round 18607
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.13326187, -2.45641238, 56.59450777]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.9259206 , -0.20766526,  0.31550939],
       [ 0.21884324,  0.97576003,  0.        ],
       [-0.30786145,  0.06904709,  0.94892246]]), 'currentState': array([-12.32091949,   5.01109015,  55.78044718,   0.9259206 ,
        -0.20766526,   0.31550939]), 'targetState': array([27.3103707 , -6.17605473, 57.        ]), 'previousTarget': array([12.38974208, -2.18212   , 56.64027767])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6282509381150936
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.3103707 , -6.17605473, 57.        ]), 'distance': 2.6412731285573976, 'localFrame': array([[ 0.58347897,  0.63722723, -0.50348162],
       [-0.73752645,  0.67531825,  0.        ],
       [ 0.34001033,  0.37133101,  0.86400593]]), 'currentState': array([25.18009972, -5.97247857, 58.54816861,  0.58347897,  0.63722723,
       -0.50348162]), 'targetState': array([27.3103707 , -6.17605473, 57.        ]), 'previousTarget': array([27.3103707 , -6.17605473, 57.        ])}
episode index:18608
target thresh 85.222926503762
target distance 33.589183632295075
model initialize at round 18608
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.8029608 , -0.95660001, 40.95177263]), 'distance': 27.5, 'localFrame': array([[-0.71339672, -0.52505803, -0.46408963],
       [ 0.59275796, -0.80538066,  0.        ],
       [-0.37376881, -0.27509282,  0.88578824]]), 'currentState': array([ 39.26897382, -17.20734842,  46.5529744 ,  -0.71339672,
        -0.52505803,  -0.46408963]), 'targetState': array([ 6.49060458,  7.60736829, 38.        ]), 'previousTarget': array([17.97822672, -0.32141542, 41.07803251])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.628259824317766
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.49060458,  7.60736829, 38.        ]), 'distance': 2.741781753215267, 'localFrame': array([[-0.85328928,  0.36214546,  0.37516405],
       [-0.39068145, -0.92052594,  0.        ],
       [ 0.34534824, -0.14656963,  0.92695843]]), 'currentState': array([ 9.12849129,  6.91757601, 38.28828371, -0.85328928,  0.36214546,
        0.37516405]), 'targetState': array([ 6.49060458,  7.60736829, 38.        ]), 'previousTarget': array([ 6.49060458,  7.60736829, 38.        ])}
episode index:18609
target thresh 85.22440413722872
target distance 55.80366234122317
model initialize at round 18609
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.04815704, -2.89446798, 49.98594063]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.15976507, -0.98475347, -0.06881667],
       [ 0.98709355,  0.16014472,  0.        ],
       [ 0.01102063, -0.06792849,  0.99762932]]), 'currentState': array([17.41956277, 17.34390234, 52.35273   ,  0.15976507, -0.98475347,
       -0.06881667]), 'targetState': array([-32.14988307, -36.97816948,  46.        ]), 'previousTarget': array([-1.5305448 , -1.82020428, 50.41020797])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628226065057996
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([-4.19960571, -4.0575255 , 49.65406421]), 'distance': 27.5, 'localFrame': array([[-0.23276475,  0.87386924, -0.42681744],
       [-0.96630842, -0.25738695,  0.        ],
       [-0.10985724,  0.41243728,  0.90433781]]), 'currentState': array([13.53542027, 16.83130035, 51.97264312, -0.23276475,  0.87386924,
       -0.42681744]), 'targetState': array([-32.14988307, -36.97816948,  46.        ]), 'previousTarget': array([-4.19960571, -4.0575255 , 49.65406421])}
episode index:18610
target thresh 85.22588162293947
target distance 59.0
model initialize at round 18610
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.5514179 , -13.75594682,  50.60248157]), 'distance': 27.5, 'localFrame': array([[-0.55791215,  0.22888532, -0.79771269],
       [-0.37955393, -0.92516961,  0.        ],
       [-0.73801954,  0.30277499,  0.60303769]]), 'currentState': array([18.00749847, -0.08608133, 71.53434713, -0.55791215,  0.22888532,
       -0.79771269]), 'targetState': array([-13.48124364, -37.6597407 ,  14.        ]), 'previousTarget': array([  7.45086958, -13.12602528,  52.02683936])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281923094261085
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 55, 'trapConfig': [], 'currentTarget': array([-13.48124364, -37.6597407 ,  14.        ]), 'distance': 13.30017763110967, 'localFrame': array([[-0.89226179, -0.28676402, -0.34876253],
       [ 0.3059759 , -0.95203926,  0.        ],
       [-0.33203562, -0.10671293,  0.93721113]]), 'currentState': array([-11.47036938, -30.49822895,  25.02560017,  -0.89226179,
        -0.28676402,  -0.34876253]), 'targetState': array([-13.48124364, -37.6597407 ,  14.        ]), 'previousTarget': array([-13.48124364, -37.6597407 ,  14.        ])}
episode index:18611
target thresh 85.22735896090904
target distance 13.583557109192336
model initialize at round 18611
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.1688814 , 11.44311594,  2.        ]), 'distance': 16.07274335477157, 'localFrame': array([[-0.71543715,  0.07660263, -0.69446506],
       [-0.10646257, -0.99431671,  0.        ],
       [-0.69051822,  0.07393454,  0.71952642]]), 'currentState': array([ 6.4250307 ,  8.7634647 , 11.62007934, -0.71543715,  0.07660263,
       -0.69446506]), 'targetState': array([-6.1688814 , 11.44311594,  2.        ]), 'previousTarget': array([-6.1688814 , 11.44311594,  2.        ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6282025025610843
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([-6.1688814 , 11.44311594,  2.        ]), 'distance': 3.7514020798836225, 'localFrame': array([[-0.75217005, -0.63258904,  0.18458418],
       [ 0.64364905, -0.76532078,  0.        ],
       [ 0.14126611,  0.11880744,  0.98281671]]), 'currentState': array([-3.28000932, 10.05519995,  0.05035263, -0.75217005, -0.63258904,
        0.18458418]), 'targetState': array([-6.1688814 , 11.44311594,  2.        ]), 'previousTarget': array([-6.1688814 , 11.44311594,  2.        ])}
episode index:18612
target thresh 85.22883615115222
target distance 56.5005934031135
model initialize at round 18612
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.73425684, -10.90940593,  49.68154939]), 'distance': 27.5, 'localFrame': array([[ 0.38526765, -0.69218429,  0.61028661],
       [ 0.87377071,  0.4863381 ,  0.        ],
       [-0.29680563,  0.53325057,  0.79218069]]), 'currentState': array([18.90876211, 14.01067086, 41.41007817,  0.38526765, -0.69218429,
        0.61028661]), 'targetState': array([  0.53676879, -41.99656985,  60.        ]), 'previousTarget': array([ 10.2184891 , -10.4014654 ,  48.81600969])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.628202931453982
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([  0.53676879, -41.99656985,  60.        ]), 'distance': 3.0634514705509703, 'localFrame': array([[ 0.3526419 , -0.82440788,  0.44271362],
       [ 0.91941768,  0.3932825 ,  0.        ],
       [-0.17411152,  0.40703873,  0.89666307]]), 'currentState': array([  1.39872192, -40.64612887,  57.38885449,   0.3526419 ,
        -0.82440788,   0.44271362]), 'targetState': array([  0.53676879, -41.99656985,  60.        ]), 'previousTarget': array([  0.53676879, -41.99656985,  60.        ])}
episode index:18613
target thresh 85.23031319368374
target distance 57.556697928454014
model initialize at round 18613
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.8772011 ,  -4.78724199,  67.81955869]), 'distance': 27.5, 'localFrame': array([[-0.86650355, -0.27810116,  0.41452545],
       [ 0.30559301, -0.95216223,  0.        ],
       [ 0.39469547,  0.12667608,  0.91003772]]), 'currentState': array([  8.13358553, -17.04623274,  62.39284039,  -0.86650355,
        -0.27810116,   0.41452545]), 'targetState': array([-47.64742287,  11.43342003,  75.        ]), 'previousTarget': array([-14.19596245,  -5.30837606,  67.44451122])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6282033603007967
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-47.64742287,  11.43342003,  75.        ]), 'distance': 3.3683941388147436, 'localFrame': array([[-0.7683834 ,  0.4531407 , -0.45194077],
       [-0.50797797, -0.86137006,  0.        ],
       [-0.38928825,  0.22957595,  0.89204795]]), 'currentState': array([-46.40453108,  10.10675948,  72.16428653,  -0.7683834 ,
         0.4531407 ,  -0.45194077]), 'targetState': array([-47.64742287,  11.43342003,  75.        ]), 'previousTarget': array([-47.64742287,  11.43342003,  75.        ])}
episode index:18614
target thresh 85.2317900885184
target distance 56.528693569540955
model initialize at round 18614
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.72976294, -11.53050964,  66.63996294]), 'distance': 27.5, 'localFrame': array([[-0.79147659, -0.16052125,  0.58974378],
       [ 0.19876568, -0.98004704,  0.        ],
       [ 0.57797665,  0.11722082,  0.80759041]]), 'currentState': array([  5.80017094, -36.47800527,  77.04012756,  -0.79147659,
        -0.16052125,   0.58974378]), 'targetState': array([-5.92014885, 21.18848361, 53.        ]), 'previousTarget': array([  1.22384553, -10.40877771,  65.85607299])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6282070245424163
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.92014885, 21.18848361, 53.        ]), 'distance': 2.8375931817020152, 'localFrame': array([[-0.44408704,  0.89413123,  0.05758501],
       [-0.89561742, -0.44482518,  0.        ],
       [ 0.02561526, -0.05157414,  0.99834061]]), 'currentState': array([-4.47599728, 18.81399118, 53.5728412 , -0.44408704,  0.89413123,
        0.05758501]), 'targetState': array([-5.92014885, 21.18848361, 53.        ]), 'previousTarget': array([-5.92014885, 21.18848361, 53.        ])}
episode index:18615
target thresh 85.23326683567096
target distance 38.0
model initialize at round 18615
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.58785656, 15.76153132, 40.48374732]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.68088652,  0.21416491,  0.70037628],
       [-0.30004591,  0.95392476,  0.        ],
       [-0.66810628, -0.21014504,  0.71377382]]), 'currentState': array([ 0.75665659,  1.15530491, 18.20723824,  0.68088652,  0.21416491,
        0.70037628]), 'targetState': array([12.03933613, 25.27952503, 55.        ]), 'previousTarget': array([ 7.01419185, 14.72800797, 39.13903553])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281732789996282
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 68, 'trapConfig': [], 'currentTarget': array([12.03933613, 25.27952503, 55.        ]), 'distance': 13.985352529067173, 'localFrame': array([[ 0.01135358,  0.36803754,  0.92974161],
       [-0.99952451,  0.0308343 ,  0.        ],
       [-0.02866793, -0.92929953,  0.36821262]]), 'currentState': array([8.45166487e+00, 2.73877926e+01, 4.16480747e+01, 1.13535777e-02,
       3.68037538e-01, 9.29741613e-01]), 'targetState': array([12.03933613, 25.27952503, 55.        ]), 'previousTarget': array([12.03933613, 25.27952503, 55.        ])}
episode index:18616
target thresh 85.23474343515619
target distance 46.51738166795301
model initialize at round 18616
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.24001206,  8.63161266, 83.00088653]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69444399, -0.1857175 , -0.69516656],
       [ 0.25835409,  0.96605029,  0.        ],
       [ 0.67156586, -0.17959913,  0.7188487 ]]), 'currentState': array([-24.40544193,  23.01402109,  90.62256918,   0.69444399,
        -0.1857175 ,  -0.69516656]), 'targetState': array([21.02822113, -6.46636808, 75.        ]), 'previousTarget': array([-3.19356777,  8.61714682, 83.8519688 ])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6281655880768842
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([21.02822113, -6.46636808, 75.        ]), 'distance': 1.6640774812514554, 'localFrame': array([[ 0.41201394, -0.24166931, -0.87854451],
       [ 0.50594369,  0.86256651,  0.        ],
       [ 0.75780307, -0.44449405,  0.47766049]]), 'currentState': array([20.83354395, -7.32195631, 76.41393897,  0.41201394, -0.24166931,
       -0.87854451]), 'targetState': array([21.02822113, -6.46636808, 75.        ]), 'previousTarget': array([21.02822113, -6.46636808, 75.        ])}
episode index:18617
target thresh 85.23621988698885
target distance 71.0
model initialize at round 18617
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.39230788, -6.79754108, 34.88549831]), 'distance': 27.499999999999996, 'localFrame': array([[-0.47573705, -0.57710724, -0.66379326],
       [ 0.77162028, -0.63608344,  0.        ],
       [-0.4222279 , -0.51219634,  0.74791611]]), 'currentState': array([-1.67895966e+00,  8.51339167e-03,  8.55391265e+00, -4.75737054e-01,
       -5.77107239e-01, -6.63793259e-01]), 'targetState': array([  9.52231806, -18.71698316,  81.        ]), 'previousTarget': array([ 2.46760676, -6.19407938, 36.18608495])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6281592404647977
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.52231806, -18.71698316,  81.        ]), 'distance': 2.9794274497390965, 'localFrame': array([[ 0.33306619, -0.7620211 ,  0.55532941],
       [ 0.91629771,  0.40049781,  0.        ],
       [-0.22240821,  0.50884707,  0.83163047]]), 'currentState': array([  9.1202983 , -17.7448423 ,  78.21247239,   0.33306619,
        -0.7620211 ,   0.55532941]), 'targetState': array([  9.52231806, -18.71698316,  81.        ]), 'previousTarget': array([  9.52231806, -18.71698316,  81.        ])}
episode index:18618
target thresh 85.23769619118372
target distance 44.0
model initialize at round 18618
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.18689642, 18.5278696 , 78.05816489]), 'distance': 27.5, 'localFrame': array([[ 0.36290672,  0.38535001,  0.84841269],
       [-0.72798842,  0.68558942,  0.        ],
       [-0.58166276, -0.61763461,  0.52933535]]), 'currentState': array([-6.74822119, 26.78345106, 54.69904799,  0.36290672,  0.38535001,
        0.84841269]), 'targetState': array([14.86504611, 11.83344431, 97.        ]), 'previousTarget': array([ 4.44847319, 18.91555602, 76.44700054])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6281656324758546
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([14.86504611, 11.83344431, 97.        ]), 'distance': 3.166123965210226, 'localFrame': array([[ 0.740572  ,  0.08681394,  0.66634559],
       [-0.11642829,  0.9931991 ,  0.        ],
       [-0.66181384, -0.07758148,  0.74564305]]), 'currentState': array([1.34263433e+01, 1.31666878e+01, 9.45146555e+01, 7.40572003e-01,
       8.68139437e-02, 6.66345592e-01]), 'targetState': array([14.86504611, 11.83344431, 97.        ]), 'previousTarget': array([14.86504611, 11.83344431, 97.        ])}
episode index:18619
target thresh 85.23917234775553
target distance 29.0
model initialize at round 18619
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.15520311, -37.70809744,  34.09535898]), 'distance': 27.5, 'localFrame': array([[ 0.05311549,  0.41294297, -0.90920671],
       [-0.99182884,  0.12757566,  0.        ],
       [ 0.11599264,  0.90177744,  0.416345  ]]), 'currentState': array([  9.59248214, -31.31080093,   7.58815965,   0.05311549,
         0.41294297,  -0.90920671]), 'targetState': array([ 13.68000945, -38.65045073,  38.        ]), 'previousTarget': array([ 13.40795368, -38.15810365,  35.70765445])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6281674647212094
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([ 13.68000945, -38.65045073,  38.        ]), 'distance': 3.8175923363559465, 'localFrame': array([[ 0.64297896, -0.75672649, -0.11808077],
       [ 0.76205786,  0.64750894,  0.        ],
       [ 0.07645835, -0.08998438,  0.99300399]]), 'currentState': array([ 10.77565452, -38.59129739,  35.52305943,   0.64297896,
        -0.75672649,  -0.11808077]), 'targetState': array([ 13.68000945, -38.65045073,  38.        ]), 'previousTarget': array([ 13.68000945, -38.65045073,  38.        ])}
episode index:18620
target thresh 85.24064835671908
target distance 47.220954544016024
model initialize at round 18620
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.88385947, -18.57030526,  26.3810307 ]), 'distance': 27.5, 'localFrame': array([[ 0.46523482, -0.8093428 ,  0.35849798],
       [ 0.86696985,  0.49836059,  0.        ],
       [-0.17866126,  0.31080694,  0.9335305 ]]), 'currentState': array([-41.92102181,   4.1689588 ,  22.76722126,   0.46523482,
        -0.8093428 ,   0.35849798]), 'targetState': array([-11.82522681, -41.34203685,  30.        ]), 'previousTarget': array([-27.69698544, -16.98534974,  26.3893824 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281337303640471
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 67, 'trapConfig': [], 'currentTarget': array([-11.82522681, -41.34203685,  30.        ]), 'distance': 11.331386701725242, 'localFrame': array([[ 0.7128811 , -0.39787736, -0.57748952],
       [ 0.48735704,  0.87320279,  0.        ],
       [ 0.50426546, -0.28144358,  0.8163981 ]]), 'currentState': array([-22.35775675, -38.18421413,  27.2624293 ,   0.7128811 ,
        -0.39787736,  -0.57748952]), 'targetState': array([-11.82522681, -41.34203685,  30.        ]), 'previousTarget': array([-11.82522681, -41.34203685,  30.        ])}
episode index:18621
target thresh 85.24212421808912
target distance 40.0
model initialize at round 18621
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.82862963, 13.12416162, 68.44995611]), 'distance': 27.499999999999996, 'localFrame': array([[-0.1451549 , -0.82301012,  0.549167  ],
       [ 0.98480038, -0.17368997,  0.        ],
       [ 0.0953848 ,  0.54081987,  0.83571263]]), 'currentState': array([35.75640447, 25.11571518, 86.50274211, -0.1451549 , -0.82301012,
        0.549167  ]), 'targetState': array([-3.1599752 , -2.45245933, 45.        ]), 'previousTarget': array([18.42195881, 13.55947963, 67.39470675])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6281373969674025
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.1599752 , -2.45245933, 45.        ]), 'distance': 2.8755847357211763, 'localFrame': array([[-0.9678057 , -0.20177179,  0.15046688],
       [ 0.2040954 , -0.978951  ,  0.        ],
       [ 0.1472997 ,  0.0307096 ,  0.98861505]]), 'currentState': array([-1.35275098, -0.5998951 , 46.25336905, -0.9678057 , -0.20177179,
        0.15046688]), 'targetState': array([-3.1599752 , -2.45245933, 45.        ]), 'previousTarget': array([-3.1599752 , -2.45245933, 45.        ])}
episode index:18622
target thresh 85.24359993188038
target distance 33.0
model initialize at round 18622
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.8783356 ,  -2.72721163,  23.75311455]), 'distance': 27.499999999999996, 'localFrame': array([[-0.72128561,  0.43123771, -0.54201578],
       [-0.51315322, -0.85829702,  0.        ],
       [-0.46521053,  0.27813715,  0.84036831]]), 'currentState': array([-1.29537506,  4.36639957, 43.67220536, -0.72128561,  0.43123771,
       -0.54201578]), 'targetState': array([-30.13575251,  -7.26886652,  11.        ]), 'previousTarget': array([-18.15698298,  -3.17086877,  24.1727582 ])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.628142207853706
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.13575251,  -7.26886652,  11.        ]), 'distance': 2.4683419547968803, 'localFrame': array([[ 0.57626111, -0.46205205, -0.674115  ],
       [ 0.62555582,  0.78017941,  0.        ],
       [ 0.52593065, -0.42169656,  0.7386264 ]]), 'currentState': array([-28.89152608,  -6.90551411,  13.10061601,   0.57626111,
        -0.46205205,  -0.674115  ]), 'targetState': array([-30.13575251,  -7.26886652,  11.        ]), 'previousTarget': array([-30.13575251,  -7.26886652,  11.        ])}
episode index:18623
target thresh 85.24507549810765
target distance 36.144904359834726
model initialize at round 18623
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.69678429, 20.2125301 , 97.68011053]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.63676653,  0.74847088, -0.18525584],
       [-0.76165488,  0.64798291,  0.        ],
       [ 0.12004262,  0.14110102,  0.98269032]]), 'currentState': array([17.34067883, -7.26070012, 96.52045111,  0.63676653,  0.74847088,
       -0.18525584]), 'targetState': array([17.79501519, 27.79095958, 98.        ]), 'previousTarget': array([17.34831928, 19.06822851, 97.51734657])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6281494136369153
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([17.79501519, 27.79095958, 98.        ]), 'distance': 2.0944618662700685, 'localFrame': array([[ 0.3738024 ,  0.82865016,  0.41666614],
       [-0.91154659,  0.41119681,  0.        ],
       [-0.17133179, -0.3798106 ,  0.90905959]]), 'currentState': array([18.4079056 , 26.05707101, 99.0023803 ,  0.3738024 ,  0.82865016,
        0.41666614]), 'targetState': array([17.79501519, 27.79095958, 98.        ]), 'previousTarget': array([17.79501519, 27.79095958, 98.        ])}
episode index:18624
target thresh 85.24655091678568
target distance 10.365741793734005
model initialize at round 18624
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.89102909, -9.79996484, 63.        ]), 'distance': 12.390379032637062, 'localFrame': array([[ 0.68147956,  0.66829592, -0.29827198],
       [-0.70016688,  0.71397923,  0.        ],
       [ 0.21296   ,  0.20884016,  0.95448092]]), 'currentState': array([  6.66192294, -19.16242705,  66.68860655,   0.68147956,
         0.66829592,  -0.29827198]), 'targetState': array([13.89102909, -9.79996484, 63.        ]), 'previousTarget': array([13.89102909, -9.79996484, 63.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6281667473623522
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.89102909, -9.79996484, 63.        ]), 'distance': 3.8237306296966675, 'localFrame': array([[ 0.69385407,  0.67461148, -0.25192435],
       [-0.69709493,  0.71697884,  0.        ],
       [ 0.18062443,  0.17561519,  0.96774693]]), 'currentState': array([ 11.56402421, -11.85263716,  65.23439043,   0.69385407,
         0.67461148,  -0.25192435]), 'targetState': array([13.89102909, -9.79996484, 63.        ]), 'previousTarget': array([13.89102909, -9.79996484, 63.        ])}
episode index:18625
target thresh 85.24802618792921
target distance 51.0
model initialize at round 18625
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.37753142, -3.25142517, 55.4648788 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.56795303,  0.24848446,  0.78465587],
       [-0.40082562, -0.91615437,  0.        ],
       [ 0.7188659 , -0.31451018,  0.61993158]]), 'currentState': array([17.36442994, -9.39545416, 31.01515551, -0.56795303,  0.24848446,
        0.78465587]), 'targetState': array([-5.09710909,  3.16535605, 81.        ]), 'previousTarget': array([ 7.45267679, -3.30071428, 54.30902321])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.628170411405662
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.09710909,  3.16535605, 81.        ]), 'distance': 1.6219808098868653, 'localFrame': array([[-0.10726496, -0.0593597 ,  0.99245688],
       [ 0.48419654, -0.87495926,  0.        ],
       [ 0.86835934,  0.48054419,  0.12259423]]), 'currentState': array([-5.10170631e+00,  3.36822238e+00,  7.93907623e+01, -1.07264961e-01,
       -5.93597043e-02,  9.92456878e-01]), 'targetState': array([-5.09710909,  3.16535605, 81.        ]), 'previousTarget': array([-5.09710909,  3.16535605, 81.        ])}
episode index:18626
target thresh 85.249501311553
target distance 32.8575977559063
model initialize at round 18626
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.97420709,  16.36751501,  65.24035871]), 'distance': 27.499999999999996, 'localFrame': array([[-0.08952501,  0.43829728,  0.89436054],
       [-0.9797705 , -0.20012438,  0.        ],
       [ 0.17898334, -0.87626807,  0.44734688]]), 'currentState': array([-4.54265897, -2.52524691, 55.47026775, -0.08952501,  0.43829728,
        0.89436054]), 'targetState': array([-34.03458736,  29.43886654,  72.        ]), 'previousTarget': array([-20.97125201,  15.31295369,  64.26156357])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6281763989485826
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.03458736,  29.43886654,  72.        ]), 'distance': 2.6770923701168674, 'localFrame': array([[-0.18767947,  0.77310536,  0.605875  ],
       [-0.97177523, -0.23590868,  0.        ],
       [ 0.14293117, -0.58877432,  0.79555985]]), 'currentState': array([-31.9327937 ,  29.02790512,  70.39363838,  -0.18767947,
         0.77310536,   0.605875  ]), 'targetState': array([-34.03458736,  29.43886654,  72.        ]), 'previousTarget': array([-34.03458736,  29.43886654,  72.        ])}
episode index:18627
target thresh 85.25097628767182
target distance 21.407196121637604
model initialize at round 18627
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.14467232,  20.08027863,  97.        ]), 'distance': 25.305691616738137, 'localFrame': array([[ 0.46766247, -0.43904173,  0.76715981],
       [ 0.68444556,  0.72906397,  0.        ],
       [-0.55930858,  0.52507913,  0.64145602]]), 'currentState': array([-34.58372096,  21.17224535,  83.60026754,   0.46766247,
        -0.43904173,   0.76715981]), 'targetState': array([-13.14467232,  20.08027863,  97.        ]), 'previousTarget': array([-13.14467232,  20.08027863,  97.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6281893133470055
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.14467232,  20.08027863,  97.        ]), 'distance': 1.5921158630110752, 'localFrame': array([[ 0.89905257, -0.22662786, -0.37462553],
       [ 0.24442804,  0.96966744,  0.        ],
       [ 0.36326217, -0.09156898,  0.92717621]]), 'currentState': array([-14.40714865,  20.97920382,  96.63541818,   0.89905257,
        -0.22662786,  -0.37462553]), 'targetState': array([-13.14467232,  20.08027863,  97.        ]), 'previousTarget': array([-13.14467232,  20.08027863,  97.        ])}
episode index:18628
target thresh 85.25245111630039
target distance 60.0
model initialize at round 18628
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.79823855, -10.1959903 ,  38.82287305]), 'distance': 27.5, 'localFrame': array([[ 0.70542262, -0.40899435, -0.57888042],
       [ 0.50157973,  0.86511142,  0.        ],
       [ 0.50079607, -0.29035469,  0.81541244]]), 'currentState': array([-25.96662672,  -9.89192262,  63.48231085,   0.70542262,
        -0.40899435,  -0.57888042]), 'targetState': array([  2.89191683, -10.61304938,   5.        ]), 'previousTarget': array([-14.40906852,  -9.67941273,  40.31173671])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.628191502538332
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.89191683, -10.61304938,   5.        ]), 'distance': 3.4841827384468407, 'localFrame': array([[ 0.34645364, -0.82779965,  0.44126819],
       [ 0.92246764,  0.38607442,  0.        ],
       [-0.17036236,  0.40705563,  0.89737527]]), 'currentState': array([ 0.01672575, -8.98424361,  6.10444439,  0.34645364, -0.82779965,
        0.44126819]), 'targetState': array([  2.89191683, -10.61304938,   5.        ]), 'previousTarget': array([  2.89191683, -10.61304938,   5.        ])}
episode index:18629
target thresh 85.25392579745348
target distance 67.0
model initialize at round 18629
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.18652658, 10.0014547 , 44.93645189]), 'distance': 27.5, 'localFrame': array([[-0.81543968, -0.01970459,  0.57850658],
       [ 0.02415732, -0.99970817,  0.        ],
       [ 0.57833776,  0.01397517,  0.81567771]]), 'currentState': array([10.26110133, 16.58862026, 18.54976792, -0.81543968, -0.01970459,
        0.57850658]), 'targetState': array([-0.,  0., 85.]), 'previousTarget': array([ 7.19627676, 10.55546248, 44.24122093])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.628194054204812
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.55111512e-17,  0.00000000e+00,  8.50000000e+01]), 'distance': 2.5431317658333636, 'localFrame': array([[-0.42295452, -0.80920318,  0.40779859],
       [ 0.88624255, -0.46322148,  0.        ],
       [ 0.18890107,  0.36140846,  0.91307191]]), 'currentState': array([-0.3199016 ,  1.26826385, 82.81901651, -0.42295452, -0.80920318,
        0.40779859]), 'targetState': array([-0.,  0., 85.]), 'previousTarget': array([ 0.,  0., 85.])}
episode index:18630
target thresh 85.25540033114582
target distance 45.69465095877832
model initialize at round 18630
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.20185042, 17.25037038, 53.88199174]), 'distance': 27.5, 'localFrame': array([[-0.57756459,  0.78466055, -0.22522648],
       [-0.80535294, -0.59279562,  0.        ],
       [-0.13351327,  0.18138681,  0.97430644]]), 'currentState': array([-9.65118949, -2.41725303, 70.38508822, -0.57756459,  0.78466055,
       -0.22522648]), 'targetState': array([12.66927423, 42.13655765, 33.        ]), 'previousTarget': array([ 0.95798778, 16.57574014, 53.69717632])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6281948280520494
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([12.66927423, 42.13655765, 33.        ]), 'distance': 3.2700682849005656, 'localFrame': array([[ 0.69052659,  0.72036294, -0.06519412],
       [-0.7218987 ,  0.69199874,  0.        ],
       [ 0.04511425,  0.04706355,  0.9978726 ]]), 'currentState': array([10.08413112, 40.22233079, 33.5883173 ,  0.69052659,  0.72036294,
       -0.06519412]), 'targetState': array([12.66927423, 42.13655765, 33.        ]), 'previousTarget': array([12.66927423, 42.13655765, 33.        ])}
episode index:18631
target thresh 85.25687471739217
target distance 9.061575053331783
model initialize at round 18631
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.61339748, 15.66087398, 27.        ]), 'distance': 11.227579960619916, 'localFrame': array([[ 0.08619879,  0.91443278,  0.39545221],
       [-0.99558647,  0.09384872,  0.        ],
       [-0.03711269, -0.39370687,  0.91848655]]), 'currentState': array([-16.4288014 ,  15.56966586,  21.54948821,   0.08619879,
         0.91443278,   0.39545221]), 'targetState': array([-6.61339748, 15.66087398, 27.        ]), 'previousTarget': array([-6.61339748, 15.66087398, 27.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6282121528278034
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.61339748, 15.66087398, 27.        ]), 'distance': 3.4515067493188774, 'localFrame': array([[ 0.80567029,  0.59177571, -0.02639879],
       [-0.59198202,  0.80595117,  0.        ],
       [ 0.02127614,  0.01562761,  0.99965149]]), 'currentState': array([-9.37141763, 15.60247984, 24.92567751,  0.80567029,  0.59177571,
       -0.02639879]), 'targetState': array([-6.61339748, 15.66087398, 27.        ]), 'previousTarget': array([-6.61339748, 15.66087398, 27.        ])}
episode index:18632
target thresh 85.25834895620726
target distance 68.0
model initialize at round 18632
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.89487514, -14.64417304,  28.99889408]), 'distance': 27.500000000000004, 'localFrame': array([[-0.67954892,  0.42702422,  0.59654302],
       [-0.53206364, -0.84670437,  0.        ],
       [ 0.50509558, -0.31739885,  0.8025811 ]]), 'currentState': array([  0.3758188 , -11.4732445 ,   1.7768596 ,  -0.67954892,
         0.42702422,   0.59654302]), 'targetState': array([ -5.23152026, -19.30365758,  69.        ]), 'previousTarget': array([ -1.41874432, -15.49764195,  28.26263964])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6282063635490948
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ -5.23152026, -19.30365758,  69.        ]), 'distance': 2.150965742769856, 'localFrame': array([[-0.90394966, -0.32701937,  0.27556006],
       [ 0.34019022, -0.94035664,  0.        ],
       [ 0.25912473,  0.09374284,  0.96128386]]), 'currentState': array([ -5.30061356, -18.74092283,  66.92510016,  -0.90394966,
        -0.32701937,   0.27556006]), 'targetState': array([ -5.23152026, -19.30365758,  69.        ]), 'previousTarget': array([ -5.23152026, -19.30365758,  69.        ])}
episode index:18633
target thresh 85.25982304760585
target distance 39.35633700264064
model initialize at round 18633
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.86958087, 11.40474082, 15.48134025]), 'distance': 27.500000000000004, 'localFrame': array([[-0.06500937,  0.76947296,  0.63536222],
       [-0.99645009, -0.08418566,  0.        ],
       [ 0.05348839, -0.63310674,  0.77221425]]), 'currentState': array([10.67030786, -8.03893676,  9.61057133, -0.06500937,  0.76947296,
        0.63536222]), 'targetState': array([-25.29750882,  29.68225139,  21.        ]), 'previousTarget': array([-7.82068222, 10.26174035, 15.07856142])}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.6281948117248802
{'scaleFactor': 20, 'timeStep': 89, 'trapCount': 27, 'trapConfig': [], 'currentTarget': array([-25.29750882,  29.68225139,  21.        ]), 'distance': 4.05624542774582, 'localFrame': array([[-0.88271747, -0.36681934, -0.29368936],
       [ 0.38374202, -0.92344034,  0.        ],
       [-0.2712046 , -0.11270095,  0.95590092]]), 'currentState': array([-22.31583417,  30.59222373,  23.59512881,  -0.88271747,
        -0.36681934,  -0.29368936]), 'targetState': array([-25.29750882,  29.68225139,  21.        ]), 'previousTarget': array([-25.29750882,  29.68225139,  21.        ])}
episode index:18634
target thresh 85.26129699160265
target distance 26.0
model initialize at round 18634
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.1028825 , 10.57396781, 24.09353347]), 'distance': 27.5, 'localFrame': array([[-0.2360532 , -0.45612211, -0.85803934],
       [ 0.88811594, -0.45961949,  0.        ],
       [-0.39437161, -0.76203841,  0.51358397]]), 'currentState': array([-4.36939607, 13.7486023 , 47.26085634, -0.2360532 , -0.45612211,
       -0.85803934]), 'targetState': array([10.78599644, 10.42412015, 23.        ]), 'previousTarget': array([ 9.23734959, 10.80172309, 25.58715723])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6282045532358272
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.78599644, 10.42412015, 23.        ]), 'distance': 2.8686777541078885, 'localFrame': array([[-0.12157443, -0.42707683,  0.89600504],
       [ 0.96178971, -0.27378924,  0.        ],
       [ 0.24531654,  0.86176843,  0.44404387]]), 'currentState': array([13.13140567, 10.91178525, 21.42185225, -0.12157443, -0.42707683,
        0.89600504]), 'targetState': array([10.78599644, 10.42412015, 23.        ]), 'previousTarget': array([10.78599644, 10.42412015, 23.        ])}
episode index:18635
target thresh 85.26277078821242
target distance 19.88920387441454
model initialize at round 18635
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 31.907729  , -20.63726799,  48.        ]), 'distance': 20.76794044398003, 'localFrame': array([[ 0.43982369,  0.35599181, -0.82451498],
       [-0.6291385 ,  0.77729322,  0.        ],
       [ 0.64088991,  0.51873411,  0.56584013]]), 'currentState': array([ 12.84539262, -28.28253721,  51.07969816,   0.43982369,
         0.35599181,  -0.82451498]), 'targetState': array([ 31.907729  , -20.63726799,  48.        ]), 'previousTarget': array([ 31.907729  , -20.63726799,  48.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6282193728066457
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 31.907729  , -20.63726799,  48.        ]), 'distance': 3.7777460251160906, 'localFrame': array([[ 0.89438457, -0.25779603,  0.36553721],
       [ 0.27696276,  0.96088065,  0.        ],
       [-0.35123764,  0.1012402 ,  0.93079673]]), 'currentState': array([ 29.5130636 , -23.52877284,  47.5803068 ,   0.89438457,
        -0.25779603,   0.36553721]), 'targetState': array([ 31.907729  , -20.63726799,  48.        ]), 'previousTarget': array([ 31.907729  , -20.63726799,  48.        ])}
episode index:18636
target thresh 85.26424443744992
target distance 58.663204998655836
model initialize at round 18636
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.91656048,  5.15580789, 67.24527208]), 'distance': 27.5, 'localFrame': array([[-0.56440311, -0.460675  , -0.68500195],
       [ 0.63232522, -0.77470305,  0.        ],
       [-0.5306731 , -0.43314401,  0.72854123]]), 'currentState': array([ 3.61429216, 32.25442602, 71.07112353, -0.56440311, -0.460675  ,
       -0.68500195]), 'targetState': array([ -2.07691825, -24.91357884,  63.        ]), 'previousTarget': array([ 1.26337085,  6.71456323, 67.85233084])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6282234093104926
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.07691825, -24.91357884,  63.        ]), 'distance': 2.3288911126017156, 'localFrame': array([[-0.13234667, -0.96400837,  0.23059103],
       [ 0.99070718, -0.13601209,  0.        ],
       [ 0.03136317,  0.22844819,  0.97305076]]), 'currentState': array([ -3.61893968, -23.260949  ,  62.43900235,  -0.13234667,
        -0.96400837,   0.23059103]), 'targetState': array([ -2.07691825, -24.91357884,  63.        ]), 'previousTarget': array([ -2.07691825, -24.91357884,  63.        ])}
episode index:18637
target thresh 85.26571793932985
target distance 22.0
model initialize at round 18637
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.58247767, -10.98934076,  94.73918296]), 'distance': 27.5, 'localFrame': array([[ 0.45273001, -0.73845625, -0.49971782],
       [ 0.85253552,  0.5226693 ,  0.        ],
       [ 0.26118716, -0.42602719,  0.86618826]]), 'currentState': array([-3.25588372,  3.48439952, 76.66755122,  0.45273001, -0.73845625,
       -0.49971782]), 'targetState': array([ 15.90206019, -15.20277875, 100.        ]), 'previousTarget': array([ 11.27628327, -10.81511947,  94.99039068])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6282301962982538
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 15.90206019, -15.20277875, 100.        ]), 'distance': 2.491363464168936, 'localFrame': array([[ 0.92502107, -0.24826299, -0.28757868],
       [ 0.25921292,  0.9658202 ,  0.        ],
       [ 0.2777493 , -0.07454411,  0.95775702]]), 'currentState': array([ 13.83849473, -13.87812204,  99.55968864,   0.92502107,
        -0.24826299,  -0.28757868]), 'targetState': array([ 15.90206019, -15.20277875, 100.        ]), 'previousTarget': array([ 15.90206019, -15.20277875, 100.        ])}
episode index:18638
target thresh 85.26719129386696
target distance 73.0
model initialize at round 18638
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.22036878,   9.5983457 ,  32.80060633]), 'distance': 27.500000000000004, 'localFrame': array([[-0.33402442,  0.76013811, -0.55733091],
       [-0.91550866, -0.40229827,  0.        ],
       [-0.22421326,  0.51024127,  0.83029047]]), 'currentState': array([-6.03646277, -2.87037528,  8.8442269 , -0.33402442,  0.76013811,
       -0.55733091]), 'targetState': array([-22.08298428,  35.72592624,  83.        ]), 'previousTarget': array([-10.41349828,   9.17111642,  33.70176039])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281964911533267
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 51, 'trapConfig': [], 'currentTarget': array([-22.08298428,  35.72592624,  83.        ]), 'distance': 18.36529157900053, 'localFrame': array([[ 0.04265545,  0.92475342, -0.37816878],
       [-0.99893788,  0.04607731,  0.        ],
       [ 0.017425  ,  0.37776711,  0.92573667]]), 'currentState': array([-1.33257194e+01,  2.27622981e+01,  7.33803018e+01,  4.26554521e-02,
        9.24753421e-01, -3.78168777e-01]), 'targetState': array([-22.08298428,  35.72592624,  83.        ]), 'previousTarget': array([-22.08298428,  35.72592624,  83.        ])}
episode index:18639
target thresh 85.26866450107599
target distance 37.0
model initialize at round 18639
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.22182734,  33.18734491,  73.47203509]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.65617458,  0.33815901,  0.6745987 ],
       [-0.45809541,  0.88890303,  0.        ],
       [-0.59965282, -0.30903057,  0.73818466]]), 'currentState': array([-33.3567974 ,  33.57473701,  49.88596825,   0.65617458,
         0.33815901,   0.6745987 ]), 'targetState': array([-11.71390284,  32.98157789,  86.        ]), 'previousTarget': array([-19.92663231,  32.70586561,  72.49352956])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6282053654984173
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.71390284,  32.98157789,  86.        ]), 'distance': 2.2579319661289996, 'localFrame': array([[ 0.88769836,  0.26879243,  0.37382114],
       [-0.28980291,  0.95708635,  0.        ],
       [-0.35777911, -0.10833445,  0.92750081]]), 'currentState': array([-13.79165609,  32.38425019,  85.34854175,   0.88769836,
         0.26879243,   0.37382114]), 'targetState': array([-11.71390284,  32.98157789,  86.        ]), 'previousTarget': array([-11.71390284,  32.98157789,  86.        ])}
episode index:18640
target thresh 85.27013756097166
target distance 75.0
model initialize at round 18640
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.73872856,  33.36464314,  46.3580384 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.49153338, -0.42180549,  0.76188914],
       [ 0.65122841, -0.75888178,  0.        ],
       [ 0.57818379,  0.49616386,  0.64770745]]), 'currentState': array([-22.921691  ,  42.78706656,  23.0683564 ,  -0.49153338,
        -0.42180549,   0.76188914]), 'targetState': array([12.57792612, 12.87617081, 97.        ]), 'previousTarget': array([-10.7701408 ,  33.51354321,  45.52512578])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6282016135952495
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([12.57792612, 12.87617081, 97.        ]), 'distance': 2.240299310203491, 'localFrame': array([[ 0.14874024, -0.26806912,  0.95184835],
       [ 0.87441617,  0.48517663,  0.        ],
       [-0.46181457,  0.83231159,  0.30656926]]), 'currentState': array([12.46793614, 13.30167652, 94.80323235,  0.14874024, -0.26806912,
        0.95184835]), 'targetState': array([12.57792612, 12.87617081, 97.        ]), 'previousTarget': array([12.57792612, 12.87617081, 97.        ])}
episode index:18641
target thresh 85.2716104735687
target distance 13.949579602636119
model initialize at round 18641
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.89499894, -2.86338556, 87.        ]), 'distance': 19.394100338903172, 'localFrame': array([[-0.16355794, -0.62381851, -0.76426387],
       [ 0.96730496, -0.25361609,  0.        ],
       [-0.19382961, -0.73927623,  0.64490366]]), 'currentState': array([-12.73664876,   9.41906117,  96.22214508,  -0.16355794,
        -0.62381851,  -0.76426387]), 'targetState': array([-0.89499894, -2.86338556, 87.        ]), 'previousTarget': array([-0.89499894, -2.86338556, 87.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6282169185858024
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.89499894, -2.86338556, 87.        ]), 'distance': 3.664746371592169, 'localFrame': array([[ 0.44825028, -0.29532535, -0.84371478],
       [ 0.55016745,  0.83505436,  0.        ],
       [ 0.7045477 , -0.46418441,  0.53679174]]), 'currentState': array([-3.59367142, -1.50068329, 89.07137041,  0.44825028, -0.29532535,
       -0.84371478]), 'targetState': array([-0.89499894, -2.86338556, 87.        ]), 'previousTarget': array([-0.89499894, -2.86338556, 87.        ])}
episode index:18642
target thresh 85.27308323888185
target distance 44.0
model initialize at round 18642
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.83672431,  13.68039012,  56.77548004]), 'distance': 27.499999999999996, 'localFrame': array([[-0.53357679,  0.37573838,  0.75770474],
       [-0.57575821, -0.81762001,  0.        ],
       [ 0.61951456, -0.43625472,  0.65259752]]), 'currentState': array([-34.2692094 ,  13.65444821,  29.49055062,  -0.53357679,
         0.37573838,   0.75770474]), 'targetState': array([-28.92145683,  13.69486526,  72.        ]), 'previousTarget': array([-30.54778206,  13.75797923,  55.36925007])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6282237041014715
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-28.92145683,  13.69486526,  72.        ]), 'distance': 2.610988097104109, 'localFrame': array([[ 0.24316684, -0.75827344,  0.60488948],
       [ 0.95223465,  0.30536727,  0.        ],
       [-0.18471345,  0.57599672,  0.79630944]]), 'currentState': array([-29.50544799,  12.69540827,  69.65963702,   0.24316684,
        -0.75827344,   0.60488948]), 'targetState': array([-28.92145683,  13.69486526,  72.        ]), 'previousTarget': array([-28.92145683,  13.69486526,  72.        ])}
episode index:18643
target thresh 85.27455585692584
target distance 68.14598351779048
model initialize at round 18643
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.31282733, 13.22386361, 19.30724514]), 'distance': 27.499999999999996, 'localFrame': array([[-0.31143639, -0.93024731,  0.19402917],
       [ 0.94826843, -0.31746966,  0.        ],
       [ 0.06159837,  0.18399173,  0.98099576]]), 'currentState': array([23.05929156, 35.97334469,  3.87500934, -0.31143639, -0.93024731,
        0.19402917]), 'targetState': array([ 20.87657534, -30.54780847,  49.        ]), 'previousTarget': array([22.19384159, 14.81166305, 18.38138417])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.628215248991918
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([ 20.87657534, -30.54780847,  49.        ]), 'distance': 3.2356031334433117, 'localFrame': array([[-0.54464536, -0.59258071,  0.59347244],
       [ 0.73625835, -0.67670055,  0.        ],
       [ 0.40160312,  0.43694904,  0.80485432]]), 'currentState': array([ 18.98533771, -29.16806576,  46.76646962,  -0.54464536,
        -0.59258071,   0.59347244]), 'targetState': array([ 20.87657534, -30.54780847,  49.        ]), 'previousTarget': array([ 20.87657534, -30.54780847,  49.        ])}
episode index:18644
target thresh 85.27602832771538
target distance 10.0
model initialize at round 18644
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.9812057 , -3.47488474, 88.        ]), 'distance': 13.687717952536225, 'localFrame': array([[-0.82140204,  0.02161904, -0.56993973],
       [-0.02631057, -0.99965382,  0.        ],
       [-0.56974243,  0.01499544,  0.8216865 ]]), 'currentState': array([ 5.54417246e+00,  4.19055878e+00,  9.64831175e+01, -8.21402044e-01,
        2.16190437e-02, -5.69939732e-01]), 'targetState': array([-1.9812057 , -3.47488474, 88.        ]), 'previousTarget': array([-1.9812057 , -3.47488474, 88.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6282320505419533
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.9812057 , -3.47488474, 88.        ]), 'distance': 3.167916156945721, 'localFrame': array([[-0.39387984, -0.24564828, -0.88572885],
       [ 0.5291829 , -0.84850778,  0.        ],
       [-0.75154782, -0.46871256,  0.46420298]]), 'currentState': array([ 0.10771513, -1.80674868, 89.69983076, -0.39387984, -0.24564828,
       -0.88572885]), 'targetState': array([-1.9812057 , -3.47488474, 88.        ]), 'previousTarget': array([-1.9812057 , -3.47488474, 88.        ])}
episode index:18645
target thresh 85.2775006512652
target distance 58.0
model initialize at round 18645
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.58039178, -14.81604162,  59.8628102 ]), 'distance': 27.5, 'localFrame': array([[-0.7420196 ,  0.36739445,  0.56073901],
       [-0.44371706, -0.89616693,  0.        ],
       [ 0.50251576, -0.24880947,  0.82799261]]), 'currentState': array([-19.56691997, -18.03043414,  34.87038895,  -0.7420196 ,
         0.36739445,   0.56073901]), 'targetState': array([-44.74236634, -10.6827269 ,  92.        ]), 'previousTarget': array([-29.89130704, -15.67931152,  58.86101093])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.628235707152889
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-44.74236634, -10.6827269 ,  92.        ]), 'distance': 2.709512809422432, 'localFrame': array([[-0.0400139 , -0.5960911 ,  0.80191913],
       [ 0.99775456, -0.06697643,  0.        ],
       [ 0.05370968,  0.80011847,  0.5974326 ]]), 'currentState': array([-4.53514900e+01, -1.04186477e+01,  8.93730835e+01, -4.00139045e-02,
       -5.96091097e-01,  8.01919130e-01]), 'targetState': array([-44.74236634, -10.6827269 ,  92.        ]), 'previousTarget': array([-44.74236634, -10.6827269 ,  92.        ])}
episode index:18646
target thresh 85.27897282759002
target distance 34.93534927079446
model initialize at round 18646
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.09690656, -15.04505024,  72.67299522]), 'distance': 27.5, 'localFrame': array([[-0.44483651, -0.44915466,  0.77484228],
       [ 0.71051395, -0.70368311,  0.        ],
       [ 0.54524343,  0.55053625,  0.6321546 ]]), 'currentState': array([ 3.11172761, -9.76622303, 82.31191355, -0.44483651, -0.44915466,
        0.77484228]), 'targetState': array([-31.70287995, -17.05659412,  69.        ]), 'previousTarget': array([-22.0898428 , -14.67444461,  72.30199778])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.628245440201694
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.70287995, -17.05659412,  69.        ]), 'distance': 3.1470649285438546, 'localFrame': array([[-0.91992347,  0.21360687,  0.32880529],
       [-0.22618316, -0.97408479,  0.        ],
       [ 0.32028423, -0.07437022,  0.94439774]]), 'currentState': array([-29.21623693, -17.8303546 ,  70.76689526,  -0.91992347,
         0.21360687,   0.32880529]), 'targetState': array([-31.70287995, -17.05659412,  69.        ]), 'previousTarget': array([-31.70287995, -17.05659412,  69.        ])}
episode index:18647
target thresh 85.28044485670459
target distance 42.97693239389914
model initialize at round 18647
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.13701188, -10.01041033,  26.87468752]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.10116922, -0.73022223,  0.67567764],
       [ 0.99053852,  0.13723495,  0.        ],
       [-0.09272659,  0.66928473,  0.73719721]]), 'currentState': array([21.37245694, -9.13353281, 39.31522707,  0.10116922, -0.73022223,
        0.67567764]), 'targetState': array([-22.591421  , -10.70643252,  17.        ]), 'previousTarget': array([-4.2906111 , -9.66990942, 25.94240204])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6282494729266404
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.591421  , -10.70643252,  17.        ]), 'distance': 3.509080741021394, 'localFrame': array([[-0.54774576,  0.52697823,  0.64982192],
       [-0.6933135 , -0.7206361 ,  0.        ],
       [ 0.46828513, -0.45053031,  0.76008649]]), 'currentState': array([-20.49786484, -11.66380866,  14.35158515,  -0.54774576,
         0.52697823,   0.64982192]), 'targetState': array([-22.591421  , -10.70643252,  17.        ]), 'previousTarget': array([-22.591421  , -10.70643252,  17.        ])}
episode index:18648
target thresh 85.2819167386236
target distance 16.34079734709733
model initialize at round 18648
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.40102008,  38.34079735,   3.        ]), 'distance': 21.92384689823326, 'localFrame': array([[ 0.38459417, -0.80105997,  0.45868318],
       [ 0.90148557,  0.43280916,  0.        ],
       [-0.19852228,  0.41349627,  0.88859988]]), 'currentState': array([ 1.02363718, 20.37594944,  1.11660025,  0.38459417, -0.80105997,
        0.45868318]), 'targetState': array([-11.40102008,  38.34079735,   3.        ]), 'previousTarget': array([-11.40102008,  38.34079735,   3.        ])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6282587699997603
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-11.40102008,  38.34079735,   3.        ]), 'distance': 3.6595597162347375, 'localFrame': array([[ 0.24895354,  0.96601427,  0.06955983],
       [-0.96835985,  0.24955802,  0.        ],
       [-0.01735921, -0.06735894,  0.99757778]]), 'currentState': array([-9.58871209, 35.58465224,  1.41520309,  0.24895354,  0.96601427,
        0.06955983]), 'targetState': array([-11.40102008,  38.34079735,   3.        ]), 'previousTarget': array([-11.40102008,  38.34079735,   3.        ])}
episode index:18649
target thresh 85.28338847336177
target distance 64.0
model initialize at round 18649
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.56127192,  35.72510825,  44.23420604]), 'distance': 27.5, 'localFrame': array([[ 0.86825381,  0.4806997 , -0.1227319 ],
       [-0.48436154,  0.87486793,  0.        ],
       [ 0.1073742 ,  0.05944661,  0.99243986]]), 'currentState': array([-17.6411329 ,  33.89976434,  71.67344331,   0.86825381,
         0.4806997 ,  -0.1227319 ]), 'targetState': array([-17.45290302,  38.20204414,   7.        ]), 'previousTarget': array([-18.35559556,  35.16452096,  43.60248092])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282250832024412
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 57, 'trapConfig': [], 'currentTarget': array([-17.45290302,  38.20204414,   7.        ]), 'distance': 7.697954699427443, 'localFrame': array([[ 0.6933762 ,  0.31670179, -0.64724757],
       [-0.41546656,  0.90960845,  0.        ],
       [ 0.58874186,  0.26890972,  0.76227986]]), 'currentState': array([-19.19236509,  37.38503255,  14.45421158,   0.6933762 ,
         0.31670179,  -0.64724757]), 'targetState': array([-17.45290302,  38.20204414,   7.        ]), 'previousTarget': array([-17.45290302,  38.20204414,   7.        ])}
episode index:18650
target thresh 85.28486006093384
target distance 68.0
model initialize at round 18650
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.16967759, -10.53943824,  69.7636139 ]), 'distance': 27.5, 'localFrame': array([[-0.64241063, -0.3754361 , -0.66809903],
       [ 0.50456934, -0.86337117,  0.        ],
       [-0.57681745, -0.33710229,  0.74407236]]), 'currentState': array([ 0.16989353,  4.77127192, 92.05339437, -0.64241063, -0.3754361 ,
       -0.66809903]), 'targetState': array([ 15.21053034, -41.28728335,  25.        ]), 'previousTarget': array([  6.19184968, -10.51121267,  70.51921619])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6282187436315361
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([ 15.21053034, -41.28728335,  25.        ]), 'distance': 2.4833355598292806, 'localFrame': array([[ 0.80203097,  0.37862188, -0.4619435 ],
       [-0.42690031,  0.90429869,  0.        ],
       [ 0.4177349 ,  0.19720382,  0.88690936]]), 'currentState': array([ 15.03022957, -41.6260785 ,  27.45350056,   0.80203097,
         0.37862188,  -0.4619435 ]), 'targetState': array([ 15.21053034, -41.28728335,  25.        ]), 'previousTarget': array([ 15.21053034, -41.28728335,  25.        ])}
episode index:18651
target thresh 85.2863315013545
target distance 18.774431444021484
model initialize at round 18651
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.98209689, 21.42845563, 90.        ]), 'distance': 25.15364757378814, 'localFrame': array([[-0.03361029,  0.58532874, -0.81009914],
       [-0.99835547, -0.05732678,  0.        ],
       [-0.04644038,  0.80876691,  0.58629291]]), 'currentState': array([ 1.45099912e+01,  6.47580121e+00,  9.54020935e+01, -3.36102851e-02,
        5.85328737e-01, -8.10099141e-01]), 'targetState': array([-4.98209689, 21.42845563, 90.        ]), 'previousTarget': array([-4.98209689, 21.42845563, 90.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.628231639142427
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.98209689, 21.42845563, 90.        ]), 'distance': 2.7336782865030758, 'localFrame': array([[-0.74803855, -0.30930297, -0.58717119],
       [ 0.38210895, -0.92411728,  0.        ],
       [-0.54261504, -0.22436336,  0.80946278]]), 'currentState': array([-3.34414172, 22.0178168 , 92.10778396, -0.74803855, -0.30930297,
       -0.58717119]), 'targetState': array([-4.98209689, 21.42845563, 90.        ]), 'previousTarget': array([-4.98209689, 21.42845563, 90.        ])}
episode index:18652
target thresh 85.28780279463847
target distance 66.05950598383373
model initialize at round 18652
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.55830818,  -9.02720472,  84.55294934]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.94341687,  0.2232919 , -0.24516388],
       [-0.23032091,  0.97311473,  0.        ],
       [ 0.23857259,  0.05646637,  0.96948165]]), 'currentState': array([-39.35301162,  -8.51884679,  98.32987428,   0.94341687,
         0.2232919 ,  -0.24516388]), 'targetState': array([25.12097602, -9.89629042, 61.        ]), 'previousTarget': array([-16.94620013,  -9.5692936 ,  84.56187038])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6282278882444162
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([25.12097602, -9.89629042, 61.        ]), 'distance': 2.922991515379232, 'localFrame': array([[ 0.81676143, -0.03136083,  0.57612262],
       [ 0.03836829,  0.99926367,  0.        ],
       [-0.5756984 ,  0.02210484,  0.81736328]]), 'currentState': array([ 2.25818860e+01, -1.12145031e+01,  6.15993468e+01,  8.16761428e-01,
       -3.13608290e-02,  5.76122616e-01]), 'targetState': array([25.12097602, -9.89629042, 61.        ]), 'previousTarget': array([25.12097602, -9.89629042, 61.        ])}
episode index:18653
target thresh 85.28927394080046
target distance 50.0
model initialize at round 18653
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.0983576 , -13.50268281,  37.75881902]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.80931575,  0.58652596,  0.03154872],
       [-0.58681807,  0.80971881,  0.        ],
       [-0.02554559, -0.01851336,  0.99950222]]), 'currentState': array([-17.9030059 ,   3.74575484,  16.75547589,   0.80931575,
         0.58652596,   0.03154872]), 'targetState': array([-27.73944463, -36.69500255,  66.        ]), 'previousTarget': array([-22.66892511, -14.41439746,  37.4655504 ])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6282253653820743
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.73944463, -36.69500255,  66.        ]), 'distance': 3.0819729639695077, 'localFrame': array([[ 0.6380669 ,  0.26301633,  0.72366639],
       [-0.38110026,  0.92453372,  0.        ],
       [-0.66905397, -0.27578945,  0.69014995]]), 'currentState': array([-27.42229609, -35.74859375,  63.08413228,   0.6380669 ,
         0.26301633,   0.72366639]), 'targetState': array([-27.73944463, -36.69500255,  66.        ]), 'previousTarget': array([-27.73944463, -36.69500255,  66.        ])}
episode index:18654
target thresh 85.2907449398552
target distance 67.97948928711357
model initialize at round 18654
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.07693328, -6.3693501 , 54.75968332]), 'distance': 27.499999999999996, 'localFrame': array([[-0.05696139, -0.80069628, -0.59635633],
       [ 0.99747913, -0.07096049,  0.        ],
       [-0.04231774, -0.59485299,  0.80271984]]), 'currentState': array([-1.30813959e+01,  1.90778335e+01,  6.14394607e+01, -5.69613918e-02,
       -8.00696280e-01, -5.96356325e-01]), 'targetState': array([  7.81653278, -47.35928436,  44.        ]), 'previousTarget': array([-4.72384568, -4.94035699, 55.23192746])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6282271909878423
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.81653278, -47.35928436,  44.        ]), 'distance': 3.0724619897630734, 'localFrame': array([[ 0.16098414, -0.74047283, -0.65252134],
       [ 0.97717314,  0.21244449,  0.        ],
       [ 0.13862456, -0.63762632,  0.75777035]]), 'currentState': array([  6.42549548, -44.86143628,  45.12507461,   0.16098414,
        -0.74047283,  -0.65252134]), 'targetState': array([  7.81653278, -47.35928436,  44.        ]), 'previousTarget': array([  7.81653278, -47.35928436,  44.        ])}
episode index:18655
target thresh 85.2922157918174
target distance 16.8997857963815
model initialize at round 18655
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.32299825, -22.11519169,  45.        ]), 'distance': 20.473904069197385, 'localFrame': array([[ 0.01708679,  0.0370016 ,  0.99916912],
       [-0.90787418,  0.41924273,  0.        ],
       [-0.41889439, -0.90711985,  0.04075631]]), 'currentState': array([-2.70387241e+01, -1.19827072e+01,  4.66329621e+01,  1.70867876e-02,
        3.70016039e-02,  9.99169116e-01]), 'targetState': array([ -9.32299825, -22.11519169,  45.        ]), 'previousTarget': array([ -9.32299825, -22.11519169,  45.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6282419934580407
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.32299825, -22.11519169,  45.        ]), 'distance': 3.6103197588470373, 'localFrame': array([[ 0.96128299, -0.08660637,  0.26159958],
       [ 0.08973113,  0.99596603,  0.        ],
       [-0.2605443 ,  0.02347362,  0.96517649]]), 'currentState': array([-12.17895797, -19.91112508,  44.85860288,   0.96128299,
        -0.08660637,   0.26159958]), 'targetState': array([ -9.32299825, -22.11519169,  45.        ]), 'previousTarget': array([ -9.32299825, -22.11519169,  45.        ])}
episode index:18656
target thresh 85.29368649670175
target distance 82.0
model initialize at round 18656
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.14108603,  7.20457979, 67.15167058]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.15200907,  0.59697758, -0.78772521],
       [-0.96907733,  0.24675724,  0.        ],
       [ 0.1943769 ,  0.76336664,  0.61602678]]), 'currentState': array([20.00850293,  7.12349579, 94.1683324 ,  0.15200907,  0.59697758,
       -0.78772521]), 'targetState': array([35.23875716,  7.36410171, 14.        ]), 'previousTarget': array([24.91763067,  7.03521175, 68.97270685])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6282410755851819
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([35.23875716,  7.36410171, 14.        ]), 'distance': 1.9287936887173467, 'localFrame': array([[-0.73448219, -0.42579098, -0.52842971],
       [ 0.50153414, -0.86513785,  0.        ],
       [-0.45716455, -0.26502554,  0.84897706]]), 'currentState': array([34.87053715,  7.41012045, 15.89276026, -0.73448219, -0.42579098,
       -0.52842971]), 'targetState': array([35.23875716,  7.36410171, 14.        ]), 'previousTarget': array([35.23875716,  7.36410171, 14.        ])}
episode index:18657
target thresh 85.29515705452295
target distance 62.0
model initialize at round 18657
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 32.52840114, -31.63855096,  62.56364222]), 'distance': 27.500000000000004, 'localFrame': array([[-0.72840855,  0.20202974, -0.65467929],
       [-0.26726807, -0.96362222,  0.        ],
       [-0.63086351,  0.17497487,  0.75590676]]), 'currentState': array([ 33.76806636, -28.79075198,  35.23960022,  -0.72840855,
         0.20202974,  -0.65467929]), 'targetState': array([ 30.87531902, -35.43606461,  99.        ]), 'previousTarget': array([ 32.83941584, -31.85069562,  64.31099797])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6282401578107124
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 30.87531902, -35.43606461,  99.        ]), 'distance': 2.3729636526515003, 'localFrame': array([[-0.04449863,  0.38582683,  0.92149744],
       [-0.99341475, -0.11457367,  0.        ],
       [ 0.10557934, -0.91542915,  0.38838443]]), 'currentState': array([ 3.19984778e+01, -3.49193002e+01,  9.69745555e+01, -4.44986296e-02,
        3.85826827e-01,  9.21497440e-01]), 'targetState': array([ 30.87531902, -35.43606461,  99.        ]), 'previousTarget': array([ 30.87531902, -35.43606461,  99.        ])}
episode index:18658
target thresh 85.29662746529574
target distance 35.0
model initialize at round 18658
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.28308222,  7.71438811, 25.76523665]), 'distance': 27.5, 'localFrame': array([[-0.80853318,  0.00649608,  0.58841473],
       [-0.00803414, -0.99996773,  0.        ],
       [ 0.58839574, -0.0047274 ,  0.80855928]]), 'currentState': array([ 1.60804558e+01,  2.47754716e+01,  6.55115582e+00, -8.08533182e-01,
        6.49607526e-03,  5.88414730e-01]), 'targetState': array([-1.48521381, -5.81327274, 41.        ]), 'previousTarget': array([ 7.28513378,  8.49909539, 24.98391556])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6282457349162295
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.48521381, -5.81327274, 41.        ]), 'distance': 3.207485131376133, 'localFrame': array([[-0.6825371 , -0.61644184, -0.39260995],
       [ 0.67026034, -0.74212605,  0.        ],
       [-0.29136607, -0.26315088,  0.91970508]]), 'currentState': array([ 0.43645647, -3.25549768, 40.7699327 , -0.6825371 , -0.61644184,
       -0.39260995]), 'targetState': array([-1.48521381, -5.81327274, 41.        ]), 'previousTarget': array([-1.48521381, -5.81327274, 41.        ])}
episode index:18659
target thresh 85.29809772903481
target distance 47.379894763453095
model initialize at round 18659
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.98128048, -9.04468169, 56.74016371]), 'distance': 27.5, 'localFrame': array([[-0.15384678,  0.77903062, -0.60781778],
       [-0.98105231, -0.19374301,  0.        ],
       [-0.11776045,  0.59630104,  0.79407653]]), 'currentState': array([  3.38495973, -33.31809055,  65.398238  ,  -0.15384678,
         0.77903062,  -0.60781778]), 'targetState': array([21.56021975, 12.65531209, 49.        ]), 'previousTarget': array([ 13.26384421, -10.22746742,  57.21038657])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6282525127057411
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.56021975, 12.65531209, 49.        ]), 'distance': 3.2538886032204064, 'localFrame': array([[-0.27692929,  0.96036176, -0.0318664 ],
       [-0.96084973, -0.27707   ,  0.        ],
       [-0.00882922,  0.03061882,  0.99949214]]), 'currentState': array([ 2.10710282e+01,  1.00261580e+01,  5.08536535e+01, -2.76929290e-01,
        9.60361755e-01, -3.18663981e-02]), 'targetState': array([21.56021975, 12.65531209, 49.        ]), 'previousTarget': array([21.56021975, 12.65531209, 49.        ])}
episode index:18660
target thresh 85.29956784575484
target distance 73.05036784454092
model initialize at round 18660
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.07274907,  2.31423481, 12.40535916]), 'distance': 27.499999999999996, 'localFrame': array([[-0.87298436, -0.1002552 , -0.47733344],
       [ 0.11409202, -0.99347019,  0.        ],
       [-0.47421654, -0.05445994,  0.87872225]]), 'currentState': array([-26.1763964 ,   9.28692471,  17.52699873,  -0.87298436,
        -0.1002552 ,  -0.47733344]), 'targetState': array([ 47.86388828, -10.49038603,   3.        ]), 'previousTarget': array([ 0.74562087,  2.55093104, 13.32017087])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6282440637549283
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([ 47.86388828, -10.49038603,   3.        ]), 'distance': 2.624680354699258, 'localFrame': array([[ 0.96436776,  0.25854126,  0.05613593],
       [-0.25894959,  0.96589084,  0.        ],
       [-0.05422118, -0.01453638,  0.99842314]]), 'currentState': array([45.43303453, -9.55166473,  2.68583547,  0.96436776,  0.25854126,
        0.05613593]), 'targetState': array([ 47.86388828, -10.49038603,   3.        ]), 'previousTarget': array([ 47.86388828, -10.49038603,   3.        ])}
episode index:18661
target thresh 85.30103781547055
target distance 62.0
model initialize at round 18661
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.27309128,   8.8628234 ,  30.98760736]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.40229069,  0.0124737 ,  0.91542701],
       [-0.03099178,  0.99951964,  0.        ],
       [-0.91498727, -0.02837071,  0.40248403]]), 'currentState': array([-1.64254223e+01,  9.81221949e+00,  3.58840813e+00,  4.02290694e-01,
        1.24736956e-02,  9.15427006e-01]), 'targetState': array([-11.67981853,   7.71892733,  64.        ]), 'previousTarget': array([-14.60134558,   9.37190229,  29.37170019])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6282469739752291
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-11.67981853,   7.71892733,  64.        ]), 'distance': 3.0254988588943563, 'localFrame': array([[-0.53377007, -0.0996051 ,  0.83974302],
       [ 0.18344021, -0.98303087,  0.        ],
       [ 0.82549331,  0.15404263,  0.54298404]]), 'currentState': array([-11.67178927,   8.28130677,  61.02723895,  -0.53377007,
        -0.0996051 ,   0.83974302]), 'targetState': array([-11.67981853,   7.71892733,  64.        ]), 'previousTarget': array([-11.67981853,   7.71892733,  64.        ])}
episode index:18662
target thresh 85.30250763819664
target distance 69.0
model initialize at round 18662
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.43588662,  3.89133202, 50.50135299]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.51986513, -0.41305351,  0.74774798],
       [ 0.6220848 ,  0.78294987,  0.        ],
       [-0.58544919,  0.46516265,  0.66398265]]), 'currentState': array([11.18350906, -0.59864002, 26.29454826,  0.51986513, -0.41305351,
        0.74774798]), 'targetState': array([45.4529125 , 11.95962981, 94.        ]), 'previousTarget': array([23.14029467,  4.66683277, 49.34199187])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6282133112750214
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([45.4529125 , 11.95962981, 94.        ]), 'distance': 21.633191742410066, 'localFrame': array([[-0.76530926, -0.14581244,  0.6269294 ],
       [ 0.18716073, -0.9823293 ,  0.        ],
       [ 0.61585112,  0.11733656,  0.77907607]]), 'currentState': array([43.98575142,  2.24734688, 74.72530198, -0.76530926, -0.14581244,
        0.6269294 ]), 'targetState': array([45.4529125 , 11.95962981, 94.        ]), 'previousTarget': array([45.4529125 , 11.95962981, 94.        ])}
episode index:18663
target thresh 85.30397731394781
target distance 24.400570488814843
model initialize at round 18663
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.44737976, -12.25494939,  99.94536821]), 'distance': 27.499999999999996, 'localFrame': array([[-0.7513613 ,  0.2957525 ,  0.58990393],
       [-0.36626901, -0.93050901,  0.        ],
       [ 0.54891092, -0.21606353,  0.80747344]]), 'currentState': array([ 20.33110838, -33.94736886,  99.1524968 ,  -0.7513613 ,
         0.2957525 ,   0.58990393]), 'targetState': array([  2.28402802, -10.76026096, 100.        ]), 'previousTarget': array([  4.33711414, -13.42436364,  99.78163603])}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.628200905815053
{'scaleFactor': 20, 'timeStep': 93, 'trapCount': 76, 'trapConfig': [], 'currentTarget': array([  2.28402802, -10.76026096, 100.        ]), 'distance': 2.303262438794649, 'localFrame': array([[-0.90102747,  0.32511345,  0.28714239],
       [-0.33940657, -0.94063977,  0.        ],
       [ 0.27009755, -0.09745801,  0.95788791]]), 'currentState': array([  4.3316335 , -10.51039647,  98.97535499,  -0.90102747,
         0.32511345,   0.28714239]), 'targetState': array([  2.28402802, -10.76026096, 100.        ]), 'previousTarget': array([  2.28402802, -10.76026096, 100.        ])}
episode index:18664
target thresh 85.30544684273875
target distance 47.903896897019735
model initialize at round 18664
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.15807058,   4.17698068,  42.16532804]), 'distance': 27.5, 'localFrame': array([[-0.88142272,  0.0192167 ,  0.47193718],
       [-0.02179674, -0.99976242,  0.        ],
       [ 0.47182506, -0.01028669,  0.88163218]]), 'currentState': array([-1.09049782e+00,  5.58607217e-02,  3.95937769e+01, -8.81422722e-01,
        1.92167045e-02,  4.71937182e-01]), 'targetState': array([-47.46941733,   7.11719173,  44.        ]), 'previousTarget': array([-26.69192258,   4.42088697,  41.83133564])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281672491900427
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 77, 'trapConfig': [], 'currentTarget': array([-47.46941733,   7.11719173,  44.        ]), 'distance': 11.68389417347061, 'localFrame': array([[ 0.15415871,  0.8687874 , -0.47057788],
       [-0.98461956,  0.17471211,  0.        ],
       [ 0.08221565,  0.46334019,  0.88235846]]), 'currentState': array([-36.63498445,   7.33206435,  48.36832656,   0.15415871,
         0.8687874 ,  -0.47057788]), 'targetState': array([-47.46941733,   7.11719173,  44.        ]), 'previousTarget': array([-47.46941733,   7.11719173,  44.        ])}
episode index:18665
target thresh 85.30691622458416
target distance 82.0
model initialize at round 18665
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.87921775, -13.41697939,  36.60647111]), 'distance': 27.499999999999996, 'localFrame': array([[-0.13395934, -0.5399068 ,  0.83099672],
       [ 0.9705713 , -0.24081396,  0.        ],
       [ 0.20011561,  0.80654156,  0.55627732]]), 'currentState': array([ -5.03429276, -11.1437137 ,  10.69061509,  -0.13395934,
        -0.5399068 ,   0.83099672]), 'targetState': array([ 22.58735113, -18.1882261 ,  91.        ]), 'previousTarget': array([  3.32208616, -13.01489875,  34.91021274])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6281623258522102
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 22.58735113, -18.1882261 ,  91.        ]), 'distance': 1.4537809595261963, 'localFrame': array([[-0.14974682, -0.19060074,  0.97017898],
       [ 0.78634019, -0.61779374,  0.        ],
       [ 0.59937049,  0.76289073,  0.24238967]]), 'currentState': array([ 22.71657893, -18.23626155,  89.55277098,  -0.14974682,
        -0.19060074,   0.97017898]), 'targetState': array([ 22.58735113, -18.1882261 ,  91.        ]), 'previousTarget': array([ 22.58735113, -18.1882261 ,  91.        ])}
episode index:18666
target thresh 85.30838545949874
target distance 37.967310594415444
model initialize at round 18666
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.44205046,  6.16753186, 82.9898749 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.61834391, -0.78163693, -0.08182006],
       [ 0.78426648, -0.62042412,  0.        ],
       [-0.05076314, -0.06416873,  0.99664712]]), 'currentState': array([ 3.20906994e+01,  1.25523369e+01,  9.87003893e+01, -6.18343908e-01,
       -7.81636929e-01, -8.18200645e-02]), 'targetState': array([-4.70168941,  1.70121036, 72.        ]), 'previousTarget': array([11.31727747,  6.78992309, 82.96978249])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281286748999494
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 78, 'trapConfig': [], 'currentTarget': array([-4.70168941,  1.70121036, 72.        ]), 'distance': 11.446631835840574, 'localFrame': array([[-0.49333534, -0.63138328, -0.59831046],
       [ 0.78798371, -0.61569608,  0.        ],
       [-0.3683774 , -0.4714589 ,  0.80126437]]), 'currentState': array([ 1.48799943,  2.47216158, 81.59785219, -0.49333534, -0.63138328,
       -0.59831046]), 'targetState': array([-4.70168941,  1.70121036, 72.        ]), 'previousTarget': array([-4.70168941,  1.70121036, 72.        ])}
episode index:18667
target thresh 85.30985454749715
target distance 36.60854967485456
model initialize at round 18667
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.82631626, -36.19768518,  91.93623437]), 'distance': 27.500000000000007, 'localFrame': array([[ 0.80647444, -0.26260501,  0.52975237],
       [ 0.3096201 ,  0.95086035,  0.        ],
       [-0.50372052,  0.16402198,  0.84815236]]), 'currentState': array([-10.83772495,  -9.98880799,  94.20825703,   0.80647444,
        -0.26260501,   0.52975237]), 'targetState': array([  0.47495193, -46.99760016,  91.        ]), 'previousTarget': array([ -3.2030575 , -36.34792362,  91.58181363])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6281346515283235
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([  0.47495193, -46.99760016,  91.        ]), 'distance': 2.931951764328078, 'localFrame': array([[ 0.09956566, -0.63199048, -0.76855365],
       [ 0.98781644,  0.15562354,  0.        ],
       [ 0.11960504, -0.75918993,  0.63978535]]), 'currentState': array([  0.51023736, -44.88342049,  93.0310934 ,   0.09956566,
        -0.63199048,  -0.76855365]), 'targetState': array([  0.47495193, -46.99760016,  91.        ]), 'previousTarget': array([  0.47495193, -46.99760016,  91.        ])}
episode index:18668
target thresh 85.31132348859414
target distance 27.0
model initialize at round 18668
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.70404108, 10.32456808, 38.73907162]), 'distance': 27.5, 'localFrame': array([[ 0.38087394,  0.30145879,  0.87410391],
       [-0.62061889,  0.78411236,  0.        ],
       [-0.68539568, -0.5424854 ,  0.48573898]]), 'currentState': array([-21.48812785,  12.08674732,  15.00842315,   0.38087394,
         0.30145879,   0.87410391]), 'targetState': array([-6.39076753, 10.15667713, 41.        ]), 'previousTarget': array([-8.41514063, 10.24858952, 37.60794836])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6281457061702391
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.39076753, 10.15667713, 41.        ]), 'distance': 1.9126818658722642, 'localFrame': array([[ 0.60406875,  0.38100879,  0.69995232],
       [-0.53348411,  0.84581009,  0.        ],
       [-0.59202673, -0.37341344,  0.71418958]]), 'currentState': array([-8.1681805 , 10.1892516 , 40.29424227,  0.60406875,  0.38100879,
        0.69995232]), 'targetState': array([-6.39076753, 10.15667713, 41.        ]), 'previousTarget': array([-6.39076753, 10.15667713, 41.        ])}
episode index:18669
target thresh 85.31279228280434
target distance 30.0
model initialize at round 18669
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.9358748 , -24.28212602,  60.86903585]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.1050732 , -0.78411829, -0.61165197],
       [ 0.9911409 ,  0.13281459,  0.        ],
       [ 0.08123631, -0.60623328,  0.79112696]]), 'currentState': array([ 6.50518542, -8.17648978, 38.86634277,  0.1050732 , -0.78411829,
       -0.61165197]), 'targetState': array([  1.45463579, -30.96585272,  70.        ]), 'previousTarget': array([  3.14524512, -23.92254456,  61.2164229 ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6281528940121339
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.45463579, -30.96585272,  70.        ]), 'distance': 3.2219040850974223, 'localFrame': array([[-0.51388937,  0.04943198,  0.85643108],
       [-0.09574992, -0.99540542,  0.        ],
       [ 0.85249614, -0.08200321,  0.51626137]]), 'currentState': array([ 3.69000096e+00, -2.98210225e+01,  6.79817898e+01, -5.13889366e-01,
        4.94319842e-02,  8.56431082e-01]), 'targetState': array([  1.45463579, -30.96585272,  70.        ]), 'previousTarget': array([  1.45463579, -30.96585272,  70.        ])}
episode index:18670
target thresh 85.31426093014247
target distance 14.015805230620241
model initialize at round 18670
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.48979428,  7.99903916,  8.        ]), 'distance': 12.915537042006362, 'localFrame': array([[ 0.42626997,  0.8972413 ,  0.11511719],
       [-0.90324615,  0.42912281,  0.        ],
       [-0.04939941, -0.10397916,  0.99335192]]), 'currentState': array([10.76504373, -4.53225866, 11.04177722,  0.42626997,  0.8972413 ,
        0.11511719]), 'targetState': array([11.48979428,  7.99903916,  8.        ]), 'previousTarget': array([11.48979428,  7.99903916,  8.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6281696755051117
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.48979428,  7.99903916,  8.        ]), 'distance': 3.09978871360063, 'localFrame': array([[-0.57890948,  0.8004293 , -0.15548873],
       [-0.81028425, -0.58603706,  0.        ],
       [-0.09112216,  0.12599007,  0.98783767]]), 'currentState': array([11.33041896,  5.89004081, 10.26614553, -0.57890948,  0.8004293 ,
       -0.15548873]), 'targetState': array([11.48979428,  7.99903916,  8.        ]), 'previousTarget': array([11.48979428,  7.99903916,  8.        ])}
episode index:18671
target thresh 85.3157294306232
target distance 42.0
model initialize at round 18671
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.18100085,  -6.72697093,  76.3531797 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69553508,  0.53623616,  0.47820678],
       [-0.61057538, -0.79195815,  0.        ],
       [ 0.37871975, -0.29198129,  0.87824727]]), 'currentState': array([-8.89992482, -6.21156617, 49.1933427 , -0.69553508,  0.53623616,
        0.47820678]), 'targetState': array([-15.48970953,  -7.00491961,  91.        ]), 'previousTarget': array([-12.77352485,  -7.25166277,  76.05329937])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.628178536077527
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.48970953,  -7.00491961,  91.        ]), 'distance': 2.8749429524779937, 'localFrame': array([[-0.34773314, -0.93524091,  0.06637842],
       [ 0.93730813, -0.34850176,  0.        ],
       [ 0.02313299,  0.06221703,  0.99779452]]), 'currentState': array([-1.67828073e+01, -6.10618349e+00,  8.85946999e+01, -3.47733144e-01,
       -9.35240914e-01,  6.63784158e-02]), 'targetState': array([-15.48970953,  -7.00491961,  91.        ]), 'previousTarget': array([-15.48970953,  -7.00491961,  91.        ])}
episode index:18672
target thresh 85.31719778426124
target distance 33.0
model initialize at round 18672
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.69100518,  4.96102677, 64.96532506]), 'distance': 27.499999999999996, 'localFrame': array([[-0.65251215, -0.71068156, -0.26298215],
       [ 0.73660971, -0.67631807,  0.        ],
       [-0.17785958, -0.19371521,  0.9648007 ]]), 'currentState': array([-3.5183723 ,  3.55261254, 38.        , -0.65251215, -0.71068156,
       -0.26298215]), 'targetState': array([ 2.85683202,  5.27622126, 71.        ]), 'previousTarget': array([ 1.69100518,  4.96102677, 64.96532506])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.628188258635881
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.85683202,  5.27622126, 71.        ]), 'distance': 2.519139960166373, 'localFrame': array([[-0.35839509, -0.01225953,  0.93348951],
       [ 0.03418674, -0.99941546,  0.        ],
       [ 0.93294385,  0.03191296,  0.35860471]]), 'currentState': array([ 4.63224351e+00,  4.43416516e+00,  6.94236366e+01, -3.58395094e-01,
       -1.22595260e-02,  9.33489508e-01]), 'targetState': array([ 2.85683202,  5.27622126, 71.        ]), 'previousTarget': array([ 2.85683202,  5.27622126, 71.        ])}
episode index:18673
target thresh 85.31866599107126
target distance 74.681888368383
model initialize at round 18673
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.14139891,  9.76982392, 84.27149261]), 'distance': 27.499999999999996, 'localFrame': array([[-0.68272998, -0.64585046,  0.34169718],
       [ 0.68721378, -0.72645524,  0.        ],
       [ 0.24822771,  0.23481901,  0.93981011]]), 'currentState': array([34.07153784, 16.61501769, 90.35599895, -0.68272998, -0.64585046,
        0.34169718]), 'targetState': array([-39.89394739,  -2.91083522,  73.        ]), 'previousTarget': array([ 8.82079181, 10.48755202, 83.43674503])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.628186054958217
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-39.89394739,  -2.91083522,  73.        ]), 'distance': 2.8093881299465155, 'localFrame': array([[-0.82281116, -0.54769802,  0.15168608],
       [ 0.55410978, -0.8324436 ,  0.        ],
       [ 0.12627011,  0.08405074,  0.98842872]]), 'currentState': array([-37.46666014,  -1.82618243,  73.90800142,  -0.82281116,
        -0.54769802,   0.15168608]), 'targetState': array([-39.89394739,  -2.91083522,  73.        ]), 'previousTarget': array([-39.89394739,  -2.91083522,  73.        ])}
episode index:18674
target thresh 85.32013405106791
target distance 64.0
model initialize at round 18674
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.44195886, -7.29503642, 38.41274614]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.5875659 , -0.32327576, -0.74179451],
       [ 0.48204985,  0.87614379,  0.        ],
       [ 0.64991865, -0.35758193,  0.67062725]]), 'currentState': array([-9.04124049, -4.64258882, 64.3986438 ,  0.5875659 , -0.32327576,
       -0.74179451]), 'targetState': array([ 11.60778686, -11.01177934,   2.        ]), 'previousTarget': array([-0.79041071, -6.70008384, 40.006631  ])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6281835371728975
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([ 11.60778686, -11.01177934,   2.        ]), 'distance': 2.097847781046854, 'localFrame': array([[-0.44053766,  0.65590741, -0.61295353],
       [-0.83013755, -0.55755866,  0.        ],
       [-0.34175755,  0.50883574,  0.79011896]]), 'currentState': array([11.82070491, -9.85166104,  3.73486505, -0.44053766,  0.65590741,
       -0.61295353]), 'targetState': array([ 11.60778686, -11.01177934,   2.        ]), 'previousTarget': array([ 11.60778686, -11.01177934,   2.        ])}
episode index:18675
target thresh 85.32160196426592
target distance 30.539961133649822
model initialize at round 18675
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.66249524, -8.83108806, 44.44819836]), 'distance': 27.5, 'localFrame': array([[-0.80347269,  0.59028388,  0.07743757],
       [-0.59206172, -0.80589262,  0.        ],
       [ 0.06240636, -0.04584782,  0.9969972 ]]), 'currentState': array([ 29.89398664, -22.58310063,  28.01019805,  -0.80347269,
         0.59028388,   0.07743757]), 'targetState': array([ 0.55306789,  0.83313619, 56.        ]), 'previousTarget': array([13.83363575, -9.76658391, 43.38909764])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6281887201242139
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.55306789,  0.83313619, 56.        ]), 'distance': 1.543057968016836, 'localFrame': array([[-0.46587875, -0.64667251,  0.60396329],
       [ 0.81137097, -0.58453157,  0.        ],
       [ 0.35303561,  0.49003828,  0.79701214]]), 'currentState': array([ 1.21791933,  0.76629705, 54.6091251 , -0.46587875, -0.64667251,
        0.60396329]), 'targetState': array([ 0.55306789,  0.83313619, 56.        ]), 'previousTarget': array([ 0.55306789,  0.83313619, 56.        ])}
episode index:18676
target thresh 85.32306973067996
target distance 59.0
model initialize at round 18676
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.95403463,  24.1004889 ,  54.27928235]), 'distance': 27.499999999999996, 'localFrame': array([[-0.17146256, -0.91960401,  0.35345304],
       [ 0.98305819, -0.18329375,  0.        ],
       [ 0.06478574,  0.34746491,  0.93545227]]), 'currentState': array([-15.05702485,  38.40644841,  30.88768226,  -0.17146256,
        -0.91960401,   0.35345304]), 'targetState': array([-9.74261174,  2.25422194, 90.        ]), 'previousTarget': array([-13.06240946,  25.38858862,  54.05509951])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.628184088564319
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.74261174,  2.25422194, 90.        ]), 'distance': 2.0105560398408193, 'localFrame': array([[ 0.39662315, -0.16102108,  0.90374902],
       [ 0.37616234,  0.92655377,  0.        ],
       [-0.83737206,  0.33995634,  0.42806274]]), 'currentState': array([-9.23270772,  2.24502857, 88.0551995 ,  0.39662315, -0.16102108,
        0.90374902]), 'targetState': array([-9.74261174,  2.25422194, 90.        ]), 'previousTarget': array([-9.74261174,  2.25422194, 90.        ])}
episode index:18677
target thresh 85.32453735032468
target distance 30.00851736242074
model initialize at round 18677
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.11275858, 32.73165821, 68.9006536 ]), 'distance': 27.5, 'localFrame': array([[ 0.3232643 , -0.90321607,  0.28231352],
       [ 0.94151482,  0.33697156,  0.        ],
       [-0.09513163,  0.26580236,  0.9593222 ]]), 'currentState': array([-0.92524565, 44.2025999 , 52.70738673,  0.3232643 , -0.90321607,
        0.28231352]), 'targetState': array([28.81062434, 26.28588833, 78.        ]), 'previousTarget': array([17.66716613, 33.60079689, 68.71642055])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6281888827844728
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([28.81062434, 26.28588833, 78.        ]), 'distance': 2.297480952322853, 'localFrame': array([[ 0.52397614,  0.81356469,  0.25211407],
       [-0.84072209,  0.54146686,  0.        ],
       [-0.13651141, -0.21195787,  0.96769752]]), 'currentState': array([27.39945703, 26.87102203, 76.28400347,  0.52397614,  0.81356469,
        0.25211407]), 'targetState': array([28.81062434, 26.28588833, 78.        ]), 'previousTarget': array([28.81062434, 26.28588833, 78.        ])}
episode index:18678
target thresh 85.32600482321477
target distance 42.0
model initialize at round 18678
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.5638215 ,  17.51740387,  77.68013941]), 'distance': 27.5, 'localFrame': array([[ 0.49082406, -0.13791086, -0.86027457],
       [ 0.27050308,  0.96271911,  0.        ],
       [ 0.82820276, -0.23270692,  0.50983102]]), 'currentState': array([-30.77970849,  31.21004856,  97.53213711,   0.49082406,
        -0.13791086,  -0.86027457]), 'targetState': array([-3.79662338,  3.25355973, 57.        ]), 'previousTarget': array([-18.29749389,  17.45233161,  78.85051244])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.628194064617182
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.79662338,  3.25355973, 57.        ]), 'distance': 3.836001334678021, 'localFrame': array([[-0.05211063, -0.86441723, -0.50006732],
       [ 0.99818785, -0.06017488,  0.        ],
       [-0.03009149, -0.49916113,  0.86598653]]), 'currentState': array([-5.49679327e+00,  5.69154006e+00,  5.94249908e+01, -5.21106315e-02,
       -8.64417233e-01, -5.00067325e-01]), 'targetState': array([-3.79662338,  3.25355973, 57.        ]), 'previousTarget': array([-3.79662338,  3.25355973, 57.        ])}
episode index:18679
target thresh 85.32747214936494
target distance 53.0
model initialize at round 18679
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.95691023, -15.76848819,  49.17833143]), 'distance': 27.5, 'localFrame': array([[ 0.30842689, -0.64028356,  0.70349827],
       [ 0.90092344,  0.43397806,  0.        ],
       [-0.30530282,  0.63379808,  0.71069697]]), 'currentState': array([-12.82417777,  -9.56951494,  23.89603927,   0.30842689,
        -0.64028356,   0.70349827]), 'targetState': array([  5.45026336, -22.34490164,  76.        ]), 'previousTarget': array([ -4.07059084, -14.80994387,  48.20107136])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6281962475772437
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.45026336, -22.34490164,  76.        ]), 'distance': 1.5403788927262239, 'localFrame': array([[-0.14497619, -0.87566961,  0.46063505],
       [ 0.98657034, -0.16333696,  0.        ],
       [ 0.07523873,  0.45444888,  0.88758963]]), 'currentState': array([  5.10827907, -21.98864114,  74.54092758,  -0.14497619,
        -0.87566961,   0.46063505]), 'targetState': array([  5.45026336, -22.34490164,  76.        ]), 'previousTarget': array([  5.45026336, -22.34490164,  76.        ])}
episode index:18680
target thresh 85.3289393287898
target distance 39.0
model initialize at round 18680
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.68379877,   6.82024775,  32.65644639]), 'distance': 27.5, 'localFrame': array([[ 0.82412964, -0.121524  ,  0.55321085],
       [ 0.14587993,  0.9893023 ,  0.        ],
       [-0.54729277,  0.08070236,  0.83304127]]), 'currentState': array([-28.43491583,  14.55436508,  11.48250627,   0.82412964,
        -0.121524  ,   0.55321085]), 'targetState': array([-0.52596585,  0.85050569, 49.        ]), 'previousTarget': array([-13.64784653,   7.37612073,  31.2204753 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281626200279916
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-0.52596585,  0.85050569, 49.        ]), 'distance': 23.118839658359228, 'localFrame': array([[-0.2394539 ,  0.00618736,  0.97088802],
       [-0.02583082, -0.99966633,  0.        ],
       [ 0.97056406, -0.02507883,  0.23953383]]), 'currentState': array([ 1.36022207e+00, -4.60481830e+00,  2.66133414e+01, -2.39453902e-01,
        6.18735510e-03,  9.70888019e-01]), 'targetState': array([-0.52596585,  0.85050569, 49.        ]), 'previousTarget': array([-0.52596585,  0.85050569, 49.        ])}
episode index:18681
target thresh 85.33040636150406
target distance 40.79694530702539
model initialize at round 18681
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.03763985, -11.4086565 ,  18.34289156]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.17021201, -0.9472415 , -0.27159052],
       [ 0.98423608,  0.17685965,  0.        ],
       [ 0.04803341, -0.26730919,  0.9624129 ]]), 'currentState': array([-24.6466934 ,  15.64201769,  14.73560613,   0.17021201,
        -0.9472415 ,  -0.27159052]), 'targetState': array([-29.59536685, -23.83514759,  20.        ]), 'previousTarget': array([-27.97622116, -10.21734206,  18.66482106])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6281681944177586
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-29.59536685, -23.83514759,  20.        ]), 'distance': 3.9414941389615317, 'localFrame': array([[-0.1757798 , -0.97844094, -0.10841947],
       [ 0.98424282, -0.17682213,  0.        ],
       [-0.01917096, -0.10671109,  0.99410523]]), 'currentState': array([-26.7476166 , -21.17144666,  19.42520278,  -0.1757798 ,
        -0.97844094,  -0.10841947]), 'targetState': array([-29.59536685, -23.83514759,  20.        ]), 'previousTarget': array([-29.59536685, -23.83514759,  20.        ])}
episode index:18682
target thresh 85.33187324752238
target distance 51.73950832265053
model initialize at round 18682
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 19.22014444, -19.1259682 ,  70.25046895]), 'distance': 27.499999999999996, 'localFrame': array([[-0.4818502 , -0.86316131,  0.15090703],
       [ 0.87316078, -0.4874323 ,  0.        ],
       [ 0.07355696,  0.1317661 ,  0.98854796]]), 'currentState': array([44.29612348, -8.05652206, 72.46691603, -0.4818502 , -0.86316131,
        0.15090703]), 'targetState': array([ -6.24074428, -30.36532744,  68.        ]), 'previousTarget': array([ 20.57356714, -18.13804036,  70.59128008])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6281718472049787
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.24074428, -30.36532744,  68.        ]), 'distance': 3.1132159481366566, 'localFrame': array([[-0.65552162, -0.38752224, -0.64816504],
       [ 0.50889337, -0.86082957,  0.        ],
       [-0.55795963, -0.32984689,  0.76149989]]), 'currentState': array([ -4.0559197 , -28.1549096 ,  67.81914634,  -0.65552162,
        -0.38752224,  -0.64816504]), 'targetState': array([ -6.24074428, -30.36532744,  68.        ]), 'previousTarget': array([ -6.24074428, -30.36532744,  68.        ])}
episode index:18683
target thresh 85.33333998685944
target distance 57.184777024867145
model initialize at round 18683
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.67524633, -9.72603496, 93.34664948]), 'distance': 27.499999999999996, 'localFrame': array([[-0.61310987,  0.19550674, -0.76542368],
       [-0.30380519, -0.95273418,  0.        ],
       [-0.72924529,  0.23253968,  0.64352669]]), 'currentState': array([29.7649581 ,  2.64177443, 98.13967334, -0.61310987,  0.19550674,
       -0.76542368]), 'targetState': array([-26.22297931, -26.10278445,  87.        ]), 'previousTarget': array([  6.63294878, -10.22126314,  93.89468697])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6281751268687907
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-26.22297931, -26.10278445,  87.        ]), 'distance': 2.5830413284459226, 'localFrame': array([[-0.54553682, -0.80818138, -0.22188384],
       [ 0.82884183, -0.55948299,  0.        ],
       [-0.12414023, -0.18390661,  0.97507311]]), 'currentState': array([-24.34378008, -25.70274703,  88.72646541,  -0.54553682,
        -0.80818138,  -0.22188384]), 'targetState': array([-26.22297931, -26.10278445,  87.        ]), 'previousTarget': array([-26.22297931, -26.10278445,  87.        ])}
episode index:18684
target thresh 85.33480657952991
target distance 49.0
model initialize at round 18684
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.63925584, -20.48567414,  69.15964817]), 'distance': 27.5, 'localFrame': array([[ 0.1443023 , -0.78080242, -0.60788521],
       [ 0.9833475 ,  0.18173523,  0.        ],
       [ 0.11047416, -0.5977624 ,  0.79402492]]), 'currentState': array([19.69586667, -6.73997638, 92.43488135,  0.1443023 , -0.78080242,
       -0.60788521]), 'targetState': array([  9.39050332, -34.7536825 ,  45.        ]), 'previousTarget': array([ 14.45317067, -19.72874396,  70.61652382])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6281688015072376
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 33, 'trapConfig': [], 'currentTarget': array([  9.39050332, -34.7536825 ,  45.        ]), 'distance': 3.528859926007818, 'localFrame': array([[-0.59046548, -0.55448839, -0.58642403],
       [ 0.68454999, -0.72896592,  0.        ],
       [-0.42748313, -0.40143656,  0.81000423]]), 'currentState': array([ 11.68921882, -32.96465474,  46.99201886,  -0.59046548,
        -0.55448839,  -0.58642403]), 'targetState': array([  9.39050332, -34.7536825 ,  45.        ]), 'previousTarget': array([  9.39050332, -34.7536825 ,  45.        ])}
episode index:18685
target thresh 85.33627302554842
target distance 40.0
model initialize at round 18685
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.60214728, -8.26318396, 47.33818385]), 'distance': 27.5, 'localFrame': array([[-0.31742856,  0.84469079,  0.43097168],
       [-0.93608502, -0.35177384,  0.        ],
       [ 0.15160456, -0.40342613,  0.90236545]]), 'currentState': array([ -7.35304471, -25.89380686,  26.41346881,  -0.31742856,
         0.84469079,   0.43097168]), 'targetState': array([-2.28021151,  6.61820485, 65.        ]), 'previousTarget': array([-4.6471139 , -9.41741715, 45.94601849])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6281743743729203
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.28021151,  6.61820485, 65.        ]), 'distance': 2.8087499863275167, 'localFrame': array([[ 0.4387713 ,  0.71589666, -0.543113  ],
       [-0.85260341,  0.52255853,  0.        ],
       [ 0.28380833,  0.46306   ,  0.83965962]]), 'currentState': array([-1.53208856,  4.12285656, 63.94994026,  0.4387713 ,  0.71589666,
       -0.543113  ]), 'targetState': array([-2.28021151,  6.61820485, 65.        ]), 'previousTarget': array([-2.28021151,  6.61820485, 65.        ])}
episode index:18686
target thresh 85.33773932492969
target distance 63.0
model initialize at round 18686
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.38245526, -22.43658426,  64.65872451]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.47750818, -0.85527405,  0.20122683],
       [ 0.8731343 ,  0.48747974,  0.        ],
       [-0.098094  ,  0.17569805,  0.97954467]]), 'currentState': array([ -8.88137781, -31.75926463,  90.52547357,   0.47750818,
        -0.85527405,   0.20122683]), 'targetState': array([-7.67537514, -9.22434911, 28.        ]), 'previousTarget': array([ -9.04321929, -21.6405072 ,  64.94848658])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6281721729712809
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-7.67537514, -9.22434911, 28.        ]), 'distance': 3.926490779663595, 'localFrame': array([[ 0.01095772,  0.82725351, -0.56172195],
       [-0.99991228,  0.01324474,  0.        ],
       [ 0.00743986,  0.56167268,  0.82732608]]), 'currentState': array([-7.28673774e+00, -1.20028838e+01,  2.52529951e+01,  1.09577209e-02,
        8.27253515e-01, -5.61721951e-01]), 'targetState': array([-7.67537514, -9.22434911, 28.        ]), 'previousTarget': array([-7.67537514, -9.22434911, 28.        ])}
episode index:18687
target thresh 85.33920547768832
target distance 35.97720644606433
model initialize at round 18687
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.39836382,   9.33545597,   1.78588624]), 'distance': 27.499999999999996, 'localFrame': array([[-0.98565222,  0.14989211, -0.07760198],
       [-0.15034549, -0.98863352,  0.        ],
       [-0.07671992,  0.01166711,  0.99698442]]), 'currentState': array([11.23000801, 17.81342792,  7.03570043, -0.98565222,  0.14989211,
       -0.07760198]), 'targetState': array([-23.11664508,   6.45141228,   0.        ]), 'previousTarget': array([-12.7853597 ,   9.72452017,   2.29729574])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6281696576802465
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([-23.11664508,   6.45141228,   0.        ]), 'distance': 4.187728084045536, 'localFrame': array([[-0.68446015,  0.34132089, -0.64421608],
       [-0.44626237, -0.89490217,  0.        ],
       [-0.57651037,  0.28748939,  0.76484354]]), 'currentState': array([-20.25125352,   7.45046692,   2.88591191,  -0.68446015,
         0.34132089,  -0.64421608]), 'targetState': array([-23.11664508,   6.45141228,   0.        ]), 'previousTarget': array([-2.31166451e+01,  6.45141228e+00,  4.44089210e-16])}
episode index:18688
target thresh 85.34067148383903
target distance 22.153207136395647
model initialize at round 18688
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.83596084, -31.61717277,  60.38324702]), 'distance': 27.499999999999996, 'localFrame': array([[-0.28462532, -0.58846178, -0.75677021],
       [ 0.90022777, -0.4354193 ,  0.        ],
       [-0.32951235, -0.68126556,  0.653681  ]]), 'currentState': array([  1.62264131, -15.03855746,  68.31073797,  -0.28462532,
        -0.58846178,  -0.75677021]), 'targetState': array([-19.82501252, -32.41865017,  60.        ]), 'previousTarget': array([-17.97282375, -30.9471103 ,  60.83608154])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6281811496560453
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.82501252, -32.41865017,  60.        ]), 'distance': 3.4540725060104647, 'localFrame': array([[-0.55966542, -0.82772757, -0.0405176 ],
       [ 0.82840784, -0.56012538,  0.        ],
       [-0.02269493, -0.03356509,  0.99917883]]), 'currentState': array([-1.88374525e+01, -2.98585812e+01,  5.79020513e+01, -5.59665423e-01,
       -8.27727575e-01, -4.05175965e-02]), 'targetState': array([-19.82501252, -32.41865017,  60.        ]), 'previousTarget': array([-19.82501252, -32.41865017,  60.        ])}
episode index:18689
target thresh 85.34213734339644
target distance 33.0
model initialize at round 18689
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.53061972, -2.40581305, 28.08373347]), 'distance': 27.5, 'localFrame': array([[-0.4900779 ,  0.09521005, -0.86646333],
       [-0.19070969, -0.98164648,  0.        ],
       [-0.85056068,  0.16524296,  0.49924072]]), 'currentState': array([ 11.34247486, -10.78307968,  52.7499576 ,  -0.4900779 ,
         0.09521005,  -0.86646333]), 'targetState': array([-0., -0., 21.]), 'previousTarget': array([ 3.16588714, -3.06458824, 29.5531996 ])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6281724658478866
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([0.00000000e+00, 1.11022302e-16, 2.10000000e+01]), 'distance': 3.239420410219989, 'localFrame': array([[-0.35884253, -0.73741785,  0.57222982],
       [ 0.89918774, -0.43756304,  0.        ],
       [ 0.25038662,  0.51454203,  0.82009331]]), 'currentState': array([ 2.19394669, -0.9239508 , 18.80301174, -0.35884253, -0.73741785,
        0.57222982]), 'targetState': array([-0., -0., 21.]), 'previousTarget': array([ 0.,  0., 21.])}
episode index:18690
target thresh 85.34360305637524
target distance 72.0
model initialize at round 18690
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.8703356 , -18.98174292,  48.25394829]), 'distance': 27.5, 'localFrame': array([[-0.55455927,  0.69149441,  0.46292494],
       [-0.78011775, -0.62563272,  0.        ],
       [ 0.28962099, -0.36113596,  0.88639748]]), 'currentState': array([ 12.42328589, -31.72988353,  25.4376832 ,  -0.55455927,
         0.69149441,   0.46292494]), 'targetState': array([-14.02784385,   7.69542702,  96.        ]), 'previousTarget': array([  4.68808119, -19.66441556,  46.80565439])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6281690274403431
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.02784385,   7.69542702,  96.        ]), 'distance': 3.5475075784304493, 'localFrame': array([[-0.73286285,  0.68011859, -0.01872798],
       [-0.6802379 , -0.73299141,  0.        ],
       [-0.01372745,  0.01273948,  0.99982462]]), 'currentState': array([-1.19453781e+01,  1.02334021e+01,  9.46558166e+01, -7.32862854e-01,
        6.80118592e-01, -1.87279815e-02]), 'targetState': array([-14.02784385,   7.69542702,  96.        ]), 'previousTarget': array([-14.02784385,   7.69542702,  96.        ])}
episode index:18691
target thresh 85.34506862279005
target distance 49.037265828849314
model initialize at round 18691
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.72657324,  -9.20542406,  68.85991119]), 'distance': 27.5, 'localFrame': array([[ 0.36226756, -0.40462008,  0.83966946],
       [ 0.74502253,  0.6670393 ,  0.        ],
       [-0.56009253,  0.62557267,  0.54309778]]), 'currentState': array([-24.42113364,  17.78249932,  71.2812939 ,   0.36226756,
        -0.40462008,   0.83966946]), 'targetState': array([-16.12059   , -29.93537336,  67.        ]), 'previousTarget': array([-19.83721966,  -7.96372066,  68.34418094])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281354211367137
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([-16.12059   , -29.93537336,  67.        ]), 'distance': 8.844402399082902, 'localFrame': array([[ 0.27957819, -0.85562507,  0.4355936 ],
       [ 0.95054302,  0.31059293,  0.        ],
       [-0.13529229,  0.41405046,  0.90014344]]), 'currentState': array([-22.36032897, -23.84593982,  65.51409601,   0.27957819,
        -0.85562507,   0.4355936 ]), 'targetState': array([-16.12059   , -29.93537336,  67.        ]), 'previousTarget': array([-16.12059   , -29.93537336,  67.        ])}
episode index:18692
target thresh 85.34653404265556
target distance 37.0
model initialize at round 18692
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 9.64427047, -0.19800734, 55.3459299 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.07885212, -0.73641082,  0.67192369],
       [ 0.99431617, -0.10646766,  0.        ],
       [ 0.07153814,  0.66810459,  0.74062038]]), 'currentState': array([-6.43164158,  6.29555321, 34.        , -0.07885212, -0.73641082,
        0.67192369]), 'targetState': array([21.43356497, -4.96006982, 71.        ]), 'previousTarget': array([ 9.64427047, -0.19800734, 55.3459299 ])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6281430126268259
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([21.43356497, -4.96006982, 71.        ]), 'distance': 4.573248923001762, 'localFrame': array([[ 0.52076382, -0.21445602,  0.82632539],
       [ 0.38078603,  0.92466318,  0.        ],
       [-0.76407267,  0.31465316,  0.56319299]]), 'currentState': array([18.72689229, -2.16962784, 68.59127372,  0.52076382, -0.21445602,
        0.82632539]), 'targetState': array([21.43356497, -4.96006982, 71.        ]), 'previousTarget': array([21.43356497, -4.96006982, 71.        ])}
episode index:18693
target thresh 85.34799931598641
target distance 52.0
model initialize at round 18693
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.50452077,  2.18441721, 39.67057819]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.68948783, -0.41249185, -0.59536292],
       [ 0.51339636,  0.85815161,  0.        ],
       [ 0.51091165, -0.30565715,  0.8034569 ]]), 'currentState': array([ 5.92478165, -3.57886176, 61.5864707 ,  0.68948783, -0.41249185,
       -0.59536292]), 'targetState': array([41.88608305,  9.72399334, 11.        ]), 'previousTarget': array([20.94301456,  2.68880897, 40.88630328])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6281462920786951
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([41.88608305,  9.72399334, 11.        ]), 'distance': 3.1747596358008794, 'localFrame': array([[ 0.63212788,  0.62875251, -0.4528627 ],
       [-0.70521133,  0.70899717,  0.        ],
       [ 0.32107837,  0.3193639 ,  0.89158027]]), 'currentState': array([39.46662994,  9.50438685, 13.04379998,  0.63212788,  0.62875251,
       -0.4528627 ]), 'targetState': array([41.88608305,  9.72399334, 11.        ]), 'previousTarget': array([41.88608305,  9.72399334, 11.        ])}
episode index:18694
target thresh 85.34946444279726
target distance 27.0
model initialize at round 18694
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.11700126,  -6.15761894,  51.31373194]), 'distance': 27.5, 'localFrame': array([[ 0.25895117,  0.73115153,  0.63115904],
       [-0.94262655,  0.33384906,  0.        ],
       [-0.21071185, -0.59494727,  0.77565345]]), 'currentState': array([-17.79357795, -17.40400748,  26.43957663,   0.25895117,
         0.73115153,   0.63115904]), 'targetState': array([-21.20869319,  -5.84733555,  52.        ]), 'previousTarget': array([-21.02290383,  -6.7602503 ,  49.97749717])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.628158697110124
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.20869319,  -5.84733555,  52.        ]), 'distance': 4.050104012821317, 'localFrame': array([[-0.58487607,  0.61242475,  0.531842  ],
       [-0.72318518, -0.69065418,  0.        ],
       [ 0.3673189 , -0.38462025,  0.84684361]]), 'currentState': array([-18.71726637,  -7.02834731,  49.03329373,  -0.58487607,
         0.61242475,   0.531842  ]), 'targetState': array([-21.20869319,  -5.84733555,  52.        ]), 'previousTarget': array([-21.20869319,  -5.84733555,  52.        ])}
episode index:18695
target thresh 85.35092942310274
target distance 52.0
model initialize at round 18695
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-1.36281435, -9.9446066 , 26.88286742]), 'distance': 27.499999999999996, 'localFrame': array([[-0.85971445,  0.51031529, -0.02166516],
       [-0.51043509, -0.85991628,  0.        ],
       [-0.01863023,  0.01105866,  0.99976528]]), 'currentState': array([  4.29131508, -11.20645428,   0.        ,  -0.85971445,
         0.51031529,  -0.02166516]), 'targetState': array([-6.64556623, -8.76564028, 52.        ]), 'previousTarget': array([-1.36281435, -9.9446066 , 26.88286742])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281250985490889
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-6.64556623, -8.76564028, 52.        ]), 'distance': 23.66776991191712, 'localFrame': array([[ 0.16993264,  0.98506468,  0.02775757],
       [-0.98544438,  0.16999814,  0.        ],
       [-0.00471874, -0.02735354,  0.99961468]]), 'currentState': array([-8.47306608e+00, -9.95288205e+00,  2.84327763e+01,  1.69932640e-01,
        9.85064676e-01,  2.77575732e-02]), 'targetState': array([-6.64556623, -8.76564028, 52.        ]), 'previousTarget': array([-6.64556623, -8.76564028, 52.        ])}
episode index:18696
target thresh 85.35239425691752
target distance 32.68438207186671
model initialize at round 18696
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.21367923, -8.98102106, 50.25816544]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.21071976, -0.67179192, -0.71013576],
       [ 0.9541621 ,  0.29929031,  0.        ],
       [ 0.21253675, -0.67758463,  0.70406477]]), 'currentState': array([ 0.25969788, 11.14291011, 67.84716151,  0.21071976, -0.67179192,
       -0.71013576]), 'targetState': array([ -9.98904952, -20.71759855,  40.        ]), 'previousTarget': array([-6.49723239, -8.17166694, 51.131678  ])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6281339496580955
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.98904952, -20.71759855,  40.        ]), 'distance': 3.2626100976695613, 'localFrame': array([[-0.29884254, -0.83100041, -0.46918169],
       [ 0.94100198, -0.33840106,  0.        ],
       [-0.15877158, -0.44150089,  0.88310166]]), 'currentState': array([-10.76504703, -18.44905049,  42.21272281,  -0.29884254,
        -0.83100041,  -0.46918169]), 'targetState': array([ -9.98904952, -20.71759855,  40.        ]), 'previousTarget': array([ -9.98904952, -20.71759855,  40.        ])}
episode index:18697
target thresh 85.35385894425623
target distance 30.77674906327319
model initialize at round 18697
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.04224377,  5.06815319, 45.56132379]), 'distance': 27.5, 'localFrame': array([[ 0.73850484,  0.50382872, -0.44807055],
       [-0.56356793,  0.82606972,  0.        ],
       [ 0.37013751,  0.25251819,  0.8939982 ]]), 'currentState': array([-1.85757281, 13.22173161, 62.70094045,  0.73850484,  0.50382872,
       -0.44807055]), 'targetState': array([27.98230116,  0.99540019, 37.        ]), 'previousTarget': array([17.51208876,  4.62691749, 45.84516821])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6281415391968775
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.98230116,  0.99540019, 37.        ]), 'distance': 2.949268262373736, 'localFrame': array([[ 0.968208  ,  0.22680301, -0.10551617],
       [-0.22807623,  0.97364328,  0.        ],
       [ 0.10273511,  0.02406573,  0.99441759]]), 'currentState': array([25.26591684,  2.03444571, 36.51028183,  0.968208  ,  0.22680301,
       -0.10551617]), 'targetState': array([27.98230116,  0.99540019, 37.        ]), 'previousTarget': array([27.98230116,  0.99540019, 37.        ])}
episode index:18698
target thresh 85.35532348513355
target distance 30.0
model initialize at round 18698
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.31438445,  2.32579339, 55.37849297]), 'distance': 27.5, 'localFrame': array([[ 0.01000475, -0.46877484, -0.88326103],
       [ 0.99977233,  0.02133749,  0.        ],
       [ 0.01884657, -0.88305994,  0.46888159]]), 'currentState': array([-1.18001962e+01, -2.10086477e+00,  7.96661778e+01,  1.00047548e-02,
       -4.68774839e-01, -8.83261035e-01]), 'targetState': array([ 2.49835573,  3.12381476, 51.        ]), 'previousTarget': array([ 0.20823869,  2.39311235, 56.11313884])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6281525757347809
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.49835573,  3.12381476, 51.        ]), 'distance': 3.160638169636005, 'localFrame': array([[ 0.69826389,  0.32684326, -0.63686814],
       [-0.4239362 ,  0.90569205,  0.        ],
       [ 0.57680642,  0.26999146,  0.77097274]]), 'currentState': array([ 1.89775517,  4.11617727, 53.94009001,  0.69826389,  0.32684326,
       -0.63686814]), 'targetState': array([ 2.49835573,  3.12381476, 51.        ]), 'previousTarget': array([ 2.49835573,  3.12381476, 51.        ])}
episode index:18699
target thresh 85.3567878795641
target distance 22.25505285137962
model initialize at round 18699
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.40498133,  27.31513204,  95.        ]), 'distance': 24.039926211505712, 'localFrame': array([[-0.28113942,  0.5454916 ,  0.78955655],
       [-0.88888923, -0.45812218,  0.        ],
       [ 0.36171337, -0.70182831,  0.61367782]]), 'currentState': array([-35.94186572,  22.49369365,  94.1712314 ,  -0.28113942,
         0.5454916 ,   0.78955655]), 'targetState': array([-12.40498133,  27.31513204,  95.        ]), 'previousTarget': array([-12.40498133,  27.31513204,  95.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6281654416832854
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.40498133,  27.31513204,  95.        ]), 'distance': 1.6500196715354531, 'localFrame': array([[ 0.92822821,  0.29807583, -0.222583  ],
       [-0.30574585,  0.95211316,  0.        ],
       [ 0.2119242 ,  0.06805383,  0.97491374]]), 'currentState': array([-14.05405597,  27.26333486,  95.02085148,   0.92822821,
         0.29807583,  -0.222583  ]), 'targetState': array([-12.40498133,  27.31513204,  95.        ]), 'previousTarget': array([-12.40498133,  27.31513204,  95.        ])}
episode index:18700
target thresh 85.35825212756252
target distance 57.0
model initialize at round 18700
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.86027501,  9.80681529, 38.64446581]), 'distance': 27.499999999999996, 'localFrame': array([[-0.90884639, -0.30842481, -0.28084227],
       [ 0.32135821, -0.94695771,  0.        ],
       [-0.26594575, -0.09025097,  0.95975394]]), 'currentState': array([29.90728038,  8.96642347, 59.37727408, -0.90884639, -0.30842481,
       -0.28084227]), 'targetState': array([-20.03713427,  11.2921765 ,   2.        ]), 'previousTarget': array([13.07584857, 10.31689496, 38.60910214])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6281608173120905
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-20.03713427,  11.2921765 ,   2.        ]), 'distance': 3.893386325607214, 'localFrame': array([[-0.98852364,  0.07230986, -0.13263593],
       [-0.07295443, -0.99733528,  0.        ],
       [-0.13228249,  0.00967638,  0.99116482]]), 'currentState': array([-17.39818173,  12.60417516,   4.54421817,  -0.98852364,
         0.07230986,  -0.13263593]), 'targetState': array([-20.03713427,  11.2921765 ,   2.        ]), 'previousTarget': array([-20.03713427,  11.2921765 ,   2.        ])}
episode index:18701
target thresh 85.35971622914347
target distance 59.0
model initialize at round 18701
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.31915272,  1.51775733, 42.67020972]), 'distance': 27.5, 'localFrame': array([[ 0.88972179,  0.01181181,  0.45635033],
       [-0.01327468,  0.99991189,  0.        ],
       [-0.45631012, -0.0060579 ,  0.88980019]]), 'currentState': array([-9.11007307e+00,  1.37474210e+01,  1.99155392e+01,  8.89721788e-01,
        1.18118099e-02,  4.56350327e-01]), 'targetState': array([ 14.95933701, -17.47049617,  78.        ]), 'previousTarget': array([-0.76849348,  2.32231386, 41.58900776])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6281583045111497
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.95933701, -17.47049617,  78.        ]), 'distance': 2.5734762390022863, 'localFrame': array([[ 0.06622969,  0.35421446,  0.93281603],
       [-0.98296532,  0.18379115,  0.        ],
       [-0.17144333, -0.9169258 ,  0.36035296]]), 'currentState': array([ 1.47604553e+01, -1.82344351e+01,  7.55505872e+01,  6.62296851e-02,
        3.54214460e-01,  9.32816030e-01]), 'targetState': array([ 14.95933701, -17.47049617,  78.        ]), 'previousTarget': array([ 14.95933701, -17.47049617,  78.        ])}
episode index:18702
target thresh 85.36118018432157
target distance 46.0
model initialize at round 18702
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.66458119, -13.78870109,  66.9106746 ]), 'distance': 27.5, 'localFrame': array([[-0.3565506 ,  0.79832254,  0.48533781],
       [-0.9130712 , -0.40780018,  0.        ],
       [ 0.19792085, -0.44314798,  0.87432672]]), 'currentState': array([  2.98779664, -31.44749845,  47.27693184,  -0.3565506 ,
         0.79832254,   0.48533781]), 'targetState': array([20.86549562,  9.67631606, 93.        ]), 'previousTarget': array([ 11.32245835, -14.70748913,  66.54501631])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6281570666221814
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.86549562,  9.67631606, 93.        ]), 'distance': 1.7040112815092887, 'localFrame': array([[ 0.44322388,  0.30884446,  0.841527  ],
       [-0.5717065 ,  0.82045821,  0.        ],
       [-0.69043774, -0.48110645,  0.54021506]]), 'currentState': array([21.11943834,  9.94556489, 91.33666822,  0.44322388,  0.30884446,
        0.841527  ]), 'targetState': array([20.86549562,  9.67631606, 93.        ]), 'previousTarget': array([20.86549562,  9.67631606, 93.        ])}
episode index:18703
target thresh 85.36264399311148
target distance 30.0
model initialize at round 18703
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 30.7392291 , -12.10643238,  64.43485437]), 'distance': 27.5, 'localFrame': array([[-0.89807755,  0.28326125, -0.33648148],
       [-0.30080093, -0.95368695,  0.        ],
       [-0.32089799,  0.10121394,  0.94169009]]), 'currentState': array([ 29.01847304, -27.66430208,  41.82420004,  -0.89807755,
         0.28326125,  -0.33648148]), 'targetState': array([31.39106899, -6.21295323, 73.        ]), 'previousTarget': array([ 31.15067561, -11.57555465,  65.43121438])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6281234825189617
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([31.39106899, -6.21295323, 73.        ]), 'distance': 9.840154512970475, 'localFrame': array([[ 0.00636192, -0.41853352,  0.90817907],
       [ 0.99988449,  0.01519874,  0.        ],
       [-0.01380318,  0.90807417,  0.41858187]]), 'currentState': array([ 3.14059122e+01, -7.48849799e+00,  6.32428792e+01,  6.36191790e-03,
       -4.18533518e-01,  9.08179068e-01]), 'targetState': array([31.39106899, -6.21295323, 73.        ]), 'previousTarget': array([31.39106899, -6.21295323, 73.        ])}
episode index:18704
target thresh 85.36410765552782
target distance 37.548341861881624
model initialize at round 18704
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.57524714, 37.17253058, 30.83211892]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.6163749 , -0.47880679, -0.62516081],
       [ 0.61346478,  0.78972208,  0.        ],
       [ 0.4937033 , -0.38351414,  0.78049597]]), 'currentState': array([-27.22684834,  33.13134708,  49.63992065,   0.6163749 ,
        -0.47880679,  -0.62516081]), 'targetState': array([10.01203357, 40.78920425, 14.        ]), 'previousTarget': array([-8.0942966 , 37.66888616, 31.84191214])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6281282730589285
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.01203357, 40.78920425, 14.        ]), 'distance': 2.315511444896226, 'localFrame': array([[ 0.8227494 ,  0.48750431, -0.29227208],
       [-0.50976301,  0.86031487,  0.        ],
       [ 0.25144601,  0.14898949,  0.95633521]]), 'currentState': array([ 8.48756926, 40.16515508, 15.62731817,  0.8227494 ,  0.48750431,
       -0.29227208]), 'targetState': array([10.01203357, 40.78920425, 14.        ]), 'previousTarget': array([10.01203357, 40.78920425, 14.        ])}
episode index:18705
target thresh 85.36557117158526
target distance 45.44920875194679
model initialize at round 18705
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.91406378,  13.34696157,  68.67534832]), 'distance': 27.499999999999996, 'localFrame': array([[-0.76264328,  0.23589701,  0.6022689 ],
       [-0.29550168, -0.95534222,  0.        ],
       [ 0.57537291, -0.17797147,  0.79829329]]), 'currentState': array([ 4.29316029, 27.54766408, 56.58162501, -0.76264328,  0.23589701,
        0.6022689 ]), 'targetState': array([-39.84891321,  -3.47334365,  83.        ]), 'previousTarget': array([-14.98108858,  12.8790578 ,  68.22677769])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6281273636697738
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-39.84891321,  -3.47334365,  83.        ]), 'distance': 2.5561199244906128, 'localFrame': array([[-0.22716149, -0.71453823,  0.66168934],
       [ 0.95299958, -0.30297162,  0.        ],
       [ 0.20047309,  0.63058966,  0.74977811]]), 'currentState': array([-37.78515889,  -2.21403539,  82.17005429,  -0.22716149,
        -0.71453823,   0.66168934]), 'targetState': array([-39.84891321,  -3.47334365,  83.        ]), 'previousTarget': array([-39.84891321,  -3.47334365,  83.        ])}
episode index:18706
target thresh 85.3670345412984
target distance 36.0
model initialize at round 18706
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.19305306,  10.32636704,  34.03164644]), 'distance': 27.499999999999996, 'localFrame': array([[-0.42429689, -0.6853417 , -0.59184364],
       [ 0.85024419, -0.52638846,  0.        ],
       [-0.31153966, -0.50321162,  0.80605279]]), 'currentState': array([-28.40793705,  22.23524634,  14.46289843,  -0.42429689,
        -0.6853417 ,  -0.59184364]), 'targetState': array([-0., -0., 51.]), 'previousTarget': array([-13.15137721,  11.22437782,  34.17726135])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6281321534901046
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 51.]), 'distance': 2.8582052327992558, 'localFrame': array([[ 0.48759766, -0.78356571,  0.38506272],
       [ 0.84903441,  0.52833756,  0.        ],
       [-0.2034431 ,  0.3269315 ,  0.9228904 ]]), 'currentState': array([ 1.06431192,  2.4340094 , 52.05459734,  0.48759766, -0.78356571,
        0.38506272]), 'targetState': array([-0., -0., 51.]), 'previousTarget': array([ 0.,  0., 51.])}
episode index:18707
target thresh 85.36849776468188
target distance 45.102396991998184
model initialize at round 18707
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.65837077, -18.72898044,  22.09420887]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.58305004,  0.58672391,  0.5619677 ],
       [-0.70932407,  0.70488252,  0.        ],
       [-0.39612121, -0.39861722,  0.82715918]]), 'currentState': array([-45.27937297,   4.3636851 ,  15.97480555,   0.58305004,
         0.58672391,   0.5619677 ]), 'targetState': array([-18.51284175, -41.01554206,  28.        ]), 'previousTarget': array([-32.61951924, -18.54064461,  21.52199248])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6281339789063701
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-18.51284175, -41.01554206,  28.        ]), 'distance': 1.8899525871837333, 'localFrame': array([[ 0.94416598, -0.2125431 , -0.25174596],
       [ 0.2196162 ,  0.97558635,  0.        ],
       [ 0.24559992, -0.05528749,  0.96779335]]), 'currentState': array([-20.00063096, -41.75879851,  27.10223951,   0.94416598,
        -0.2125431 ,  -0.25174596]), 'targetState': array([-18.51284175, -41.01554206,  28.        ]), 'previousTarget': array([-18.51284175, -41.01554206,  28.        ])}
episode index:18708
target thresh 85.36996084175033
target distance 44.0
model initialize at round 18708
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.53542553,  -8.19909742,  29.5326934 ]), 'distance': 27.5, 'localFrame': array([[-0.77503896, -0.51427782,  0.36719604],
       [ 0.55290155, -0.83324659,  0.        ],
       [ 0.30596485,  0.20302326,  0.93014357]]), 'currentState': array([-13.99355842,   1.05650878,   6.87689231,  -0.77503896,
        -0.51427782,   0.36719604]), 'targetState': array([-38.41936225, -16.96916629,  51.        ]), 'previousTarget': array([-25.71307079,  -7.35908056,  29.20224413])}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6281296506506369
{'scaleFactor': 20, 'timeStep': 61, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-38.41936225, -16.96916629,  51.        ]), 'distance': 2.975093804717325, 'localFrame': array([[-0.71288375,  0.29209555,  0.63755545],
       [-0.37914571, -0.92533698,  0.        ],
       [ 0.58995364, -0.24172641,  0.77040447]]), 'currentState': array([-38.0808866 , -16.29333478,  48.12252382,  -0.71288375,
         0.29209555,   0.63755545]), 'targetState': array([-38.41936225, -16.96916629,  51.        ]), 'previousTarget': array([-38.41936225, -16.96916629,  51.        ])}
episode index:18709
target thresh 85.3714237725184
target distance 19.0
model initialize at round 18709
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-25.02754168,  10.13025948,  54.        ]), 'distance': 21.703974263536978, 'localFrame': array([[ 0.24032687,  0.94527863, -0.22066107],
       [-0.96916809,  0.24640051,  0.        ],
       [ 0.054371  ,  0.21385767,  0.97535055]]), 'currentState': array([-17.82525635,   2.50204968,  73.        ,   0.24032687,
         0.94527863,  -0.22066107]), 'targetState': array([-25.02754168,  10.13025948,  54.        ]), 'previousTarget': array([-25.02754168,  10.13025948,  54.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628096078782617
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 87, 'trapConfig': [], 'currentTarget': array([-25.02754168,  10.13025948,  54.        ]), 'distance': 13.587775457576384, 'localFrame': array([[ 0.54743864,  0.66492878, -0.5081148 ],
       [-0.77201556,  0.63560363,  0.        ],
       [ 0.32295961,  0.39227253,  0.86128935]]), 'currentState': array([-29.87896185,  12.66364627,  66.43677272,   0.54743864,
         0.66492878,  -0.5081148 ]), 'targetState': array([-25.02754168,  10.13025948,  54.        ]), 'previousTarget': array([-25.02754168,  10.13025948,  54.        ])}
episode index:18710
target thresh 85.3728865570007
target distance 31.689514523839414
model initialize at round 18710
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.56296118,  4.05519656, 52.72701997]), 'distance': 27.5, 'localFrame': array([[ 0.4310828 ,  0.273035  , -0.86001134],
       [-0.53507431,  0.844805  ,  0.        ],
       [ 0.72654188,  0.46016998,  0.51027492]]), 'currentState': array([27.80690729,  6.60796898, 70.00209179,  0.4310828 ,  0.273035  ,
       -0.86001134]), 'targetState': array([-5.39880026,  2.61781507, 43.        ]), 'previousTarget': array([ 5.75408953,  3.86025117, 52.85439249])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6281036650723408
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.39880026,  2.61781507, 43.        ]), 'distance': 2.742078784741627, 'localFrame': array([[-0.40470905, -0.3571263 , -0.84182622],
       [ 0.66165296, -0.74981021,  0.        ],
       [-0.6312099 , -0.55699682,  0.53974865]]), 'currentState': array([-6.63974973,  3.28953984, 45.35113298, -0.40470905, -0.3571263 ,
       -0.84182622]), 'targetState': array([-5.39880026,  2.61781507, 43.        ]), 'previousTarget': array([-5.39880026,  2.61781507, 43.        ])}
episode index:18711
target thresh 85.37434919521188
target distance 53.0
model initialize at round 18711
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.466214  , -13.48651288,  60.94322407]), 'distance': 27.5, 'localFrame': array([[-0.5659711 , -0.12168712, -0.81539497],
       [ 0.21020221, -0.97765793,  0.        ],
       [-0.79717736, -0.17139783,  0.57890504]]), 'currentState': array([-40.03370349, -27.80801537,  80.77256531,  -0.5659711 ,
        -0.12168712,  -0.81539497]), 'targetState': array([-7.22115755,  9.58409535, 29.        ]), 'previousTarget': array([-26.748324  , -13.96359741,  61.86937177])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6281021073736124
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-7.22115755,  9.58409535, 29.        ]), 'distance': 2.5306515595066728, 'localFrame': array([[ 0.33903112,  0.87677249,  0.34106876],
       [-0.93269864,  0.36065669,  0.        ],
       [-0.12300873, -0.31811437,  0.94003835]]), 'currentState': array([-8.93785756,  7.72601546, 28.93160742,  0.33903112,  0.87677249,
        0.34106876]), 'targetState': array([-7.22115755,  9.58409535, 29.        ]), 'previousTarget': array([-7.22115755,  9.58409535, 29.        ])}
episode index:18712
target thresh 85.37581168716653
target distance 59.0
model initialize at round 18712
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.76895779, -2.01646854, 42.17689873]), 'distance': 27.5, 'localFrame': array([[ 0.39368468, -0.64180172, -0.65810556],
       [ 0.85241045,  0.52287323,  0.        ],
       [ 0.34410578, -0.56097606,  0.75292568]]), 'currentState': array([ 2.8835704 , -2.950904  , 14.71538987,  0.39368468, -0.64180172,
       -0.65810556]), 'targetState': array([ 0.43672791, -0.89959365, 75.        ]), 'previousTarget': array([ 1.06553405, -1.76991848, 43.48405437])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280685423595915
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.53073017, -4.51234119, 54.4888049 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.8932842 , -0.42596237, -0.14352488],
       [ 0.43041862,  0.90262939,  0.        ],
       [ 0.12954978, -0.06177578,  0.98964671]]), 'currentState': array([-4.11705855, -9.26149015, 27.52574703,  0.8932842 , -0.42596237,
       -0.14352488]), 'targetState': array([ 0.43672791, -0.89959365, 75.        ]), 'previousTarget': array([-2.35513929, -4.53201379, 54.08228727])}
episode index:18713
target thresh 85.37727403287931
target distance 35.78311260211688
model initialize at round 18713
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.95475883, -17.84316127,  41.24993203]), 'distance': 27.499999999999996, 'localFrame': array([[-0.52162273, -0.06694528, -0.85054574],
       [ 0.12729633, -0.99186473,  0.        ],
       [-0.84362632, -0.10827135,  0.52590108]]), 'currentState': array([ 2.16980043, -1.16550706, 57.04086478, -0.52162273, -0.06694528,
       -0.85054574]), 'targetState': array([-30.43454818, -37.11789699,  23.        ]), 'previousTarget': array([-11.74498346, -17.56620865,  42.12380009])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6280669867041521
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([-30.43454818, -37.11789699,  23.        ]), 'distance': 2.7524979495674797, 'localFrame': array([[ 0.58265717,  0.28303493, -0.76184109],
       [-0.43694146,  0.89948994,  0.        ],
       [ 0.6852684 ,  0.33287996,  0.64776397]]), 'currentState': array([-29.20243652, -37.88597847,  25.33841755,   0.58265717,
         0.28303493,  -0.76184109]), 'targetState': array([-30.43454818, -37.11789699,  23.        ]), 'previousTarget': array([-30.43454818, -37.11789699,  23.        ])}
episode index:18714
target thresh 85.37873623236483
target distance 72.0
model initialize at round 18714
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.13765028,  7.69623923, 51.0732424 ]), 'distance': 27.5, 'localFrame': array([[-0.80014125, -0.50466234, -0.32417574],
       [ 0.53347146, -0.84581806,  0.        ],
       [-0.2741937 , -0.17293851,  0.94599688]]), 'currentState': array([20.59308753,  4.69812926, 72.11056749, -0.80014125, -0.50466234,
       -0.32417574]), 'targetState': array([-39.23968497,  14.9748831 ,   0.        ]), 'previousTarget': array([ 4.62380808,  7.98699087, 51.22320463])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6280606772603662
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-39.23968497,  14.9748831 ,   0.        ]), 'distance': 2.4155751151111344, 'localFrame': array([[ 0.0262479 ,  0.72922232, -0.68377325],
       [-0.99935283,  0.03597108,  0.        ],
       [ 0.02459606,  0.68333073,  0.72969455]]), 'currentState': array([-3.92790513e+01,  1.48570099e+01,  2.41237628e+00,  2.62479035e-02,
        7.29222318e-01, -6.83773251e-01]), 'targetState': array([-39.23968497,  14.9748831 ,   0.        ]), 'previousTarget': array([-39.23968497,  14.9748831 ,   0.        ])}
episode index:18715
target thresh 85.3801982856377
target distance 85.0
model initialize at round 18715
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.81904957,   3.55223105,  27.60205965]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.32298886,  0.32610995,  0.88844274],
       [-0.71049858,  0.70369863,  0.        ],
       [-0.62519594, -0.6312373 ,  0.45898748]]), 'currentState': array([-29.65763035,  -7.23692513,   2.37382686,   0.32298886,
         0.32610995,   0.88844274]), 'targetState': array([-23.56312995,  28.52681032,  86.        ]), 'previousTarget': array([-27.95388463,   2.42373279,  26.15007901])}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.6280515165994601
{'scaleFactor': 20, 'timeStep': 79, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-23.56312995,  28.52681032,  86.        ]), 'distance': 3.436020514957101, 'localFrame': array([[-0.41691769,  0.79778136,  0.43557381],
       [-0.88627305, -0.46316313,  0.        ],
       [ 0.20174173, -0.38603733,  0.90015302]]), 'currentState': array([-22.10855333,  30.57896112,  83.65924779,  -0.41691769,
         0.79778136,   0.43557381]), 'targetState': array([-23.56312995,  28.52681032,  86.        ]), 'previousTarget': array([-23.56312995,  28.52681032,  86.        ])}
episode index:18716
target thresh 85.38166019271257
target distance 49.00061896970311
model initialize at round 18716
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.36955608, -21.26310437,  56.88086191]), 'distance': 27.5, 'localFrame': array([[ 0.4836432 , -0.52031335, -0.70382048],
       [ 0.73244588,  0.68082525,  0.        ],
       [ 0.47917876, -0.51551041,  0.71037788]]), 'currentState': array([ 7.44091344,  4.90074086, 63.66205185,  0.4836432 , -0.52031335,
       -0.70382048]), 'targetState': array([ -2.02848505, -43.95321659,  51.        ]), 'previousTarget': array([  1.81972678, -21.05589989,  57.54200785])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6280578808981049
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.02848505, -43.95321659,  51.        ]), 'distance': 2.977821409007151, 'localFrame': array([[ 0.06350964, -0.99685515, -0.0473955 ],
       [ 0.99797668,  0.06358109,  0.        ],
       [ 0.00301346, -0.0472996 ,  0.9988762 ]]), 'currentState': array([ 8.31878748e-01, -4.32019638e+01,  5.13483655e+01,  6.35096357e-02,
       -9.96855151e-01, -4.73955005e-02]), 'targetState': array([ -2.02848505, -43.95321659,  51.        ]), 'previousTarget': array([ -2.02848505, -43.95321659,  51.        ])}
episode index:18717
target thresh 85.38312195360405
target distance 33.05673121929984
model initialize at round 18717
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-21.0883044 ,   8.33688542,  30.8412505 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.74686604,  0.51345447,  0.42255842],
       [-0.56651689,  0.82405013,  0.        ],
       [-0.34820932, -0.23938648,  0.90633569]]), 'currentState': array([-41.80188751,  -4.07459259,  44.        ,   0.74686604,
         0.51345447,   0.42255842]), 'targetState': array([-8.74515629, 15.73283959, 23.        ]), 'previousTarget': array([-21.0883044 ,   8.33688542,  30.8412505 ])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6280638453436915
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-8.74515629, 15.73283959, 23.        ]), 'distance': 1.773692506141921, 'localFrame': array([[-0.08738705,  0.84969992,  0.51997457],
       [-0.99475308, -0.10230498,  0.        ],
       [ 0.05319599, -0.51724631,  0.85418174]]), 'currentState': array([-9.17467385, 14.23034156, 23.83904675, -0.08738705,  0.84969992,
        0.51997457]), 'targetState': array([-8.74515629, 15.73283959, 23.        ]), 'previousTarget': array([-8.74515629, 15.73283959, 23.        ])}
episode index:18718
target thresh 85.38458356832672
target distance 18.0
model initialize at round 18718
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.72438351, -2.44695696,  8.        ]), 'distance': 18.989493431092647, 'localFrame': array([[-0.87466358,  0.48372161,  0.0312575 ],
       [-0.48395809, -0.87509118,  0.        ],
       [ 0.02735317, -0.01512732,  0.99951136]]), 'currentState': array([ 9.65339054, -3.81943114, 26.90952536, -0.87466358,  0.48372161,
        0.0312575 ]), 'targetState': array([10.72438351, -2.44695696,  8.        ]), 'previousTarget': array([10.72438351, -2.44695696,  8.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6280781235855268
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.72438351, -2.44695696,  8.        ]), 'distance': 2.5800249876712495, 'localFrame': array([[-0.82017829,  0.24779119, -0.5156618 ],
       [-0.28920803, -0.95726627,  0.        ],
       [-0.49362565,  0.14913353,  0.85679222]]), 'currentState': array([11.55374521, -1.3878929 , 10.20160655, -0.82017829,  0.24779119,
       -0.5156618 ]), 'targetState': array([10.72438351, -2.44695696,  8.        ]), 'previousTarget': array([10.72438351, -2.44695696,  8.        ])}
episode index:18719
target thresh 85.38604503689524
target distance 49.0
model initialize at round 18719
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.4318425 , -2.04159803, 43.06729221]), 'distance': 27.5, 'localFrame': array([[-0.2743849 , -0.73775603, -0.61678924],
       [ 0.93727539, -0.34858979,  0.        ],
       [-0.21500643, -0.57810138,  0.78712834]]), 'currentState': array([ 0.32210704, -1.12574546, 15.59289005, -0.2743849 , -0.73775603,
       -0.61678924]), 'targetState': array([-1.06115935, -2.80605432, 66.        ]), 'previousTarget': array([-0.59443556, -1.5718831 , 44.44860342])}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.6280661927013971
{'scaleFactor': 20, 'timeStep': 91, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-1.06115935, -2.80605432, 66.        ]), 'distance': 3.4521311538988373, 'localFrame': array([[ 0.30037049,  0.50350939,  0.8100962 ],
       [-0.85879582,  0.512318  ,  0.        ],
       [-0.41502687, -0.69570724,  0.58629697]]), 'currentState': array([-2.53836367, -5.04114609, 63.82297412,  0.30037049,  0.50350939,
        0.8100962 ]), 'targetState': array([-1.06115935, -2.80605432, 66.        ]), 'previousTarget': array([-1.06115935, -2.80605432, 66.        ])}
episode index:18720
target thresh 85.38750635932422
target distance 71.72131005172773
model initialize at round 18720
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.21022132, 11.49248705, 35.22480039]), 'distance': 27.499999999999996, 'localFrame': array([[-0.2576622 , -0.81216405,  0.52344985],
       [ 0.953181  , -0.30240038,  0.        ],
       [ 0.15829143,  0.49894245,  0.85205648]]), 'currentState': array([-2.54442282, 38.83714358, 33.4722986 , -0.2576622 , -0.81216405,
        0.52344985]), 'targetState': array([  3.48613729, -31.80954018,  38.        ]), 'previousTarget': array([-0.31718644, 12.6067262 , 34.28426198])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.628065287356962
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.48613729, -31.80954018,  38.        ]), 'distance': 4.0189929036075105, 'localFrame': array([[ 0.99133122, -0.07908191,  0.10492127],
       [ 0.07952083,  0.9968332 ,  0.        ],
       [-0.104589  ,  0.00834343,  0.99448053]]), 'currentState': array([  0.7282626 , -29.02008716,  38.87486167,   0.99133122,
        -0.07908191,   0.10492127]), 'targetState': array([  3.48613729, -31.80954018,  38.        ]), 'previousTarget': array([  3.48613729, -31.80954018,  38.        ])}
episode index:18721
target thresh 85.38896753562825
target distance 52.2926469814174
model initialize at round 18721
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.11448295, 12.34750611, 34.06638835]), 'distance': 27.5, 'localFrame': array([[ 0.38608051,  0.69570145,  0.60575683],
       [-0.87438144,  0.48523922,  0.        ],
       [-0.29393697, -0.52966253,  0.79564983]]), 'currentState': array([  5.19453594, -14.91887012,  37.4764049 ,   0.38608051,
         0.69570145,   0.60575683]), 'targetState': array([ 3.14326715, 36.86624298, 31.        ]), 'previousTarget': array([ 3.66870997, 11.94271739, 33.38308128])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6280689380316065
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 3.14326715, 36.86624298, 31.        ]), 'distance': 3.962328324143347, 'localFrame': array([[-0.9374055 , -0.03073213, -0.34688104],
       [ 0.03276664, -0.99946303,  0.        ],
       [-0.34669478, -0.01136613,  0.93790913]]), 'currentState': array([ 4.72684002e+00,  3.41506230e+01,  3.34120014e+01, -9.37405505e-01,
       -3.07321287e-02, -3.46881040e-01]), 'targetState': array([ 3.14326715, 36.86624298, 31.        ]), 'previousTarget': array([ 3.14326715, 36.86624298, 31.        ])}
episode index:18722
target thresh 85.39042856582196
target distance 67.715272259655
model initialize at round 18722
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.41393276, 23.43345525, 17.12198896]), 'distance': 27.499999999999996, 'localFrame': array([[-0.88043378, -0.05853217, -0.47054261],
       [ 0.06633463, -0.99779743,  0.        ],
       [-0.46950621, -0.03121327,  0.88237727]]), 'currentState': array([19.67276877, 32.1176305 , 16.55709657, -0.88043378, -0.05853217,
       -0.47054261]), 'targetState': array([-46.96043713,   9.93566024,  18.        ]), 'previousTarget': array([-5.41126512, 23.37057723, 18.        ])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6280677062380426
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-46.96043713,   9.93566024,  18.        ]), 'distance': 1.9896245864296407, 'localFrame': array([[-0.12009542, -0.78556034,  0.60701898],
       [ 0.98851499, -0.15112285,  0.        ],
       [ 0.09173444,  0.60004736,  0.79468734]]), 'currentState': array([-47.72876413,  10.22909092,  16.18832176,  -0.12009542,
        -0.78556034,   0.60701898]), 'targetState': array([-46.96043713,   9.93566024,  18.        ]), 'previousTarget': array([-46.96043713,   9.93566024,  18.        ])}
episode index:18723
target thresh 85.39188944991996
target distance 75.0
model initialize at round 18723
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.46333613, -2.00525687, 28.15428381]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.63291594,  0.25422575,  0.73129111],
       [-0.37272924,  0.92794014,  0.        ],
       [-0.67859438, -0.27257358,  0.68206548]]), 'currentState': array([-17.69184559,  -5.2695144 ,   2.8368547 ,   0.63291594,
         0.25422575,   0.73129111]), 'targetState': array([12.27085046,  4.29257836, 77.        ]), 'previousTarget': array([-8.72632163, -1.95701581, 27.17412093])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280341627801149
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([10.81144266,  2.02535082, 68.24956204]), 'distance': 27.5, 'localFrame': array([[-0.58869202,  0.63609233,  0.49882688],
       [-0.73392307, -0.67923261,  0.        ],
       [ 0.33881948, -0.36610055,  0.86670165]]), 'currentState': array([ 6.42832833, -4.78392999, 41.9689227 , -0.58869202,  0.63609233,
        0.49882688]), 'targetState': array([12.27085046,  4.29257836, 77.        ]), 'previousTarget': array([11.13170924,  1.7878543 , 67.46030972])}
episode index:18724
target thresh 85.39335018793685
target distance 85.0
model initialize at round 18724
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.28229666,  15.2843367 ,  68.44341973]), 'distance': 27.5, 'localFrame': array([[ 0.38014274, -0.61075055, -0.69460439],
       [ 0.84898166,  0.52842231,  0.        ],
       [ 0.36704445, -0.58970639,  0.71939193]]), 'currentState': array([-44.72756637,  16.32070622,  87.86138626,   0.38014274,
        -0.61075055,  -0.69460439]), 'targetState': array([39.25173557, 11.84488305,  4.        ]), 'previousTarget': array([-25.31024556,  16.38794123,  69.44656952])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280006229049331
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 32, 'trapConfig': [], 'currentTarget': array([31.88286314, 14.19767438,  8.70401641]), 'distance': 27.5, 'localFrame': array([[-0.55485663,  0.45325876, -0.69763215],
       [-0.63263994, -0.77444607,  0.        ],
       [-0.54027848,  0.44134996,  0.71645613]]), 'currentState': array([ 9.49962003, 21.34437162, 22.99265338, -0.55485663,  0.45325876,
       -0.69763215]), 'targetState': array([39.25173557, 11.84488305,  4.        ]), 'previousTarget': array([31.59912584, 14.09784042,  9.34090494])}
episode index:18725
target thresh 85.39481077988724
target distance 39.26361598032864
model initialize at round 18725
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.99344704, -20.44839258,  77.24038811]), 'distance': 27.5, 'localFrame': array([[-0.84424363, -0.53478151,  0.03551668],
       [ 0.53511913, -0.84477661,  0.        ],
       [ 0.03000366,  0.01900566,  0.99936908]]), 'currentState': array([-6.13782028e-02,  8.10822998e-01,  6.28998943e+01, -8.44243627e-01,
       -5.34781510e-01,  3.55166843e-02]), 'targetState': array([-18.13802537, -37.88155271,  89.        ]), 'previousTarget': array([ -9.00790555, -19.57643228,  76.41230935])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.627999394956852
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-18.13802537, -37.88155271,  89.        ]), 'distance': 2.5034319169666364, 'localFrame': array([[-0.24177356, -0.58436518,  0.77463726],
       [ 0.92403511, -0.38230762,  0.        ],
       [ 0.29614973,  0.71579202,  0.63240582]]), 'currentState': array([-18.25744569, -35.90497354,  87.46831958,  -0.24177356,
        -0.58436518,   0.77463726]), 'targetState': array([-18.13802537, -37.88155271,  89.        ]), 'previousTarget': array([-18.13802537, -37.88155271,  89.        ])}
episode index:18726
target thresh 85.39627122578574
target distance 75.0
model initialize at round 18726
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.92656026,  0.90325473, 63.78060525]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.11679534, -0.7678431 , -0.62990144],
       [ 0.98862848,  0.15037863,  0.        ],
       [ 0.09472372, -0.6227385 ,  0.77667508]]), 'currentState': array([ 4.27894113, -2.5347896 , 90.47394242,  0.11679534, -0.7678431 ,
       -0.62990144]), 'targetState': array([19.82412531,  6.92849591, 17.        ]), 'previousTarget': array([10.12153276,  1.43154638, 65.20750055])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279658605202121
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([19.82412531,  6.92849591, 17.        ]), 'distance': 4.811954216309509, 'localFrame': array([[-0.77363429, -0.18467453,  0.60612317],
       [ 0.23218672, -0.97267123,  0.        ],
       [ 0.58955857,  0.14073375,  0.79537079]]), 'currentState': array([23.69372424,  7.21089704, 14.15371169, -0.77363429, -0.18467453,
        0.60612317]), 'targetState': array([19.82412531,  6.92849591, 17.        ]), 'previousTarget': array([19.82412531,  6.92849591, 17.        ])}
episode index:18727
target thresh 85.39773152564695
target distance 63.66908164436942
model initialize at round 18727
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.5512127 ,  8.87313564, 66.44747597]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13651347, -0.61836665, -0.77394235],
       [ 0.97648753, -0.21557388,  0.        ],
       [-0.16684176, -0.75574506,  0.63325606]]), 'currentState': array([30.50643912, 27.91235406, 78.24531103, -0.13651347, -0.61836665,
       -0.77394235]), 'targetState': array([-22.56836821, -35.42130371,  39.        ]), 'previousTarget': array([15.18427153,  9.38166682, 67.85108037])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279323296647806
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 54, 'trapConfig': [], 'currentTarget': array([-22.56836821, -35.42130371,  39.        ]), 'distance': 17.6053488687885, 'localFrame': array([[-0.67481049,  0.05341047, -0.73605579],
       [-0.07890208, -0.99688237,  0.        ],
       [-0.73376104,  0.05807633,  0.67692088]]), 'currentState': array([-13.79532205, -24.19383784,  49.34050193,  -0.67481049,
         0.05341047,  -0.73605579]), 'targetState': array([-22.56836821, -35.42130371,  39.        ]), 'previousTarget': array([-22.56836821, -35.42130371,  39.        ])}
episode index:18728
target thresh 85.39919167948547
target distance 14.0
model initialize at round 18728
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.11645514, -13.78851536,  15.        ]), 'distance': 13.771228966518168, 'localFrame': array([[-0.80654038, -0.03166424, -0.59033041],
       [ 0.03922911, -0.99923024,  0.        ],
       [-0.589876  , -0.02315814,  0.8071617 ]]), 'currentState': array([ -7.7741944 , -16.34704293,  28.52714092,  -0.80654038,
        -0.03166424,  -0.59033041]), 'targetState': array([ -8.11645514, -13.78851536,  15.        ]), 'previousTarget': array([ -8.11645514, -13.78851536,  15.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6279485682796688
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.11645514, -13.78851536,  15.        ]), 'distance': 3.563247569676728, 'localFrame': array([[-0.79260878,  0.22017381, -0.5685902 ],
       [-0.26764917, -0.96351643,  0.        ],
       [-0.547846  ,  0.1521827 ,  0.82262093]]), 'currentState': array([-10.29228521, -13.98549899,  17.81490572,  -0.79260878,
         0.22017381,  -0.5685902 ]), 'targetState': array([ -8.11645514, -13.78851536,  15.        ]), 'previousTarget': array([ -8.11645514, -13.78851536,  15.        ])}
episode index:18729
target thresh 85.40065168731591
target distance 20.138093741266708
model initialize at round 18729
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.71952751, -7.95069933, 89.        ]), 'distance': 23.05792776971104, 'localFrame': array([[ 0.48817174,  0.75494093,  0.43789558],
       [-0.83973219,  0.54300079,  0.        ],
       [-0.23777764, -0.36771501,  0.89902584]]), 'currentState': array([ -5.8509441 , -11.1116741 ,  75.70286288,   0.48817174,
         0.75494093,   0.43789558]), 'targetState': array([12.71952751, -7.95069933, 89.        ]), 'previousTarget': array([12.71952751, -7.95069933, 89.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6279628442906661
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.71952751, -7.95069933, 89.        ]), 'distance': 3.972140687727914, 'localFrame': array([[ 0.95062715,  0.2422658 , -0.19394667],
       [-0.24695497,  0.96902696,  0.        ],
       [ 0.18793955,  0.04789609,  0.98101207]]), 'currentState': array([ 9.72418075, -8.6576859 , 86.48883107,  0.95062715,  0.2422658 ,
       -0.19394667]), 'targetState': array([12.71952751, -7.95069933, 89.        ]), 'previousTarget': array([12.71952751, -7.95069933, 89.        ])}
episode index:18730
target thresh 85.40211154915288
target distance 20.0
model initialize at round 18730
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.53913745, -40.46743731,  76.        ]), 'distance': 23.411663681437883, 'localFrame': array([[ 0.17039536, -0.98426326, -0.04681084],
       [ 0.98534342,  0.17058235,  0.        ],
       [ 0.0079851 , -0.04612476,  0.99890377]]), 'currentState': array([-2.09096732e+01, -3.18472851e+01,  9.68138234e+01,  1.70395356e-01,
       -9.84263261e-01, -4.68108420e-02]), 'targetState': array([-14.53913745, -40.46743731,  76.        ]), 'previousTarget': array([-14.53913745, -40.46743731,  76.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6279766407792372
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.53913745, -40.46743731,  76.        ]), 'distance': 3.302126960759449, 'localFrame': array([[-0.25439272, -0.41736838, -0.87240356],
       [ 0.85388728, -0.52045799,  0.        ],
       [-0.4540494 , -0.7449343 ,  0.48878628]]), 'currentState': array([-13.6231014 , -37.6154495 ,  77.38963519,  -0.25439272,
        -0.41736838,  -0.87240356]), 'targetState': array([-14.53913745, -40.46743731,  76.        ]), 'previousTarget': array([-14.53913745, -40.46743731,  76.        ])}
episode index:18731
target thresh 85.40357126501095
target distance 60.0
model initialize at round 18731
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([12.89982626, 36.13094261, 39.52192707]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.62918381, -0.09289318,  0.77168555],
       [ 0.14605748,  0.98927611,  0.        ],
       [-0.76341008,  0.11271045,  0.63600425]]), 'currentState': array([ 9.42924118, 48.08419037, 15.        ,  0.62918381, -0.09289318,
        0.77168555]), 'targetState': array([17.9210332 , 18.83710617, 75.        ]), 'previousTarget': array([12.89982626, 36.13094261, 39.52192707])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6279795544005393
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([17.9210332 , 18.83710617, 75.        ]), 'distance': 2.7487637249703485, 'localFrame': array([[-0.87997816, -0.27346262,  0.38840267],
       [ 0.29676143, -0.95495165,  0.        ],
       [ 0.37090577,  0.11526293,  0.92148975]]), 'currentState': array([19.51941786, 19.41635739, 72.84006099, -0.87997816, -0.27346262,
        0.38840267]), 'targetState': array([17.9210332 , 18.83710617, 75.        ]), 'previousTarget': array([17.9210332 , 18.83710617, 75.        ])}
episode index:18732
target thresh 85.40503083490474
target distance 44.04503193281501
model initialize at round 18732
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.33773156, 17.03941691, 24.09706704]), 'distance': 27.499999999999996, 'localFrame': array([[-0.30674966,  0.65534474,  0.69023758],
       [-0.90569418, -0.42393166,  0.        ],
       [ 0.29261356, -0.62514416,  0.72358281]]), 'currentState': array([12.74063862, -8.49914341, 13.91495445, -0.30674966,  0.65534474,
        0.69023758]), 'targetState': array([13.74252895, 34.35320797, 31.        ]), 'previousTarget': array([13.83713693, 15.76416445, 23.40316629])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6279851233865668
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.74252895, 34.35320797, 31.        ]), 'distance': 1.8872150765979452, 'localFrame': array([[-0.11836733,  0.85793109,  0.49994342],
       [-0.99061613, -0.13667367,  0.        ],
       [ 0.0683291 , -0.49525202,  0.86605807]]), 'currentState': array([13.09442493, 33.06342196, 32.21572775, -0.11836733,  0.85793109,
        0.49994342]), 'targetState': array([13.74252895, 34.35320797, 31.        ]), 'previousTarget': array([13.74252895, 34.35320797, 31.        ])}
episode index:18733
target thresh 85.40649025884885
target distance 41.165668287424694
model initialize at round 18733
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.99612498, 19.48459508, 51.02307969]), 'distance': 27.499999999999996, 'localFrame': array([[-0.90640352, -0.35994775, -0.2210662 ],
       [ 0.36907922, -0.92939794,  0.        ],
       [-0.20545847, -0.08159094,  0.9752588 ]]), 'currentState': array([12.41113816, 27.16769182, 66.48105851, -0.90640352, -0.35994775,
       -0.2210662 ]), 'targetState': array([-27.03137378,  13.01171901,  38.        ]), 'previousTarget': array([-7.687942  , 19.86205712, 51.15698523])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.627991086621861
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.03137378,  13.01171901,  38.        ]), 'distance': 1.8996496282692004, 'localFrame': array([[-0.05418081, -0.98543185, -0.16120953],
       [ 0.99849192, -0.05489888,  0.        ],
       [-0.00885022, -0.16096641,  0.9869202 ]]), 'currentState': array([-26.52450425,  14.81335948,  38.32533614,  -0.05418081,
        -0.98543185,  -0.16120953]), 'targetState': array([-27.03137378,  13.01171901,  38.        ]), 'previousTarget': array([-27.03137378,  13.01171901,  38.        ])}
episode index:18734
target thresh 85.40794953685784
target distance 65.0
model initialize at round 18734
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.89672497, -19.43905672,  64.67502177]), 'distance': 27.5, 'localFrame': array([[ 0.30175563,  0.94064715, -0.155327  ],
       [-0.95220395,  0.305463  ,  0.        ],
       [ 0.04744665,  0.14790298,  0.98786311]]), 'currentState': array([-23.75551929, -24.8300611 ,  91.61726966,   0.30175563,
         0.94064715,  -0.155327  ]), 'targetState': array([-26.53490137, -11.70038501,  26.        ]), 'previousTarget': array([-25.31449282, -20.3468787 ,  64.19179416])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6279901859628223
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-26.53490137, -11.70038501,  26.        ]), 'distance': 3.4869665428378807, 'localFrame': array([[-0.58448532,  0.81069823,  0.03384219],
       [-0.81116287, -0.58482031,  0.        ],
       [ 0.0197916 , -0.02745152,  0.99942719]]), 'currentState': array([-23.79055299, -11.85740228,  28.14542145,  -0.58448532,
         0.81069823,   0.03384219]), 'targetState': array([-26.53490137, -11.70038501,  26.        ]), 'previousTarget': array([-26.53490137, -11.70038501,  26.        ])}
episode index:18735
target thresh 85.40940866894633
target distance 17.0
model initialize at round 18735
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.17443176,  6.99782635, 55.        ]), 'distance': 22.494709260043315, 'localFrame': array([[-0.94215071,  0.22147398,  0.25159753],
       [-0.22883516, -0.97346519,  0.        ],
       [ 0.24492143, -0.05757436,  0.96783195]]), 'currentState': array([ 9.40125907, 17.69518614, 37.68282404, -0.94215071,  0.22147398,
        0.25159753]), 'targetState': array([-0.17443176,  6.99782635, 55.        ]), 'previousTarget': array([-0.17443176,  6.99782635, 55.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6280030358575067
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.17443176,  6.99782635, 55.        ]), 'distance': 1.9749305932718373, 'localFrame': array([[-0.73132605,  0.04746137,  0.68037462],
       [-0.06476146, -0.99790077,  0.        ],
       [ 0.67894636, -0.04406205,  0.7328645 ]]), 'currentState': array([ 1.19418917e-02,  7.56368918e+00,  5.31170728e+01, -7.31326054e-01,
        4.74613721e-02,  6.80374618e-01]), 'targetState': array([-0.17443176,  6.99782635, 55.        ]), 'previousTarget': array([-0.17443176,  6.99782635, 55.        ])}
episode index:18736
target thresh 85.41086765512891
target distance 19.665664875627495
model initialize at round 18736
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.67810696, -1.76306949, 52.09960161]), 'distance': 27.499999999999996, 'localFrame': array([[-0.38389194, -0.80257692,  0.45661501],
       [ 0.90211199, -0.43150197,  0.        ],
       [ 0.19703028,  0.41191788,  0.88966439]]), 'currentState': array([-7.51136925, 17.00614672, 70.4544103 , -0.38389194, -0.80257692,
        0.45661501]), 'targetState': array([ 0.72254681, -1.86491987, 52.        ]), 'previousTarget': array([ 0.72254681, -1.86491987, 52.        ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6280131710926958
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.72254681, -1.86491987, 52.        ]), 'distance': 2.5646159387022633, 'localFrame': array([[ 0.04873747, -0.79126032, -0.60953405],
       [ 0.99810842,  0.06147823,  0.        ],
       [ 0.03747307, -0.60838107,  0.79275989]]), 'currentState': array([-3.35920022e-02, -3.66180424e-01,  5.39388886e+01,  4.87374739e-02,
       -7.91260324e-01, -6.09534051e-01]), 'targetState': array([ 0.72254681, -1.86491987, 52.        ]), 'previousTarget': array([ 0.72254681, -1.86491987, 52.        ])}
episode index:18737
target thresh 85.41232649542016
target distance 57.295418282455444
model initialize at round 18737
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.4380984 ,  12.80522012,  75.37519292]), 'distance': 27.499999999999996, 'localFrame': array([[-0.44687775,  0.45109484,  0.7725372 ],
       [-0.71041969, -0.70377828,  0.        ],
       [ 0.5436949 , -0.54882564,  0.63496951]]), 'currentState': array([14.55037547,  7.53686641, 75.71684691, -0.44687775,  0.45109484,
        0.7725372 ]), 'targetState': array([-42.07593205,  18.59074884,  75.        ]), 'previousTarget': array([-11.78187549,  12.76587685,  74.47126564])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6280106709989305
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-42.07593205,  18.59074884,  75.        ]), 'distance': 2.5710168296217426, 'localFrame': array([[ 0.02098363,  0.50733066,  0.86149596],
       [-0.99914574,  0.04132552,  0.        ],
       [-0.03560177, -0.86076002,  0.50776442]]), 'currentState': array([-4.11012160e+01,  1.88802039e+01,  7.26385869e+01,  2.09836281e-02,
        5.07330657e-01,  8.61495961e-01]), 'targetState': array([-42.07593205,  18.59074884,  75.        ]), 'previousTarget': array([-42.07593205,  18.59074884,  75.        ])}
episode index:18738
target thresh 85.41378518983468
target distance 26.0
model initialize at round 18738
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.09577657, -12.14575692,  29.03979831]), 'distance': 27.500000000000004, 'localFrame': array([[-0.598254  ,  0.33534251, -0.72776202],
       [-0.48895883, -0.87230686,  0.        ],
       [-0.63483181,  0.35584566,  0.68582975]]), 'currentState': array([-1.87953493,  0.09771571,  4.44573126, -0.598254  ,  0.33534251,
       -0.72776202]), 'targetState': array([ -3.24216637, -13.61941104,  32.        ]), 'previousTarget': array([ -3.07689267, -12.61997543,  30.14264404])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.628016631279756
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.24216637, -13.61941104,  32.        ]), 'distance': 1.9872582218944606, 'localFrame': array([[-0.01530089,  0.90746869,  0.41984099],
       [-0.99985788, -0.01685867,  0.        ],
       [ 0.00707796, -0.41978132,  0.90759768]]), 'currentState': array([-2.36196879e+00, -1.40932991e+01,  3.02824792e+01, -1.53008853e-02,
        9.07468691e-01,  4.19840991e-01]), 'targetState': array([ -3.24216637, -13.61941104,  32.        ]), 'previousTarget': array([ -3.24216637, -13.61941104,  32.        ])}
episode index:18739
target thresh 85.41524373838706
target distance 14.986596280640722
model initialize at round 18739
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.87204104, -4.85209292, 28.        ]), 'distance': 14.158112859221534, 'localFrame': array([[ 0.28549505, -0.71263671,  0.64081315],
       [ 0.92827876,  0.37188513,  0.        ],
       [-0.23830888,  0.59485324,  0.76769688]]), 'currentState': array([ 7.68196612,  9.02819479, 29.73012855,  0.28549505, -0.71263671,
        0.64081315]), 'targetState': array([ 9.87204104, -4.85209292, 28.        ]), 'previousTarget': array([ 9.87204104, -4.85209292, 28.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6280323584976402
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.87204104, -4.85209292, 28.        ]), 'distance': 2.6353728987792713, 'localFrame': array([[ 0.79681467, -0.10346719, -0.59529903],
       [ 0.12876993,  0.9916745 ,  0.        ],
       [ 0.59034286, -0.07665662,  0.80350424]]), 'currentState': array([ 9.56238778, -2.31275175, 28.63328637,  0.79681467, -0.10346719,
       -0.59529903]), 'targetState': array([ 9.87204104, -4.85209292, 28.        ]), 'previousTarget': array([ 9.87204104, -4.85209292, 28.        ])}
episode index:18740
target thresh 85.41670214109187
target distance 78.0
model initialize at round 18740
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.69936411,  8.89102594, 58.42556022]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.18868471,  0.15498049, -0.96973147],
       [-0.63471369,  0.77274739,  0.        ],
       [ 0.74935747,  0.61550184,  0.24417386]]), 'currentState': array([13.51667737, -2.39906109, 83.50045923,  0.18868471,  0.15498049,
       -0.96973147]), 'targetState': array([14.07403231, 32.04561771,  7.        ]), 'previousTarget': array([13.72397461,  7.91260834, 59.97490522])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6280292406727631
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([14.07403231, 32.04561771,  7.        ]), 'distance': 3.4907149155463135, 'localFrame': array([[-0.29382467,  0.179946  , -0.93876861],
       [-0.52226646, -0.85278236,  0.        ],
       [-0.80056531,  0.49028735,  0.34454826]]), 'currentState': array([14.23923363, 29.13618789,  8.92172247, -0.29382467,  0.179946  ,
       -0.93876861]), 'targetState': array([14.07403231, 32.04561771,  7.        ]), 'previousTarget': array([14.07403231, 32.04561771,  7.        ])}
episode index:18741
target thresh 85.4181603979637
target distance 70.96064471566069
model initialize at round 18741
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.44044918, -8.39260391, 11.56602696]), 'distance': 27.5, 'localFrame': array([[-0.21472515,  0.86185332, -0.45945834],
       [-0.97033774, -0.24175333,  0.        ],
       [-0.11107558,  0.44582976,  0.88819932]]), 'currentState': array([-15.22733514, -32.90545101,   0.52599626,  -0.21472515,
         0.86185332,  -0.45945834]), 'targetState': array([ 1.27048765, 36.97818088, 32.        ]), 'previousTarget': array([-9.07500155, -9.17623197, 12.48731954])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6280224008365252
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([ 1.27048765, 36.97818088, 32.        ]), 'distance': 2.045017028884513, 'localFrame': array([[ 0.24267917,  0.79470954,  0.55636639],
       [-0.95640158,  0.29205481,  0.        ],
       [-0.16248948, -0.5321097 ,  0.83093708]]), 'currentState': array([ 0.65712729, 35.13251459, 31.36801925,  0.24267917,  0.79470954,
        0.55636639]), 'targetState': array([ 1.27048765, 36.97818088, 32.        ]), 'previousTarget': array([ 1.27048765, 36.97818088, 32.        ])}
episode index:18742
target thresh 85.41961850901714
target distance 44.12227956778641
model initialize at round 18742
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.91239467,  15.64624409,  71.9214908 ]), 'distance': 27.5, 'localFrame': array([[-0.13862043, -0.22843384, -0.96364016],
       [ 0.85490627, -0.51878249,  0.        ],
       [-0.49991965, -0.82382201,  0.26720337]]), 'currentState': array([ 3.30077661, 12.95863957, 90.37289966, -0.13862043, -0.22843384,
       -0.96364016]), 'targetState': array([-39.83149426,  18.69363703,  51.        ]), 'previousTarget': array([-15.77484657,  15.76719869,  73.3542973 ])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6280235255352143
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-39.83149426,  18.69363703,  51.        ]), 'distance': 1.4394317879632312, 'localFrame': array([[-0.65971707, -0.27492994, -0.69941898],
       [ 0.3846724 , -0.92305317,  0.        ],
       [-0.64560091, -0.26904718,  0.71471189]]), 'currentState': array([-38.55908183,  18.05228727,  50.79603683,  -0.65971707,
        -0.27492994,  -0.69941898]), 'targetState': array([-39.83149426,  18.69363703,  51.        ]), 'previousTarget': array([-39.83149426,  18.69363703,  51.        ])}
episode index:18743
target thresh 85.42107647426677
target distance 35.991302490234375
model initialize at round 18743
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.13993907, -11.60321968,  45.63140953]), 'distance': 27.499999999999996, 'localFrame': array([[-0.80854909, -0.4646071 ,  0.36109363],
       [ 0.49822238, -0.86704928,  0.        ],
       [ 0.31308598,  0.17990493,  0.93252956]]), 'currentState': array([ -0.45030913, -37.33793465,  35.94233754,  -0.80854909,
        -0.4646071 ,   0.36109363]), 'targetState': array([ 0.,  0., 50.]), 'previousTarget': array([  0.22795848, -10.36736737,  45.96727173])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6280323598693004
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 50.]), 'distance': 3.969129012546183, 'localFrame': array([[ 0.36616809,  0.45629856,  0.81099479],
       [-0.77992639,  0.62587126,  0.        ],
       [-0.50757833, -0.63251624,  0.58505337]]), 'currentState': array([-2.76018237, -1.3397445 , 47.48197239,  0.36616809,  0.45629856,
        0.81099479]), 'targetState': array([ 0.,  0., 50.]), 'previousTarget': array([ 0.,  0., 50.])}
episode index:18744
target thresh 85.42253429372715
target distance 35.0
model initialize at round 18744
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.77846497, -8.23916569, 49.07431002]), 'distance': 27.5, 'localFrame': array([[-0.48447368,  0.27270414,  0.8312146 ],
       [-0.49051768, -0.87143124,  0.        ],
       [ 0.72434637, -0.40772546,  0.5559517 ]]), 'currentState': array([ -2.21851393, -21.75940496,  25.13397617,  -0.48447368,
         0.27270414,   0.8312146 ]), 'targetState': array([-3.01062132, -2.63365892, 59.        ]), 'previousTarget': array([-2.27924205, -8.7034866 , 48.02431943])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6280407698869551
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.01062132, -2.63365892, 59.        ]), 'distance': 3.5196012219039465, 'localFrame': array([[-0.5342813 ,  0.59329648,  0.60211525],
       [-0.74309826, -0.66918232,  0.        ],
       [ 0.40292488, -0.4474308 ,  0.79840918]]), 'currentState': array([-0.09346183, -4.42329053, 58.17842102, -0.5342813 ,  0.59329648,
        0.60211525]), 'targetState': array([-3.01062132, -2.63365892, 59.        ]), 'previousTarget': array([-3.01062132, -2.63365892, 59.        ])}
episode index:18745
target thresh 85.42399196741289
target distance 27.26450133259889
model initialize at round 18745
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.96938651, 33.08900528, 15.65377734]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.67029359,  0.39608401, -0.62755395],
       [-0.50873068,  0.86092572,  0.        ],
       [ 0.54027734,  0.31925595,  0.77857308]]), 'currentState': array([28.7071468 , 21.76646769,  7.61830104,  0.67029359,  0.39608401,
       -0.62755395]), 'targetState': array([ 0.99248338, 34.98592541, 17.        ]), 'previousTarget': array([ 4.69722124, 33.03833616, 15.91294902])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6280487598895963
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 0.99248338, 34.98592541, 17.        ]), 'distance': 1.9086486615809926, 'localFrame': array([[-0.10093971, -0.5118123 ,  0.85314673],
       [ 0.98110165, -0.19349304,  0.        ],
       [ 0.16507796,  0.83702366,  0.52167103]]), 'currentState': array([ 2.38874303, 35.28105196, 15.73260147, -0.10093971, -0.5118123 ,
        0.85314673]), 'targetState': array([ 0.99248338, 34.98592541, 17.        ]), 'previousTarget': array([ 0.99248338, 34.98592541, 17.        ])}
episode index:18746
target thresh 85.42544949533853
target distance 51.0
model initialize at round 18746
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.70715323,   9.59951822,  69.37546694]), 'distance': 27.499999999999996, 'localFrame': array([[-0.54238595,  0.52223864, -0.65809139],
       [-0.69360106, -0.72035933,  0.        ],
       [-0.47406227,  0.45645289,  0.75293806]]), 'currentState': array([-0.1629799 ,  1.28683845, 89.70868321, -0.54238595,  0.52223864,
       -0.65809139]), 'targetState': array([-40.60857691,  21.60887507,  40.        ]), 'previousTarget': array([-16.25988711,   8.65230687,  70.5793326 ])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6280465722340807
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-40.60857691,  21.60887507,  40.        ]), 'distance': 3.423260097708985, 'localFrame': array([[-0.28726438,  0.19997399, -0.93674414],
       [-0.57132996, -0.82072046,  0.        ],
       [-0.76880508,  0.53518999,  0.35001489]]), 'currentState': array([-37.66282528,  19.92579969,  40.45663372,  -0.28726438,
         0.19997399,  -0.93674414]), 'targetState': array([-40.60857691,  21.60887507,  40.        ]), 'previousTarget': array([-40.60857691,  21.60887507,  40.        ])}
episode index:18747
target thresh 85.42690687751868
target distance 53.0
model initialize at round 18747
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.31194889, -8.38362001, 50.78400234]), 'distance': 27.5, 'localFrame': array([[-0.38731921,  0.58617075,  0.71160922],
       [-0.83431711, -0.55128483,  0.        ],
       [ 0.39229937, -0.59370774,  0.70257549]]), 'currentState': array([ 16.40630696, -18.14705398,  26.73783823,  -0.38731921,
         0.58617075,   0.71160922]), 'targetState': array([-2.98125326,  2.66685751, 78.        ]), 'previousTarget': array([ 7.57041116, -9.06902647, 49.129957  ])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6280480450657608
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-2.98125326,  2.66685751, 78.        ]), 'distance': 1.8105480246894512, 'localFrame': array([[ 0.06706245,  0.72828951,  0.68198022],
       [-0.99578721,  0.09169421,  0.        ],
       [-0.06253364, -0.67910718,  0.73137062]]), 'currentState': array([-3.40441934e+00,  2.75579538e+00,  7.62418462e+01,  6.70624514e-02,
        7.28289506e-01,  6.81980222e-01]), 'targetState': array([-2.98125326,  2.66685751, 78.        ]), 'previousTarget': array([-2.98125326,  2.66685751, 78.        ])}
episode index:18748
target thresh 85.42836411396789
target distance 51.0
model initialize at round 18748
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.84118059, -0.72706952, 66.32182218]), 'distance': 27.5, 'localFrame': array([[-0.14828402, -0.65270141,  0.74296213],
       [ 0.97515132, -0.22153983,  0.        ],
       [ 0.1645957 ,  0.7245005 ,  0.66933346]]), 'currentState': array([24.22321299,  6.83879689, 44.13703439, -0.14828402, -0.65270141,
        0.74296213]), 'targetState': array([ -8.10212604, -10.16639334,  94.        ]), 'previousTarget': array([ 9.73733895,  0.12686244, 65.33339868])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6280513199625981
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ -8.10212604, -10.16639334,  94.        ]), 'distance': 3.4135174223828013, 'localFrame': array([[-0.99776757, -0.02251871, -0.06287116],
       [ 0.02256334, -0.99974542,  0.        ],
       [-0.06285516, -0.00141858,  0.99802165]]), 'currentState': array([-5.42463591e+00, -8.12345584e+00,  9.34436241e+01, -9.97767570e-01,
       -2.25187063e-02, -6.28711632e-02]), 'targetState': array([ -8.10212604, -10.16639334,  94.        ]), 'previousTarget': array([ -8.10212604, -10.16639334,  94.        ])}
episode index:18749
target thresh 85.42982120470073
target distance 42.682596893640756
model initialize at round 18749
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.73646329, -5.97951796, 53.40010799]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.29196521,  0.74859707,  0.59528038],
       [-0.93164935,  0.36335862,  0.        ],
       [-0.21630026, -0.55459258,  0.80351806]]), 'currentState': array([-30.08906759,  -5.9620926 ,  50.55674131,   0.29196521,
         0.74859707,   0.59528038]), 'targetState': array([12.65416977, -5.98932279, 55.        ]), 'previousTarget': array([-2.736814  , -6.60626323, 53.19704225])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6280597267263764
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.65416977, -5.98932279, 55.        ]), 'distance': 3.5227408315975812, 'localFrame': array([[ 0.88735391,  0.36656877,  0.27969693],
       [-0.38180734,  0.92424194,  0.        ],
       [-0.25850763, -0.10679034,  0.96008834]]), 'currentState': array([10.68749119, -4.15900334, 57.27855415,  0.88735391,  0.36656877,
        0.27969693]), 'targetState': array([12.65416977, -5.98932279, 55.        ]), 'previousTarget': array([12.65416977, -5.98932279, 55.        ])}
episode index:18750
target thresh 85.4312781497318
target distance 76.0
model initialize at round 18750
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.6776429 , -4.07694798, 44.90619724]), 'distance': 27.499999999999996, 'localFrame': array([[-0.34441538, -0.66948781,  0.65815204],
       [ 0.8892298 , -0.45746078,  0.        ],
       [ 0.30107875,  0.58524841,  0.75288504]]), 'currentState': array([ 17.31999299, -11.66316407,  21.69261695,  -0.34441538,
        -0.66948781,   0.65815204]), 'targetState': array([-23.69316777,  12.94734726,  97.        ]), 'previousTarget': array([ 4.71648899, -2.9065493 , 44.38919204])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6280545454462436
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-23.69316777,  12.94734726,  97.        ]), 'distance': 3.4979115408421335, 'localFrame': array([[ 0.21670041, -0.18128008,  0.95925933],
       [ 0.64163795,  0.76700765,  0.        ],
       [-0.73575925,  0.61549719,  0.28252705]]), 'currentState': array([-22.06831724,  11.72505675,  94.15373019,   0.21670041,
        -0.18128008,   0.95925933]), 'targetState': array([-23.69316777,  12.94734726,  97.        ]), 'previousTarget': array([-23.69316777,  12.94734726,  97.        ])}
episode index:18751
target thresh 85.43273494907564
target distance 40.91152448857151
model initialize at round 18751
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.26799047, -28.34589438,  80.69820733]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.86366542, -0.420341  ,  0.27820046],
       [ 0.43761678,  0.89916159,  0.        ],
       [-0.25014717,  0.12174519,  0.96052304]]), 'currentState': array([-27.93141058, -31.8271363 ,  97.27611985,   0.86366542,
        -0.420341  ,   0.27820046]), 'targetState': array([ 11.63233443, -25.46936976,  67.        ]), 'previousTarget': array([ -7.00400689, -28.21272282,  80.21030944])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6280597142703963
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 11.63233443, -25.46936976,  67.        ]), 'distance': 1.667027104915686, 'localFrame': array([[ 0.7420758 ,  0.61845798,  0.25852123],
       [-0.64022196,  0.76818998,  0.        ],
       [-0.19859341, -0.16551097,  0.96600558]]), 'currentState': array([ 10.24796187, -25.0514678 ,  67.8293672 ,   0.7420758 ,
         0.61845798,   0.25852123]), 'targetState': array([ 11.63233443, -25.46936976,  67.        ]), 'previousTarget': array([ 11.63233443, -25.46936976,  67.        ])}
episode index:18752
target thresh 85.43419160274684
target distance 23.139817906217793
model initialize at round 18752
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.62779251, -45.71937151,  63.        ]), 'distance': 23.881824530630915, 'localFrame': array([[ 0.39289579, -0.3717563 , -0.84108867],
       [ 0.68729558,  0.72637785,  0.        ],
       [ 0.61094818, -0.57807653,  0.54089726]]), 'currentState': array([ 17.78277536, -22.51132675,  68.63064666,   0.39289579,
        -0.3717563 ,  -0.84108867]), 'targetState': array([ 17.62779251, -45.71937151,  63.        ]), 'previousTarget': array([ 17.62779251, -45.71937151,  63.        ])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6280461380194596
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([ 17.62779251, -45.71937151,  63.        ]), 'distance': 3.143436271008126, 'localFrame': array([[-0.42561896, -0.20514687, -0.88134174],
       [ 0.43419225, -0.90082023,  0.        ],
       [-0.79393047, -0.38267175,  0.47247935]]), 'currentState': array([ 19.25072554, -44.77724809,  65.52184128,  -0.42561896,
        -0.20514687,  -0.88134174]), 'targetState': array([ 17.62779251, -45.71937151,  63.        ]), 'previousTarget': array([ 17.62779251, -45.71937151,  63.        ])}
episode index:18753
target thresh 85.43564811075994
target distance 46.44007029141034
model initialize at round 18753
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.18487963, -24.79500865,  71.19620798]), 'distance': 27.5, 'localFrame': array([[-0.04051021, -0.73543962,  0.67637821],
       [ 0.99848638, -0.0549996 ,  0.        ],
       [ 0.03720053,  0.67535443,  0.73655449]]), 'currentState': array([ 2.98578292e+00, -3.16856855e+00,  5.45080689e+01, -4.05102052e-02,
       -7.35439621e-01,  6.76378213e-01]), 'targetState': array([ -3.56751062, -47.86724212,  89.        ]), 'previousTarget': array([ -0.27957993, -23.26452127,  70.45792579])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6280505374590061
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.56751062, -47.86724212,  89.        ]), 'distance': 3.064811469087899, 'localFrame': array([[ 0.38867935,  0.29091497,  0.87424072],
       [-0.59921597,  0.80058743,  0.        ],
       [-0.69990613, -0.523859  ,  0.48549269]]), 'currentState': array([ -2.18395697, -48.61426442,  86.36925746,   0.38867935,
         0.29091497,   0.87424072]), 'targetState': array([ -3.56751062, -47.86724212,  89.        ]), 'previousTarget': array([ -3.56751062, -47.86724212,  89.        ])}
episode index:18754
target thresh 85.43710447312954
target distance 12.640493170077892
model initialize at round 18754
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.84677483, -1.09650508, 41.        ]), 'distance': 13.785253976092845, 'localFrame': array([[ 0.28891412, -0.81022578,  0.50996355],
       [ 0.94190832,  0.3358701 ,  0.        ],
       [-0.17128151,  0.48033891,  0.86019601]]), 'currentState': array([-9.17470512,  3.34591185, 40.14022158,  0.28891412, -0.81022578,
        0.50996355]), 'targetState': array([ 3.84677483, -1.09650508, 41.        ]), 'previousTarget': array([ 3.84677483, -1.09650508, 41.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6280667472596164
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.84677483, -1.09650508, 41.        ]), 'distance': 2.2358330003631166, 'localFrame': array([[ 0.93511178,  0.34116837, -0.09576063],
       [-0.34274348,  0.93942903,  0.        ],
       [ 0.08996032,  0.03282133,  0.99540439]]), 'currentState': array([ 2.201232  , -0.24187732, 39.75070044,  0.93511178,  0.34116837,
       -0.09576063]), 'targetState': array([ 3.84677483, -1.09650508, 41.        ]), 'previousTarget': array([ 3.84677483, -1.09650508, 41.        ])}
episode index:18755
target thresh 85.43856068987017
target distance 22.998628456183802
model initialize at round 18755
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.99862846,  6.86025431, 49.        ]), 'distance': 24.741213759473215, 'localFrame': array([[-0.39869404,  0.84043309, -0.36703581],
       [-0.90349058, -0.42860795,  0.        ],
       [-0.15731447,  0.33161339,  0.93020681]]), 'currentState': array([-0.22876688,  1.20481962, 42.62569041, -0.39869404,  0.84043309,
       -0.36703581]), 'targetState': array([22.99862846,  6.86025431, 49.        ]), 'previousTarget': array([22.99862846,  6.86025431, 49.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6280777542448047
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([22.99862846,  6.86025431, 49.        ]), 'distance': 2.388283344329763, 'localFrame': array([[ 0.96632364,  0.10561044,  0.23465947],
       [-0.10864404,  0.99408072,  0.        ],
       [-0.23327046, -0.02549435,  0.97207764]]), 'currentState': array([20.8933419 ,  5.87386249, 49.54653184,  0.96632364,  0.10561044,
        0.23465947]), 'targetState': array([22.99862846,  6.86025431, 49.        ]), 'previousTarget': array([22.99862846,  6.86025431, 49.        ])}
episode index:18756
target thresh 85.44001676099641
target distance 34.290214097874596
model initialize at round 18756
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.91829294, -5.01217583, 22.99772485]), 'distance': 27.5, 'localFrame': array([[-0.35151308,  0.85548926,  0.38023241],
       [-0.92496227, -0.38005894,  0.        ],
       [ 0.14451072, -0.35170063,  0.92489098]]), 'currentState': array([ 21.66611527, -25.48741707,   5.92489878,  -0.35151308,
         0.85548926,   0.38023241]), 'targetState': array([10.96501799,  6.98343614, 33.        ]), 'previousTarget': array([15.21388767, -6.34780554, 22.50302485])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6280740323481422
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([10.96501799,  6.98343614, 33.        ]), 'distance': 2.883956886660119, 'localFrame': array([[-0.92476004, -0.16905722, -0.34093773],
       [ 0.17983167, -0.9836974 ,  0.        ],
       [-0.33537955, -0.0613114 ,  0.94008588]]), 'currentState': array([13.00200008,  7.84990656, 34.84855087, -0.92476004, -0.16905722,
       -0.34093773]), 'targetState': array([10.96501799,  6.98343614, 33.        ]), 'previousTarget': array([10.96501799,  6.98343614, 33.        ])}
episode index:18757
target thresh 85.44147268652283
target distance 49.0
model initialize at round 18757
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.58796474, 21.31062601, 47.20018645]), 'distance': 27.5, 'localFrame': array([[-0.82180089,  0.17399398, -0.54255819],
       [-0.2071312 , -0.97831317,  0.        ],
       [-0.53079183,  0.11238073,  0.84001822]]), 'currentState': array([ 5.36833584, 21.86385907, 74.63691752, -0.82180089,  0.17399398,
       -0.54255819]), 'targetState': array([ 2.21228339, 20.88314637, 26.        ]), 'previousTarget': array([ 4.27282045, 20.88884733, 47.62400784])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6280788119888422
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.21228339, 20.88314637, 26.        ]), 'distance': 3.2941176535408134, 'localFrame': array([[-0.30330875,  0.51078904, -0.80442424],
       [-0.85983423, -0.5105733 ,  0.        ],
       [-0.41071754,  0.6916715 ,  0.59405525]]), 'currentState': array([-0.30059267, 21.14230606, 28.1141195 , -0.30330875,  0.51078904,
       -0.80442424]), 'targetState': array([ 2.21228339, 20.88314637, 26.        ]), 'previousTarget': array([ 2.21228339, 20.88314637, 26.        ])}
episode index:18758
target thresh 85.44292846646397
target distance 55.0
model initialize at round 18758
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.55626505,  5.74801398, 38.398023  ]), 'distance': 27.5, 'localFrame': array([[-0.27514233,  0.8169426 , -0.5068545 ],
       [-0.94769447, -0.31917893,  0.        ],
       [-0.16177728,  0.48034321,  0.86203162]]), 'currentState': array([-5.86772863,  2.91419799, 64.72488769, -0.27514233,  0.8169426 ,
       -0.5068545 ]), 'targetState': array([ 9.56431105,  8.80476883, 10.        ]), 'previousTarget': array([ 2.10446857,  4.99237654, 38.60831557])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6280839775906317
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.56431105,  8.80476883, 10.        ]), 'distance': 2.2152444575836587, 'localFrame': array([[ 0.41659343,  0.76474579, -0.49154226],
       [-0.87815642,  0.4783736 ,  0.        ],
       [ 0.23514084,  0.43165099,  0.87085372]]), 'currentState': array([10.01731111,  8.57999008, 12.15675067,  0.41659343,  0.76474579,
       -0.49154226]), 'targetState': array([ 9.56431105,  8.80476883, 10.        ]), 'previousTarget': array([ 9.56431105,  8.80476883, 10.        ])}
episode index:18759
target thresh 85.4443841008344
target distance 35.0
model initialize at round 18759
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.65653645, -8.54430966, 76.08649148]), 'distance': 27.5, 'localFrame': array([[ 0.74436891,  0.54878911, -0.38045424],
       [-0.593414  ,  0.8048974 ,  0.        ],
       [ 0.30622663,  0.22576687,  0.92479975]]), 'currentState': array([ 17.90985287, -10.92876649,  53.66760832,   0.74436891,
         0.54878911,  -0.38045424]), 'targetState': array([43.42917387, -7.06447856, 90.        ]), 'previousTarget': array([33.60190843, -8.92017995, 76.87414287])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.628081166864213
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([43.42917387, -7.06447856, 90.        ]), 'distance': 2.704295581448144, 'localFrame': array([[ 0.66684148,  0.27937283,  0.69084966],
       [-0.38640862,  0.92232769,  0.        ],
       [-0.63718978, -0.26695027,  0.72299844]]), 'currentState': array([42.61050517, -7.0037984 , 87.42331335,  0.66684148,  0.27937283,
        0.69084966]), 'targetState': array([43.42917387, -7.06447856, 90.        ]), 'previousTarget': array([43.42917387, -7.06447856, 90.        ])}
episode index:18760
target thresh 85.44583958964866
target distance 59.0
model initialize at round 18760
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-33.87019742, -30.37871188,  38.27099561]), 'distance': 27.5, 'localFrame': array([[-0.32765809,  0.75443459,  0.56874302],
       [-0.91722867, -0.3983611 ,  0.        ],
       [ 0.2265651 , -0.5216674 ,  0.82251527]]), 'currentState': array([-27.06146387, -36.73098945,  12.39553006,  -0.32765809,
         0.75443459,   0.56874302]), 'targetState': array([-42.48233504, -22.34393005,  71.        ]), 'previousTarget': array([-33.71069815, -31.46889372,  37.70520221])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6280829898413529
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-42.48233504, -22.34393005,  71.        ]), 'distance': 1.991505479994942, 'localFrame': array([[-0.36233181, -0.46591508,  0.80724147],
       [ 0.78939046, -0.61389144,  0.        ],
       [ 0.49555863,  0.63722872,  0.59022132]]), 'currentState': array([-41.70054813, -23.10413687,  69.33357002,  -0.36233181,
        -0.46591508,   0.80724147]), 'targetState': array([-42.48233504, -22.34393005,  71.        ]), 'previousTarget': array([-42.48233504, -22.34393005,  71.        ])}
episode index:18761
target thresh 85.44729493292131
target distance 36.0
model initialize at round 18761
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.34317882, -5.53625485, 58.10343853]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.23924942,  0.96677945,  0.08998454],
       [-0.97071749,  0.24022397,  0.        ],
       [-0.02161644, -0.08734956,  0.99594316]]), 'currentState': array([ -4.80870295, -20.89682398,  35.73466542,   0.23924942,
         0.96677945,   0.08998454]), 'targetState': array([ 2.23138823,  3.31977508, 71.        ]), 'previousTarget': array([-0.90084654, -6.58156158, 57.0555719 ])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6280905562071968
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.23138823,  3.31977508, 71.        ]), 'distance': 2.0810055934730323, 'localFrame': array([[ 0.81856808,  0.02578162,  0.57383064],
       [-0.03148038,  0.99950437,  0.        ],
       [-0.57354623, -0.01806441,  0.81897399]]), 'currentState': array([1.71470501e+00, 2.68476360e+00, 6.90867872e+01, 8.18568085e-01,
       2.57816153e-02, 5.73830636e-01]), 'targetState': array([ 2.23138823,  3.31977508, 71.        ]), 'previousTarget': array([ 2.23138823,  3.31977508, 71.        ])}
episode index:18762
target thresh 85.44875013066692
target distance 47.09170921992331
model initialize at round 18762
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.40901267, -4.71306964, 24.27310474]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.71954766, -0.13827309, -0.68053782],
       [ 0.18871386,  0.98203212,  0.        ],
       [ 0.66830999, -0.12842692,  0.73271296]]), 'currentState': array([-31.89632431,  -5.44679492,  24.67228611,   0.71954766,
        -0.13827309,  -0.68053782]), 'targetState': array([14.39676293, -4.21108265, 24.        ]), 'previousTarget': array([-5.22015016, -4.32171168, 24.83313659])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6280938263947821
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.39676293, -4.21108265, 24.        ]), 'distance': 3.210070178194965, 'localFrame': array([[ 0.65666553,  0.26721624,  0.70525589],
       [-0.37691681,  0.92624712,  0.        ],
       [-0.65324123, -0.2658228 ,  0.70895284]]), 'currentState': array([12.93978112, -3.89834605, 21.15677113,  0.65666553,  0.26721624,
        0.70525589]), 'targetState': array([14.39676293, -4.21108265, 24.        ]), 'previousTarget': array([14.39676293, -4.21108265, 24.        ])}
episode index:18763
target thresh 85.45020518290004
target distance 65.0
model initialize at round 18763
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.1406649 , -13.27816319,  47.65810411]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.68016596,  0.11546526, -0.72390748],
       [-0.16736592,  0.98589485,  0.        ],
       [ 0.71369665,  0.12115744,  0.68989706]]), 'currentState': array([ -0.92127191, -21.90199469,  72.79788491,   0.68016596,
         0.11546526,  -0.72390748]), 'targetState': array([16.99999143, -0.01706999,  9.        ]), 'previousTarget': array([  5.41138502, -14.10859748,  48.98171707])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6280901050300491
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([16.99999143, -0.01706999,  9.        ]), 'distance': 2.9298451098618803, 'localFrame': array([[ 0.0810746 ,  0.58751405, -0.80514232],
       [-0.99061241,  0.13670056,  0.        ],
       [ 0.11006341,  0.79758398,  0.59308165]]), 'currentState': array([15.99554962, -2.66535661,  8.25055556,  0.0810746 ,  0.58751405,
       -0.80514232]), 'targetState': array([16.99999143, -0.01706999,  9.        ]), 'previousTarget': array([16.99999143, -0.01706999,  9.        ])}
episode index:18764
target thresh 85.4516600896352
target distance 24.320555170460157
model initialize at round 18764
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.3478571 ,  9.60282984, 30.46231783]), 'distance': 27.5, 'localFrame': array([[-0.0269974 ,  0.66848552, -0.74323499],
       [-0.99918549, -0.04035301,  0.        ],
       [-0.02999177,  0.74262962,  0.66903045]]), 'currentState': array([ 3.54532313e+00, -6.33300735e-02,  4.24147150e+01, -2.69973952e-02,
        6.68485516e-01, -7.43234994e-01]), 'targetState': array([27.22985741,  9.97671618, 30.        ]), 'previousTarget': array([25.17634129,  9.07251348, 31.18209578])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6281015547016907
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.22985741,  9.97671618, 30.        ]), 'distance': 3.579599477373472, 'localFrame': array([[ 0.40744815,  0.89437169, -0.18462198],
       [-0.91001521,  0.41457486,  0.        ],
       [ 0.07653963,  0.16800881,  0.98280961]]), 'currentState': array([25.88967016,  7.50603141, 32.21656205,  0.40744815,  0.89437169,
       -0.18462198]), 'targetState': array([27.22985741,  9.97671618, 30.        ]), 'previousTarget': array([27.22985741,  9.97671618, 30.        ])}
episode index:18765
target thresh 85.45311485088696
target distance 54.31745928260475
model initialize at round 18765
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.98963037,  6.55520735, 60.58401174]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.96638881,  0.25553622,  0.02817641],
       [-0.25563772,  0.96677265,  0.        ],
       [-0.02724018, -0.00720295,  0.99960297]]), 'currentState': array([-2.49658787e+01,  1.06303016e+01,  7.66055009e+01,  9.66388808e-01,
        2.55536223e-01,  2.81764051e-02]), 'targetState': array([27.98825472,  0.81092388, 38.        ]), 'previousTarget': array([-3.98830502,  5.94234441, 60.37051008])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6281037325874346
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.98825472,  0.81092388, 38.        ]), 'distance': 4.808775848417147, 'localFrame': array([[ 0.41831633, -0.68956872, -0.59119069],
       [ 0.85498018,  0.51866067,  0.        ],
       [ 0.30662736, -0.50545632,  0.80653182]]), 'currentState': array([25.0854975 ,  3.19998965, 40.99844801,  0.41831633, -0.68956872,
       -0.59119069]), 'targetState': array([27.98825472,  0.81092388, 38.        ]), 'previousTarget': array([27.98825472,  0.81092388, 38.        ])}
episode index:18766
target thresh 85.45456946666987
target distance 38.900060152001984
model initialize at round 18766
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.12049227, -0.47825189, 50.92853535]), 'distance': 27.5, 'localFrame': array([[ 0.04571405, -0.84171561, -0.5379824 ],
       [ 0.99852844,  0.05423064,  0.        ],
       [ 0.02917513, -0.53719072,  0.84295607]]), 'currentState': array([ 2.84600058e+01,  2.03887609e+01,  6.55542743e+01,  4.57140489e-02,
       -8.41715610e-01, -5.37982395e-01]), 'targetState': array([  9.6877387 , -17.49707744,  39.        ]), 'previousTarget': array([18.6934308 ,  0.69298468, 52.09308359])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6280985533798024
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 30, 'trapConfig': [], 'currentTarget': array([  9.6877387 , -17.49707744,  39.        ]), 'distance': 3.515347087343344, 'localFrame': array([[ 0.23448839, -0.81743763,  0.52612824],
       [ 0.96123305,  0.27573723,  0.        ],
       [-0.14507314,  0.50573185,  0.85040524]]), 'currentState': array([ 11.4915183 , -15.57459038,  36.67447043,   0.23448839,
        -0.81743763,   0.52612824]), 'targetState': array([  9.6877387 , -17.49707744,  39.        ]), 'previousTarget': array([  9.6877387 , -17.49707744,  39.        ])}
episode index:18767
target thresh 85.45602393699848
target distance 72.0
model initialize at round 18767
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-3.55008702, 12.61221377, 56.11024983]), 'distance': 27.499999999999996, 'localFrame': array([[-0.74216956,  0.61418506,  0.26825556],
       [-0.63755279, -0.77040667,  0.        ],
       [ 0.20666587, -0.17102708,  0.96334778]]), 'currentState': array([ 1.52376342,  9.88322544, 83.        , -0.74216956,  0.61418506,
        0.26825556]), 'targetState': array([-12.06197943,  17.1903651 ,  11.        ]), 'previousTarget': array([-3.55008702, 12.61221377, 56.11024983])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6280989842692251
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-12.06197943,  17.1903651 ,  11.        ]), 'distance': 1.8025552346241398, 'localFrame': array([[-0.63450596,  0.19005483, -0.74918712],
       [-0.28693654, -0.95794959,  0.        ],
       [-0.7176835 ,  0.21496916,  0.66235841]]), 'currentState': array([-11.84123534,  16.65436882,  12.70680561,  -0.63450596,
         0.19005483,  -0.74918712]), 'targetState': array([-12.06197943,  17.1903651 ,  11.        ]), 'previousTarget': array([-12.06197943,  17.1903651 ,  11.        ])}
episode index:18768
target thresh 85.4574782618873
target distance 69.86554355494633
model initialize at round 18768
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.67288588, -11.07522111,  40.46992192]), 'distance': 27.5, 'localFrame': array([[ 0.63570904,  0.60447438,  0.48008827],
       [-0.68907942,  0.72468583,  0.        ],
       [-0.34791316, -0.33081894,  0.87722019]]), 'currentState': array([ -3.77023603, -31.73955675,  23.16074353,   0.63570904,
         0.60447438,   0.48008827]), 'targetState': array([14.41815611, 37.31108112, 81.        ]), 'previousTarget': array([  0.46082729, -11.88229736,  40.16132893])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280655195676283
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([12.50387653, 34.41447927, 68.87010476]), 'distance': 27.5, 'localFrame': array([[-0.47237602, -0.54816604,  0.69019917],
       [ 0.75753359, -0.65279619,  0.        ],
       [ 0.45055939,  0.52284905,  0.72361945]]), 'currentState': array([ 8.33152051, 28.1010576 , 42.43183416, -0.47237602, -0.54816604,
        0.69019917]), 'targetState': array([14.41815611, 37.31108112, 81.        ]), 'previousTarget': array([12.50387653, 34.41447927, 68.87010476])}
episode index:18769
target thresh 85.45893244135094
target distance 67.17639316502859
model initialize at round 18769
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.53815187,  3.28450489, 33.99179634]), 'distance': 27.499999999999996, 'localFrame': array([[-0.80343145, -0.10781682,  0.58555396],
       [ 0.13300317, -0.99111561,  0.        ],
       [ 0.58035167,  0.07788053,  0.81063343]]), 'currentState': array([23.28608811, 12.24139311, 21.39991202, -0.80343145, -0.10781682,
        0.58555396]), 'targetState': array([-42.83399738, -13.79306595,  58.        ]), 'previousTarget': array([ 1.61272055,  3.06170191, 32.85760694])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6280659521710644
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-42.83399738, -13.79306595,  58.        ]), 'distance': 3.6630892268301762, 'localFrame': array([[-0.64373329,  0.35705919,  0.67684281],
       [-0.48505069, -0.87448604,  0.        ],
       [ 0.59188959, -0.32830307,  0.73612758]]), 'currentState': array([-39.87107441, -13.00509274,  55.9954032 ,  -0.64373329,
         0.35705919,   0.67684281]), 'targetState': array([-42.83399738, -13.79306595,  58.        ]), 'previousTarget': array([-42.83399738, -13.79306595,  58.        ])}
episode index:18770
target thresh 85.46038647540388
target distance 24.0
model initialize at round 18770
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.92609029,  0.39718408, 96.75683277]), 'distance': 27.5, 'localFrame': array([[ 0.96643372, -0.1380382 ,  0.21668255],
       [ 0.1413975 ,  0.9899529 ,  0.        ],
       [-0.21450551,  0.03063837,  0.97624212]]), 'currentState': array([-6.93702909,  2.4763463 , 76.8543251 ,  0.96643372, -0.1380382 ,
        0.21668255]), 'targetState': array([1.49998864e+01, 5.83789954e-02, 1.00000000e+02]), 'previousTarget': array([10.58116241,  0.47899352, 95.52483204])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6280773994696214
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.49998864e+01, 5.83789954e-02, 1.00000000e+02]), 'distance': 2.8124234023388417, 'localFrame': array([[ 0.55157031,  0.70943563,  0.43871548],
       [-0.78946705,  0.61379295,  0.        ],
       [-0.26928047, -0.34635141,  0.89862602]]), 'currentState': array([13.14575228,  0.34077146, 97.90425036,  0.55157031,  0.70943563,
        0.43871548]), 'targetState': array([1.49998864e+01, 5.83789954e-02, 1.00000000e+02]), 'previousTarget': array([1.49998864e+01, 5.83789954e-02, 1.00000000e+02])}
episode index:18771
target thresh 85.4618403640607
target distance 30.0
model initialize at round 18771
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.71842405, 19.00843674, 74.39712489]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.70168193,  0.70811921,  0.0788013 ],
       [-0.71032809,  0.70387073,  0.        ],
       [-0.05546593, -0.05597477,  0.99689034]]), 'currentState': array([-3.04321335,  7.48209079, 50.33813673,  0.70168193,  0.70811921,
        0.0788013 ]), 'targetState': array([-11.55040394,  22.17178768,  81.        ]), 'previousTarget': array([-10.12016266,  19.00387045,  74.89661722])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6280879519533399
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.55040394,  22.17178768,  81.        ]), 'distance': 3.128700951165866, 'localFrame': array([[-0.83788209, -0.37181729,  0.39963171],
       [ 0.40561483, -0.9140441 ,  0.        ],
       [ 0.365281  ,  0.16209655,  0.91667579]]), 'currentState': array([-9.65275185, 20.20082523, 79.4824384 , -0.83788209, -0.37181729,
        0.39963171]), 'targetState': array([-11.55040394,  22.17178768,  81.        ]), 'previousTarget': array([-11.55040394,  22.17178768,  81.        ])}
episode index:18772
target thresh 85.46329410733591
target distance 67.2512302098699
model initialize at round 18772
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.06857103,  -3.19157025,  37.91979855]), 'distance': 27.500000000000004, 'localFrame': array([[-0.4119434 ,  0.89426531,  0.17490623],
       [-0.90826614, -0.41839289,  0.        ],
       [ 0.07317952, -0.15886141,  0.9845851 ]]), 'currentState': array([-27.89756278, -26.18796899,  50.14536805,  -0.4119434 ,
         0.89426531,   0.17490623]), 'targetState': array([-2.51648219, 39.92076299, 15.        ]), 'previousTarget': array([-18.1381426 ,  -4.06374046,  37.89114438])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.628084533066561
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-2.51648219, 39.92076299, 15.        ]), 'distance': 2.2118469547110746, 'localFrame': array([[ 0.80095885, -0.29621806, -0.5203074 ],
       [ 0.34686806,  0.93791393,  0.        ],
       [ 0.48800356, -0.18047802,  0.85397904]]), 'currentState': array([-3.59183002, 38.94072127, 16.66595685,  0.80095885, -0.29621806,
       -0.5203074 ]), 'targetState': array([-2.51648219, 39.92076299, 15.        ]), 'previousTarget': array([-2.51648219, 39.92076299, 15.        ])}
episode index:18773
target thresh 85.46474770524408
target distance 52.0
model initialize at round 18773
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.6876585 , -15.11190648,  73.83660588]), 'distance': 27.5, 'localFrame': array([[ 0.05565536, -0.88385429, -0.46443953],
       [ 0.99802333,  0.06284446,  0.        ],
       [ 0.02918745, -0.46352149,  0.88560483]]), 'currentState': array([ 1.47790569e+00,  3.58092216e+00,  9.35623793e+01,  5.56553607e-02,
       -8.83854288e-01, -4.64439532e-01]), 'targetState': array([ 12.26861657, -44.33374615,  43.        ]), 'previousTarget': array([  5.7814836 , -13.99241928,  75.19786799])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6280829815311076
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 12.26861657, -44.33374615,  43.        ]), 'distance': 2.1107853787217077, 'localFrame': array([[ 0.82166844,  0.35475169,  0.44610785],
       [-0.39637981,  0.91808662,  0.        ],
       [-0.40956565, -0.17682814,  0.89497921]]), 'currentState': array([ 10.53827448, -44.13795629,  44.19289458,   0.82166844,
         0.35475169,   0.44610785]), 'targetState': array([ 12.26861657, -44.33374615,  43.        ]), 'previousTarget': array([ 12.26861657, -44.33374615,  43.        ])}
episode index:18774
target thresh 85.46620115779972
target distance 27.266416001796955
model initialize at round 18774
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.62852809,   7.15320399,  15.01271123]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.73602149,  0.67141931,  0.08642036],
       [-0.67394068,  0.73878546,  0.        ],
       [-0.06384611, -0.0582422 ,  0.99625876]]), 'currentState': array([-29.58454943,  26.45177812,  19.96087167,   0.73602149,
         0.67141931,   0.08642036]), 'targetState': array([-2.91798646, -0.69667424, 13.        ]), 'previousTarget': array([-10.405881  ,   6.27670659,  14.92233778])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.628049528376299
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 78, 'trapConfig': [], 'currentTarget': array([-2.91798646, -0.69667424, 13.        ]), 'distance': 10.820098262716684, 'localFrame': array([[ 0.5019909 , -0.69364958, -0.51658049],
       [ 0.81011245,  0.58627452,  0.        ],
       [ 0.30285798, -0.41848829,  0.85623863]]), 'currentState': array([-11.17667466,   5.55884191,  16.1204348 ,   0.5019909 ,
        -0.69364958,  -0.51658049]), 'targetState': array([-2.91798646, -0.69667424, 13.        ]), 'previousTarget': array([-2.91798646, -0.69667424, 13.        ])}
episode index:18775
target thresh 85.46765446501736
target distance 63.0
model initialize at round 18775
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.15918756, 28.92800031, 73.42794568]), 'distance': 27.499999999999996, 'localFrame': array([[-0.67849475,  0.05167796, -0.73278528],
       [-0.07594564, -0.99711196,  0.        ],
       [-0.73066897,  0.05565184,  0.68045994]]), 'currentState': array([ 3.47977286e+00,  3.44994225e+01,  9.81670195e+01, -6.78494749e-01,
        5.16779629e-02, -7.32785278e-01]), 'targetState': array([-23.25495715,  20.49895041,  36.        ]), 'previousTarget': array([-5.8713755 , 29.10900276, 74.44810587])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.628043240360634
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 21, 'trapConfig': [], 'currentTarget': array([-23.25495715,  20.49895041,  36.        ]), 'distance': 2.734352897931501, 'localFrame': array([[-0.21252798, -0.59325129, -0.77645655],
       [ 0.94141349, -0.33725457,  0.        ],
       [-0.26186352, -0.73096666,  0.6301708 ]]), 'currentState': array([-25.03528914,  19.37472628,  37.74448384,  -0.21252798,
        -0.59325129,  -0.77645655]), 'targetState': array([-23.25495715,  20.49895041,  36.        ]), 'previousTarget': array([-23.25495715,  20.49895041,  36.        ])}
episode index:18776
target thresh 85.46910762691155
target distance 64.0
model initialize at round 18776
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.39750192,   2.56739311,  58.00625872]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.42390414, -0.5730217 ,  0.70139248],
       [ 0.80393039,  0.59472341,  0.        ],
       [-0.41713452,  0.56387073,  0.71277527]]), 'currentState': array([-30.02597836,  12.6455954 ,  36.35113304,   0.42390414,
        -0.5730217 ,   0.70139248]), 'targetState': array([  9.40157428, -16.51091763,  99.        ]), 'previousTarget': array([-16.59150204,   3.56112261,  56.71833501])}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.6280313475520021
{'scaleFactor': 20, 'timeStep': 91, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([  9.40157428, -16.51091763,  99.        ]), 'distance': 3.080034589141139, 'localFrame': array([[ 0.43732769,  0.20512985,  0.87559479],
       [-0.42465874,  0.9053535 ,  0.        ],
       [-0.79272281, -0.37182898,  0.48304633]]), 'currentState': array([  6.65782071, -16.37173424,  97.60749959,   0.43732769,
         0.20512985,   0.87559479]), 'targetState': array([  9.40157428, -16.51091763,  99.        ]), 'previousTarget': array([  9.40157428, -16.51091763,  99.        ])}
episode index:18777
target thresh 85.47056064349682
target distance 48.0
model initialize at round 18777
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.54602364, 11.09916512, 53.77981452]), 'distance': 27.5, 'localFrame': array([[ 0.55056988, -0.345136  , -0.76010128],
       [ 0.5311381 ,  0.84728526,  0.        ],
       [ 0.64402261, -0.40371875,  0.64980462]]), 'currentState': array([21.2864686 , -2.49443473, 77.62168724,  0.55056988, -0.345136  ,
       -0.76010128]), 'targetState': array([17.8831084 , 24.08722553, 31.        ]), 'previousTarget': array([19.40487692, 11.5649552 , 54.71991516])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6280297991793806
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([17.8831084 , 24.08722553, 31.        ]), 'distance': 1.4029499178048874, 'localFrame': array([[-0.31164354,  0.90247411, -0.29735297],
       [-0.94522906, -0.32640774,  0.        ],
       [-0.09705831,  0.28106667,  0.95476762]]), 'currentState': array([17.9239022 , 22.68811381, 31.09534532, -0.31164354,  0.90247411,
       -0.29735297]), 'targetState': array([17.8831084 , 24.08722553, 31.        ]), 'previousTarget': array([17.8831084 , 24.08722553, 31.        ])}
episode index:18778
target thresh 85.4720135147877
target distance 32.26980145421478
model initialize at round 18778
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.44374847,   0.28449754,  33.89639427]), 'distance': 27.500000000000004, 'localFrame': array([[-0.08855461, -0.67853418, -0.72921152],
       [ 0.99159101, -0.12941125,  0.        ],
       [-0.09436818, -0.72307959,  0.68428836]]), 'currentState': array([-6.14884587, 12.52039515, 46.26813355, -0.08855461, -0.67853418,
       -0.72921152]), 'targetState': array([-37.59293921,  -5.54715434,  28.        ]), 'previousTarget': array([-26.32540936,   1.28633066,  34.63416127])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6280386167140984
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-37.59293921,  -5.54715434,  28.        ]), 'distance': 2.276553170186912, 'localFrame': array([[-0.37459584,  0.49426529, -0.78446146],
       [-0.79697383, -0.60401384,  0.        ],
       [-0.47382558,  0.62519525,  0.62017757]]), 'currentState': array([-35.92926013,  -5.97893887,  29.49279214,  -0.37459584,
         0.49426529,  -0.78446146]), 'targetState': array([-37.59293921,  -5.54715434,  28.        ]), 'previousTarget': array([-37.59293921,  -5.54715434,  28.        ])}
episode index:18779
target thresh 85.47346624079871
target distance 38.0
model initialize at round 18779
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.71450327, -3.18001876, 19.63971033]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.66378135, -0.25805164, -0.70199976],
       [ 0.36234191,  0.93204525,  0.        ],
       [ 0.65429554, -0.25436393,  0.71217718]]), 'currentState': array([-4.61512789, -3.90033387, 46.85213899,  0.66378135, -0.25805164,
       -0.70199976]), 'targetState': array([ 0.66725108, -2.92485487, 10.        ]), 'previousTarget': array([-1.05285204, -2.85827487, 20.84029804])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6280370681193036
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([ 0.66725108, -2.92485487, 10.        ]), 'distance': 2.3875648270859426, 'localFrame': array([[ 0.18643287,  0.43956736, -0.87864858],
       [-0.92061962,  0.39046064,  0.        ],
       [ 0.34307769,  0.80890112,  0.47746904]]), 'currentState': array([ 1.5804028 , -3.03106036, 12.20348364,  0.18643287,  0.43956736,
       -0.87864858]), 'targetState': array([ 0.66725108, -2.92485487, 10.        ]), 'previousTarget': array([ 0.66725108, -2.92485487, 10.        ])}
episode index:18780
target thresh 85.47491882154439
target distance 29.225729656374263
model initialize at round 18780
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.93213049, 11.56802436, 54.3058913 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.80943062, -0.48001119,  0.33824745],
       [ 0.51007652,  0.86012903,  0.        ],
       [-0.29093645,  0.17253208,  0.9410572 ]]), 'currentState': array([-19.12477133,  30.48016479,  68.34689875,   0.80943062,
        -0.48001119,   0.33824745]), 'targetState': array([ 2.4526595 ,  1.72755938, 47.        ]), 'previousTarget': array([-5.41952553, 11.85247868, 53.92877093])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6280458843279998
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.4526595 ,  1.72755938, 47.        ]), 'distance': 4.150954761955524, 'localFrame': array([[ 0.70817957, -0.43655852, -0.5548859 ],
       [ 0.52475614,  0.8512526 ,  0.        ],
       [ 0.47234807, -0.29117978,  0.83192646]]), 'currentState': array([-0.42608853,  4.45190252, 45.76663484,  0.70817957, -0.43655852,
       -0.5548859 ]), 'targetState': array([ 2.4526595 ,  1.72755938, 47.        ]), 'previousTarget': array([ 2.4526595 ,  1.72755938, 47.        ])}
episode index:18781
target thresh 85.47637125703925
target distance 18.0
model initialize at round 18781
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.83865408,  1.96119086, 83.        ]), 'distance': 22.9077550037589, 'localFrame': array([[ 0.71299693,  0.65749274,  0.24359531],
       [-0.67791355,  0.73514163,  0.        ],
       [-0.17907705, -0.16513656,  0.96987696]]), 'currentState': array([-0.83404778, -3.17613976, 64.62132483,  0.71299693,  0.65749274,
        0.24359531]), 'targetState': array([11.83865408,  1.96119086, 83.        ]), 'previousTarget': array([11.83865408,  1.96119086, 83.        ])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6280495243734541
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([11.83865408,  1.96119086, 83.        ]), 'distance': 1.8631714553233505, 'localFrame': array([[-0.28450706, -0.64647254,  0.70790465],
       [ 0.91528416, -0.40280877,  0.        ],
       [ 0.2851502 ,  0.64793391,  0.70630801]]), 'currentState': array([11.95748713,  1.91299206, 81.1412468 , -0.28450706, -0.64647254,
        0.70790465]), 'targetState': array([11.83865408,  1.96119086, 83.        ]), 'previousTarget': array([11.83865408,  1.96119086, 83.        ])}
episode index:18782
target thresh 85.47782354729782
target distance 37.0
model initialize at round 18782
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.26593819, -3.75575179, 88.98492937]), 'distance': 27.499999999999996, 'localFrame': array([[-0.226616  , -0.85083366,  0.47405408],
       [ 0.96631208, -0.25737319,  0.        ],
       [ 0.12200881,  0.45808418,  0.88049573]]), 'currentState': array([-0.08312466, -2.01145267, 63.18178142, -0.226616  , -0.85083366,
        0.47405408]), 'targetState': array([ 13.25694688,  -4.50037326, 100.        ]), 'previousTarget': array([ 9.5584293 , -3.29635062, 88.94198622])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6280399132191864
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 13.25694688,  -4.50037326, 100.        ]), 'distance': 3.1710460032251113, 'localFrame': array([[-0.55406548,  0.67830233,  0.48261517],
       [-0.77446579, -0.63261579,  0.        ],
       [ 0.30530997, -0.37376894,  0.87583252]]), 'currentState': array([12.25399192, -5.4468043 , 97.14449962, -0.55406548,  0.67830233,
        0.48261517]), 'targetState': array([ 13.25694688,  -4.50037326, 100.        ]), 'previousTarget': array([ 13.25694688,  -4.50037326, 100.        ])}
episode index:18783
target thresh 85.47927569233462
target distance 60.0
model initialize at round 18783
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.38735435, -12.08460953,  54.77693968]), 'distance': 27.499999999999996, 'localFrame': array([[-0.49228108, -0.66805928, -0.55799295],
       [ 0.80504037, -0.59322002,  0.        ],
       [-0.33101259, -0.44920686,  0.82984569]]), 'currentState': array([ -7.04285251, -10.74803405,  29.36681668,  -0.49228108,
        -0.66805928,  -0.55799295]), 'targetState': array([ 18.25599597, -13.98994679,  91.        ]), 'previousTarget': array([  3.84730104, -11.66353032,  56.34409938])}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.628029135440767
{'scaleFactor': 20, 'timeStep': 86, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.25599597, -13.98994679,  91.        ]), 'distance': 2.0695865857654057, 'localFrame': array([[-0.06780306, -0.31792057,  0.94568983],
       [ 0.97800539, -0.20857963,  0.        ],
       [ 0.19725164,  0.92488975,  0.32507037]]), 'currentState': array([ 1.89327407e+01, -1.26936261e+01,  8.95355009e+01, -6.78030580e-02,
       -3.17920573e-01,  9.45689830e-01]), 'targetState': array([ 18.25599597, -13.98994679,  91.        ]), 'previousTarget': array([ 18.25599597, -13.98994679,  91.        ])}
episode index:18784
target thresh 85.4807276921642
target distance 73.0
model initialize at round 18784
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.940732  ,  -8.65454339,  46.86255628]), 'distance': 27.5, 'localFrame': array([[-0.46867498, -0.50197217,  0.72688906],
       [ 0.73093351, -0.68244868,  0.        ],
       [ 0.49606448,  0.53130757,  0.6867549 ]]), 'currentState': array([-35.8675054 , -18.31362527,  22.7116794 ,  -0.46867498,
        -0.50197217,   0.72688906]), 'targetState': array([-9.14791836, 10.59790497, 95.        ]), 'previousTarget': array([-26.50007244,  -7.59438918,  46.42297054])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6280239651670121
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-9.14791836, 10.59790497, 95.        ]), 'distance': 2.8589916831826097, 'localFrame': array([[-0.73315201,  0.28793536,  0.61610174],
       [-0.36555488, -0.93078979,  0.        ],
       [ 0.57346121, -0.225219  ,  0.78766658]]), 'currentState': array([-8.80581685,  8.28882608, 93.34925632, -0.73315201,  0.28793536,
        0.61610174]), 'targetState': array([-9.14791836, 10.59790497, 95.        ]), 'previousTarget': array([-9.14791836, 10.59790497, 95.        ])}
episode index:18785
target thresh 85.48217954680104
target distance 49.0
model initialize at round 18785
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.42003234, 14.62969908, 37.79860339]), 'distance': 27.5, 'localFrame': array([[ 0.57368516,  0.58476451, -0.57352925],
       [-0.71383687,  0.70031202,  0.        ],
       [ 0.40164943,  0.40940632,  0.81918508]]), 'currentState': array([-21.8908577 ,  23.16864825,  14.82438171,   0.57368516,
         0.58476451,  -0.57352925]), 'targetState': array([ 5.34538045,  4.51961368, 65.        ]), 'previousTarget': array([-9.24236763, 13.68651075, 38.93992202])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6280261448643082
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.34538045,  4.51961368, 65.        ]), 'distance': 3.5387507861904495, 'localFrame': array([[ 0.16513368, -0.59501133,  0.78657002],
       [ 0.96357937,  0.26742249,  0.        ],
       [-0.21034651,  0.75792264,  0.6175011 ]]), 'currentState': array([ 3.91046727,  5.89898894, 62.07406337,  0.16513368, -0.59501133,
        0.78657002]), 'targetState': array([ 5.34538045,  4.51961368, 65.        ]), 'previousTarget': array([ 5.34538045,  4.51961368, 65.        ])}
episode index:18786
target thresh 85.48363125625967
target distance 48.0
model initialize at round 18786
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.69219259, -10.78832543,  50.41829014]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.97498903,  0.07933259,  0.20761197],
       [-0.08109964,  0.996706  ,  0.        ],
       [-0.2069281 , -0.01683726,  0.97821126]]), 'currentState': array([-18.73398451,  -8.15759982,  27.54748388,   0.97498903,
         0.07933259,   0.20761197]), 'targetState': array([ 13.13252063, -13.73087404,  76.        ]), 'previousTarget': array([ -4.7929313 , -10.67136137,  50.41502844])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6280301594249158
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.13252063, -13.73087404,  76.        ]), 'distance': 2.809349597873968, 'localFrame': array([[ 0.59744838,  0.11413488,  0.79374345],
       [-0.18764386,  0.98223713,  0.        ],
       [-0.77964428, -0.14894108,  0.6082527 ]]), 'currentState': array([ 12.93625489, -13.21685121,  73.24505799,   0.59744838,
         0.11413488,   0.79374345]), 'targetState': array([ 13.13252063, -13.73087404,  76.        ]), 'previousTarget': array([ 13.13252063, -13.73087404,  76.        ])}
episode index:18787
target thresh 85.48508282055462
target distance 55.0
model initialize at round 18787
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.43988205,  0.81522843, 67.70618019]), 'distance': 27.500000000000004, 'localFrame': array([[-0.9231388 , -0.36947379,  0.10631965],
       [ 0.37157991, -0.92840098,  0.        ],
       [ 0.09870727,  0.03950625,  0.994332  ]]), 'currentState': array([ 0.86269748,  8.73864323, 41.37576574, -0.9231388 , -0.36947379,
        0.10631965]), 'targetState': array([-3.05199260e-02, -7.99994178e+00,  9.70000000e+01]), 'previousTarget': array([ 1.17955164,  1.30353619, 68.15179093])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6280323385604889
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.05199260e-02, -7.99994178e+00,  9.70000000e+01]), 'distance': 2.3704630336209918, 'localFrame': array([[ 0.52075911,  0.29676624,  0.80046221],
       [-0.49511935,  0.86882497,  0.        ],
       [-0.69546155, -0.39632433,  0.59938323]]), 'currentState': array([ 0.29501484, -6.68571347, 95.05425437,  0.52075911,  0.29676624,
        0.80046221]), 'targetState': array([-3.05199260e-02, -7.99994178e+00,  9.70000000e+01]), 'previousTarget': array([-3.05199260e-02, -7.99994178e+00,  9.70000000e+01])}
episode index:18788
target thresh 85.48653423970039
target distance 44.71726745259049
model initialize at round 18788
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.631095  ,  8.37345371, 57.90228291]), 'distance': 27.5, 'localFrame': array([[-0.83195393,  0.46028195, -0.30982765],
       [-0.48410335, -0.87501082,  0.        ],
       [-0.27110255,  0.14998861,  0.95079273]]), 'currentState': array([14.25169165, -0.5127125 , 70.29841638, -0.83195393,  0.46028195,
       -0.30982765]), 'targetState': array([-28.75629085,  16.18875339,  47.        ]), 'previousTarget': array([-7.16283155,  7.83210419, 58.10643812])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6280390811731157
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.75629085,  16.18875339,  47.        ]), 'distance': 3.058402728470721, 'localFrame': array([[-0.92111705, -0.28466016, -0.26554092],
       [ 0.29526012, -0.95541691,  0.        ],
       [-0.25370229, -0.07840364,  0.96409959]]), 'currentState': array([-26.95298743,  16.27392642,  49.46873845,  -0.92111705,
        -0.28466016,  -0.26554092]), 'targetState': array([-28.75629085,  16.18875339,  47.        ]), 'previousTarget': array([-28.75629085,  16.18875339,  47.        ])}
episode index:18789
target thresh 85.48798551371152
target distance 44.0
model initialize at round 18789
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.90008611, -15.51932289,  24.81735081]), 'distance': 27.5, 'localFrame': array([[ 0.6168644 , -0.0879952 ,  0.782135  ],
       [ 0.14121959,  0.9899783 ,  0.        ],
       [-0.77429667,  0.11045278,  0.62310902]]), 'currentState': array([-22.02191324, -31.35785042,   5.28030888,   0.6168644 ,
        -0.0879952 ,   0.782135  ]), 'targetState': array([ 2.29707125,  3.27467001, 48.        ]), 'previousTarget': array([-11.48809822, -15.14934826,  23.89274647])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280056570602273
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 2.29707125,  3.27467001, 48.        ]), 'distance': 19.342090271770417, 'localFrame': array([[-0.29098254, -0.73192546,  0.61612846],
       [ 0.92925725, -0.36943329,  0.        ],
       [ 0.22761836,  0.57254184,  0.78764568]]), 'currentState': array([ 4.74189796, -7.1951083 , 31.92134892, -0.29098254, -0.73192546,
        0.61612846]), 'targetState': array([ 2.29707125,  3.27467001, 48.        ]), 'previousTarget': array([ 2.29707125,  3.27467001, 48.        ])}
episode index:18790
target thresh 85.48943664260248
target distance 22.40675439245697
model initialize at round 18790
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.74089537, -12.27030275,  18.        ]), 'distance': 25.415699349845195, 'localFrame': array([[-0.65380659, -0.2075644 ,  0.72763587],
       [ 0.30258807, -0.95312143,  0.        ],
       [ 0.69352534,  0.22017393,  0.68596359]]), 'currentState': array([ 28.11554687, -10.06345343,   4.42753292,  -0.65380659,
        -0.2075644 ,   0.72763587]), 'targetState': array([  6.74089537, -12.27030275,  18.        ]), 'previousTarget': array([  6.74089537, -12.27030275,  18.        ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6280128060707795
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([  6.74089537, -12.27030275,  18.        ]), 'distance': 2.0793710746437504, 'localFrame': array([[ 0.54009229, -0.83895844, -0.0667013 ],
       [ 0.84083098,  0.54129776,  0.        ],
       [ 0.03610526, -0.05608452,  0.99777299]]), 'currentState': array([  7.87430572, -10.63746136,  17.38926762,   0.54009229,
        -0.83895844,  -0.0667013 ]), 'targetState': array([  6.74089537, -12.27030275,  18.        ]), 'previousTarget': array([  6.74089537, -12.27030275,  18.        ])}
episode index:18791
target thresh 85.49088762638783
target distance 43.83377660803382
model initialize at round 18791
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.58142597, -19.7643107 ,  17.38605869]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.43894887, -0.62461103, -0.64589856],
       [ 0.81817135,  0.57497447,  0.        ],
       [ 0.37137518, -0.5284557 ,  0.76342325]]), 'currentState': array([ 7.13442519,  4.7047588 , 29.45390122,  0.43894887, -0.62461103,
       -0.64589856]), 'targetState': array([ 13.26241484, -38.79572596,   8.        ]), 'previousTarget': array([ 10.09072352, -19.07016338,  18.35019052])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6280149856659529
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.26241484, -38.79572596,   8.        ]), 'distance': 1.5953520989920973, 'localFrame': array([[ 0.6582355 , -0.14529688, -0.73865746],
       [ 0.21554811,  0.97649322,  0.        ],
       [ 0.72129401, -0.15921622,  0.67408097]]), 'currentState': array([ 12.70229613, -38.86963086,   9.49196294,   0.6582355 ,
        -0.14529688,  -0.73865746]), 'targetState': array([ 13.26241484, -38.79572596,   8.        ]), 'previousTarget': array([ 13.26241484, -38.79572596,   8.        ])}
episode index:18792
target thresh 85.49233846508206
target distance 52.77561265747231
model initialize at round 18792
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.55406557,  -9.27970292,  40.72002611]), 'distance': 27.5, 'localFrame': array([[ 0.98522509, -0.11150724, -0.12999103],
       [ 0.11246146,  0.99365609,  0.        ],
       [ 0.12916637, -0.01461898,  0.99151517]]), 'currentState': array([-34.33842206,  -7.76758497,  54.44110061,   0.98522509,
        -0.11150724,  -0.12999103]), 'targetState': array([ 16.69530947, -11.01211341,  25.        ]), 'previousTarget': array([-12.00550894,  -9.4818742 ,  40.77099141])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6280178877895811
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.69530947, -11.01211341,  25.        ]), 'distance': 2.772924026339838, 'localFrame': array([[ 0.41589815,  0.51542862, -0.74924099],
       [-0.77824339,  0.62796276,  0.        ],
       [ 0.47049544,  0.58309185,  0.66229746]]), 'currentState': array([ 14.09094079, -10.70903307,  24.09749582,   0.41589815,
         0.51542862,  -0.74924099]), 'targetState': array([ 16.69530947, -11.01211341,  25.        ]), 'previousTarget': array([ 16.69530947, -11.01211341,  25.        ])}
episode index:18793
target thresh 85.49378915869966
target distance 30.600152464239564
model initialize at round 18793
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.20000757,  2.78483642, 53.58521878]), 'distance': 27.5, 'localFrame': array([[ 0.63757886,  0.05422774,  0.76847417],
       [-0.08474662,  0.99640253,  0.        ],
       [-0.76570961, -0.06512559,  0.63988081]]), 'currentState': array([-27.46649151,  -8.69126336,  38.96267582,   0.63757886,
         0.05422774,   0.76847417]), 'targetState': array([ 1.69072101,  7.81930064, 60.        ]), 'previousTarget': array([-8.32425376,  2.63687643, 52.79972722])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6280258586032242
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.69072101,  7.81930064, 60.        ]), 'distance': 1.9992100739833814, 'localFrame': array([[-0.10436675,  0.99148065,  0.07793396],
       [-0.99450542, -0.10468514,  0.        ],
       [ 0.00815853, -0.07750575,  0.99695852]]), 'currentState': array([ 0.54808521,  6.18613509, 60.15490205, -0.10436675,  0.99148065,
        0.07793396]), 'targetState': array([ 1.69072101,  7.81930064, 60.        ]), 'previousTarget': array([ 1.69072101,  7.81930064, 60.        ])}
episode index:18794
target thresh 85.49523970725514
target distance 31.823412352671195
model initialize at round 18794
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.25589707,  2.13191234, 30.05928528]), 'distance': 27.499999999999996, 'localFrame': array([[-0.06093829,  0.72617985,  0.68479877],
       [-0.99649752, -0.08362233,  0.        ],
       [ 0.05726447, -0.68240027,  0.72873222]]), 'currentState': array([  2.02019081, -22.4015506 ,  17.76105517,  -0.06093829,
         0.72617985,   0.68479877]), 'targetState': array([-0.16597539,  7.99827808, 33.        ]), 'previousTarget': array([ 0.539701  ,  0.65424526, 29.30760763])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6280350953540057
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.16597539,  7.99827808, 33.        ]), 'distance': 3.267590085770712, 'localFrame': array([[-0.81030356,  0.3152948 ,  0.49396086],
       [-0.36262283, -0.93193599,  0.        ],
       [ 0.4603399 , -0.17912148,  0.86948414]]), 'currentState': array([ 1.52501094,  5.23998743, 32.54224142, -0.81030356,  0.3152948 ,
        0.49396086]), 'targetState': array([-0.16597539,  7.99827808, 33.        ]), 'previousTarget': array([-0.16597539,  7.99827808, 33.        ])}
episode index:18795
target thresh 85.49669011076304
target distance 42.18976867543978
model initialize at round 18795
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.48001763, 10.61244126, 68.22203051]), 'distance': 27.5, 'localFrame': array([[-0.94393945, -0.12742643,  0.30453377],
       [ 0.13378081, -0.99101095,  0.        ],
       [ 0.3017963 ,  0.04074078,  0.95250154]]), 'currentState': array([24.522177  , 14.62763123, 48.79204297, -0.94393945, -0.12742643,
        0.30453377]), 'targetState': array([-15.86325729,   6.11204287,  90.        ]), 'previousTarget': array([ 6.79711862, 10.55524704, 67.97865592])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6280016821227142
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 75, 'trapConfig': [], 'currentTarget': array([-15.86325729,   6.11204287,  90.        ]), 'distance': 19.254987535352107, 'localFrame': array([[-0.85751539,  0.13481792,  0.49647909],
       [-0.15531147, -0.98786555,  0.        ],
       [ 0.49045459, -0.0771089 ,  0.86804868]]), 'currentState': array([-1.04626581,  8.59908591, 77.95732897, -0.85751539,  0.13481792,
        0.49647909]), 'targetState': array([-15.86325729,   6.11204287,  90.        ]), 'previousTarget': array([-15.86325729,   6.11204287,  90.        ])}
episode index:18796
target thresh 85.49814036923783
target distance 24.569716029457545
model initialize at round 18796
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.95881706,   1.14723934,  75.        ]), 'distance': 25.57287506643788, 'localFrame': array([[-0.67201367,  0.22670674, -0.70498346],
       [-0.31965475, -0.94753408,  0.        ],
       [-0.66799586,  0.22535131,  0.70922374]]), 'currentState': array([ 7.96516258,  3.68867459, 83.66927012, -0.67201367,  0.22670674,
       -0.70498346]), 'targetState': array([-15.95881706,   1.14723934,  75.        ]), 'previousTarget': array([-15.95881706,   1.14723934,  75.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6280144897053417
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.95881706,   1.14723934,  75.        ]), 'distance': 3.374137648986762, 'localFrame': array([[-0.95231888, -0.00417327,  0.30507594],
       [ 0.00438218, -0.9999904 ,  0.        ],
       [ 0.30507301,  0.0013369 ,  0.95232803]]), 'currentState': array([-1.38250763e+01,  1.11303246e+00,  7.76135771e+01, -9.52318884e-01,
       -4.17327228e-03,  3.05075936e-01]), 'targetState': array([-15.95881706,   1.14723934,  75.        ]), 'previousTarget': array([-15.95881706,   1.14723934,  75.        ])}
episode index:18797
target thresh 85.49959048269402
target distance 33.0
model initialize at round 18797
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.6418393 , -7.10408121, 50.8275217 ]), 'distance': 27.5, 'localFrame': array([[ 0.82680016, -0.46232986, -0.32039446],
       [ 0.48805823,  0.87281107,  0.        ],
       [ 0.27964383, -0.15637115,  0.94728422]]), 'currentState': array([ 9.29835718,  2.70247261, 72.14291573,  0.82680016, -0.46232986,
       -0.32039446]), 'targetState': array([ 31.60077479, -12.5455583 ,  39.        ]), 'previousTarget': array([22.81889113, -6.35577403, 51.29350173])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6280228769620233
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 31.60077479, -12.5455583 ,  39.        ]), 'distance': 2.1112205081927895, 'localFrame': array([[ 0.91750125, -0.37390385,  0.13560005],
       [ 0.37738955,  0.9260546 ,  0.        ],
       [-0.12557305,  0.05117404,  0.99076366]]), 'currentState': array([ 30.2332121 , -12.20414179,  40.57176942,   0.91750125,
        -0.37390385,   0.13560005]), 'targetState': array([ 31.60077479, -12.5455583 ,  39.        ]), 'previousTarget': array([ 31.60077479, -12.5455583 ,  39.        ])}
episode index:18798
target thresh 85.50104045114612
target distance 41.0
model initialize at round 18798
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.69400889, -2.91981998, 71.74138873]), 'distance': 27.5, 'localFrame': array([[-0.28217253, -0.61432186,  0.73687673],
       [ 0.90872367, -0.41739824,  0.        ],
       [ 0.30757105,  0.66961733,  0.67602713]]), 'currentState': array([ 3.38502546, -9.82538933, 45.17630685, -0.28217253, -0.61432186,
        0.73687673]), 'targetState': array([ 0.85002385,  0.5267442 , 85.        ]), 'previousTarget': array([ 1.61434359, -2.58157864, 70.82461853])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279894697128632
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 70, 'trapConfig': [], 'currentTarget': array([ 0.85002385,  0.5267442 , 85.        ]), 'distance': 9.28863278667016, 'localFrame': array([[-0.64071657,  0.05026283,  0.76613049],
       [-0.07820756, -0.9969371 ,  0.        ],
       [ 0.76378391, -0.0599172 ,  0.64268505]]), 'currentState': array([ 2.79919603e+00,  1.90889962e+00,  7.60239723e+01, -6.40716567e-01,
        5.02628326e-02,  7.66130491e-01]), 'targetState': array([ 0.85002385,  0.5267442 , 85.        ]), 'previousTarget': array([ 0.85002385,  0.5267442 , 85.        ])}
episode index:18799
target thresh 85.50249027460862
target distance 36.968766266442614
model initialize at round 18799
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.55634052, 16.64660874, 14.71876925]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.68251078,  0.5827689 ,  0.44108893],
       [-0.64935116,  0.76048871,  0.        ],
       [-0.33544315, -0.28642161,  0.8974634 ]]), 'currentState': array([-1.58182974, -5.64852816, 22.41870514,  0.68251078,  0.5827689 ,
        0.44108893]), 'targetState': array([21.22066762, 30.30978828, 10.        ]), 'previousTarget': array([12.09630374, 15.88155028, 14.29310021])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6279954117822076
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.22066762, 30.30978828, 10.        ]), 'distance': 2.471715260375835, 'localFrame': array([[ 0.79631583,  0.5419963 , -0.26855373],
       [-0.56266601,  0.82668432,  0.        ],
       [ 0.22200916,  0.15110605,  0.96326471]]), 'currentState': array([19.80598183, 28.31444537,  9.64412488,  0.79631583,  0.5419963 ,
       -0.26855373]), 'targetState': array([21.22066762, 30.30978828, 10.        ]), 'previousTarget': array([21.22066762, 30.30978828, 10.        ])}
episode index:18800
target thresh 85.50393995309604
target distance 84.0
model initialize at round 18800
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.75182683,   6.06506411,  38.86081754]), 'distance': 27.5, 'localFrame': array([[ 0.28238905,  0.95770796,  0.05524399],
       [-0.95917272,  0.28282095,  0.        ],
       [-0.01562416, -0.05298853,  0.99847288]]), 'currentState': array([-30.38001214,   1.66208307,  11.84311223,   0.28238905,
         0.95770796,   0.05524399]), 'targetState': array([-22.19353244,  15.37683705,  96.        ]), 'previousTarget': array([-27.49705191,   4.82949713,  38.92984451])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279620095476572
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-20.99831848,  13.93239621,  86.10789895]), 'distance': 27.5, 'localFrame': array([[ 0.44353629, -0.61609687, -0.65092259],
       [ 0.81156773,  0.58425835,  0.        ],
       [ 0.38030695, -0.52826777,  0.75914411]]), 'currentState': array([-17.73374381,   9.98709005,  59.08888506,   0.44353629,
        -0.61609687,  -0.65092259]), 'targetState': array([-22.19353244,  15.37683705,  96.        ]), 'previousTarget': array([-21.4515054 ,  14.23774777,  86.97130925])}
episode index:18801
target thresh 85.50538948662285
target distance 71.0
model initialize at round 18801
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.21587071,  4.15482703, 74.64476681]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.24042442,  0.89036726,  0.38657761],
       [-0.96542219,  0.26069138,  0.        ],
       [-0.10077745, -0.3732106 ,  0.92225688]]), 'currentState': array([-15.9887793 ,   2.61294117,  98.39723146,   0.24042442,
         0.89036726,   0.38657761]), 'targetState': array([25.99088325,  7.31259104, 26.        ]), 'previousTarget': array([-1.99194544,  3.35506007, 73.3820894 ])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6279598329054059
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.99088325,  7.31259104, 26.        ]), 'distance': 2.8221186036403547, 'localFrame': array([[ 0.47029007, -0.05258799, -0.88094367],
       [ 0.11112774,  0.99380613,  0.        ],
       [ 0.87548722, -0.09789728,  0.47322114]]), 'currentState': array([24.04899287,  8.70605192, 27.50056054,  0.47029007, -0.05258799,
       -0.88094367]), 'targetState': array([25.99088325,  7.31259104, 26.        ]), 'previousTarget': array([25.99088325,  7.31259104, 26.        ])}
episode index:18802
target thresh 85.50683887520354
target distance 40.0
model initialize at round 18802
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.98153095, 32.12751309, 29.87314864]), 'distance': 27.5, 'localFrame': array([[ 0.92306744, -0.3612443 , -0.13209487],
       [ 0.36443784,  0.93122772,  0.        ],
       [ 0.1230104 , -0.04814037,  0.99123708]]), 'currentState': array([14.81794101, 43.71653128,  5.2843468 ,  0.92306744, -0.3612443 ,
       -0.13209487]), 'targetState': array([21.54294108, 24.99803371, 45.        ]), 'previousTarget': array([18.35994856, 32.90796096, 29.20076198])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6279682208386027
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.54294108, 24.99803371, 45.        ]), 'distance': 3.264895127461886, 'localFrame': array([[ 0.47676908, -0.80031681, -0.36357152],
       [ 0.85910881,  0.51179297,  0.        ],
       [ 0.18607335, -0.31234749,  0.93156629]]), 'currentState': array([20.98979   , 26.52926134, 42.16999894,  0.47676908, -0.80031681,
       -0.36357152]), 'targetState': array([21.54294108, 24.99803371, 45.        ]), 'previousTarget': array([21.54294108, 24.99803371, 45.        ])}
episode index:18803
target thresh 85.50828811885263
target distance 10.329071998596191
model initialize at round 18803
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 21.]), 'distance': 11.939144343416364, 'localFrame': array([[ 0.638118  , -0.60948121, -0.4704658 ],
       [ 0.69069427,  0.72314689,  0.        ],
       [ 0.34021588, -0.32494803,  0.88241823]]), 'currentState': array([-9.33583617, -6.59099555, 17.54397508,  0.638118  , -0.60948121,
       -0.4704658 ]), 'targetState': array([-0.,  0., 21.]), 'previousTarget': array([ 0.,  0., 21.])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6279792049664803
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 21.]), 'distance': 4.068706334022964, 'localFrame': array([[-0.82326038,  0.1824985 , -0.53752828],
       [-0.21642386, -0.9762995 ,  0.        ],
       [-0.52478859,  0.11633395,  0.84324572]]), 'currentState': array([-2.22792013, -2.81251101, 22.91846942, -0.82326038,  0.1824985 ,
       -0.53752828]), 'targetState': array([-0.,  0., 21.]), 'previousTarget': array([ 0.,  0., 21.])}
episode index:18804
target thresh 85.5097372175846
target distance 81.0
model initialize at round 18804
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.47466356,   0.45467169,  62.38915801]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.92532578,  0.09863022, -0.36612058],
       [-0.10598933,  0.99436727,  0.        ],
       [ 0.36405832,  0.03880488,  0.93056742]]), 'currentState': array([-37.34101312,  10.53056441,  81.63110798,   0.92532578,
         0.09863022,  -0.36612058]), 'targetState': array([ 32.4588827 , -31.16762637,   2.        ]), 'previousTarget': array([-21.65320157,   0.70919373,  63.72384304])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.6279691352135871
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 32.4588827 , -31.16762637,   2.        ]), 'distance': 1.4616308475100752, 'localFrame': array([[ 0.04141881, -0.96648146,  0.25337338],
       [ 0.99908298,  0.04281595,  0.        ],
       [-0.01084842,  0.25314103,  0.96736856]]), 'currentState': array([ 32.82323603, -29.92841062,   2.68407288,   0.04141881,
        -0.96648146,   0.25337338]), 'targetState': array([ 32.4588827 , -31.16762637,   2.        ]), 'previousTarget': array([ 32.4588827 , -31.16762637,   2.        ])}
episode index:18805
target thresh 85.51118617141394
target distance 41.13965046866699
model initialize at round 18805
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.4546187 ,  -4.71214645,  48.31495087]), 'distance': 27.5, 'localFrame': array([[ 0.8023557 , -0.43361336, -0.41012777],
       [ 0.47543861,  0.8797489 ,  0.        ],
       [ 0.36080945, -0.19499058,  0.91202808]]), 'currentState': array([-39.60287266, -12.07273648,  52.59753526,   0.8023557 ,
        -0.43361336,  -0.41012777]), 'targetState': array([ 0.67981923, -0.73337972, 46.        ]), 'previousTarget': array([-14.28010034,  -4.56471638,  48.90909999])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6279771035334949
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.67981923, -0.73337972, 46.        ]), 'distance': 2.565105360059563, 'localFrame': array([[ 0.55836324,  0.74126201, -0.37250653],
       [-0.79874827,  0.60166536,  0.        ],
       [ 0.22412428,  0.29753895,  0.92802957]]), 'currentState': array([-0.92004563, -0.26215122, 47.94888215,  0.55836324,  0.74126201,
       -0.37250653]), 'targetState': array([ 0.67981923, -0.73337972, 46.        ]), 'previousTarget': array([ 0.67981923, -0.73337972, 46.        ])}
episode index:18806
target thresh 85.51263498035514
target distance 48.67967364610596
model initialize at round 18806
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.76679135,  -7.87732047,  46.97077908]), 'distance': 27.5, 'localFrame': array([[ 0.74329528,  0.02218226,  0.6685956 ],
       [-0.02982985,  0.99955499,  0.        ],
       [-0.66829806, -0.01994411,  0.7436262 ]]), 'currentState': array([-4.00701107e+01,  3.18410383e+00,  3.74392419e+01,  7.43295282e-01,
        2.21822581e-02,  6.68595596e-01]), 'targetState': array([  7.7532973 , -19.51631064,  57.        ]), 'previousTarget': array([-17.59974412,  -8.07284202,  46.0629121 ])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6279789275850423
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([  7.7532973 , -19.51631064,  57.        ]), 'distance': 1.9761364216765083, 'localFrame': array([[ 0.89538639, -0.0138909 ,  0.44507332],
       [ 0.01551199,  0.99987968,  0.        ],
       [-0.44501977,  0.00690397,  0.89549413]]), 'currentState': array([ 5.87525169e+00, -2.01296833e+01,  5.70428223e+01,  8.95386389e-01,
       -1.38908993e-02,  4.45073318e-01]), 'targetState': array([  7.7532973 , -19.51631064,  57.        ]), 'previousTarget': array([  7.7532973 , -19.51631064,  57.        ])}
episode index:18807
target thresh 85.5140836444227
target distance 41.999672381829825
model initialize at round 18807
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.35810664, -28.39541218,  42.19212049]), 'distance': 27.5, 'localFrame': array([[ 0.09620604, -0.98621323, -0.13463973],
       [ 0.9952756 ,  0.09709008,  0.        ],
       [ 0.01307218, -0.13400363,  0.99089462]]), 'currentState': array([ 0.73634676, -1.62508066, 48.47354428,  0.09620604, -0.98621323,
       -0.13463973]), 'targetState': array([  0.16589099, -41.99967238,  39.        ]), 'previousTarget': array([  0.10620802, -26.88935664,  42.23795006])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6279873122731123
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.16589099, -41.99967238,  39.        ]), 'distance': 2.7962260776453447, 'localFrame': array([[ 0.23488494, -0.97060192,  0.05254505],
       [ 0.97194461,  0.23520987,  0.        ],
       [-0.01235911,  0.05107088,  0.99861855]]), 'currentState': array([  1.17313746, -39.89290871,  40.53814227,   0.23488494,
        -0.97060192,   0.05254505]), 'targetState': array([  0.16589099, -41.99967238,  39.        ]), 'previousTarget': array([  0.16589099, -41.99967238,  39.        ])}
episode index:18808
target thresh 85.5155321636311
target distance 43.0
model initialize at round 18808
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.74880568, 18.04386877, 30.37083577]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55376632,  0.44603901,  0.70313019],
       [-0.62728644,  0.77878862,  0.        ],
       [-0.54758979, -0.44106404,  0.71106113]]), 'currentState': array([21.53118945,  6.16102467,  5.57162894,  0.55376632,  0.44603901,
        0.70313019]), 'targetState': array([21.89472873, 26.01193675, 47.        ]), 'previousTarget': array([21.67150326, 17.19748942, 28.75914   ])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6279905799520743
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.89472873, 26.01193675, 47.        ]), 'distance': 3.314258547366544, 'localFrame': array([[-0.86457361,  0.07553589,  0.49679654],
       [-0.08703625, -0.99620515,  0.        ],
       [ 0.49491127, -0.04323931,  0.86786704]]), 'currentState': array([23.43141989, 26.50365215, 44.10498601, -0.86457361,  0.07553589,
        0.49679654]), 'targetState': array([21.89472873, 26.01193675, 47.        ]), 'previousTarget': array([21.89472873, 26.01193675, 47.        ])}
episode index:18809
target thresh 85.5169805379948
target distance 71.0
model initialize at round 18809
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.27808158, -22.73431538,  49.96034727]), 'distance': 27.5, 'localFrame': array([[ 0.38346871,  0.92274394, -0.03867005],
       [-0.92343464,  0.38375575,  0.        ],
       [ 0.01483985,  0.03570926,  0.99925203]]), 'currentState': array([ -0.23634826, -24.29149319,  24.20572007,   0.38346871,
         0.92274394,  -0.03867005]), 'targetState': array([ 26.28632801, -19.95066314,  96.        ]), 'previousTarget': array([  9.25294106, -23.81172108,  50.66174383])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6279910156195976
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.28632801, -19.95066314,  96.        ]), 'distance': 2.033442771021522, 'localFrame': array([[ 0.01109382, -0.73553436,  0.67739658],
       [ 0.99988628,  0.01508095,  0.        ],
       [-0.01021578,  0.67731955,  0.73561802]]), 'currentState': array([ 2.63595020e+01, -1.92864720e+01,  9.40794831e+01,  1.10938195e-02,
       -7.35534360e-01,  6.77396585e-01]), 'targetState': array([ 26.28632801, -19.95066314,  96.        ]), 'previousTarget': array([ 26.28632801, -19.95066314,  96.        ])}
episode index:18810
target thresh 85.51842876752832
target distance 41.3605329709781
model initialize at round 18810
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.91261126,  -6.85135173,  21.85566177]), 'distance': 27.499999999999996, 'localFrame': array([[-0.62751393,  0.52692594, -0.5732149 ],
       [-0.64305916, -0.76581651,  0.        ],
       [-0.43897743,  0.36861109,  0.81940507]]), 'currentState': array([ 6.51952146, -8.76005562, 21.56421045, -0.62751393,  0.52692594,
       -0.5732149 ]), 'targetState': array([-34.49808905,  -5.90608602,  22.        ]), 'previousTarget': array([-20.50582136,  -7.23832577,  22.33829999])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6279973513316125
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.49808905,  -5.90608602,  22.        ]), 'distance': 2.5212626867417818, 'localFrame': array([[ 0.32003685,  0.87088566, -0.37300749],
       [-0.93862801,  0.3449311 ,  0.        ],
       [ 0.12866189,  0.35011528,  0.92782833]]), 'currentState': array([-33.57918116,  -7.89910713,  20.75893566,   0.32003685,
         0.87088566,  -0.37300749]), 'targetState': array([-34.49808905,  -5.90608602,  22.        ]), 'previousTarget': array([-34.49808905,  -5.90608602,  22.        ])}
episode index:18811
target thresh 85.51987685224613
target distance 64.0
model initialize at round 18811
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.43437034,  10.97107855,  67.37910184]), 'distance': 27.5, 'localFrame': array([[-0.34923025,  0.66405996, -0.66110711],
       [-0.88506923, -0.4654594 ,  0.        ],
       [-0.30771852,  0.58512556,  0.75029154]]), 'currentState': array([-12.38697304,  17.31047085,  92.18055166,  -0.34923025,
         0.66405996,  -0.66110711]), 'targetState': array([-37.98225514,   1.16116081,  29.        ]), 'previousTarget': array([-22.39905018,  10.01023817,  68.01057735])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6279945529794247
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-37.98225514,   1.16116081,  29.        ]), 'distance': 3.159574872412936, 'localFrame': array([[-0.18042546, -0.52247584, -0.83334606],
       [ 0.94522716, -0.32641326,  0.        ],
       [-0.2720152 , -0.78770133,  0.55275162]]), 'currentState': array([-38.3511325 ,   2.36214009,  31.89905013,  -0.18042546,
        -0.52247584,  -0.83334606]), 'targetState': array([-37.98225514,   1.16116081,  29.        ]), 'previousTarget': array([-37.98225514,   1.16116081,  29.        ])}
episode index:18812
target thresh 85.5213247921627
target distance 71.0
model initialize at round 18812
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.97093982,  18.76490463,  71.45751804]), 'distance': 27.5, 'localFrame': array([[ 0.33759293, -0.70492453, -0.62378877],
       [ 0.90190745,  0.43192933,  0.        ],
       [ 0.26943267, -0.56259974,  0.78159297]]), 'currentState': array([-32.84285649,  32.95669205,  93.98794039,   0.33759293,
        -0.70492453,  -0.62378877]), 'targetState': array([-11.49610644, -11.12832138,  24.        ]), 'previousTarget': array([-27.0055823 ,  19.52962283,  72.55206317])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6279849600734971
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([-11.49610644, -11.12832138,  24.        ]), 'distance': 1.97469784395112, 'localFrame': array([[ 0.71933053,  0.2535957 , -0.64672467],
       [-0.33248705,  0.94310782,  0.        ],
       [ 0.6099311 ,  0.21502758,  0.76272354]]), 'currentState': array([-11.87872612, -11.9495713 ,  25.75458893,   0.71933053,
         0.2535957 ,  -0.64672467]), 'targetState': array([-11.49610644, -11.12832138,  24.        ]), 'previousTarget': array([-11.49610644, -11.12832138,  24.        ])}
episode index:18813
target thresh 85.52277258729252
target distance 15.464353466769804
model initialize at round 18813
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.2677756 ,  -6.24531546,   3.        ]), 'distance': 19.02611726795974, 'localFrame': array([[-0.81111379, -0.51528316,  0.27672673],
       [ 0.53622341, -0.8440761 ,  0.        ],
       [ 0.23357842,  0.14838735,  0.96094866]]), 'currentState': array([-14.06513412,   8.29491093,   4.2924767 ,  -0.81111379,
        -0.51528316,   0.27672673]), 'targetState': array([-26.2677756 ,  -6.24531546,   3.        ]), 'previousTarget': array([-26.2677756 ,  -6.24531546,   3.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6279996511075641
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.2677756 ,  -6.24531546,   3.        ]), 'distance': 2.5165257263567975, 'localFrame': array([[ 0.02707948, -0.92411638,  0.38115038],
       [ 0.99957094,  0.02929054,  0.        ],
       [-0.0111641 ,  0.38098685,  0.92451305]]), 'currentState': array([-23.91123184,  -6.39768374,   3.8697052 ,   0.02707948,
        -0.92411638,   0.38115038]), 'targetState': array([-26.2677756 ,  -6.24531546,   3.        ]), 'previousTarget': array([-26.2677756 ,  -6.24531546,   3.        ])}
episode index:18814
target thresh 85.52422023765006
target distance 73.0
model initialize at round 18814
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.06063077,  6.28800809, 75.58623859]), 'distance': 27.5, 'localFrame': array([[ 0.58510641, -0.77833861,  0.22768289],
       [ 0.79933282,  0.60088855,  0.        ],
       [-0.13681204,  0.1819944 ,  0.97373533]]), 'currentState': array([-13.35282803,   6.83388567,  99.        ,   0.58510641,
        -0.77833861,   0.22768289]), 'targetState': array([31.5858076 ,  5.13193517, 26.        ]), 'previousTarget': array([ 1.06063077,  6.28800809, 75.58623859])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6279965472835602
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([31.5858076 ,  5.13193517, 26.        ]), 'distance': 2.6791634136234306, 'localFrame': array([[-0.51175423, -0.32672399, -0.79458105],
       [ 0.53812012, -0.84286816,  0.        ],
       [-0.66972706, -0.42758005,  0.6071581 ]]), 'currentState': array([30.24097862,  7.4189407 , 26.37276977, -0.51175423, -0.32672399,
       -0.79458105]), 'targetState': array([31.5858076 ,  5.13193517, 26.        ]), 'previousTarget': array([31.5858076 ,  5.13193517, 26.        ])}
episode index:18815
target thresh 85.52566774324981
target distance 64.0
model initialize at round 18815
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.10082812,  1.2612434 , 41.41446489]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.53488507,  0.10933221, -0.83782124],
       [-0.20026244,  0.97974229,  0.        ],
       [ 0.8208489 ,  0.16778413,  0.54594466]]), 'currentState': array([-0.9764431 , -0.21300693, 68.57054056,  0.53488507,  0.10933221,
       -0.83782124]), 'targetState': array([8.41803153, 3.18382556, 6.        ]), 'previousTarget': array([ 2.66719674,  0.62609453, 42.89168674])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6279980174511459
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([8.41803153, 3.18382556, 6.        ]), 'distance': 3.497785680506843, 'localFrame': array([[-0.4181522 , -0.22543747, -0.87995834],
       [ 0.47455439, -0.88022618,  0.        ],
       [-0.77456237, -0.41758809,  0.47505085]]), 'currentState': array([ 8.57441568,  0.8433795 ,  8.59467934, -0.4181522 , -0.22543747,
       -0.87995834]), 'targetState': array([8.41803153, 3.18382556, 6.        ]), 'previousTarget': array([8.41803153, 3.18382556, 6.        ])}
episode index:18816
target thresh 85.52711510410624
target distance 32.65340690650165
model initialize at round 18816
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.93459638, -36.75993859,  28.88325921]), 'distance': 27.5, 'localFrame': array([[-0.60920914, -0.37662479, -0.69786675],
       [ 0.5258451 , -0.85058035,  0.        ],
       [-0.59359174, -0.36696981,  0.71622762]]), 'currentState': array([-15.27610097, -24.46161742,  41.33747995,  -0.60920914,
        -0.37662479,  -0.69786675]), 'targetState': array([ 17.6574276 , -43.55703446,  22.        ]), 'previousTarget': array([  5.58477888, -36.19540836,  29.76414001])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6280047518545977
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.6574276 , -43.55703446,  22.        ]), 'distance': 3.8478162545980377, 'localFrame': array([[ 0.85712946,  0.11068916, -0.50306759],
       [-0.12807583,  0.99176438,  0.        ],
       [ 0.49892452,  0.0644308 ,  0.86424707]]), 'currentState': array([ 15.00580553, -46.08560156,  23.17513349,   0.85712946,
         0.11068916,  -0.50306759]), 'targetState': array([ 17.6574276 , -43.55703446,  22.        ]), 'previousTarget': array([ 17.6574276 , -43.55703446,  22.        ])}
episode index:18817
target thresh 85.52856232023382
target distance 39.82947339250801
model initialize at round 18817
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.0925588 ,  10.07295534,  34.91580514]), 'distance': 27.500000000000004, 'localFrame': array([[-0.49618966, -0.32588791,  0.80473156],
       [ 0.54896659, -0.83584429,  0.        ],
       [ 0.67263028,  0.44177074,  0.59363887]]), 'currentState': array([ 7.25627801, 15.29939373, 40.80419545, -0.49618966, -0.32588791,
        0.80473156]), 'targetState': array([-32.13994016,   7.48493464,  32.        ]), 'previousTarget': array([-18.92016653,  10.03285826,  34.32336527])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6280131307146759
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.13994016,   7.48493464,  32.        ]), 'distance': 3.6140046432733826, 'localFrame': array([[ 0.32097246, -0.62772716, -0.7091793 ],
       [ 0.89035757,  0.4552619 ,  0.        ],
       [ 0.32286232, -0.63142316,  0.70502817]]), 'currentState': array([-29.55716093,   6.44552823,  34.30432535,   0.32097246,
        -0.62772716,  -0.7091793 ]), 'targetState': array([-32.13994016,   7.48493464,  32.        ]), 'previousTarget': array([-32.13994016,   7.48493464,  32.        ])}
episode index:18818
target thresh 85.53000939164701
target distance 15.0
model initialize at round 18818
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.78847044,  2.24320413, 84.        ]), 'distance': 23.492106879165288, 'localFrame': array([[-0.70890445,  0.52041935, -0.47604431],
       [-0.59177478, -0.80610335,  0.        ],
       [-0.38374091,  0.28171102,  0.8794213 ]]), 'currentState': array([-0.58550693, -9.08266719, 67.55346819, -0.70890445,  0.52041935,
       -0.47604431]), 'targetState': array([11.78847044,  2.24320413, 84.        ]), 'previousTarget': array([11.78847044,  2.24320413, 84.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279797594871551
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 87, 'trapConfig': [], 'currentTarget': array([11.78847044,  2.24320413, 84.        ]), 'distance': 8.721145851395255, 'localFrame': array([[-0.0133301 ,  0.31850268,  0.94782823],
       [-0.99912534, -0.04181579,  0.        ],
       [ 0.03963419, -0.9469992 ,  0.3187815 ]]), 'currentState': array([ 9.50731353e+00,  9.55290503e-01,  7.56815875e+01, -1.33301010e-02,
        3.18502677e-01,  9.47828230e-01]), 'targetState': array([11.78847044,  2.24320413, 84.        ]), 'previousTarget': array([11.78847044,  2.24320413, 84.        ])}
episode index:18819
target thresh 85.53145631836031
target distance 34.0
model initialize at round 18819
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.09956941,   6.27851299,  51.30141543]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.3301128 ,  0.87059368, -0.364818  ],
       [-0.93503755,  0.3545487 ,  0.        ],
       [ 0.12934575,  0.34111853,  0.93107885]]), 'currentState': array([-31.53178353,   8.37320603,  70.64699156,   0.3301128 ,
         0.87059368,  -0.364818  ]), 'targetState': array([ 1.2614156 ,  4.83826732, 38.        ]), 'previousTarget': array([-12.91953854,   5.89479255,  52.41965449])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6279805369495673
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 1.2614156 ,  4.83826732, 38.        ]), 'distance': 2.06691419775597, 'localFrame': array([[-0.19716483, -0.48977225, -0.8492639 ],
       [ 0.92765415, -0.37344046,  0.        ],
       [-0.3171495 , -0.78782318,  0.52796858]]), 'currentState': array([ 0.54459191,  5.31723546, 39.87853337, -0.19716483, -0.48977225,
       -0.8492639 ]), 'targetState': array([ 1.2614156 ,  4.83826732, 38.        ]), 'previousTarget': array([ 1.2614156 ,  4.83826732, 38.        ])}
episode index:18820
target thresh 85.53290310038817
target distance 45.360625810139844
model initialize at round 18820
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.39192133,  1.15574858, 53.54226139]), 'distance': 27.499999999999993, 'localFrame': array([[ 0.6599734 ,  0.14088687, -0.7379607 ],
       [-0.20876964,  0.97796485,  0.        ],
       [ 0.72169962,  0.15406379,  0.67484369]]), 'currentState': array([-2.02271013,  3.10146845, 72.92101933,  0.6599734 ,  0.14088687,
       -0.7379607 ]), 'targetState': array([41.9796148, -1.3084117, 29.       ]), 'previousTarget': array([16.08256382,  1.54159029, 54.69116438])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279471709999924
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 66, 'trapConfig': [], 'currentTarget': array([41.9796148, -1.3084117, 29.       ]), 'distance': 21.759337461469592, 'localFrame': array([[ 0.45861613,  0.67674489,  0.57592325],
       [-0.82781869,  0.56099574,  0.        ],
       [-0.32309049, -0.47676003,  0.81750377]]), 'currentState': array([21.44249781, -8.01994304, 31.57894158,  0.45861613,  0.67674489,
        0.57592325]), 'targetState': array([41.9796148, -1.3084117, 29.       ]), 'previousTarget': array([41.9796148, -1.3084117, 29.       ])}
episode index:18821
target thresh 85.53434973774505
target distance 23.0
model initialize at round 18821
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.33996629, 12.93075753, 71.        ]), 'distance': 25.733829988822755, 'localFrame': array([[ 0.13875318,  0.96879063,  0.20540755],
       [-0.98989872,  0.14177634,  0.        ],
       [-0.02912193, -0.20333267,  0.97867652]]), 'currentState': array([-3.31625459e-02,  2.35815813e+00,  4.75747484e+01,  1.38753177e-01,
        9.68790634e-01,  2.05407552e-01]), 'targetState': array([-1.33996629, 12.93075753, 71.        ]), 'previousTarget': array([-1.33996629, 12.93075753, 71.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6279604306882295
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.33996629, 12.93075753, 71.        ]), 'distance': 2.763017635357184, 'localFrame': array([[-0.41726065, -0.52432546,  0.74227782],
       [ 0.78246757, -0.62269134,  0.        ],
       [ 0.46220997,  0.58080832,  0.67009226]]), 'currentState': array([-1.09959114, 12.32170545, 68.31568597, -0.41726065, -0.52432546,
        0.74227782]), 'targetState': array([-1.33996629, 12.93075753, 71.        ]), 'previousTarget': array([-1.33996629, 12.93075753, 71.        ])}
episode index:18822
target thresh 85.53579623044544
target distance 27.499955808572135
model initialize at round 18822
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.30859026,  -8.50982902,  36.04706821]), 'distance': 27.499999999999996, 'localFrame': array([[-0.58136002, -0.69265175,  0.42691226],
       [ 0.76595938, -0.64288897,  0.        ],
       [ 0.27445718,  0.32699745,  0.90429305]]), 'currentState': array([-1.53621672,  1.2598997 , 47.97272977, -0.58136002, -0.69265175,
        0.42691226]), 'targetState': array([-28.21752242, -10.18682623,  34.        ]), 'previousTarget': array([-23.27607295,  -7.83294134,  36.51565104])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6279709607946496
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.21752242, -10.18682623,  34.        ]), 'distance': 3.3680769792345213, 'localFrame': array([[ 0.48144744, -0.46628982, -0.742147  ],
       [ 0.6957087 ,  0.71832402,  0.        ],
       [ 0.53310202, -0.51631813,  0.67023715]]), 'currentState': array([-25.55742604,  -8.58054007,  35.2991053 ,   0.48144744,
        -0.46628982,  -0.742147  ]), 'targetState': array([-28.21752242, -10.18682623,  34.        ]), 'previousTarget': array([-28.21752242, -10.18682623,  34.        ])}
episode index:18823
target thresh 85.5372425785038
target distance 38.70527784654641
model initialize at round 18823
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.60871364, -6.66235975, 43.55781137]), 'distance': 27.499999999999996, 'localFrame': array([[-0.6897518 ,  0.68359821,  0.23861252],
       [-0.70393137, -0.71026799,  0.        ],
       [ 0.16947883, -0.16796684,  0.97111486]]), 'currentState': array([ 17.79024668, -11.56051907,  37.61197846,  -0.6897518 ,
         0.68359821,   0.23861252]), 'targetState': array([-19.45181029,  -4.65049208,  46.        ]), 'previousTarget': array([-7.15088051, -7.1711711 , 43.45751888])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6279797603761865
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.45181029,  -4.65049208,  46.        ]), 'distance': 3.4021742393192964, 'localFrame': array([[-0.94693851,  0.02283873, -0.32060233],
       [-0.02411148, -0.99970928,  0.        ],
       [-0.32050913,  0.0077302 ,  0.94721388]]), 'currentState': array([-1.72650564e+01, -7.07910703e+00,  4.50541001e+01, -9.46938507e-01,
        2.28387299e-02, -3.20602334e-01]), 'targetState': array([-19.45181029,  -4.65049208,  46.        ]), 'previousTarget': array([-19.45181029,  -4.65049208,  46.        ])}
episode index:18824
target thresh 85.53868878193455
target distance 27.544100723426283
model initialize at round 18824
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.00804706, -4.70202035, 10.56234388]), 'distance': 27.5, 'localFrame': array([[-0.31159401, -0.87323913, -0.37464997],
       [ 0.94183654, -0.33607132,  0.        ],
       [-0.12590911, -0.35285903,  0.92716633]]), 'currentState': array([14.17246434, 15.71144677, 27.08150398, -0.31159401, -0.87323913,
       -0.37464997]), 'targetState': array([  3.75315765, -10.33991333,   6.        ]), 'previousTarget': array([ 6.59148639, -3.54863178, 11.17776616])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6279885590228409
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.75315765, -10.33991333,   6.        ]), 'distance': 3.099316097413461, 'localFrame': array([[-0.53723545, -0.83882433,  0.08804436],
       [ 0.84209456, -0.53932991,  0.        ],
       [ 0.04748496,  0.07414167,  0.99611655]]), 'currentState': array([  5.64184909, -10.00729757,   8.43474263,  -0.53723545,
        -0.83882433,   0.08804436]), 'targetState': array([  3.75315765, -10.33991333,   6.        ]), 'previousTarget': array([  3.75315765, -10.33991333,   6.        ])}
episode index:18825
target thresh 85.54013484075223
target distance 47.500059106335065
model initialize at round 18825
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.63221292,  10.13252142,  19.82291494]), 'distance': 27.5, 'localFrame': array([[-0.1867297 ,  0.80009288, -0.57007315],
       [-0.97383007, -0.22727735,  0.        ],
       [-0.12956472,  0.55515438,  0.82159394]]), 'currentState': array([ 8.07743708, -0.75614486, 28.51425795, -0.1867297 ,  0.80009288,
       -0.57007315]), 'targetState': array([-39.70078609,  21.18602331,  11.        ]), 'previousTarget': array([-15.49424248,   9.48186845,  20.6826054 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279552015088165
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 58, 'trapConfig': [], 'currentTarget': array([-35.3544947 ,  15.81553925,  15.8247272 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.7282087 ,  0.31059238,  0.61093737],
       [-0.39232114, -0.91982831,  0.        ],
       [ 0.56195749, -0.23968364,  0.79167893]]), 'currentState': array([-21.17075555,  -1.7105625 ,  31.56979949,  -0.7282087 ,
         0.31059238,   0.61093737]), 'targetState': array([-39.70078609,  21.18602331,  11.        ]), 'previousTarget': array([-35.23264693,  16.04259649,  15.35260326])}
episode index:18826
target thresh 85.54158075497122
target distance 21.36778450661499
model initialize at round 18826
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([32.65377331,  4.76771311, 19.        ]), 'distance': 23.72595712852006, 'localFrame': array([[ 0.83781309,  0.54538129, -0.02506932],
       [-0.54555275,  0.83807649,  0.        ],
       [ 0.02101001,  0.01367664,  0.99968572]]), 'currentState': array([12.60018034, -7.31056738, 22.85870323,  0.83781309,  0.54538129,
       -0.02506932]), 'targetState': array([32.65377331,  4.76771311, 19.        ]), 'previousTarget': array([32.65377331,  4.76771311, 19.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279218475383747
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 81, 'trapConfig': [], 'currentTarget': array([32.65377331,  4.76771311, 19.        ]), 'distance': 16.20688846579736, 'localFrame': array([[ 0.60847969, -0.55774208, -0.56451416],
       [ 0.67570424,  0.73717283,  0.        ],
       [ 0.4161445 , -0.38144461,  0.82542338]]), 'currentState': array([17.75638098,  5.18232083, 12.63140236,  0.60847969, -0.55774208,
       -0.56451416]), 'targetState': array([32.65377331,  4.76771311, 19.        ]), 'previousTarget': array([32.65377331,  4.76771311, 19.        ])}
episode index:18827
target thresh 85.54302652460605
target distance 19.0
model initialize at round 18827
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.61884943,  9.32222767, 39.        ]), 'distance': 22.55103466522491, 'localFrame': array([[ 0.97608284, -0.21708401, -0.01169693],
       [ 0.21709886,  0.97614962,  0.        ],
       [ 0.01141796, -0.00253939,  0.99993159]]), 'currentState': array([-1.68741463e+00,  1.87456907e+01,  5.87886601e+01,  9.76082843e-01,
       -2.17084005e-01, -1.16969322e-02]), 'targetState': array([ 3.61884943,  9.32222767, 39.        ]), 'previousTarget': array([ 3.61884943,  9.32222767, 39.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6279346382737279
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 3.61884943,  9.32222767, 39.        ]), 'distance': 2.3021864859464025, 'localFrame': array([[ 0.12790918, -0.68823116, -0.71412682],
       [ 0.98316441,  0.18272313,  0.        ],
       [ 0.13048748, -0.70210407,  0.70001635]]), 'currentState': array([ 3.71322085, 10.14422388, 41.14836656,  0.12790918, -0.68823116,
       -0.71412682]), 'targetState': array([ 3.61884943,  9.32222767, 39.        ]), 'previousTarget': array([ 3.61884943,  9.32222767, 39.        ])}
episode index:18828
target thresh 85.54447214967114
target distance 51.0
model initialize at round 18828
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.10871635, 26.44965273, 52.64332125]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.56733645, -0.41840778, -0.70927025],
       [ 0.59353953,  0.80480484,  0.        ],
       [ 0.57082413, -0.42097993,  0.70493668]]), 'currentState': array([-13.79301352,  16.7212532 ,  72.86072043,   0.56733645,
        -0.41840778,  -0.70927025]), 'targetState': array([25.42428147, 40.71370668, 23.        ]), 'previousTarget': array([ 1.71720855, 27.17883076, 53.50688284])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.627936817737337
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.42428147, 40.71370668, 23.        ]), 'distance': 2.0855044712829423, 'localFrame': array([[ 0.35417021, -0.90171846, -0.24792597],
       [ 0.93077831,  0.36558411,  0.        ],
       [ 0.0906378 , -0.23076411,  0.96877898]]), 'currentState': array([23.71994325, 39.56508811, 23.35388627,  0.35417021, -0.90171846,
       -0.24792597]), 'targetState': array([25.42428147, 40.71370668, 23.        ]), 'previousTarget': array([25.42428147, 40.71370668, 23.        ])}
episode index:18829
target thresh 85.54591763018094
target distance 46.0
model initialize at round 18829
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.19833845, -2.44847841, 65.75830196]), 'distance': 27.5, 'localFrame': array([[ 0.47499883,  0.57769902,  0.66380717],
       [-0.77242427,  0.63510688,  0.        ],
       [-0.4215885 , -0.51274076,  0.74790377]]), 'currentState': array([-17.05350093,   6.80688145,  46.44606875,   0.47499883,
         0.57769902,   0.66380717]), 'targetState': array([ 22.74703676, -14.54552573,  91.        ]), 'previousTarget': array([ 3.06060800e-03, -2.98749628e+00,  6.47284356e+01])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6279389969694577
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.74703676, -14.54552573,  91.        ]), 'distance': 2.675978778612311, 'localFrame': array([[ 0.76182178, -0.26608392,  0.59061572],
       [ 0.32973907,  0.94407211,  0.        ],
       [-0.55758382,  0.19474908,  0.80695296]]), 'currentState': array([ 20.6823791 , -12.91537656,  91.49057605,   0.76182178,
        -0.26608392,   0.59061572]), 'targetState': array([ 22.74703676, -14.54552573,  91.        ]), 'previousTarget': array([ 22.74703676, -14.54552573,  91.        ])}
episode index:18830
target thresh 85.5473629661499
target distance 57.0
model initialize at round 18830
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.38220651, 17.39393314, 60.25271824]), 'distance': 27.5, 'localFrame': array([[-0.5076035 , -0.04191329, -0.86057072],
       [ 0.08229088, -0.99660835,  0.        ],
       [-0.85765197, -0.07081712,  0.50933097]]), 'currentState': array([12.86858076, 25.22556642, 34.27603704, -0.5076035 , -0.04191329,
       -0.86057072]), 'targetState': array([ 2.72649704,  7.52105138, 93.        ]), 'previousTarget': array([ 8.54939524, 16.81962473, 61.93074375])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6279387625898667
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.72649704,  7.52105138, 93.        ]), 'distance': 3.1341519835458076, 'localFrame': array([[-0.32481608,  0.8599191 ,  0.39374314],
       [-0.93548729, -0.35336035,  0.        ],
       [ 0.13913321, -0.3683417 ,  0.91922051]]), 'currentState': array([ 2.88626578,  8.44726526, 90.01009859, -0.32481608,  0.8599191 ,
        0.39374314]), 'targetState': array([ 2.72649704,  7.52105138, 93.        ]), 'previousTarget': array([ 2.72649704,  7.52105138, 93.        ])}
episode index:18831
target thresh 85.54880815759252
target distance 5.832942626291452
model initialize at round 18831
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.42981894,  3.17741718,  9.        ]), 'distance': 7.178688389501994, 'localFrame': array([[ 0.75334177,  0.07589705,  0.65323489],
       [-0.10023973,  0.99496331,  0.        ],
       [-0.64994475, -0.06548009,  0.75715532]]), 'currentState': array([-5.32390656, -3.38837224,  9.21962041,  0.75334177,  0.07589705,
        0.65323489]), 'targetState': array([-2.42981894,  3.17741718,  9.        ]), 'previousTarget': array([-2.42981894,  3.17741718,  9.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.627955916970034
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-2.42981894,  3.17741718,  9.        ]), 'distance': 3.2924808249286315, 'localFrame': array([[ 0.08404208,  0.85513694, -0.51154446],
       [-0.99520532,  0.09780788,  0.        ],
       [ 0.05003308,  0.50909177,  0.85925681]]), 'currentState': array([-3.2208129 ,  1.07452493,  6.59321735,  0.08404208,  0.85513694,
       -0.51154446]), 'targetState': array([-2.42981894,  3.17741718,  9.        ]), 'previousTarget': array([-2.42981894,  3.17741718,  9.        ])}
episode index:18832
target thresh 85.5502532045232
target distance 39.0
model initialize at round 18832
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.06347812, -31.66748967,  65.11465865]), 'distance': 27.5, 'localFrame': array([[ 0.20362109, -0.66852199,  0.71527393],
       [ 0.95661093,  0.29136836,  0.        ],
       [-0.2084082 ,  0.68423887,  0.69884419]]), 'currentState': array([  4.85354125, -15.7791266 ,  42.84302946,   0.20362109,
        -0.66852199,   0.71527393]), 'targetState': array([ 7.34528343e-02, -4.29999373e+01,  8.10000000e+01]), 'previousTarget': array([  1.64321269, -30.70159436,  64.15008395])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6279606838481536
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 7.34528343e-02, -4.29999373e+01,  8.10000000e+01]), 'distance': 4.14389459017526, 'localFrame': array([[-0.34141773, -0.09361967,  0.93523756],
       [ 0.26444682, -0.96440027,  0.        ],
       [ 0.90194335,  0.2473206 ,  0.35402077]]), 'currentState': array([  1.99432422, -40.73621035,  78.10903893,  -0.34141773,
        -0.09361967,   0.93523756]), 'targetState': array([ 7.34528343e-02, -4.29999373e+01,  8.10000000e+01]), 'previousTarget': array([ 7.34528343e-02, -4.29999373e+01,  8.10000000e+01])}
episode index:18833
target thresh 85.55169810695644
target distance 44.52869862932287
model initialize at round 18833
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.26282413, 21.36757404, 82.51176486]), 'distance': 27.500000000000004, 'localFrame': array([[-0.28357009, -0.87164874, -0.39977053],
       [ 0.95094287, -0.30936654,  0.        ],
       [-0.12367563, -0.38015894,  0.91661525]]), 'currentState': array([-17.57607225,  42.81545296,  97.58254438,  -0.28357009,
        -0.87164874,  -0.39977053]), 'targetState': array([-0.70632247, -0.70789022, 67.        ]), 'previousTarget': array([-9.1627135 , 22.4134231 , 83.61584661])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6279621545148588
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-0.70632247, -0.70789022, 67.        ]), 'distance': 2.7776111091029008, 'localFrame': array([[-0.71214999, -0.49799221, -0.49481931],
       [ 0.57306624, -0.81950905,  0.        ],
       [-0.4055089 , -0.28356424,  0.86899589]]), 'currentState': array([ 1.4634418 ,  0.89471128, 66.33749346, -0.71214999, -0.49799221,
       -0.49481931]), 'targetState': array([-0.70632247, -0.70789022, 67.        ]), 'previousTarget': array([-0.70632247, -0.70789022, 67.        ])}
episode index:18834
target thresh 85.55314286490663
target distance 32.581639576876924
model initialize at round 18834
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.10198706,  7.93574418, 20.13386931]), 'distance': 27.499999999999996, 'localFrame': array([[-0.53804055, -0.37709533, -0.75386436],
       [ 0.57393922, -0.8188979 ,  0.        ],
       [-0.61733794, -0.43267233,  0.65703008]]), 'currentState': array([-4.25832464, 34.04845056, 28.48412522, -0.53804055, -0.37709533,
       -0.75386436]), 'targetState': array([-1.55094496,  1.26276274, 18.        ]), 'previousTarget': array([-1.90571717,  8.06980656, 20.50707229])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6279639766484649
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-1.55094496,  1.26276274, 18.        ]), 'distance': 3.602008528137379, 'localFrame': array([[-0.89807546, -0.05554774, -0.43631975],
       [ 0.061734  , -0.99809264,  0.        ],
       [-0.43548753, -0.02693576,  0.89979169]]), 'currentState': array([ 0.7239668 ,  2.23806514, 20.61687354, -0.89807546, -0.05554774,
       -0.43631975]), 'targetState': array([-1.55094496,  1.26276274, 18.        ]), 'previousTarget': array([-1.55094496,  1.26276274, 18.        ])}
episode index:18835
target thresh 85.55458747838827
target distance 37.04626205679796
model initialize at round 18835
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.3184545 , -15.87867585,  37.40230762]), 'distance': 27.499999999999996, 'localFrame': array([[-0.46981318, -0.86120684, -0.19390296],
       [ 0.87786815, -0.47890241,  0.        ],
       [-0.09286059, -0.17022123,  0.98102071]]), 'currentState': array([25.09389249,  8.77040845, 40.56643521, -0.46981318, -0.86120684,
       -0.19390296]), 'targetState': array([  8.09970592, -26.80288723,  36.        ]), 'previousTarget': array([ 14.11908523, -14.38619225,  37.34066913])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6279727709947697
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.09970592, -26.80288723,  36.        ]), 'distance': 2.2453862356637653, 'localFrame': array([[-0.84525179, -0.48378813,  0.22693271],
       [ 0.49674807, -0.86789478,  0.        ],
       [ 0.19695371,  0.11272838,  0.97391044]]), 'currentState': array([  7.16373936, -24.79409058,  35.63880482,  -0.84525179,
        -0.48378813,   0.22693271]), 'targetState': array([  8.09970592, -26.80288723,  36.        ]), 'previousTarget': array([  8.09970592, -26.80288723,  36.        ])}
episode index:18836
target thresh 85.55603194741576
target distance 61.0
model initialize at round 18836
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.5049897 , -9.47645885, 57.46171268]), 'distance': 27.5, 'localFrame': array([[-0.51552189,  0.52622922,  0.67625438],
       [-0.71433684, -0.69980203,  0.        ],
       [ 0.47324418, -0.48307341,  0.73666819]]), 'currentState': array([ 19.98361424, -17.69775865,  32.03165854,  -0.51552189,
         0.52622922,   0.67625438]), 'targetState': array([ 4.70592877,  1.68944796, 92.        ]), 'previousTarget': array([14.5144715 , -9.80672935, 56.32215776])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279394337982418
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 43, 'trapConfig': [], 'currentTarget': array([ 4.70592877,  1.68944796, 92.        ]), 'distance': 17.24982387007813, 'localFrame': array([[-0.5421803 ,  0.13848773,  0.82877118],
       [-0.24748176, -0.96889255,  0.        ],
       [ 0.80299022, -0.20510575,  0.55958764]]), 'currentState': array([ 8.58024856, -0.50063524, 75.33417853, -0.5421803 ,  0.13848773,
        0.82877118]), 'targetState': array([ 4.70592877,  1.68944796, 92.        ]), 'previousTarget': array([ 4.70592877,  1.68944796, 92.        ])}
episode index:18837
target thresh 85.5574762720036
target distance 52.0
model initialize at round 18837
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-0.57483492, 13.33555976, 70.76362974]), 'distance': 27.5, 'localFrame': array([[ 0.39986413, -0.60025272, -0.69267983],
       [ 0.832245  ,  0.55440803,  0.        ],
       [ 0.38402726, -0.57647933,  0.72124521]]), 'currentState': array([ 9.04115105, 13.20066643, 45.        ,  0.39986413, -0.60025272,
       -0.69267983]), 'targetState': array([-10.36726599,  13.47292826,  97.        ]), 'previousTarget': array([-0.57483492, 13.33555976, 70.76362974])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6279385408056595
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-10.36726599,  13.47292826,  97.        ]), 'distance': 3.8137986972047395, 'localFrame': array([[-0.3096469 , -0.7470141 ,  0.58829307],
       [ 0.92378177, -0.38291937,  0.        ],
       [ 0.22526881,  0.54345441,  0.8086478 ]]), 'currentState': array([-8.18604044, 15.21051191, 94.39843918, -0.3096469 , -0.7470141 ,
        0.58829307]), 'targetState': array([-10.36726599,  13.47292826,  97.        ]), 'previousTarget': array([-10.36726599,  13.47292826,  97.        ])}
episode index:18838
target thresh 85.55892045216619
target distance 52.0
model initialize at round 18838
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.01045197, 16.08011985, 40.65446906]), 'distance': 27.500000000000004, 'localFrame': array([[-0.20112287,  0.05948942, -0.97775794],
       [-0.28363883, -0.95893118,  0.        ],
       [-0.93760258,  0.27733012,  0.2097365 ]]), 'currentState': array([-4.89431029e+00,  1.63700825e+01,  6.75114924e+01, -2.01122874e-01,
        5.94894172e-02, -9.77757945e-01]), 'targetState': array([ 6.21110337, 15.82473364, 17.        ]), 'previousTarget': array([ 1.10496025, 15.61381375, 42.05481337])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6279400122574228
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 6.21110337, 15.82473364, 17.        ]), 'distance': 3.0434750432545794, 'localFrame': array([[-0.6631962 ,  0.46204274, -0.58880159],
       [-0.57163868, -0.82050547,  0.        ],
       [-0.48311492,  0.33658176,  0.80827761]]), 'currentState': array([ 6.73298339, 17.48212059, 19.49868967, -0.6631962 ,  0.46204274,
       -0.58880159]), 'targetState': array([ 6.21110337, 15.82473364, 17.        ]), 'previousTarget': array([ 6.21110337, 15.82473364, 17.        ])}
episode index:18839
target thresh 85.56036448791798
target distance 36.39361913874295
model initialize at round 18839
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.38615011, 17.25244373, 47.65843198]), 'distance': 27.5, 'localFrame': array([[-0.64308737, -0.50103935,  0.57913574],
       [ 0.61459791, -0.78884054,  0.        ],
       [ 0.45684575,  0.35593562,  0.81523113]]), 'currentState': array([15.71568223,  3.02942554, 62.70101884, -0.64308737, -0.50103935,
        0.57913574]), 'targetState': array([-20.02570112,  31.11223705,  33.        ]), 'previousTarget': array([-2.31774466, 18.20860308, 47.11046084])}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6279311631042997
{'scaleFactor': 20, 'timeStep': 78, 'trapCount': 34, 'trapConfig': [], 'currentTarget': array([-20.02570112,  31.11223705,  33.        ]), 'distance': 1.864414301215679, 'localFrame': array([[-0.42797613,  0.10073621, -0.89815848],
       [-0.2291168 , -0.97339894,  0.        ],
       [-0.87426651,  0.2057832 ,  0.43967187]]), 'currentState': array([-20.61611664,  31.82459527,  34.61864016,  -0.42797613,
         0.10073621,  -0.89815848]), 'targetState': array([-20.02570112,  31.11223705,  33.        ]), 'previousTarget': array([-20.02570112,  31.11223705,  33.        ])}
episode index:18840
target thresh 85.5618083792734
target distance 49.0
model initialize at round 18840
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.15012952,  1.2670466 , 60.39631625]), 'distance': 27.499999999999996, 'localFrame': array([[-0.45698103, -0.04932514, -0.88810775],
       [ 0.10731363, -0.99422522,  0.        ],
       [-0.88297912, -0.09530607,  0.45963532]]), 'currentState': array([-5.33263747e-02, -5.80456546e-01,  8.62445876e+01, -4.56981025e-01,
       -4.93251355e-02, -8.88107749e-01]), 'targetState': array([16.76843599,  2.7963466 , 39.        ]), 'previousTarget': array([ 8.89096931,  1.48268043, 62.01919317])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279299463378878
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([16.76843599,  2.7963466 , 39.        ]), 'distance': 2.701282847235007, 'localFrame': array([[-0.77856769, -0.03814796, -0.6264001 ],
       [ 0.04893891, -0.99880177,  0.        ],
       [-0.62564953, -0.03065534,  0.77950171]]), 'currentState': array([ 1.49201486e+01,  2.90450275e+00,  4.09669939e+01, -7.78567690e-01,
       -3.81479605e-02, -6.26400100e-01]), 'targetState': array([16.76843599,  2.7963466 , 39.        ]), 'previousTarget': array([16.76843599,  2.7963466 , 39.        ])}
episode index:18841
target thresh 85.56325212624694
target distance 51.0
model initialize at round 18841
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.49329673, 30.68399347, 57.31305295]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.6654581 ,  0.66696062,  0.33515527],
       [-0.70790371,  0.70630896,  0.        ],
       [-0.23672317, -0.23725766,  0.94216291]]), 'currentState': array([ 5.42800774, 17.35962857, 33.34545229,  0.6654581 ,  0.66696062,
        0.33515527]), 'targetState': array([ 9.70674185, 44.96419868, 83.        ]), 'previousTarget': array([ 7.05604739, 29.74556193, 55.89267285])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279287297006305
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([ 9.70674185, 44.96419868, 83.        ]), 'distance': 3.2001842384638386, 'localFrame': array([[-0.3156316 ,  0.67678888,  0.66508158],
       [-0.90628739, -0.422662  ,  0.        ],
       [ 0.28110471, -0.60275505,  0.74677071]]), 'currentState': array([10.59947019, 43.36901235, 80.37329182, -0.3156316 ,  0.67678888,
        0.66508158]), 'targetState': array([ 9.70674185, 44.96419868, 83.        ]), 'previousTarget': array([ 9.70674185, 44.96419868, 83.        ])}
episode index:18842
target thresh 85.56469572885298
target distance 21.0
model initialize at round 18842
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.49523937, -17.85823618,  86.        ]), 'distance': 27.416000005463786, 'localFrame': array([[ 0.72533758, -0.27743005,  0.63001425],
       [ 0.35724429,  0.93401098,  0.        ],
       [-0.58844023,  0.225069  ,  0.77658357]]), 'currentState': array([ 2.70826616, -5.72246846, 66.36061046,  0.72533758, -0.27743005,
        0.63001425]), 'targetState': array([ 17.49523937, -17.85823618,  86.        ]), 'previousTarget': array([ 16.52288395, -17.03549943,  84.69318414])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6279392503127482
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.49523937, -17.85823618,  86.        ]), 'distance': 2.652496210424729, 'localFrame': array([[-0.69893989,  0.18155792,  0.69175122],
       [-0.25141795, -0.96787862,  0.        ],
       [ 0.66953121, -0.17391867,  0.7221359 ]]), 'currentState': array([ 16.9198734 , -15.73532733,  84.51745214,  -0.69893989,
         0.18155792,   0.69175122]), 'targetState': array([ 17.49523937, -17.85823618,  86.        ]), 'previousTarget': array([ 17.49523937, -17.85823618,  86.        ])}
episode index:18843
target thresh 85.56613918710599
target distance 62.0
model initialize at round 18843
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.54523635, -16.48061455,  38.84505278]), 'distance': 27.5, 'localFrame': array([[ 0.35739049,  0.31942425,  0.87763329],
       [-0.66639365,  0.7456001 ,  0.        ],
       [-0.65436347, -0.58484925,  0.47933267]]), 'currentState': array([-34.04736762, -22.65681125,  63.90127148,   0.35739049,
         0.31942425,   0.87763329]), 'targetState': array([-9.8139317 , -6.90555897,  0.        ]), 'previousTarget': array([-24.40869441, -16.29381322,  37.09136724])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279380333108816
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-9.8139317 , -6.90555897,  0.        ]), 'distance': 3.2970426693809425, 'localFrame': array([[ 0.40121494, -0.08498374, -0.91203308],
       [ 0.20721845,  0.9782947 ,  0.        ],
       [ 0.89223712, -0.18899008,  0.41011665]]), 'currentState': array([-12.16958474,  -8.06508394,   1.99421437,   0.40121494,
        -0.08498374,  -0.91203308]), 'targetState': array([-9.8139317 , -6.90555897,  0.        ]), 'previousTarget': array([-9.81393170e+00, -6.90555897e+00,  4.44089210e-16])}
episode index:18844
target thresh 85.56758250102037
target distance 41.0
model initialize at round 18844
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.25760729, -20.32367013,  20.45964105]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.71032713,  0.13675429, -0.690459  ],
       [-0.18905125,  0.98196722,  0.        ],
       [ 0.6780081 ,  0.13053214,  0.72337153]]), 'currentState': array([-1.72452821, -5.90708527, 41.64381089,  0.71032713,  0.13675429,
       -0.690459  ]), 'targetState': array([ 16.95592492, -32.88611576,   2.        ]), 'previousTarget': array([  7.413302  , -19.56161399,  21.81094173])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6279374682860809
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ 16.95592492, -32.88611576,   2.        ]), 'distance': 2.3240834688282197, 'localFrame': array([[-0.18918581, -0.85212456, -0.4879472 ],
       [ 0.97622955, -0.21673918,  0.        ],
       [-0.10575728, -0.47634848,  0.87287315]]), 'currentState': array([ 18.27197901, -31.04702576,   2.5358298 ,  -0.18918581,
        -0.85212456,  -0.4879472 ]), 'targetState': array([ 16.95592492, -32.88611576,   2.        ]), 'previousTarget': array([ 16.95592492, -32.88611576,   2.        ])}
episode index:18845
target thresh 85.56902567061059
target distance 41.95626181179948
model initialize at round 18845
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.9489086 , -38.66819139,  69.96935251]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.54956762, -0.3077802 , -0.77668963],
       [ 0.48863035,  0.8724909 ,  0.        ],
       [ 0.67765464, -0.37951413,  0.6298835 ]]), 'currentState': array([ -4.87096484, -47.30027471,  82.65755624,   0.54956762,
        -0.3077802 ,  -0.77668963]), 'targetState': array([ 35.87889832, -31.88580649,  60.        ]), 'previousTarget': array([ 16.62174834, -39.10462505,  71.01555715])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.627944195539552
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 35.87889832, -31.88580649,  60.        ]), 'distance': 3.562145839263411, 'localFrame': array([[ 0.2110876 ,  0.47149566, -0.85623237],
       [-0.91270613,  0.40861659,  0.        ],
       [ 0.34987075,  0.78148853,  0.51659088]]), 'currentState': array([ 33.90051157, -33.60375959,  62.41319415,   0.2110876 ,
         0.47149566,  -0.85623237]), 'targetState': array([ 35.87889832, -31.88580649,  60.        ]), 'previousTarget': array([ 35.87889832, -31.88580649,  60.        ])}
episode index:18846
target thresh 85.57046869589105
target distance 24.98681422712626
model initialize at round 18846
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.15742692, 16.11510935, 46.67856774]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.0313416 , -0.38710344,  0.92150346],
       [ 0.99673841,  0.08070033,  0.        ],
       [-0.07436563,  0.9184979 ,  0.38837014]]), 'currentState': array([ 5.03094398e+00,  2.18977238e+01,  3.14066941e+01,  3.13415978e-02,
       -3.87103439e-01,  9.21503463e-01]), 'targetState': array([29.07196924, 15.61475599, 48.        ]), 'previousTarget': array([25.84030697, 16.52262668, 45.67197529])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6279525647200723
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([29.07196924, 15.61475599, 48.        ]), 'distance': 1.6076220129847505, 'localFrame': array([[ 0.94178654, -0.12941751, -0.31030504],
       [ 0.13613766,  0.99068993,  0.        ],
       [ 0.30741608, -0.0422442 ,  0.95063704]]), 'currentState': array([27.59563759, 16.24175753, 47.89154499,  0.94178654, -0.12941751,
       -0.31030504]), 'targetState': array([29.07196924, 15.61475599, 48.        ]), 'previousTarget': array([29.07196924, 15.61475599, 48.        ])}
episode index:18847
target thresh 85.57191157687622
target distance 31.0
model initialize at round 18847
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.44487265, 15.48032942, 90.71125162]), 'distance': 27.500000000000004, 'localFrame': array([[-0.45711738,  0.78434438,  0.41934185],
       [-0.86397868, -0.50352839,  0.        ],
       [ 0.21115053, -0.36230242,  0.9078284 ]]), 'currentState': array([ 4.96088059, 11.32542581, 66.03544223, -0.45711738,  0.78434438,
        0.41934185]), 'targetState': array([-9.35168358, 16.53922653, 97.        ]), 'previousTarget': array([-6.1668763 , 15.036742  , 90.32378378])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6279630812766891
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-9.35168358, 16.53922653, 97.        ]), 'distance': 3.3307595575686526, 'localFrame': array([[-0.27372396,  0.36893112,  0.88823703],
       [-0.80309711, -0.59584816,  0.        ],
       [ 0.5292544 , -0.7133406 ,  0.45938543]]), 'currentState': array([-7.8701689 , 16.66795036, 94.01964705, -0.27372396,  0.36893112,
        0.88823703]), 'targetState': array([-9.35168358, 16.53922653, 97.        ]), 'previousTarget': array([-9.35168358, 16.53922653, 97.        ])}
episode index:18848
target thresh 85.57335431358048
target distance 50.48963908663296
model initialize at round 18848
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.36314845, -11.82743505,  11.85574112]), 'distance': 27.499999999999996, 'localFrame': array([[-0.50300107, -0.62438745,  0.59760374],
       [ 0.77874033, -0.6273464 ,  0.        ],
       [ 0.37490455,  0.46537813,  0.8017916 ]]), 'currentState': array([  9.68138969, -19.51830031,   3.49681172,  -0.50300107,
        -0.62438745,   0.59760374]), 'targetState': array([-39.76450328,  -4.33408337,  20.        ]), 'previousTarget': array([-14.23476296, -11.85529583,  10.89842324])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279297658179763
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 80, 'trapConfig': [], 'currentTarget': array([-18.6251962 , -10.43324768,  12.16544224]), 'distance': 27.5, 'localFrame': array([[-0.83938934,  0.26389266,  0.47516965],
       [-0.29991408, -0.95396622,  0.        ],
       [ 0.4532958 , -0.14251007,  0.8798942 ]]), 'currentState': array([  6.26600225, -17.61491683,   2.94037487,  -0.83938934,
         0.26389266,   0.47516965]), 'targetState': array([-39.76450328,  -4.33408337,  20.        ]), 'previousTarget': array([-18.6251962 , -10.43324768,  12.16544224])}
episode index:18849
target thresh 85.5747969060183
target distance 33.0
model initialize at round 18849
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.73457343, 29.82080022, 61.69431806]), 'distance': 27.499999999999996, 'localFrame': array([[-0.19900562,  0.57512064,  0.79349418],
       [-0.94502396, -0.3270011 ,  0.        ],
       [ 0.25947347, -0.74987101,  0.60857784]]), 'currentState': array([17.92947505, 22.09142705, 36.81506901, -0.19900562,  0.57512064,
        0.79349418]), 'targetState': array([28.96623845, 31.77982112, 68.        ]), 'previousTarget': array([26.24174555, 29.33098   , 60.00212714])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6279407251811399
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.96623845, 31.77982112, 68.        ]), 'distance': 3.482744834599622, 'localFrame': array([[-0.22568119,  0.81621165,  0.531852  ],
       [-0.96383523, -0.26649887,  0.        ],
       [ 0.14173796, -0.51261769,  0.84683732]]), 'currentState': array([27.65996227, 29.99827633, 65.30755645, -0.22568119,  0.81621165,
        0.531852  ]), 'targetState': array([28.96623845, 31.77982112, 68.        ]), 'previousTarget': array([28.96623845, 31.77982112, 68.        ])}
episode index:18850
target thresh 85.57623935420409
target distance 56.0
model initialize at round 18850
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.14659305,  5.70893348, 39.70208558]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.14556262,  0.68734684,  0.71159387],
       [-0.9783029 ,  0.20717973,  0.        ],
       [-0.14742783, -0.69615435,  0.70259103]]), 'currentState': array([ 6.87916172, -0.51847643, 13.25859118,  0.14556262,  0.68734684,
        0.71159387]), 'targetState': array([15.7132893 , 12.37305699, 68.        ]), 'previousTarget': array([10.48546111,  5.10359367, 38.32741895])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6279415034357101
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([15.7132893 , 12.37305699, 68.        ]), 'distance': 2.738821054624824, 'localFrame': array([[-0.71227007,  0.48288165,  0.50940814],
       [-0.56114771, -0.82771568,  0.        ],
       [ 0.42164511, -0.28585321,  0.86052504]]), 'currentState': array([16.00338346, 11.60905331, 65.38594482, -0.71227007,  0.48288165,
        0.50940814]), 'targetState': array([15.7132893 , 12.37305699, 68.        ]), 'previousTarget': array([15.7132893 , 12.37305699, 68.        ])}
episode index:18851
target thresh 85.57768165815227
target distance 14.0
model initialize at round 18851
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.00183857,  -4.99558513, 100.        ]), 'distance': 18.478435224135204, 'localFrame': array([[-0.5479498 ,  0.37040191,  0.75003563],
       [-0.56002926, -0.82847283,  0.        ],
       [ 0.62138414, -0.4200419 ,  0.66139743]]), 'currentState': array([-1.27742606,  3.03664821, 87.2751121 , -0.5479498 ,  0.37040191,
        0.75003563]), 'targetState': array([-12.00183857,  -4.99558513, 100.        ]), 'previousTarget': array([-12.00183857,  -4.99558513, 100.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6279561671621886
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.00183857,  -4.99558513, 100.        ]), 'distance': 2.8325336537816557, 'localFrame': array([[-0.47679386,  0.03921663,  0.87813989],
       [-0.08197389, -0.99663448,  0.        ],
       [ 0.87518449, -0.07198454,  0.47840394]]), 'currentState': array([-1.36108027e+01, -4.88087122e+00,  9.76716267e+01, -4.76793864e-01,
        3.92166309e-02,  8.78139891e-01]), 'targetState': array([-12.00183857,  -4.99558513, 100.        ]), 'previousTarget': array([-12.00183857,  -4.99558513, 100.        ])}
episode index:18852
target thresh 85.57912381787727
target distance 40.88890120087647
model initialize at round 18852
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.60048158,  -7.79079615,  81.86580965]), 'distance': 27.5, 'localFrame': array([[ 0.71155647,  0.6296337 ,  0.3118474 ],
       [-0.6626801 ,  0.74890259,  0.        ],
       [-0.23354333, -0.20665507,  0.9501322 ]]), 'currentState': array([ -3.2396739 , -16.91876481,  97.93962994,   0.71155647,
         0.6296337 ,   0.3118474 ]), 'targetState': array([-44.96450707,   1.78692583,  65.        ]), 'previousTarget': array([-24.03187934,  -8.62830002,  81.89399066])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6279609289701468
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-44.96450707,   1.78692583,  65.        ]), 'distance': 2.75184809065836, 'localFrame': array([[-0.87462247, -0.39172216, -0.28563837],
       [ 0.40875179, -0.91264559,  0.        ],
       [-0.2606866 , -0.1167552 ,  0.95833748]]), 'currentState': array([-43.57546802,   0.49106463,  66.99097518,  -0.87462247,
        -0.39172216,  -0.28563837]), 'targetState': array([-44.96450707,   1.78692583,  65.        ]), 'previousTarget': array([-44.96450707,   1.78692583,  65.        ])}
episode index:18853
target thresh 85.5805658333935
target distance 33.50319425736589
model initialize at round 18853
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 9.79637111, -6.88713799,  6.46389223]), 'distance': 27.5, 'localFrame': array([[ 0.95460905,  0.24659491, -0.16707037],
       [-0.25011022,  0.96821737,  0.        ],
       [ 0.16176044,  0.04178601,  0.98594497]]), 'currentState': array([-13.02434826,   5.1348176 ,  16.        ,   0.95460905,
         0.24659491,  -0.16707037]), 'targetState': array([ 20.478846  , -12.51466606,   2.        ]), 'previousTarget': array([ 9.79637111, -6.88713799,  6.46389223])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279276224607074
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 95, 'trapConfig': [], 'currentTarget': array([12.67377133, -6.07140944,  6.54599463]), 'distance': 27.5, 'localFrame': array([[ 0.84287491,  0.16954141, -0.51070304],
       [-0.19719685,  0.98036391,  0.        ],
       [ 0.50067483,  0.10070903,  0.85975718]]), 'currentState': array([-6.67169906,  9.8986915 , 17.8135876 ,  0.84287491,  0.16954141,
       -0.51070304]), 'targetState': array([ 20.478846  , -12.51466606,   2.        ]), 'previousTarget': array([12.67377133, -6.07140944,  6.54599463])}
episode index:18854
target thresh 85.58200770471541
target distance 64.13381933209577
model initialize at round 18854
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.7192695 , 14.8364842 , 33.57392816]), 'distance': 27.5, 'localFrame': array([[-0.32420561, -0.36124062, -0.8742974 ],
       [ 0.7442266 , -0.66792722,  0.        ],
       [-0.58396703, -0.65067538,  0.48539063]]), 'currentState': array([ 0.74667932, 35.23459755, 15.29590671, -0.32420561, -0.36124062,
       -0.8742974 ]), 'targetState': array([ -7.03837233, -29.16266989,  73.        ]), 'previousTarget': array([-1.29802174, 14.35804768, 34.9988298 ])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6279221951928774
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ -7.03837233, -29.16266989,  73.        ]), 'distance': 4.193303946789878, 'localFrame': array([[-0.53242761, -0.56659879,  0.62887729],
       [ 0.72874015, -0.68479033,  0.        ],
       [ 0.43064909,  0.45828813,  0.77750457]]), 'currentState': array([ -5.53967277, -26.59377895,  70.04390512,  -0.53242761,
        -0.56659879,   0.62887729]), 'targetState': array([ -7.03837233, -29.16266989,  73.        ]), 'previousTarget': array([ -7.03837233, -29.16266989,  73.        ])}
episode index:18855
target thresh 85.58344943185736
target distance 66.0
model initialize at round 18855
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.24335256, -10.92695088,  38.28410428]), 'distance': 27.499999999999996, 'localFrame': array([[-0.66130019,  0.66633958, -0.34449039],
       [-0.70978567, -0.70441771,  0.        ],
       [-0.24266513,  0.24451434,  0.93878984]]), 'currentState': array([ 19.78060093, -10.05389362,  11.17504401,  -0.66130019,
         0.66633958,  -0.34449039]), 'targetState': array([  8.76346602, -12.17381055,  77.        ]), 'previousTarget': array([ 16.17211632, -11.50948752,  38.01275149])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278888942703491
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([  8.76346602, -12.17381055,  77.        ]), 'distance': 9.477500436679552, 'localFrame': array([[ 0.19470171,  0.62980803,  0.75195285],
       [-0.95538819,  0.29535304,  0.        ],
       [-0.22209156, -0.71840687,  0.6592169 ]]), 'currentState': array([  4.39553326, -12.85215335,  68.61644294,   0.19470171,
         0.62980803,   0.75195285]), 'targetState': array([  8.76346602, -12.17381055,  77.        ]), 'previousTarget': array([  8.76346602, -12.17381055,  77.        ])}
episode index:18856
target thresh 85.58489101483383
target distance 48.0
model initialize at round 18856
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-18.01867488,  -4.43746541,  50.6872924 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.21472405, -0.86270375,  0.45786004],
       [ 0.97039391, -0.24152776,  0.        ],
       [ 0.11058591,  0.4443046 ,  0.88902429]]), 'currentState': array([-3.96426988, -0.53344643, 74.        , -0.21472405, -0.86270375,
        0.45786004]), 'targetState': array([-32.90176833,  -8.57167666,  26.        ]), 'previousTarget': array([-18.01867488,  -4.43746541,  50.6872924 ])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6278936586357481
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-32.90176833,  -8.57167666,  26.        ]), 'distance': 2.6833817375384217, 'localFrame': array([[ 0.35305084,  0.47263191, -0.80744918],
       [-0.80115641,  0.59845502,  0.        ],
       [ 0.48322201,  0.64689309,  0.58993713]]), 'currentState': array([-32.19942218,  -8.87007294,  28.57258763,   0.35305084,
         0.47263191,  -0.80744918]), 'targetState': array([-32.90176833,  -8.57167666,  26.        ]), 'previousTarget': array([-32.90176833,  -8.57167666,  26.        ])}
episode index:18857
target thresh 85.58633245365921
target distance 35.890270289252285
model initialize at round 18857
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.76807732,   8.05847556,  82.35714643]), 'distance': 27.5, 'localFrame': array([[ 0.35723583, -0.80868836, -0.46733895],
       [ 0.9147249 ,  0.40407717,  0.        ],
       [ 0.188841  , -0.42748657,  0.88407822]]), 'currentState': array([-34.86449074,  22.19428032,  77.56290917,   0.35723583,
        -0.80868836,  -0.46733895]), 'targetState': array([ 0.96396643,  0.26602393, 85.        ]), 'previousTarget': array([-12.01608276,   8.60627353,  82.83004434])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278603627582088
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([ 0.96396643,  0.26602393, 85.        ]), 'distance': 18.879316830483827, 'localFrame': array([[ 0.0627544 , -0.60499961,  0.79374892],
       [ 0.99466345,  0.10317281,  0.        ],
       [-0.08189331,  0.78951304,  0.60824555]]), 'currentState': array([-1.49520512e+01,  6.72312941e+00,  7.71632421e+01,  6.27544043e-02,
       -6.04999613e-01,  7.93748923e-01]), 'targetState': array([ 0.96396643,  0.26602393, 85.        ]), 'previousTarget': array([ 0.96396643,  0.26602393, 85.        ])}
episode index:18858
target thresh 85.5877737483479
target distance 42.0
model initialize at round 18858
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.5889683 ,  9.12321764, 64.86147154]), 'distance': 27.500000000000004, 'localFrame': array([[-0.38469559,  0.88707864, -0.25514856],
       [-0.91744434, -0.39786416,  0.        ],
       [-0.10151447,  0.2340846 ,  0.96690186]]), 'currentState': array([-2.78827882, 21.29208018, 40.22957273, -0.38469559,  0.88707864,
       -0.25514856]), 'targetState': array([-0.75450493,  0.65629438, 82.        ]), 'previousTarget': array([-1.06643563,  8.46793426, 64.9878045 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278270704117027
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([-0.75450493,  0.65629438, 82.        ]), 'distance': 6.617785590025751, 'localFrame': array([[ 0.21793125, -0.44340408,  0.8694244 ],
       [ 0.89745922,  0.44109745,  0.        ],
       [-0.38350088,  0.78027294,  0.494066  ]]), 'currentState': array([ 0.08768214,  2.2302248 , 75.62751617,  0.21793125, -0.44340408,
        0.8694244 ]), 'targetState': array([-0.75450493,  0.65629438, 82.        ]), 'previousTarget': array([-0.75450493,  0.65629438, 82.        ])}
episode index:18859
target thresh 85.58921489891434
target distance 65.0
model initialize at round 18859
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.7749483 , 13.17834046, 51.72839571]), 'distance': 27.499999999999996, 'localFrame': array([[-0.14175009, -0.38105743, -0.91362035],
       [ 0.93725297, -0.34865006,  0.        ],
       [-0.31853379, -0.85629339,  0.40656839]]), 'currentState': array([ 4.17988095, 21.4528725 , 76.13570614, -0.14175009, -0.38105743,
       -0.91362035]), 'targetState': array([28.99995912,  0.04869425, 13.        ]), 'previousTarget': array([13.28376923, 13.52585159, 53.51234336])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6278236811285659
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([28.99995912,  0.04869425, 13.        ]), 'distance': 4.134779138511879, 'localFrame': array([[-0.82432073, -0.0960958 , -0.55790764],
       [ 0.1157916 , -0.99327353,  0.        ],
       [-0.55415489, -0.06460102,  0.82990305]]), 'currentState': array([30.21838635,  2.97089169, 15.65943522, -0.82432073, -0.0960958 ,
       -0.55790764]), 'targetState': array([28.99995912,  0.04869425, 13.        ]), 'previousTarget': array([28.99995912,  0.04869425, 13.        ])}
episode index:18860
target thresh 85.59065590537293
target distance 12.81799532102675
model initialize at round 18860
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.16771676, 12.81799532, 83.        ]), 'distance': 12.711981803639752, 'localFrame': array([[ 0.05087165,  0.76459201, -0.6425038 ],
       [-0.99779391,  0.06638759,  0.        ],
       [ 0.04265428,  0.64108638,  0.7662825 ]]), 'currentState': array([ 9.89488733e-01,  1.17431583e+00,  8.79629616e+01,  5.08716464e-02,
        7.64592009e-01, -6.42503802e-01]), 'targetState': array([ 2.16771676, 12.81799532, 83.        ]), 'previousTarget': array([ 2.16771676, 12.81799532, 83.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6278403110245561
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.16771676, 12.81799532, 83.        ]), 'distance': 3.8175375925202646, 'localFrame': array([[-0.53350861, -0.03075491, -0.84523529],
       [ 0.05755096, -0.99834257,  0.        ],
       [-0.84383437, -0.0486441 ,  0.53439433]]), 'currentState': array([ 1.33332166e+00,  1.04281829e+01,  8.58576519e+01, -5.33508613e-01,
       -3.07549086e-02, -8.45235290e-01]), 'targetState': array([ 2.16771676, 12.81799532, 83.        ]), 'previousTarget': array([ 2.16771676, 12.81799532, 83.        ])}
episode index:18861
target thresh 85.59209676773807
target distance 66.0
model initialize at round 18861
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.21802896, -19.33651602,  33.8261444 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.20431789, -0.72340317,  0.65950136],
       [ 0.96235191, -0.27180654,  0.        ],
       [ 0.17925679,  0.6346724 ,  0.75170337]]), 'currentState': array([-12.49923215, -15.62417506,   6.77614938,  -0.20431789,
        -0.72340317,   0.65950136]), 'targetState': array([ -4.58748553, -24.57549545,  72.        ]), 'previousTarget': array([ -9.46992349, -18.32141351,  32.94937459])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6278421370096033
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.58748553, -24.57549545,  72.        ]), 'distance': 3.2654631970991415, 'localFrame': array([[-0.4786831 , -0.74921313,  0.45775777],
       [ 0.84268648, -0.53840458,  0.        ],
       [ 0.24645888,  0.38574629,  0.88907695]]), 'currentState': array([ -3.00221022, -24.27447661,  69.16106715,  -0.4786831 ,
        -0.74921313,   0.45775777]), 'targetState': array([ -4.58748553, -24.57549545,  72.        ]), 'previousTarget': array([ -4.58748553, -24.57549545,  72.        ])}
episode index:18862
target thresh 85.59353748602419
target distance 36.712726874686695
model initialize at round 18862
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.25406036, -13.15166766,  63.3219538 ]), 'distance': 27.5, 'localFrame': array([[-0.1417614 , -0.9896728 ,  0.02124756],
       [ 0.98989627, -0.14179342,  0.        ],
       [ 0.00301276,  0.02103288,  0.99977425]]), 'currentState': array([-3.45602014e+01,  5.66675066e+00,  4.50702374e+01, -1.41761405e-01,
       -9.89672797e-01,  2.12475630e-02]), 'targetState': array([-19.11916632, -29.31650524,  79.        ]), 'previousTarget': array([-26.80295822, -11.80919076,  62.78631219])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6278412503586212
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-19.11916632, -29.31650524,  79.        ]), 'distance': 1.782060370177438, 'localFrame': array([[ 0.80016177,  0.0526807 ,  0.59746622],
       [-0.06569534,  0.99783973,  0.        ],
       [-0.59617553, -0.03925075,  0.80189408]]), 'currentState': array([-1.88989208e+01, -2.90655170e+01,  7.72495041e+01,  8.00161771e-01,
        5.26807036e-02,  5.97466220e-01]), 'targetState': array([-19.11916632, -29.31650524,  79.        ]), 'previousTarget': array([-19.11916632, -29.31650524,  79.        ])}
episode index:18863
target thresh 85.59497806024568
target distance 76.0
model initialize at round 18863
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.67283368e+01, 1.47923776e-02, 5.71539777e+01]), 'distance': 27.5, 'localFrame': array([[ 0.06287786,  0.98147826, -0.18096077],
       [-0.99795417,  0.06393339,  0.        ],
       [ 0.01156943,  0.18059056,  0.98349031]]), 'currentState': array([ 1.30382023e+01, -1.56763312e+01,  7.94344971e+01,  6.28778650e-02,
        9.81478259e-01, -1.80960775e-01]), 'targetState': array([25.69740338, 38.15289582,  3.        ]), 'previousTarget': array([16.98763997, -1.32408106, 56.98744032])}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.6278314542777935
{'scaleFactor': 20, 'timeStep': 82, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([25.69740338, 38.15289582,  3.        ]), 'distance': 2.707949832754844, 'localFrame': array([[ 0.89954082, -0.17414618, -0.40062379],
       [ 0.19006557,  0.9817714 ,  0.        ],
       [ 0.39332098, -0.07614479,  0.91624264]]), 'currentState': array([26.03596067, 38.0649639 ,  5.68526335,  0.89954082, -0.17414618,
       -0.40062379]), 'targetState': array([25.69740338, 38.15289582,  3.        ]), 'previousTarget': array([25.69740338, 38.15289582,  3.        ])}
episode index:18864
target thresh 85.59641849041694
target distance 40.0
model initialize at round 18864
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.34507316, 23.73310367, 61.50787265]), 'distance': 27.499999999999996, 'localFrame': array([[-0.63449202, -0.31460901,  0.70600358],
       [ 0.44423233, -0.89591162,  0.        ],
       [ 0.63251681,  0.31362962,  0.70820827]]), 'currentState': array([12.46207482, 28.98061154, 36.09933192, -0.63449202, -0.31460901,
        0.70600358]), 'targetState': array([-1.4961237 , 20.94663729, 75.        ]), 'previousTarget': array([ 3.91119261, 24.38949512, 60.22728185])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6278296061459157
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-1.4961237 , 20.94663729, 75.        ]), 'distance': 2.601821983340231, 'localFrame': array([[ 0.56030692, -0.75401262,  0.34281354],
       [ 0.80265045,  0.59644971,  0.        ],
       [-0.20447104,  0.27515945,  0.93940347]]), 'currentState': array([-3.05157719, 20.06528355, 73.10969376,  0.56030692, -0.75401262,
        0.34281354]), 'targetState': array([-1.4961237 , 20.94663729, 75.        ]), 'previousTarget': array([-1.4961237 , 20.94663729, 75.        ])}
episode index:18865
target thresh 85.59785877655239
target distance 48.762074875872344
model initialize at round 18865
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.06760521, -21.70741575,  77.58849691]), 'distance': 27.5, 'localFrame': array([[-0.76594618, -0.22025333, -0.60399911],
       [ 0.27635819, -0.96105471,  0.        ],
       [-0.58047619, -0.1669201 ,  0.79698499]]), 'currentState': array([-4.62258515,  3.06844988, 89.42017134, -0.76594618, -0.22025333,
       -0.60399911]), 'targetState': array([ -1.54458491, -45.9740607 ,  66.        ]), 'previousTarget': array([ -2.19869043, -21.87805642,  77.85971073])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6278355359014145
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.54458491, -45.9740607 ,  66.        ]), 'distance': 4.542515645742345, 'localFrame': array([[ 0.80620948, -0.10603033, -0.58205141],
       [ 0.13039424,  0.99146222,  0.        ],
       [ 0.57708199, -0.07589615,  0.81315199]]), 'currentState': array([ -4.39811604, -43.99849312,  68.93068954,   0.80620948,
        -0.10603033,  -0.58205141]), 'targetState': array([ -1.54458491, -45.9740607 ,  66.        ]), 'previousTarget': array([ -1.54458491, -45.9740607 ,  66.        ])}
episode index:18866
target thresh 85.59929891866642
target distance 46.0
model initialize at round 18866
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.24733453, -9.35502165, 42.37794713]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.13048132,  0.51179629, -0.84914026],
       [-0.96900391,  0.24704538,  0.        ],
       [ 0.20977618,  0.82282023,  0.52816741]]), 'currentState': array([ 0.35466692, -6.61456971, 69.46275951,  0.13048132,  0.51179629,
       -0.84914026]), 'targetState': array([  6.74491771, -11.11332916,  25.        ]), 'previousTarget': array([ 3.71120204, -9.40733508, 43.95231541])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6278279647896519
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([  6.74491771, -11.11332916,  25.        ]), 'distance': 3.235027739519817, 'localFrame': array([[-0.24508163, -0.82523643, -0.50884166],
       [ 0.95861848, -0.2846939 ,  0.        ],
       [-0.14486411, -0.48778502,  0.86086013]]), 'currentState': array([ 5.6769841 , -9.26742082, 27.43260039, -0.24508163, -0.82523643,
       -0.50884166]), 'targetState': array([  6.74491771, -11.11332916,  25.        ]), 'previousTarget': array([  6.74491771, -11.11332916,  25.        ])}
episode index:18867
target thresh 85.60073891677345
target distance 24.0
model initialize at round 18867
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.63759929, -1.2687291 , 74.83964426]), 'distance': 27.5, 'localFrame': array([[ 0.65092845,  0.18195971, -0.73700937],
       [-0.26921796,  0.96307928,  0.        ],
       [ 0.70979845,  0.19841616,  0.67588253]]), 'currentState': array([-13.76416635, -15.59688012,  93.37456359,   0.65092845,
         0.18195971,  -0.73700937]), 'targetState': array([ 3.6210305,  1.6994523, 71.       ]), 'previousTarget': array([-5.42665393e-02, -1.72233336e+00,  7.58649881e+01])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278335019639609
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ 3.6210305,  1.6994523, 71.       ]), 'distance': 2.0680212451691595, 'localFrame': array([[ 0.9576098 , -0.28000585,  0.06767707],
       [ 0.2806493 ,  0.95981038,  0.        ],
       [-0.06495715,  0.01899352,  0.99770728]]), 'currentState': array([ 1.83550656e+00,  2.40233710e+00,  7.17710831e+01,  9.57609805e-01,
       -2.80005850e-01,  6.76770698e-02]), 'targetState': array([ 3.6210305,  1.6994523, 71.       ]), 'previousTarget': array([ 3.6210305,  1.6994523, 71.       ])}
episode index:18868
target thresh 85.60217877088787
target distance 30.77120163230716
model initialize at round 18868
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.05597766, 27.43432084, 95.74659591]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.55656134,  0.66211643,  0.50183792],
       [-0.76548644,  0.64345203,  0.        ],
       [-0.32290863, -0.38415012,  0.86496167]]), 'currentState': array([ 7.38233652,  2.34700937, 84.56099911,  0.55656134,  0.66211643,
        0.50183792]), 'targetState': array([ 5.78877488, 32.4883069 , 98.        ]), 'previousTarget': array([ 5.78136406, 26.74815322, 95.38839735])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278002286849338
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 88, 'trapConfig': [], 'currentTarget': array([ 5.78877488, 32.4883069 , 98.        ]), 'distance': 13.877219902733692, 'localFrame': array([[ 0.46133123,  0.87557136,  0.14334672],
       [-0.88470816,  0.46614533,  0.        ],
       [-0.0668204 , -0.12682001,  0.98967253]]), 'currentState': array([ 5.0675735 , 20.3184208 , 91.37044699,  0.46133123,  0.87557136,
        0.14334672]), 'targetState': array([ 5.78877488, 32.4883069 , 98.        ]), 'previousTarget': array([ 5.78877488, 32.4883069 , 98.        ])}
episode index:18869
target thresh 85.60361848102407
target distance 26.10502672361764
model initialize at round 18869
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.15884813,  31.01873732,  74.31366405]), 'distance': 27.5, 'localFrame': array([[ 0.85776918,  0.38555486,  0.33996983],
       [-0.40997444,  0.91209701,  0.        ],
       [-0.31008546, -0.13937894,  0.94043634]]), 'currentState': array([-35.37676865,  21.61135504,  83.32718081,   0.85776918,
         0.38555486,   0.33996983]), 'targetState': array([-10.31608152,  31.34610761,  74.        ]), 'previousTarget': array([-11.85903926,  30.7296619 ,  74.47284617])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.627802410536014
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([-10.31608152,  31.34610761,  74.        ]), 'distance': 2.703164502324624, 'localFrame': array([[ 0.87220725,  0.36752541,  0.32276862],
       [-0.38830849,  0.92152944,  0.        ],
       [-0.29744078, -0.1253338 ,  0.9464779 ]]), 'currentState': array([-12.8277325 ,  30.6587011 ,  74.72538266,   0.87220725,
         0.36752541,   0.32276862]), 'targetState': array([-10.31608152,  31.34610761,  74.        ]), 'previousTarget': array([-10.31608152,  31.34610761,  74.        ])}
episode index:18870
target thresh 85.60505804719647
target distance 39.64248582074556
model initialize at round 18870
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.77791439, 16.34205347, 15.30091447]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.38668557, -0.36171863,  0.84831239],
       [ 0.68313751,  0.73028976,  0.        ],
       [-0.61951385,  0.57951401,  0.52949608]]), 'currentState': array([-27.07440215,  35.46363492,  19.57416649,   0.38668557,
        -0.36171863,   0.84831239]), 'targetState': array([11.64348718, -2.90330955, 11.        ]), 'previousTarget': array([-8.33819366, 16.31698291, 14.52832985])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6278035392635767
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([11.64348718, -2.90330955, 11.        ]), 'distance': 2.9998664940504383, 'localFrame': array([[ 0.34306719, -0.90375248,  0.25600072],
       [ 0.93490676,  0.35489345,  0.        ],
       [-0.09085298,  0.2393368 ,  0.96667659]]), 'currentState': array([10.90397664, -0.13327689, 10.11725301,  0.34306719, -0.90375248,
        0.25600072]), 'targetState': array([11.64348718, -2.90330955, 11.        ]), 'previousTarget': array([11.64348718, -2.90330955, 11.        ])}
episode index:18871
target thresh 85.60649746941942
target distance 33.44353143785668
model initialize at round 18871
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.77791503, -16.73928507,  14.9645389 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.40117743,  0.8023533 , -0.4419116 ],
       [-0.89442684, -0.44721429,  0.        ],
       [-0.19762918,  0.3952576 ,  0.8970586 ]]), 'currentState': array([ 0.24703176,  6.78546437, 10.5202885 , -0.40117743,  0.8023533 ,
       -0.4419116 ]), 'targetState': array([ 19.97504255, -27.51359074,  17.        ]), 'previousTarget': array([ 14.41779576, -17.76301479,  15.5422329 ])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6278071747912783
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 19.97504255, -27.51359074,  17.        ]), 'distance': 2.688428699901994, 'localFrame': array([[ 0.1592712 , -0.02889358, -0.98681196],
       [ 0.1784978 ,  0.98394031,  0.        ],
       [ 0.97096407, -0.17614376,  0.16187079]]), 'currentState': array([ 19.32860459, -26.09488433,  19.19021437,   0.1592712 ,
        -0.02889358,  -0.98681196]), 'targetState': array([ 19.97504255, -27.51359074,  17.        ]), 'previousTarget': array([ 19.97504255, -27.51359074,  17.        ])}
episode index:18872
target thresh 85.60793674770737
target distance 14.740530981523428
model initialize at round 18872
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.36494447, 11.33523188, 48.        ]), 'distance': 13.85457466316279, 'localFrame': array([[ 0.64643239,  0.29783074,  0.70244004],
       [-0.41845269,  0.9082386 ,  0.        ],
       [-0.63798315, -0.29393792,  0.71174293]]), 'currentState': array([-7.10838404,  9.60858054, 45.27285259,  0.64643239,  0.29783074,
        0.70244004]), 'targetState': array([ 6.36494447, 11.33523188, 48.        ]), 'previousTarget': array([ 6.36494447, 11.33523188, 48.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278232961378112
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.36494447, 11.33523188, 48.        ]), 'distance': 2.546356771540421, 'localFrame': array([[ 0.63636538,  0.63709132, -0.43491809],
       [-0.70750976,  0.70670358,  0.        ],
       [ 0.30735817,  0.30770879,  0.90047002]]), 'currentState': array([ 4.13712914, 11.8659544 , 49.11315105,  0.63636538,  0.63709132,
       -0.43491809]), 'targetState': array([ 6.36494447, 11.33523188, 48.        ]), 'previousTarget': array([ 6.36494447, 11.33523188, 48.        ])}
episode index:18873
target thresh 85.60937588207469
target distance 55.16659311344702
model initialize at round 18873
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.9727671 ,  -9.6162018 ,  26.84297329]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.44226381,  0.21091941,  0.87173145],
       [-0.43046186,  0.90260877,  0.        ],
       [-0.78683245, -0.37524714,  0.48998396]]), 'currentState': array([-41.05051766, -16.96976856,  41.49725547,   0.44226381,
         0.21091941,   0.87173145]), 'targetState': array([13.93527369,  1.34467368,  5.        ]), 'previousTarget': array([-18.96848325, -10.20952681,  25.87552317])}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6278144691096921
{'scaleFactor': 20, 'timeStep': 78, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([13.93527369,  1.34467368,  5.        ]), 'distance': 2.9923577987772596, 'localFrame': array([[ 0.06265744, -0.79307075, -0.60589836],
       [ 0.99689355,  0.07876069,  0.        ],
       [ 0.04772097, -0.60401617,  0.79554206]]), 'currentState': array([11.09637699,  1.64332296,  5.89759642,  0.06265744, -0.79307075,
       -0.60589836]), 'targetState': array([13.93527369,  1.34467368,  5.        ]), 'previousTarget': array([13.93527369,  1.34467368,  5.        ])}
episode index:18874
target thresh 85.61081487253576
target distance 82.0
model initialize at round 18874
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.33081063, -29.32691747,  43.19110371]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.87529453,  0.15660736,  0.45752992],
       [-0.17612278,  0.98436821,  0.        ],
       [-0.45037791, -0.08058144,  0.88919423]]), 'currentState': array([ 19.44720555, -37.84050963,  18.02911507,   0.87529453,
         0.15660736,   0.45752992]), 'targetState': array([ -3.45324174, -10.44390356,  99.        ]), 'previousTarget': array([ 11.63890447, -30.15222268,  42.19877933])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6278113849631155
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.45324174, -10.44390356,  99.        ]), 'distance': 2.999974070480782, 'localFrame': array([[-0.28515872,  0.06085165,  0.95654669],
       [-0.20869684, -0.97798038,  0.        ],
       [ 0.9354839 , -0.19962827,  0.29157918]]), 'currentState': array([-3.95427566e+00, -8.16572879e+00,  9.71135406e+01, -2.85158723e-01,
        6.08516544e-02,  9.56546695e-01]), 'targetState': array([ -3.45324174, -10.44390356,  99.        ]), 'previousTarget': array([ -3.45324174, -10.44390356,  99.        ])}
episode index:18875
target thresh 85.61225371910498
target distance 42.806685604023805
model initialize at round 18875
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.16590068, -10.95331512,   7.61961521]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.64396398,  0.28030648, -0.71185579],
       [-0.39911183,  0.91690226,  0.        ],
       [ 0.65270218,  0.28411007,  0.70232566]]), 'currentState': array([ -4.60972007, -36.92015038,  14.59210914,   0.64396398,
         0.28030648,  -0.71185579]), 'targetState': array([4.99252939, 6.25097195, 3.        ]), 'previousTarget': array([  0.68544218, -10.96535362,   8.22844105])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278185120731697
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([4.99252939, 6.25097195, 3.        ]), 'distance': 2.396436319780127, 'localFrame': array([[-1.74016267e-01,  9.84742673e-01,  4.55380335e-04],
       [-9.84742775e-01, -1.74016285e-01,  0.00000000e+00],
       [ 7.92435941e-05, -4.48432495e-04,  9.99999896e-01]]), 'currentState': array([ 4.02567089e+00,  4.12645833e+00,  3.54270946e+00, -1.74016267e-01,
        9.84742673e-01,  4.55380335e-04]), 'targetState': array([4.99252939, 6.25097195, 3.        ]), 'previousTarget': array([4.99252939, 6.25097195, 3.        ])}
episode index:18876
target thresh 85.61369242179673
target distance 24.0
model initialize at round 18876
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.99713357,  0.13111191, 43.        ]), 'distance': 23.611039930081247, 'localFrame': array([[-0.76698549,  0.10482624, -0.63304401],
       [-0.13541415, -0.99078908,  0.        ],
       [-0.62721309,  0.08572312,  0.77411581]]), 'currentState': array([ 0.52371438,  1.58641684, 66.30165067, -0.76698549,  0.10482624,
       -0.63304401]), 'targetState': array([-2.99713357,  0.13111191, 43.        ]), 'previousTarget': array([-2.99713357,  0.13111191, 43.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6278322095017677
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.99713357,  0.13111191, 43.        ]), 'distance': 2.7266621567605567, 'localFrame': array([[ 0.25666046, -0.21489641, -0.9423083 ],
       [ 0.64196807,  0.76673137,  0.        ],
       [ 0.72249734, -0.60493185,  0.33474626]]), 'currentState': array([-2.75337423, -1.0602484 , 45.44047711,  0.25666046, -0.21489641,
       -0.9423083 ]), 'targetState': array([-2.99713357,  0.13111191, 43.        ]), 'previousTarget': array([-2.99713357,  0.13111191, 43.        ])}
episode index:18877
target thresh 85.61513098062541
target distance 18.960571118654762
model initialize at round 18877
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.34884799, -3.12852327, 34.96861358]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.34190183, -0.81834373, -0.46197043],
       [ 0.9227061 ,  0.38550415,  0.        ],
       [ 0.17809152, -0.42626293,  0.88689533]]), 'currentState': array([-4.28875887, 11.18003395, 50.47441942,  0.34190183, -0.81834373,
       -0.46197043]), 'targetState': array([14.45063049, -4.02234736, 34.        ]), 'previousTarget': array([12.43890345, -2.30192844, 35.90980992])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6278440553308594
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.45063049, -4.02234736, 34.        ]), 'distance': 3.068147943659937, 'localFrame': array([[ 0.8278922 , -0.5541802 , -0.08648018],
       [ 0.5562642 ,  0.8310055 ,  0.        ],
       [ 0.0718655 , -0.04810583,  0.99625357]]), 'currentState': array([11.68434698, -4.4397137 , 35.25976691,  0.8278922 , -0.5541802 ,
       -0.08648018]), 'targetState': array([14.45063049, -4.02234736, 34.        ]), 'previousTarget': array([14.45063049, -4.02234736, 34.        ])}
episode index:18878
target thresh 85.6165693956054
target distance 36.03576267636994
model initialize at round 18878
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.49218064, 25.52027367, 21.08316165]), 'distance': 27.5, 'localFrame': array([[ 0.08358545,  0.79545887,  0.60021551],
       [-0.99452458,  0.10450293,  0.        ],
       [-0.06272428, -0.59692907,  0.79983832]]), 'currentState': array([-4.53989348, 14.34940841, 37.49175464,  0.08358545,  0.79545887,
        0.60021551]), 'targetState': array([31.98690737, 35.78879373,  6.        ]), 'previousTarget': array([15.02349182, 25.25035122, 20.12215058])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.627842845627581
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([31.98690737, 35.78879373,  6.        ]), 'distance': 3.3120487734855932, 'localFrame': array([[-0.81986282,  0.06873591, -0.56841915],
       [-0.0835452 , -0.99650399,  0.        ],
       [-0.56643195,  0.04748869,  0.82273913]]), 'currentState': array([33.9501031 , 34.32080109,  8.22722411, -0.81986282,  0.06873591,
       -0.56841915]), 'targetState': array([31.98690737, 35.78879373,  6.        ]), 'previousTarget': array([31.98690737, 35.78879373,  6.        ])}
episode index:18879
target thresh 85.61800766675108
target distance 55.68458256519598
model initialize at round 18879
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.70558963, 16.42058915, 47.25672828]), 'distance': 27.500000000000004, 'localFrame': array([[-0.64386513,  0.68295545, -0.34497761],
       [-0.72762359, -0.68597661,  0.        ],
       [-0.23664657,  0.25101385,  0.93861091]]), 'currentState': array([18.19449753,  4.27111434, 41.13904116, -0.64386513,  0.68295545,
       -0.34497761]), 'targetState': array([-35.95637975,  31.79840807,  55.        ]), 'previousTarget': array([-4.15877012, 15.51696652, 47.00556933])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6278432875047227
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-35.95637975,  31.79840807,  55.        ]), 'distance': 3.7420291968936157, 'localFrame': array([[-0.33452869,  0.32244197,  0.88550648],
       [-0.69398024, -0.71999405,  0.        ],
       [ 0.6375594 , -0.614524  ,  0.46462702]]), 'currentState': array([-33.37499854,  32.45698073,  52.37216141,  -0.33452869,
         0.32244197,   0.88550648]), 'targetState': array([-35.95637975,  31.79840807,  55.        ]), 'previousTarget': array([-35.95637975,  31.79840807,  55.        ])}
episode index:18880
target thresh 85.61944579407684
target distance 50.9713207042723
model initialize at round 18880
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.1207442 , -16.0388833 ,  31.62867518]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.4490947 , -0.6010421 , -0.66110691],
       [ 0.80107789,  0.59856012,  0.        ],
       [ 0.39571223, -0.52959812,  0.75029172]]), 'currentState': array([-5.99855481,  4.57662355, 47.91756284,  0.4490947 , -0.6010421 ,
       -0.66110691]), 'targetState': array([ 13.8986067 , -45.94375618,   8.        ]), 'previousTarget': array([  1.07250584, -15.34177546,  32.61543457])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278458236925071
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.8986067 , -45.94375618,   8.        ]), 'distance': 3.0690396228815837, 'localFrame': array([[ 0.75497867, -0.39213   , -0.52558659],
       [ 0.46092808,  0.8874375 ,  0.        ],
       [ 0.46642525, -0.24225762,  0.85074011]]), 'currentState': array([ 14.8973447 , -44.2974311 ,  10.38979923,   0.75497867,
        -0.39213   ,  -0.52558659]), 'targetState': array([ 13.8986067 , -45.94375618,   8.        ]), 'previousTarget': array([ 13.8986067 , -45.94375618,   8.        ])}
episode index:18881
target thresh 85.62088377759707
target distance 54.0
model initialize at round 18881
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.8583377 , -9.84391246, 36.07160875]), 'distance': 27.5, 'localFrame': array([[-0.68460041, -0.16071414,  0.71098048],
       [ 0.22854303, -0.97353381,  0.        ],
       [ 0.69216353,  0.16248963,  0.70321175]]), 'currentState': array([ 8.92157707,  0.58843851, 11.36021094, -0.68460041, -0.16071414,
        0.71098048]), 'targetState': array([ -3.99422979, -21.63437377,  64.        ]), 'previousTarget': array([ 3.56871108, -9.93217128, 34.81659071])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278487210959239
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.99422979, -21.63437377,  64.        ]), 'distance': 3.4757926091995786, 'localFrame': array([[-0.27097585, -0.92257208,  0.27465042],
       [ 0.95946929, -0.28181322,  0.        ],
       [ 0.07740012,  0.26351864,  0.96154415]]), 'currentState': array([ -4.17982242, -18.92904977,  61.82562383,  -0.27097585,
        -0.92257208,   0.27465042]), 'targetState': array([ -3.99422979, -21.63437377,  64.        ]), 'previousTarget': array([ -3.99422979, -21.63437377,  64.        ])}
episode index:18882
target thresh 85.62232161732612
target distance 69.0
model initialize at round 18882
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.66543701, -17.39098833,  49.51130702]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.08552206, -0.60208262,  0.79384035],
       [ 0.99006192,  0.14063209,  0.        ],
       [-0.11163943,  0.7859511 ,  0.60812622]]), 'currentState': array([-41.54216043,  -5.30491438,  27.33308112,   0.08552206,
        -0.60208262,   0.79384035]), 'targetState': array([ -8.35671246, -42.18015359,  95.        ]), 'previousTarget': array([-30.30484391, -16.51926839,  48.28695737])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278154716799892
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([-13.96498282, -39.12890112,  68.24574946]), 'distance': 27.499999999999996, 'localFrame': array([[-0.35682155, -0.64207455,  0.67854156],
       [ 0.87409169, -0.48576097,  0.        ],
       [ 0.32960901,  0.59310754,  0.73456201]]), 'currentState': array([-19.5721309 , -36.07825924,  41.49685276,  -0.35682155,
        -0.64207455,   0.67854156]), 'targetState': array([ -8.35671246, -42.18015359,  95.        ]), 'previousTarget': array([-14.25655801, -38.44789636,  66.77247841])}
episode index:18883
target thresh 85.62375931327838
target distance 47.633938569122215
model initialize at round 18883
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.17629602,  6.23937023, 47.23307936]), 'distance': 27.5, 'localFrame': array([[ 0.32441435, -0.74486666,  0.58303429],
       [ 0.91681816,  0.39930498,  0.        ],
       [-0.23280849,  0.53453642,  0.81244755]]), 'currentState': array([-23.21445257,   4.23628058,  28.50582879,   0.32441435,
        -0.74486666,   0.58303429]), 'targetState': array([24.39430692,  8.99543162, 73.        ]), 'previousTarget': array([-3.27031228,  7.25186162, 46.8651075 ])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278180089378467
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.39430692,  8.99543162, 73.        ]), 'distance': 2.8972742886785996, 'localFrame': array([[ 0.30040901,  0.71248325,  0.63413093],
       [-0.92144282,  0.38851401,  0.        ],
       [-0.24636875, -0.58431539,  0.77322568]]), 'currentState': array([22.26245129,  8.46478158, 71.11113779,  0.30040901,  0.71248325,
        0.63413093]), 'targetState': array([24.39430692,  8.99543162, 73.        ]), 'previousTarget': array([24.39430692,  8.99543162, 73.        ])}
episode index:18884
target thresh 85.62519686546825
target distance 55.50407357931109
model initialize at round 18884
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.53020216, -11.13868145,  66.56375269]), 'distance': 27.5, 'localFrame': array([[-0.88672309,  0.3948089 ,  0.2405163 ],
       [-0.40674897, -0.91353997,  0.        ],
       [ 0.21972125, -0.09782976,  0.9706451 ]]), 'currentState': array([ 28.45821788, -15.08736412,  77.48400622,  -0.88672309,
         0.3948089 ,   0.2405163 ]), 'targetState': array([-25.14948221,  -6.59572169,  54.        ]), 'previousTarget': array([  5.2129139 , -11.36631297,  66.58169114])}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6278107053775642
{'scaleFactor': 20, 'timeStep': 72, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([-25.14948221,  -6.59572169,  54.        ]), 'distance': 3.6090288427990465, 'localFrame': array([[ 0.37260487,  0.62018569, -0.69031538],
       [-0.85719188,  0.51499716,  0.        ],
       [ 0.35551046,  0.59173274,  0.72350859]]), 'currentState': array([-23.9122075 ,  -9.51337856,  55.72670751,   0.37260487,
         0.62018569,  -0.69031538]), 'targetState': array([-25.14948221,  -6.59572169,  54.        ]), 'previousTarget': array([-25.14948221,  -6.59572169,  54.        ])}
episode index:18885
target thresh 85.62663427391008
target distance 23.0
model initialize at round 18885
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.53097672,  -2.37494658,  60.39999889]), 'distance': 27.5, 'localFrame': array([[-0.55880202,  0.43233022,  0.70769406],
       [-0.61191591, -0.79092283,  0.        ],
       [ 0.55973139, -0.43304925,  0.70651901]]), 'currentState': array([-1.99328889, -8.48541468, 40.11843865, -0.55880202,  0.43233022,
        0.70769406]), 'targetState': array([-20.91451525,  -1.89289511,  62.        ]), 'previousTarget': array([-18.68863913,  -2.81213405,  59.35037547])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6278225473274593
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.91451525,  -1.89289511,  62.        ]), 'distance': 2.852359226925248, 'localFrame': array([[-0.46087533, -0.56003909,  0.68844037],
       [ 0.77215572, -0.63543335,  0.        ],
       [ 0.43745797,  0.53158317,  0.72529294]]), 'currentState': array([-19.04859712,  -2.52627136,  59.93768644,  -0.46087533,
        -0.56003909,   0.68844037]), 'targetState': array([-20.91451525,  -1.89289511,  62.        ]), 'previousTarget': array([-20.91451525,  -1.89289511,  62.        ])}
episode index:18886
target thresh 85.62807153861827
target distance 36.78947554373399
model initialize at round 18886
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.53670401,  2.90887551, 31.19739757]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.62999147,  0.59579684,  0.49813339],
       [-0.68711401,  0.72654961,  0.        ],
       [-0.36191862, -0.34227443,  0.86710041]]), 'currentState': array([ 25.60221181, -20.01690113,  43.47883131,   0.62999147,
         0.59579684,   0.49813339]), 'targetState': array([39.77266373, 16.34427176, 24.        ]), 'previousTarget': array([34.16061395,  2.72140955, 30.66526272])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6278300774062691
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([39.77266373, 16.34427176, 24.        ]), 'distance': 2.0930900016955216, 'localFrame': array([[-0.2762431 ,  0.73664333,  0.61728952],
       [-0.9363284 , -0.3511255 ,  0.        ],
       [ 0.2167461 , -0.57798571,  0.78673607]]), 'currentState': array([38.60153612, 14.65199595, 24.38169155, -0.2762431 ,  0.73664333,
        0.61728952]), 'targetState': array([39.77266373, 16.34427176, 24.        ]), 'previousTarget': array([39.77266373, 16.34427176, 24.        ])}
episode index:18887
target thresh 85.62950865960715
target distance 53.36759673164471
model initialize at round 18887
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.15800327,   5.03250405,  61.39381703]), 'distance': 27.499999999999996, 'localFrame': array([[-0.8927681 ,  0.40629203,  0.19465843],
       [-0.41421552, -0.91017883,  0.        ],
       [ 0.17717398, -0.08063054,  0.98087109]]), 'currentState': array([ 8.65985855, 13.63255817, 74.107363  , -0.8927681 ,  0.40629203,
        0.19465843]), 'targetState': array([-43.58109675,  -6.05706246,  45.        ]), 'previousTarget': array([-13.4697912 ,   4.50242191,  60.79828599])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6278272991699587
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([-43.58109675,  -6.05706246,  45.        ]), 'distance': 2.9500324725430263, 'localFrame': array([[-0.41449061, -0.6301561 , -0.65658269],
       [ 0.83546929, -0.54953713,  0.        ],
       [-0.36081657, -0.54855468,  0.75425405]]), 'currentState': array([-41.18562658,  -4.68461708,  46.03961913,  -0.41449061,
        -0.6301561 ,  -0.65658269]), 'targetState': array([-43.58109675,  -6.05706246,  45.        ]), 'previousTarget': array([-43.58109675,  -6.05706246,  45.        ])}
episode index:18888
target thresh 85.63094563689113
target distance 48.25055517341873
model initialize at round 18888
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.01619955, -8.81334272, 31.27712654]), 'distance': 27.5, 'localFrame': array([[ 0.67910764,  0.60107758, -0.42132952],
       [-0.66277707,  0.74881677,  0.        ],
       [ 0.31549861,  0.27924754,  0.90690762]]), 'currentState': array([-27.38714557, -20.55925855,  48.03397003,   0.67910764,
         0.60107758,  -0.42132952]), 'targetState': array([19.79207052,  9.60593277,  5.        ]), 'previousTarget': array([-10.02071577,  -9.95222796,  31.56860229])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6278270714234248
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.79207052,  9.60593277,  5.        ]), 'distance': 3.4876822538901133, 'localFrame': array([[ 0.10149102,  0.98961663,  0.10177671],
       [-0.99478227,  0.10202078,  0.        ],
       [-0.01038334, -0.10124567,  0.99480727]]), 'currentState': array([18.31325977,  7.35131916,  2.78781472,  0.10149102,  0.98961663,
        0.10177671]), 'targetState': array([19.79207052,  9.60593277,  5.        ]), 'previousTarget': array([19.79207052,  9.60593277,  5.        ])}
episode index:18889
target thresh 85.63238247048457
target distance 28.108454898372102
model initialize at round 18889
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  2.32463963, -27.56319285,  64.49475825]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.86922103, -0.43196038, -0.2405515 ],
       [ 0.44502802,  0.89551665,  0.        ],
       [ 0.21541787, -0.10705216,  0.97063638]]), 'currentState': array([-24.04545403, -25.43258095,  72.        ,   0.86922103,
        -0.43196038,  -0.2405515 ]), 'targetState': array([  4.06300087, -27.7036464 ,  64.        ]), 'previousTarget': array([  2.32463963, -27.56319285,  64.49475825])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278371338832539
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  4.06300087, -27.7036464 ,  64.        ]), 'distance': 1.627138218250557, 'localFrame': array([[ 0.89395755, -0.17856804,  0.41103936],
       [ 0.19588042,  0.98062779,  0.        ],
       [-0.40307662,  0.08051456,  0.9116176 ]]), 'currentState': array([  2.44562595, -27.88006024,  63.97643514,   0.89395755,
        -0.17856804,   0.41103936]), 'targetState': array([  4.06300087, -27.7036464 ,  64.        ]), 'previousTarget': array([  4.06300087, -27.7036464 ,  64.        ])}
episode index:18890
target thresh 85.63381916040183
target distance 50.0
model initialize at round 18890
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.50476823,   6.90698815,  33.11477489]), 'distance': 27.5, 'localFrame': array([[-0.72704257,  0.27658188,  0.6284199 ],
       [-0.35556115, -0.93465302,  0.        ],
       [ 0.58735456, -0.2234417 ,  0.7778743 ]]), 'currentState': array([-12.13449488, -12.05883549,  13.34284181,  -0.72704257,
         0.27658188,   0.6284199 ]), 'targetState': array([-17.9675495 ,  34.61455135,  62.        ]), 'previousTarget': array([-13.76045128,   6.90727829,  32.07290896])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6278365755754385
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.9675495 ,  34.61455135,  62.        ]), 'distance': 1.964282522787989, 'localFrame': array([[-0.27470311,  0.96127745, -0.0219971 ],
       [-0.9615101 , -0.2747696 ,  0.        ],
       [-0.00604414,  0.02115044,  0.99975803]]), 'currentState': array([-1.80894210e+01,  3.29784988e+01,  6.30802247e+01, -2.74703112e-01,
        9.61277446e-01, -2.19971040e-02]), 'targetState': array([-17.9675495 ,  34.61455135,  62.        ]), 'previousTarget': array([-17.9675495 ,  34.61455135,  62.        ])}
episode index:18891
target thresh 85.63525570665728
target distance 31.807549012745103
model initialize at round 18891
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([20.78601636, 13.0899168 , 28.8782007 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.00239937,  0.86422131,  0.50310612],
       [-0.99999615,  0.00277633,  0.        ],
       [-0.00139679, -0.50310418,  0.86422464]]), 'currentState': array([-1.99455225e+00, -1.47516757e-01,  2.10000000e+01,  2.39937333e-03,
        8.64221313e-01,  5.03106118e-01]), 'targetState': array([29.81299676, 18.33535449, 32.        ]), 'previousTarget': array([20.78601636, 13.0899168 , 28.8782007 ])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278445146387364
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([29.81299676, 18.33535449, 32.        ]), 'distance': 2.99361352173395, 'localFrame': array([[ 0.15127625,  0.89308611, -0.42368938],
       [-0.9859557 ,  0.16700705,  0.        ],
       [ 0.07075911,  0.41773896,  0.90580754]]), 'currentState': array([27.04327893, 17.27582576, 32.40961435,  0.15127625,  0.89308611,
       -0.42368938]), 'targetState': array([29.81299676, 18.33535449, 32.        ]), 'previousTarget': array([29.81299676, 18.33535449, 32.        ])}
episode index:18892
target thresh 85.63669210926528
target distance 52.31691434516544
model initialize at round 18892
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.90830577, 14.75946174, 35.37427673]), 'distance': 27.499999999999996, 'localFrame': array([[-0.96519765,  0.05793675, -0.25502318],
       [-0.05991794, -0.99820331,  0.        ],
       [-0.25456498,  0.01528046,  0.96693494]]), 'currentState': array([17.49895583, -5.97233268, 18.19326011, -0.96519765,  0.05793675,
       -0.25502318]), 'targetState': array([ 3.24437716, 46.88788774, 62.        ]), 'previousTarget': array([12.98015204, 15.0630611 , 35.23442351])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6278408318791909
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([ 3.24437716, 46.88788774, 62.        ]), 'distance': 2.0357416374913826, 'localFrame': array([[-0.84802162, -0.31840574,  0.42364739],
       [ 0.35150827, -0.93618478,  0.        ],
       [ 0.39661223,  0.14891556,  0.90582719]]), 'currentState': array([ 4.01312179, 46.88814078, 60.11498657, -0.84802162, -0.31840574,
        0.42364739]), 'targetState': array([ 3.24437716, 46.88788774, 62.        ]), 'previousTarget': array([ 3.24437716, 46.88788774, 62.        ])}
episode index:18893
target thresh 85.6381283682402
target distance 42.0
model initialize at round 18893
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.6957914 ,   5.10461045,  60.71068841]), 'distance': 27.500000000000004, 'localFrame': array([[-0.45131868,  0.10701543, -0.88592277],
       [-0.23071984, -0.97302022,  0.        ],
       [-0.86202077,  0.20439996,  0.46383278]]), 'currentState': array([ 0.70904571,  3.01167629, 85.64659259, -0.45131868,  0.10701543,
       -0.88592277]), 'targetState': array([-17.88132759,   6.42324867,  45.        ]), 'previousTarget': array([-9.74998068,  5.21558155, 62.1906929 ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278479506408331
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.88132759,   6.42324867,  45.        ]), 'distance': 2.2931820059956327, 'localFrame': array([[-0.98515687, -0.15940121, -0.06369607],
       [ 0.15972555, -0.98716146,  0.        ],
       [-0.06287831, -0.01017389,  0.99796934]]), 'currentState': array([-16.31614377,   6.0963048 ,  46.6437734 ,  -0.98515687,
        -0.15940121,  -0.06369607]), 'targetState': array([-17.88132759,   6.42324867,  45.        ]), 'previousTarget': array([-17.88132759,   6.42324867,  45.        ])}
episode index:18894
target thresh 85.63956448359642
target distance 73.95635871835708
model initialize at round 18894
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.03087729,  -5.72863855,  72.02992259]), 'distance': 27.5, 'localFrame': array([[-0.82506737, -0.14263297, -0.54673547],
       [ 0.17034761, -0.98538403,  0.        ],
       [-0.53874441, -0.09313508,  0.83730539]]), 'currentState': array([-29.17290565, -32.52781026,  72.60427714,  -0.82506737,
        -0.14263297,  -0.54673547]), 'targetState': array([-12.01709884,  42.32717018,  71.        ]), 'previousTarget': array([-21.95376347,  -4.73188778,  72.27261695])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6278461045702198
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-12.01709884,  42.32717018,  71.        ]), 'distance': 1.8488980732627605, 'localFrame': array([[ 0.21505479,  0.87282389,  0.43809804],
       [-0.97096171,  0.23923494,  0.        ],
       [-0.10480836, -0.42537642,  0.8989272 ]]), 'currentState': array([-12.97152993,  40.96819971,  71.8128251 ,   0.21505479,
         0.87282389,   0.43809804]), 'targetState': array([-12.01709884,  42.32717018,  71.        ]), 'previousTarget': array([-12.01709884,  42.32717018,  71.        ])}
episode index:18895
target thresh 85.64100045534828
target distance 36.0
model initialize at round 18895
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.86631629, -5.75068051, 14.5212375 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.86233534,  0.12953497, -0.48948795],
       [-0.14854756,  0.98890526,  0.        ],
       [ 0.48405721,  0.07271224,  0.87201006]]), 'currentState': array([ 0.9086914 , -5.54640115, 41.76013591,  0.86233534,  0.12953497,
       -0.48948795]), 'targetState': array([-3.90867597, -5.80708638,  7.        ]), 'previousTarget': array([-3.05135209, -5.61062529, 15.64135073])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6278452192577177
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-3.90867597, -5.80708638,  7.        ]), 'distance': 1.666581793779224, 'localFrame': array([[-0.84967236, -0.17225579,  0.4983822 ],
       [ 0.19869002, -0.98006238,  0.        ],
       [ 0.48844565,  0.09902357,  0.86695743]]), 'currentState': array([-2.69583144, -4.67750597,  6.82521118, -0.84967236, -0.17225579,
        0.4983822 ]), 'targetState': array([-3.90867597, -5.80708638,  7.        ]), 'previousTarget': array([-3.90867597, -5.80708638,  7.        ])}
episode index:18896
target thresh 85.64243628351014
target distance 13.875155956976378
model initialize at round 18896
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.17062524,  22.41251587,  12.        ]), 'distance': 15.505256933797941, 'localFrame': array([[-0.59593852, -0.39231734, -0.70067423],
       [ 0.5498635 , -0.83525453,  0.        ],
       [-0.58524133, -0.38527519,  0.71348134]]), 'currentState': array([-27.51094442,  24.73701575,  20.64535566,  -0.59593852,
        -0.39231734,  -0.70067423]), 'targetState': array([-40.17062524,  22.41251587,  12.        ]), 'previousTarget': array([-40.17062524,  22.41251587,  12.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6278608248816353
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.17062524,  22.41251587,  12.        ]), 'distance': 2.524426506782763, 'localFrame': array([[-0.81802191,  0.38680937,  0.42569785],
       [-0.42747726, -0.9040261 ,  0.        ],
       [ 0.38484196, -0.18197615,  0.90486537]]), 'currentState': array([-38.29367514,  23.19848929,  13.4939991 ,  -0.81802191,
         0.38680937,   0.42569785]), 'targetState': array([-40.17062524,  22.41251587,  12.        ]), 'previousTarget': array([-40.17062524,  22.41251587,  12.        ])}
episode index:18897
target thresh 85.64387196809636
target distance 58.0
model initialize at round 18897
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.34614064, -22.14163266,  71.6433622 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.96421456, -0.25905733, -0.05638783],
       [ 0.25947017,  0.96575112,  0.        ],
       [ 0.05445661, -0.01463096,  0.99840894]]), 'currentState': array([-1.24821041e+01, -2.13092637e+01,  9.16697039e+01,  9.64214556e-01,
       -2.59057333e-01, -5.63878349e-02]), 'targetState': array([ 41.73744898, -23.70623025,  34.        ]), 'previousTarget': array([  4.83763928, -22.41214268,  72.2213941 ])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.627858354016424
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 41.73744898, -23.70623025,  34.        ]), 'distance': 3.6705552705307043, 'localFrame': array([[ 0.37739827,  0.21203389, -0.90145004],
       [-0.48981783,  0.8718248 ,  0.        ],
       [ 0.78590651,  0.44154631,  0.43288315]]), 'currentState': array([ 40.06423892, -26.11200993,  36.21033216,   0.37739827,
         0.21203389,  -0.90145004]), 'targetState': array([ 41.73744898, -23.70623025,  34.        ]), 'previousTarget': array([ 41.73744898, -23.70623025,  34.        ])}
episode index:18898
target thresh 85.64530750912131
target distance 57.115529672635546
model initialize at round 18898
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.8009844 , -7.1997163 , 77.91756463]), 'distance': 27.499999999999996, 'localFrame': array([[-0.93567865,  0.06509761, -0.34679644],
       [-0.06940484, -0.99758858,  0.        ],
       [-0.34596017,  0.02406935,  0.93794042]]), 'currentState': array([ 2.61023108e+01, -2.55002096e+01,  8.89631994e+01, -9.35678646e-01,
        6.50976086e-02, -3.46796442e-01]), 'targetState': array([-27.0959355,  30.7702824,  55.       ]), 'previousTarget': array([10.03310661, -8.15438414, 78.85276539])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6278549700721114
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.0959355,  30.7702824,  55.       ]), 'distance': 2.8460217893980175, 'localFrame': array([[ 0.27268345,  0.07243844, -0.95937292],
       [-0.25674547,  0.96647906,  0.        ],
       [ 0.92721384,  0.24631465,  0.28214108]]), 'currentState': array([-27.75751094,  32.32228114,  57.2920423 ,   0.27268345,
         0.07243844,  -0.95937292]), 'targetState': array([-27.0959355,  30.7702824,  55.       ]), 'previousTarget': array([-27.0959355,  30.7702824,  55.       ])}
episode index:18899
target thresh 85.64674290659933
target distance 53.0
model initialize at round 18899
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.79353844,  30.35693862,  72.06289926]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.12312303, -0.75577572,  0.64315145],
       [ 0.98698867,  0.16078981,  0.        ],
       [-0.1034122 ,  0.63478319,  0.765739  ]]), 'currentState': array([-5.65965635, 43.98585746, 48.73596578,  0.12312303, -0.75577572,
        0.64315145]), 'targetState': array([-16.94204404,  14.03449834, 100.        ]), 'previousTarget': array([-10.7392854 ,  31.09731215,  70.43437653])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6278540844779029
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-16.94204404,  14.03449834, 100.        ]), 'distance': 2.719150312593501, 'localFrame': array([[-0.82846965, -0.51916878,  0.21000432],
       [ 0.53101006, -0.84736551,  0.        ],
       [ 0.17795042,  0.11151441,  0.97770046]]), 'currentState': array([-17.65528655,  14.01761425,  97.37611386,  -0.82846965,
        -0.51916878,   0.21000432]), 'targetState': array([-16.94204404,  14.03449834, 100.        ]), 'previousTarget': array([-16.94204404,  14.03449834, 100.        ])}
episode index:18900
target thresh 85.64817816054477
target distance 47.265137197883476
model initialize at round 18900
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.9327427 , -13.7843464 ,  80.23341961]), 'distance': 27.499999999999996, 'localFrame': array([[-0.00525534,  0.96290281,  0.26979726],
       [-0.99998511, -0.00545772,  0.        ],
       [ 0.00147248, -0.26979324,  0.96291715]]), 'currentState': array([ 1.03870402e+01, -3.93561102e+01,  8.93160476e+01, -5.25533660e-03,
        9.62902810e-01,  2.69797257e-01]), 'targetState': array([ 2.38533296,  6.58104754, 73.        ]), 'previousTarget': array([  6.02671005, -14.81074419,  79.78887008])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6278478483878427
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 28, 'trapConfig': [], 'currentTarget': array([ 2.38533296,  6.58104754, 73.        ]), 'distance': 2.204801404431801, 'localFrame': array([[ 0.17972525,  0.70978436, -0.68110572],
       [-0.96940565,  0.24546423,  0.        ],
       [ 0.16718709,  0.66026773,  0.73218509]]), 'currentState': array([ 2.08244093,  6.48158294, 75.18163069,  0.17972525,  0.70978436,
       -0.68110572]), 'targetState': array([ 2.38533296,  6.58104754, 73.        ]), 'previousTarget': array([ 2.38533296,  6.58104754, 73.        ])}
episode index:18901
target thresh 85.649613270972
target distance 35.45067840236556
model initialize at round 18901
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.44705249, -29.86802583,   9.13127413]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.94696601, -0.31588647, -0.05891609],
       [ 0.31643614,  0.94861381,  0.        ],
       [ 0.05588862, -0.01864318,  0.99826294]]), 'currentState': array([-18.74790814, -31.72124364,   5.49126039,   0.94696601,
        -0.31588647,  -0.05891609]), 'targetState': array([ 14.93740326, -29.42573676,  10.        ]), 'previousTarget': array([  6.74313933, -30.01789641,   9.0754181 ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278579033602905
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 14.93740326, -29.42573676,  10.        ]), 'distance': 2.1344387389481376, 'localFrame': array([[ 0.84685602, -0.39775809,  0.35302037],
       [ 0.4251298 ,  0.9051324 ,  0.        ],
       [-0.31953017,  0.15007948,  0.93561564]]), 'currentState': array([ 13.28800985, -30.77831929,  10.07648941,   0.84685602,
        -0.39775809,   0.35302037]), 'targetState': array([ 14.93740326, -29.42573676,  10.        ]), 'previousTarget': array([ 14.93740326, -29.42573676,  10.        ])}
episode index:18902
target thresh 85.65104823789535
target distance 55.0
model initialize at round 18902
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.51528682, 20.85170985, 58.77928518]), 'distance': 27.5, 'localFrame': array([[-0.25992928,  0.83234257, -0.48953305],
       [-0.954538  , -0.29808925,  0.        ],
       [-0.14592454,  0.4672779 ,  0.87198474]]), 'currentState': array([ 5.99594858, 20.85138222, 84.92902461, -0.25992928,  0.83234257,
       -0.48953305]), 'targetState': array([-11.8823886 ,  20.85207043,  30.        ]), 'previousTarget': array([-2.39326549, 19.88966843, 58.88661651])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278246886375819
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([-11.8823886 ,  20.85207043,  30.        ]), 'distance': 16.340365276368246, 'localFrame': array([[-0.00684494, -0.01282437, -0.99989434],
       [ 0.88220232, -0.47087054,  0.        ],
       [-0.47082078, -0.8821091 ,  0.01453677]]), 'currentState': array([-1.38231715e+01,  2.02885438e+01,  4.62149109e+01, -6.84493645e-03,
       -1.28243717e-02, -9.99894336e-01]), 'targetState': array([-11.8823886 ,  20.85207043,  30.        ]), 'previousTarget': array([-11.8823886 ,  20.85207043,  30.        ])}
episode index:18903
target thresh 85.65248306132919
target distance 69.0
model initialize at round 18903
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.7960684 ,  8.87594246, 51.62552736]), 'distance': 27.499999999999996, 'localFrame': array([[-0.81219692,  0.40466315,  0.42021888],
       [-0.44594775, -0.895059  ,  0.        ],
       [ 0.37612069, -0.18739566,  0.90742278]]), 'currentState': array([17.02336033, 14.34682384, 25.40453783, -0.81219692,  0.40466315,
        0.42021888]), 'targetState': array([ 0.96993582,  0.24336084, 93.        ]), 'previousTarget': array([11.63851344,  8.84480484, 50.1907447 ])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6278261610525172
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.96993582,  0.24336084, 93.        ]), 'distance': 2.9212021944889015, 'localFrame': array([[ 0.15469369,  0.06965556,  0.98550391],
       [-0.41057739,  0.91182575,  0.        ],
       [-0.89860785, -0.40462563,  0.16965269]]), 'currentState': array([6.10329453e-01, 8.65965218e-01, 9.01686630e+01, 1.54693692e-01,
       6.96555590e-02, 9.85503914e-01]), 'targetState': array([ 0.96993582,  0.24336084, 93.        ]), 'previousTarget': array([ 0.96993582,  0.24336084, 93.        ])}
episode index:18904
target thresh 85.65391774128787
target distance 30.0
model initialize at round 18904
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 6.44128858, 17.3874515 , 76.89528595]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.06231113,  0.73196349,  0.67848859],
       [-0.99639612,  0.08482194,  0.        ],
       [-0.05755072, -0.67604341,  0.73461094]]), 'currentState': array([12.91146851,  5.41239071, 53.        ,  0.06231113,  0.73196349,
        0.67848859]), 'targetState': array([ 4.78830153, 20.44681316, 83.        ]), 'previousTarget': array([ 6.44128858, 17.3874515 , 76.89528595])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278362155765344
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 4.78830153, 20.44681316, 83.        ]), 'distance': 3.4442113669286694, 'localFrame': array([[ 0.34373961,  0.5169656 ,  0.78395768],
       [-0.83272185,  0.55369154,  0.        ],
       [-0.43407074, -0.65281869,  0.62081426]]), 'currentState': array([ 5.75269937, 19.02813603, 80.01338252,  0.34373961,  0.5169656 ,
        0.78395768]), 'targetState': array([ 4.78830153, 20.44681316, 83.        ]), 'previousTarget': array([ 4.78830153, 20.44681316, 83.        ])}
episode index:18905
target thresh 85.65535227778574
target distance 30.0
model initialize at round 18905
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.83885891, -1.151323  , 70.76636843]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.24573988,  0.66050394,  0.70946913],
       [-0.93723531,  0.34869754,  0.        ],
       [-0.24739014, -0.66493953,  0.70473651]]), 'currentState': array([-12.46885811,  -7.80684315,  46.29276696,   0.24573988,
         0.66050394,   0.70946913]), 'targetState': array([ 0.,  0., 75.]), 'previousTarget': array([-2.59787134, -1.61476512, 69.26470581])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6278475932861401
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([2.77555756e-17, 0.00000000e+00, 7.50000000e+01]), 'distance': 2.789713539095518, 'localFrame': array([[ 0.64953621, -0.38537605,  0.65542964],
       [ 0.51025868,  0.86002098,  0.        ],
       [-0.56368324,  0.33443866,  0.75525624]]), 'currentState': array([-0.23662237, -1.26632353, 72.52554325,  0.64953621, -0.38537605,
        0.65542964]), 'targetState': array([ 0.,  0., 75.]), 'previousTarget': array([ 0.,  0., 75.])}
episode index:18906
target thresh 85.6567866708371
target distance 13.50259692232406
model initialize at round 18906
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.44545262,  3.16540068, 99.        ]), 'distance': 12.710592873135246, 'localFrame': array([[ 0.5838516 ,  0.21214651,  0.78365245],
       [-0.34151106,  0.93987775,  0.        ],
       [-0.73653751, -0.26762598,  0.62119951]]), 'currentState': array([-14.37626513,  -0.57813734,  96.71947215,   0.5838516 ,
         0.21214651,   0.78365245]), 'targetState': array([-2.44545262,  3.16540068, 99.        ]), 'previousTarget': array([-2.44545262,  3.16540068, 99.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6278641814575113
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.44545262,  3.16540068, 99.        ]), 'distance': 2.49402190232098, 'localFrame': array([[ 0.45280878,  0.76361677, -0.46027561],
       [-0.8601457 ,  0.51004841,  0.        ],
       [ 0.23476284,  0.39590409,  0.88777608]]), 'currentState': array([-4.85133003,  3.58679359, 98.4956914 ,  0.45280878,  0.76361677,
       -0.46027561]), 'targetState': array([-2.44545262,  3.16540068, 99.        ]), 'previousTarget': array([-2.44545262,  3.16540068, 99.        ])}
episode index:18907
target thresh 85.65822092045634
target distance 64.0
model initialize at round 18907
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.31378946, 26.68694254, 56.44746188]), 'distance': 27.5, 'localFrame': array([[ 0.18085922, -0.47535679,  0.86100282],
       [ 0.93463749,  0.35560196,  0.        ],
       [-0.30617429,  0.80472552,  0.50860017]]), 'currentState': array([-4.55669241, 36.44117555, 30.88375021,  0.18085922, -0.47535679,
        0.86100282]), 'targetState': array([-11.25605327,  12.73975137,  93.        ]), 'previousTarget': array([-7.11994119, 27.13430068, 54.6186324 ])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6278556145331254
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-11.25605327,  12.73975137,  93.        ]), 'distance': 1.9729440252888848, 'localFrame': array([[ 0.92989487,  0.36280363,  0.06057281],
       [-0.36347104,  0.9316055 ,  0.        ],
       [-0.05642997, -0.02201646,  0.99816378]]), 'currentState': array([-1.27059546e+01,  1.19273873e+01,  9.40631834e+01,  9.29894867e-01,
        3.62803626e-01,  6.05728147e-02]), 'targetState': array([-11.25605327,  12.73975137,  93.        ]), 'previousTarget': array([-11.25605327,  12.73975137,  93.        ])}
episode index:18908
target thresh 85.65965502665779
target distance 79.0
model initialize at round 18908
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.57389345,  12.87188823,  43.19437434]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.63039214, -0.27285941,  0.72674169],
       [ 0.39722682,  0.91772046,  0.        ],
       [-0.66694572,  0.28868129,  0.68691085]]), 'currentState': array([-22.91874358,  19.35083234,  17.00838838,   0.63039214,
        -0.27285941,   0.72674169]), 'targetState': array([-6.99979074e+00,  5.41255722e-02,  9.50000000e+01]), 'previousTarget': array([-18.65805935,  12.79237011,  42.14054046])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6278550557794319
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.99979074e+00,  5.41255722e-02,  9.50000000e+01]), 'distance': 2.8193408229350556, 'localFrame': array([[ 0.56793599,  0.37904304,  0.73059913],
       [-0.555125  ,  0.83176694,  0.        ],
       [-0.6076882 , -0.40557384,  0.68280664]]), 'currentState': array([-7.93185788,  0.92565765, 92.48596632,  0.56793599,  0.37904304,
        0.73059913]), 'targetState': array([-6.99979074e+00,  5.41255722e-02,  9.50000000e+01]), 'previousTarget': array([-6.99979074e+00,  5.41255722e-02,  9.50000000e+01])}
episode index:18909
target thresh 85.66108898945579
target distance 51.0
model initialize at round 18909
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.29755275, -37.74264223,  53.9070912 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23980931, -0.78553884,  0.57045616],
       [ 0.95642519, -0.2919775 ,  0.        ],
       [ 0.16656036,  0.54559864,  0.82132805]]), 'currentState': array([ 15.01457537, -43.10316287,  80.67903319,  -0.23980931,
        -0.78553884,   0.57045616]), 'targetState': array([ 21.47447562, -32.55528985,  28.        ]), 'previousTarget': array([ 18.56938596, -37.17569366,  52.17511316])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627821853502553
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 37, 'trapConfig': [], 'currentTarget': array([ 21.47447562, -32.55528985,  28.        ]), 'distance': 13.622776078313064, 'localFrame': array([[ 0.06805289, -0.56642137, -0.82130118],
       [ 0.99285976,  0.11928748,  0.        ],
       [ 0.09797095, -0.81543689,  0.57049484]]), 'currentState': array([ 22.16206416, -38.08796384,  40.42967291,   0.06805289,
        -0.56642137,  -0.82130118]), 'targetState': array([ 21.47447562, -32.55528985,  28.        ]), 'previousTarget': array([ 21.47447562, -32.55528985,  28.        ])}
episode index:18910
target thresh 85.66252280886468
target distance 48.09823507190111
model initialize at round 18910
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.71610029, -27.0236943 ,  58.97061514]), 'distance': 27.500000000000004, 'localFrame': array([[-0.0947892 ,  0.99424445,  0.04992982],
       [-0.99548609, -0.09490758,  0.        ],
       [ 0.00473872, -0.04970444,  0.99875273]]), 'currentState': array([ 1.57081861e+01, -4.37974319e+01,  6.29570521e+01, -9.47892027e-02,
        9.94244447e-01,  4.99298219e-02]), 'targetState': array([-32.42965025,  -6.10882843,  54.        ]), 'previousTarget': array([ -5.46037231, -28.12341596,  58.48569939])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6278222957653927
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-32.42965025,  -6.10882843,  54.        ]), 'distance': 2.405553525611855, 'localFrame': array([[-0.98187924, -0.12096617,  0.14587787],
       [ 0.12227418, -0.99249636,  0.        ],
       [ 0.14478325,  0.0178371 ,  0.98930261]]), 'currentState': array([-30.6305246 ,  -7.35775683,  54.99499374,  -0.98187924,
        -0.12096617,   0.14587787]), 'targetState': array([-32.42965025,  -6.10882843,  54.        ]), 'previousTarget': array([-32.42965025,  -6.10882843,  54.        ])}
episode index:18911
target thresh 85.66395648489879
target distance 43.29606933536839
model initialize at round 18911
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.12855534, -6.80443551, 47.57033667]), 'distance': 27.5, 'localFrame': array([[-0.36517805,  0.80506788, -0.46745128],
       [-0.91069087, -0.41308854,  0.        ],
       [-0.19309877,  0.42570361,  0.88401883]]), 'currentState': array([ 43.76908225, -15.77694071,  30.54113079,  -0.36517805,
         0.80506788,  -0.46745128]), 'targetState': array([ 0.56613236,  3.9597341 , 68.        ]), 'previousTarget': array([24.03209451, -7.33793274, 48.48842191])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6278259226119602
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 0.56613236,  3.9597341 , 68.        ]), 'distance': 3.801923496586878, 'localFrame': array([[ 0.03922689,  0.58191032,  0.81230637],
       [-0.99773562,  0.06725789,  0.        ],
       [-0.05463401, -0.810467  ,  0.58323097]]), 'currentState': array([2.98201698e+00, 1.32119907e+00, 6.67130437e+01, 3.92268862e-02,
       5.81910320e-01, 8.12306365e-01]), 'targetState': array([ 0.56613236,  3.9597341 , 68.        ]), 'previousTarget': array([ 0.56613236,  3.9597341 , 68.        ])}
episode index:18912
target thresh 85.66539001757248
target distance 32.98869214228361
model initialize at round 18912
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.89864115, -27.50228093,  53.94318712]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.84903827, -0.20848266, -0.48545751],
       [ 0.23846751,  0.97115048,  0.        ],
       [ 0.47145229, -0.11576584,  0.87426026]]), 'currentState': array([ -4.85774694, -38.34685339,  62.56172954,   0.84903827,
        -0.20848266,  -0.48545751]), 'targetState': array([ 27.01132331, -23.79891621,  51.        ]), 'previousTarget': array([ 17.64758416, -27.98290487,  54.69001015])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6278346884534994
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 27.01132331, -23.79891621,  51.        ]), 'distance': 3.262283994018732, 'localFrame': array([[ 0.75887625, -0.24471053,  0.6035094 ],
       [ 0.30690252,  0.95174096,  0.        ],
       [-0.57438462,  0.18521855,  0.79735588]]), 'currentState': array([ 24.04409115, -25.15431217,  50.96947134,   0.75887625,
        -0.24471053,   0.6035094 ]), 'targetState': array([ 27.01132331, -23.79891621,  51.        ]), 'previousTarget': array([ 27.01132331, -23.79891621,  51.        ])}
episode index:18913
target thresh 85.66682340690006
target distance 44.0
model initialize at round 18913
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.04588073, -1.77226332, 24.55298232]), 'distance': 27.499999999999996, 'localFrame': array([[-0.69802955,  0.09023683,  0.71036052],
       [-0.12820682, -0.99174745,  0.        ],
       [ 0.70449823, -0.09107307,  0.70383801]]), 'currentState': array([19.28815091, -8.64652784,  5.15674387, -0.69802955,  0.09023683,
        0.71036052]), 'targetState': array([-21.00616985,   6.53764699,  48.        ]), 'previousTarget': array([ 2.15983372, -1.54297989, 23.45102167])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6278331616119014
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.00616985,   6.53764699,  48.        ]), 'distance': 4.0834340566655065, 'localFrame': array([[-0.56877934,  0.11329543,  0.81464974],
       [-0.19535269, -0.98073306,  0.        ],
       [ 0.79895393, -0.15914402,  0.57995327]]), 'currentState': array([-18.35988929,   4.43828158,  45.70559425,  -0.56877934,
         0.11329543,   0.81464974]), 'targetState': array([-21.00616985,   6.53764699,  48.        ]), 'previousTarget': array([-21.00616985,   6.53764699,  48.        ])}
episode index:18914
target thresh 85.66825665289588
target distance 38.0
model initialize at round 18914
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.09615166, 13.1011016 , 40.53600124]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.32486411,  0.46834971,  0.82165191],
       [-0.82168115,  0.56994744,  0.        ],
       [-0.4682984 , -0.67513589,  0.56998959]]), 'currentState': array([-12.35216086,  18.42673968,  13.89441787,   0.32486411,
         0.46834971,   0.82165191]), 'targetState': array([-6.58427208, 11.20925337, 50.        ]), 'previousTarget': array([-8.3330078 , 13.25324953, 38.74649693])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6278415065751155
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.58427208, 11.20925337, 50.        ]), 'distance': 1.904773893569475, 'localFrame': array([[ 0.00353131,  0.46597886,  0.88478881],
       [-0.99997129,  0.00757805,  0.        ],
       [-0.00670497, -0.8847634 ,  0.46599224]]), 'currentState': array([-5.79988479e+00,  1.16366056e+01,  4.83176593e+01,  3.53131306e-03,
        4.65978857e-01,  8.84788808e-01]), 'targetState': array([-6.58427208, 11.20925337, 50.        ]), 'previousTarget': array([-6.58427208, 11.20925337, 50.        ])}
episode index:18915
target thresh 85.66968975557425
target distance 69.93605136323983
model initialize at round 18915
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.97759493, -4.8464632 , 31.81826522]), 'distance': 27.499999999999996, 'localFrame': array([[-0.39308146,  0.15721367,  0.90596403],
       [-0.37135217, -0.92849209,  0.        ],
       [ 0.84118044, -0.33643171,  0.42335467]]), 'currentState': array([ 39.90267004, -17.33834896,  37.09022606,  -0.39308146,
         0.15721367,   0.90596403]), 'targetState': array([-28.57940006,  18.41786883,  22.        ]), 'previousTarget': array([17.30538254, -5.30369356, 31.18534781])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278083155460092
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 49, 'trapConfig': [], 'currentTarget': array([-24.59996639,  16.55573267,  21.20247427]), 'distance': 27.500000000000004, 'localFrame': array([[-0.55292616,  0.65711891,  0.51231572],
       [-0.76516193, -0.64383788,  0.        ],
       [ 0.32984827, -0.39200449,  0.85879718]]), 'currentState': array([-0.09258366,  5.08774817, 16.29090393, -0.55292616,  0.65711891,
        0.51231572]), 'targetState': array([-28.57940006,  18.41786883,  22.        ]), 'previousTarget': array([-24.59996639,  16.55573267,  21.20247427])}
episode index:18916
target thresh 85.67112271494955
target distance 68.09694107312248
model initialize at round 18916
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.43037792,  9.26195668, 52.57862073]), 'distance': 27.500000000000004, 'localFrame': array([[-0.85869536,  0.36412182,  0.36063498],
       [-0.39039248, -0.92064853,  0.        ],
       [ 0.33201806, -0.14078918,  0.93270704]]), 'currentState': array([43.29993113,  8.91355421, 58.42291075, -0.85869536,  0.36412182,
        0.36063498]), 'targetState': array([-23.01046189,   9.773364  ,  44.        ]), 'previousTarget': array([18.15103478,  9.37959212, 52.46236181])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6278090980848123
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-23.01046189,   9.773364  ,  44.        ]), 'distance': 2.7491123158331656, 'localFrame': array([[-0.69572293,  0.46161578,  0.55034578],
       [-0.55287509, -0.83326415,  0.        ],
       [ 0.45858341, -0.30427247,  0.83493684]]), 'currentState': array([-20.81905459,   8.73003025,  42.7089511 ,  -0.69572293,
         0.46161578,   0.55034578]), 'targetState': array([-23.01046189,   9.773364  ,  44.        ]), 'previousTarget': array([-23.01046189,   9.773364  ,  44.        ])}
episode index:18917
target thresh 85.67255553103604
target distance 38.737373015996205
model initialize at round 18917
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.1631009 ,  14.06139047,  51.93533983]), 'distance': 27.5, 'localFrame': array([[ 0.72243148, -0.10902364, -0.68279323],
       [ 0.14922242,  0.98880365,  0.        ],
       [ 0.67514844, -0.10188806,  0.73061166]]), 'currentState': array([-34.69891624,  29.12470437,  60.03195837,   0.72243148,
        -0.10902364,  -0.68279323]), 'targetState': array([ 2.62403073,  3.01901685, 46.        ]), 'previousTarget': array([-14.25193826,  14.11659199,  52.53476256])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.627815806520647
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.62403073,  3.01901685, 46.        ]), 'distance': 3.927957603843175, 'localFrame': array([[ 0.92957318,  0.25534415, -0.26588169],
       [-0.26487827,  0.96428186,  0.        ],
       [ 0.25638489,  0.07042628,  0.96400567]]), 'currentState': array([-0.08139566,  3.02583283, 48.84771356,  0.92957318,  0.25534415,
       -0.26588169]), 'targetState': array([ 2.62403073,  3.01901685, 46.        ]), 'previousTarget': array([ 2.62403073,  3.01901685, 46.        ])}
episode index:18918
target thresh 85.67398820384811
target distance 26.0
model initialize at round 18918
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.99467823,  -8.50570169,  52.4676913 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.27111904,  0.90133458,  0.33777276],
       [-0.95761588, -0.2880483 ,  0.        ],
       [ 0.09729487, -0.32345656,  0.94122769]]), 'currentState': array([ -1.22068704, -19.51138057,  29.23852582,  -0.27111904,
         0.90133458,   0.33777276]), 'targetState': array([-11.63941821,  -7.7797136 ,  54.        ]), 'previousTarget': array([-10.49187104,  -9.24180023,  51.11583817])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6278271774909341
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.63941821,  -7.7797136 ,  54.        ]), 'distance': 2.4136351098859867, 'localFrame': array([[-0.53913431,  0.83970207,  0.06507402],
       [-0.84148565, -0.54027947,  0.        ],
       [ 0.03515815, -0.05475885,  0.99788044]]), 'currentState': array([-9.65224111, -7.46311015, 55.33286304, -0.53913431,  0.83970207,
        0.06507402]), 'targetState': array([-11.63941821,  -7.7797136 ,  54.        ]), 'previousTarget': array([-11.63941821,  -7.7797136 ,  54.        ])}
episode index:18919
target thresh 85.67542073340005
target distance 11.0
model initialize at round 18919
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.41344865, -26.01558037,  82.        ]), 'distance': 14.01481574901671, 'localFrame': array([[ 0.8263308 , -0.42075763, -0.37435334],
       [ 0.45375167,  0.89112818,  0.        ],
       [ 0.33359681, -0.16986345,  0.92728613]]), 'currentState': array([-17.70597082, -18.5793475 ,  92.4183587 ,   0.8263308 ,
        -0.42075763,  -0.37435334]), 'targetState': array([-23.41344865, -26.01558037,  82.        ]), 'previousTarget': array([-23.41344865, -26.01558037,  82.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278432577324994
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.41344865, -26.01558037,  82.        ]), 'distance': 2.8018575195861, 'localFrame': array([[-0.4757445 , -0.74195593, -0.47240721],
       [ 0.84181123, -0.53977204,  0.        ],
       [-0.2549922 , -0.39767769,  0.88138041]]), 'currentState': array([-22.39580513, -23.72349729,  83.24946483,  -0.4757445 ,
        -0.74195593,  -0.47240721]), 'targetState': array([-23.41344865, -26.01558037,  82.        ]), 'previousTarget': array([-23.41344865, -26.01558037,  82.        ])}
episode index:18920
target thresh 85.6768531197062
target distance 26.631093865253845
model initialize at round 18920
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.88659354, 12.56889492, 39.69031199]), 'distance': 27.499999999999996, 'localFrame': array([[-0.93000528, -0.36080857,  0.07005256],
       [ 0.36169715, -0.93229565,  0.        ],
       [ 0.06530969,  0.02533781,  0.9975433 ]]), 'currentState': array([ 6.29589297, 34.39736156, 27.25166094, -0.93000528, -0.36080857,
        0.07005256]), 'targetState': array([-6.96302891,  8.51564609, 42.        ]), 'previousTarget': array([-4.3168769 , 13.2918178 , 39.48916045])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6278546260500117
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.96302891,  8.51564609, 42.        ]), 'distance': 3.9844331438680247, 'localFrame': array([[-0.84085455, -0.23927105,  0.48550282],
       [ 0.27369188, -0.96181742,  0.        ],
       [ 0.46696507,  0.13287818,  0.8742351 ]]), 'currentState': array([-6.71080113, 11.33223495, 39.19305934, -0.84085455, -0.23927105,
        0.48550282]), 'targetState': array([-6.96302891,  8.51564609, 42.        ]), 'previousTarget': array([-6.96302891,  8.51564609, 42.        ])}
episode index:18921
target thresh 85.67828536278088
target distance 32.18018876802477
model initialize at round 18921
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.55720735, -10.61733779,  57.0857902 ]), 'distance': 27.5, 'localFrame': array([[-0.8320698 ,  0.02410009,  0.55414712],
       [-0.02895189, -0.99958081,  0.        ],
       [ 0.55391483, -0.0160436 ,  0.83241875]]), 'currentState': array([ 9.59774669e+00, -2.84709904e+01,  6.74730291e+01, -8.32069801e-01,
        2.41000921e-02,  5.54147121e-01]), 'targetState': array([-20.94184553,   1.56176369,  50.        ]), 'previousTarget': array([ -7.6824244 , -10.54273126,  57.00462514])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6278621405051305
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.94184553,   1.56176369,  50.        ]), 'distance': 1.9529468872254407, 'localFrame': array([[-9.22935009e-01,  3.84954498e-01, -1.00161344e-03],
       [-3.84954691e-01, -9.22935472e-01,  0.00000000e+00],
       [-9.24424574e-04,  3.85575792e-04,  9.99999498e-01]]), 'currentState': array([-1.93705534e+01,  9.23806848e-01,  5.09685316e+01, -9.22935009e-01,
        3.84954498e-01, -1.00161344e-03]), 'targetState': array([-20.94184553,   1.56176369,  50.        ]), 'previousTarget': array([-20.94184553,   1.56176369,  50.        ])}
episode index:18922
target thresh 85.67971746263842
target distance 24.958089735334735
model initialize at round 18922
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.112785  , -4.16417298,  8.0384362 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.0502636 , -0.75298058,  0.65612028],
       [ 0.99777945,  0.06660462,  0.        ],
       [-0.04370064,  0.65466333,  0.75465633]]), 'currentState': array([10.45035781, 17.78486046,  8.42860464,  0.0502636 , -0.75298058,
        0.65612028]), 'targetState': array([-7.74445026, -6.32641211,  8.        ]), 'previousTarget': array([-6.05048601, -3.90118161,  7.90282788])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278730611636384
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.74445026, -6.32641211,  8.        ]), 'distance': 4.453881529193839, 'localFrame': array([[-0.8603246 , -0.08293506,  0.50295464],
       [ 0.09595492, -0.99538568,  0.        ],
       [ 0.50063384,  0.04826097,  0.86431281]]), 'currentState': array([-4.98167218, -4.32370368,  5.13760992, -0.8603246 , -0.08293506,
        0.50295464]), 'targetState': array([-7.74445026, -6.32641211,  8.        ]), 'previousTarget': array([-7.74445026, -6.32641211,  8.        ])}
episode index:18923
target thresh 85.68114941929312
target distance 21.0
model initialize at round 18923
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.33843695,  7.4845593 , 71.97875424]), 'distance': 27.5, 'localFrame': array([[ 0.69279993,  0.71418188,  0.09986238],
       [-0.71776983,  0.69628046,  0.        ],
       [-0.06953222, -0.0716782 ,  0.99500126]]), 'currentState': array([-11.07378095,  -3.64240788,  51.36981015,   0.69279993,
         0.71418188,   0.09986238]), 'targetState': array([ 4.0526131 ,  8.03593971, 73.        ]), 'previousTarget': array([ 2.96539168,  7.13822191, 71.57158078])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6278853305196666
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.0526131 ,  8.03593971, 73.        ]), 'distance': 2.827334675618418, 'localFrame': array([[ 0.06777909,  0.77400547,  0.62954073],
       [-0.99618772,  0.08723543,  0.        ],
       [-0.05491826, -0.62714075,  0.77696748]]), 'currentState': array([4.35515312e+00, 6.11727282e+00, 7.09454908e+01, 6.77790942e-02,
       7.74005466e-01, 6.29540732e-01]), 'targetState': array([ 4.0526131 ,  8.03593971, 73.        ]), 'previousTarget': array([ 4.0526131 ,  8.03593971, 73.        ])}
episode index:18924
target thresh 85.68258123275933
target distance 79.0
model initialize at round 18924
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.35126933, -28.33167164,  41.02013692]), 'distance': 27.5, 'localFrame': array([[ 0.55622713,  0.62291433,  0.55008101],
       [-0.74590576,  0.6660515 ,  0.        ],
       [-0.36638228, -0.41030859,  0.8351113 ]]), 'currentState': array([-11.93673344, -45.80765095,  19.79521338,   0.55622713,
         0.62291433,   0.55008101]), 'targetState': array([-9.779548  , 18.58387583, 98.        ]), 'previousTarget': array([-11.55513891, -29.82364142,  40.09303761])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.6278753295775947
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.779548  , 18.58387583, 98.        ]), 'distance': 3.1979960260101254, 'localFrame': array([[-0.51752072,  0.85122478,  0.08711305],
       [-0.85447311, -0.51949562,  0.        ],
       [ 0.04525485, -0.07443576,  0.99619843]]), 'currentState': array([-8.86705533e+00,  1.60104131e+01,  9.63351201e+01, -5.17520722e-01,
        8.51224776e-01,  8.71130473e-02]), 'targetState': array([-9.779548  , 18.58387583, 98.        ]), 'previousTarget': array([-9.779548  , 18.58387583, 98.        ])}
episode index:18925
target thresh 85.68401290305135
target distance 43.72602416263993
model initialize at round 18925
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.45672087, -27.54357988,  40.83372217]), 'distance': 27.499999999999996, 'localFrame': array([[-0.26045446, -0.86011372, -0.43859761],
       [ 0.95708185, -0.28981776,  0.        ],
       [-0.12711338, -0.41977381,  0.89868356]]), 'currentState': array([26.36105777, -3.63868761, 51.10596192, -0.26045446, -0.86011372,
       -0.43859761]), 'targetState': array([ 10.66617635, -45.77371169,  33.        ]), 'previousTarget': array([ 17.53946999, -26.07000956,  41.56172835])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6278816329044865
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.66617635, -45.77371169,  33.        ]), 'distance': 3.428857244075556, 'localFrame': array([[-0.20016069, -0.96976981,  0.13957871],
       [ 0.97935675, -0.20213944,  0.        ],
       [ 0.02821436,  0.13669735,  0.99021098]]), 'currentState': array([ 11.71228281, -43.97727988,  35.72682159,  -0.20016069,
        -0.96976981,   0.13957871]), 'targetState': array([ 10.66617635, -45.77371169,  33.        ]), 'previousTarget': array([ 10.66617635, -45.77371169,  33.        ])}
episode index:18926
target thresh 85.6854444301835
target distance 49.020933347241915
model initialize at round 18926
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-15.49167917,  -2.53772607,   1.98128658]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23484822,  0.13241978, -0.96297005],
       [-0.49115601, -0.87107162,  0.        ],
       [-0.83881588,  0.47296853,  0.26960839]]), 'currentState': array([  8.56011295, -15.83428097,   1.        ,  -0.23484822,
         0.13241978,  -0.96297005]), 'targetState': array([-40.46082039,  11.26596703,   3.        ]), 'previousTarget': array([-15.49167917,  -2.53772607,   1.98128658])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278484590452957
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 96, 'trapConfig': [], 'currentTarget': array([-19.00352322,  -1.99471375,   1.64345651]), 'distance': 27.5, 'localFrame': array([[-0.76739876, -0.44312065,  0.46340395],
       [ 0.50005312, -0.86599473,  0.        ],
       [ 0.40130538,  0.23172659,  0.88614715]]), 'currentState': array([  4.3559389 , -16.43093876,   0.16665699,  -0.76739876,
        -0.44312065,   0.46340395]), 'targetState': array([-40.46082039,  11.26596703,   3.        ]), 'previousTarget': array([-19.00352322,  -1.99471375,   1.64345651])}
episode index:18927
target thresh 85.68687581417008
target distance 65.0
model initialize at round 18927
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.40945957,   1.10066805,  28.17149174]), 'distance': 27.499999999999996, 'localFrame': array([[-0.67260845, -0.39911628,  0.62314049],
       [ 0.51030714, -0.85999222,  0.        ],
       [ 0.53589597,  0.31799304,  0.78210993]]), 'currentState': array([-20.38198533,   3.2777698 ,   1.65934146,  -0.67260845,
        -0.39911628,   0.62314049]), 'targetState': array([-3.46080413, -2.00570056, 66.        ]), 'previousTarget': array([-12.43537664,   1.20253414,  27.68669342])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278152886913732
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.81850406, -3.81968155, 54.92635723]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.61301359, -0.52907199, -0.58676842],
       [ 0.65337309,  0.75703607,  0.        ],
       [ 0.44420486, -0.38337869,  0.80975479]]), 'currentState': array([-4.69467812, -8.26296753, 27.80183827,  0.61301359, -0.52907199,
       -0.58676842]), 'targetState': array([-3.46080413, -2.00570056, 66.        ]), 'previousTarget': array([-4.22030527, -3.67093216, 55.8735542 ])}
episode index:18928
target thresh 85.68830705502543
target distance 71.73690467740713
model initialize at round 18928
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.39483572, -22.41376686,  66.11813447]), 'distance': 27.500000000000004, 'localFrame': array([[-0.8521731 , -0.06409347,  0.51931978],
       [ 0.07499996, -0.99718354,  0.        ],
       [ 0.51785714,  0.03894897,  0.85457999]]), 'currentState': array([ 30.58054805, -19.72377938,  51.57727098,  -0.8521731 ,
        -0.06409347,   0.51931978]), 'targetState': array([-40.25242668, -27.94176348,  96.        ]), 'previousTarget': array([  8.44945008, -22.16670422,  64.77079739])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6278147326584211
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.25242668, -27.94176348,  96.        ]), 'distance': 4.174562800087114, 'localFrame': array([[-0.96982536,  0.01470752, -0.24335665],
       [-0.01516337, -0.99988503,  0.        ],
       [-0.24332867,  0.00369011,  0.96993688]]), 'currentState': array([-3.73564206e+01, -2.74649549e+01,  9.30313679e+01, -9.69825361e-01,
        1.47075160e-02, -2.43356648e-01]), 'targetState': array([-40.25242668, -27.94176348,  96.        ]), 'previousTarget': array([-40.25242668, -27.94176348,  96.        ])}
episode index:18929
target thresh 85.68973815276385
target distance 40.71362353716691
model initialize at round 18929
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.27240412, -2.15056223, 10.17347122]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.48864061, -0.6873454 ,  0.53738873],
       [ 0.81503279,  0.57941483,  0.        ],
       [-0.311371  ,  0.43798944,  0.84333466]]), 'currentState': array([-7.4495077 , 15.39947869,  2.47332461,  0.48864061, -0.6873454 ,
        0.53738873]), 'targetState': array([ 32.31801082, -19.9886512 ,  18.        ]), 'previousTarget': array([11.28578651, -1.43753265,  9.21798105])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277815675906632
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 97, 'trapConfig': [], 'currentTarget': array([14.28130583, -4.71957365, 10.70265645]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.04012366, -0.86045204, -0.50794919],
       [ 0.99891455,  0.04658029,  0.        ],
       [ 0.02366042, -0.50739784,  0.86138703]]), 'currentState': array([-5.77329513, 12.25776547,  2.58890658,  0.04012366, -0.86045204,
       -0.50794919]), 'targetState': array([ 32.31801082, -19.9886512 ,  18.        ]), 'previousTarget': array([14.28130583, -4.71957365, 10.70265645])}
episode index:18930
target thresh 85.69116910739965
target distance 34.0
model initialize at round 18930
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.24483797, 13.52998403, 53.59319706]), 'distance': 27.499999999999996, 'localFrame': array([[-0.87627778, -0.25270266,  0.41021777],
       [ 0.27709001, -0.96084396,  0.        ],
       [ 0.39415527,  0.11366725,  0.9119876 ]]), 'currentState': array([35.21678497, 24.77009628, 34.23326974, -0.87627778, -0.25270266,
        0.41021777]), 'targetState': array([ 8.18422072,  5.74617536, 67.        ]), 'previousTarget': array([20.33341176, 13.7648337 , 52.44452299])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6277737687363492
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 43, 'trapConfig': [], 'currentTarget': array([ 8.18422072,  5.74617536, 67.        ]), 'distance': 2.4170297045435585, 'localFrame': array([[-0.34754986, -0.33978479,  0.873931  ],
       [ 0.69907372, -0.7150496 ,  0.        ],
       [ 0.62490401,  0.6109422 ,  0.48605001]]), 'currentState': array([ 9.14311798,  7.93777131, 66.65437617, -0.34754986, -0.33978479,
        0.873931  ]), 'targetState': array([ 8.18422072,  5.74617536, 67.        ]), 'previousTarget': array([ 8.18422072,  5.74617536, 67.        ])}
episode index:18931
target thresh 85.69259991894714
target distance 44.67432403412007
model initialize at round 18931
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.96259367,   4.49616545,  45.63557747]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83392694, -0.26364843, -0.48482508],
       [ 0.30144646, -0.95348311,  0.        ],
       [-0.46227252, -0.1461488 ,  0.87461114]]), 'currentState': array([-45.43515964,   5.25278381,  46.60286633,  -0.83392694,
        -0.26364843,  -0.48482508]), 'targetState': array([ 0.08883285,  3.99901347, 45.        ]), 'previousTarget': array([-17.17726631,   4.80861419,  46.15946461])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6277773943146987
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 0.08883285,  3.99901347, 45.        ]), 'distance': 2.8404783106905147, 'localFrame': array([[ 0.68111151, -0.46730557, -0.56366002],
       [ 0.56574053,  0.82458332,  0.        ],
       [ 0.46478465, -0.31888532,  0.82600689]]), 'currentState': array([-2.27085277,  4.62446772, 46.45224232,  0.68111151, -0.46730557,
       -0.56366002]), 'targetState': array([ 0.08883285,  3.99901347, 45.        ]), 'previousTarget': array([ 0.08883285,  3.99901347, 45.        ])}
episode index:18932
target thresh 85.69403058742063
target distance 77.0
model initialize at round 18932
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.15538736, 17.94037334, 59.89551132]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.53116705,  0.80202504, -0.27316187],
       [-0.83373352,  0.55216702,  0.        ],
       [ 0.15083097,  0.2277442 ,  0.96196808]]), 'currentState': array([-22.00208778,  15.7806957 ,  80.70611143,   0.53116705,
         0.80202504,  -0.27316187]), 'targetState': array([42.92176275, 23.637307  ,  5.        ]), 'previousTarget': array([-4.90940726, 17.09189029, 61.14800824])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627744236474192
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 38, 'trapConfig': [], 'currentTarget': array([42.92176275, 23.637307  ,  5.        ]), 'distance': 18.93201180694378, 'localFrame': array([[ 0.63325111, -0.46916312, -0.61553148],
       [ 0.59529982,  0.80350365,  0.        ],
       [ 0.49458179, -0.36642578,  0.7881123 ]]), 'currentState': array([34.51772178, 18.09425216, 21.0333312 ,  0.63325111, -0.46916312,
       -0.61553148]), 'targetState': array([42.92176275, 23.637307  ,  5.        ]), 'previousTarget': array([42.92176275, 23.637307  ,  5.        ])}
episode index:18933
target thresh 85.69546111283442
target distance 32.42632446767702
model initialize at round 18933
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.31203605,  19.60511024,  21.76437282]), 'distance': 27.5, 'localFrame': array([[-0.49349847, -0.72527464,  0.48003746],
       [ 0.82676124, -0.56255298,  0.        ],
       [ 0.2700465 ,  0.39687636,  0.87724799]]), 'currentState': array([-17.25075922,  -4.78238436,  11.19846174,  -0.49349847,
        -0.72527464,   0.48003746]), 'targetState': array([-27.14273715,  29.38148771,  26.        ]), 'previousTarget': array([-24.42310195,  20.91625048,  22.0840903 ])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.627752162803701
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.14273715,  29.38148771,  26.        ]), 'distance': 2.69242860063146, 'localFrame': array([[-0.36021342,  0.30275071,  0.88237651],
       [-0.64340567, -0.76552541,  0.        ],
       [ 0.67548163, -0.56772605,  0.47054404]]), 'currentState': array([-26.11555453,  28.42579233,  23.70201959,  -0.36021342,
         0.30275071,   0.88237651]), 'targetState': array([-27.14273715,  29.38148771,  26.        ]), 'previousTarget': array([-27.14273715,  29.38148771,  26.        ])}
episode index:18934
target thresh 85.69689149520283
target distance 41.961359489296235
model initialize at round 18934
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.73698778, -25.02898085,  65.03563171]), 'distance': 27.5, 'localFrame': array([[-0.89127879, -0.31774946,  0.32350796],
       [ 0.33580735, -0.94193069,  0.        ],
       [ 0.30472208,  0.10863635,  0.94622545]]), 'currentState': array([-0.69929263,  0.74044181, 68.27707002, -0.89127879, -0.31774946,
        0.32350796]), 'targetState': array([-15.41268389, -41.21224545,  63.        ]), 'previousTarget': array([ -9.1366818 , -24.82981729,  64.56166801])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6277596775110156
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.41268389, -41.21224545,  63.        ]), 'distance': 3.2488023452836248, 'localFrame': array([[-0.81207679, -0.58319986,  0.020229  ],
       [ 0.58331923, -0.81224299,  0.        ],
       [ 0.01643087,  0.01179997,  0.99979537]]), 'currentState': array([-1.52490376e+01, -3.85806243e+01,  6.48980270e+01, -8.12076787e-01,
       -5.83199863e-01,  2.02290045e-02]), 'targetState': array([-15.41268389, -41.21224545,  63.        ]), 'previousTarget': array([-15.41268389, -41.21224545,  63.        ])}
episode index:18935
target thresh 85.69832173454014
target distance 26.176218071218194
model initialize at round 18935
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.21689024, -14.96097909,  23.98277809]), 'distance': 27.5, 'localFrame': array([[ 0.63257831, -0.6658586 , -0.39557174],
       [ 0.72499256,  0.6887567 ,  0.        ],
       [ 0.27245268, -0.28678657,  0.91843508]]), 'currentState': array([-21.36550393,  10.51859612,  17.60772185,   0.63257831,
        -0.6658586 ,  -0.39557174]), 'targetState': array([-13.19487715, -15.02981094,  24.        ]), 'previousTarget': array([-13.41874413, -14.39165789,  23.87810442])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6277697190857983
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.19487715, -15.02981094,  24.        ]), 'distance': 3.4576506636170965, 'localFrame': array([[-0.39016489,  0.47544634,  0.78849359],
       [-0.77302946, -0.63437013,  0.        ],
       [ 0.50019678, -0.60952877,  0.61504298]]), 'currentState': array([-11.5141561 , -12.97751434,  21.78220752,  -0.39016489,
         0.47544634,   0.78849359]), 'targetState': array([-13.19487715, -15.02981094,  24.        ]), 'previousTarget': array([-13.19487715, -15.02981094,  24.        ])}
episode index:18936
target thresh 85.69975183086069
target distance 26.91962389807164
model initialize at round 18936
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.26243062, -8.47033176, 51.30143343]), 'distance': 27.500000000000004, 'localFrame': array([[-0.35437695, -0.72992067, -0.58449362],
       [ 0.89958368, -0.43674845,  0.        ],
       [-0.25527668, -0.52580092,  0.8113983 ]]), 'currentState': array([43.83083085,  4.22727578, 67.12026005, -0.35437695, -0.72992067,
       -0.58449362]), 'targetState': array([ 16.69188733, -14.3311164 ,  44.        ]), 'previousTarget': array([25.69527676, -7.58678484, 52.02690807])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6277776428139661
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.69188733, -14.3311164 ,  44.        ]), 'distance': 3.526481989261727, 'localFrame': array([[-0.44941142,  0.60001247, -0.66182657],
       [-0.80038269, -0.5994894 ,  0.        ],
       [-0.39675802,  0.52971453,  0.74965698]]), 'currentState': array([ 18.61078409, -13.43321445,  46.81916345,  -0.44941142,
         0.60001247,  -0.66182657]), 'targetState': array([ 16.69188733, -14.3311164 ,  44.        ]), 'previousTarget': array([ 16.69188733, -14.3311164 ,  44.        ])}
episode index:18937
target thresh 85.70118178417874
target distance 29.36544177234674
model initialize at round 18937
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.63087725,  0.20475394, 48.19779557]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.70796344, -0.24857199, -0.66105955],
       [ 0.33128203,  0.94353178,  0.        ],
       [ 0.62373069, -0.21899715,  0.75033344]]), 'currentState': array([ 3.58370949, -5.5486733 , 67.18077414,  0.70796344, -0.24857199,
       -0.66105955]), 'targetState': array([31.85977432,  2.99245384, 39.        ]), 'previousTarget': array([21.75998386,  0.47284153, 48.97410241])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6277863996331038
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([31.85977432,  2.99245384, 39.        ]), 'distance': 2.7874888898530066, 'localFrame': array([[ 0.41210261,  0.76113569, -0.50084318],
       [-0.87937867,  0.47612305,  0.        ],
       [ 0.23846298,  0.44043081,  0.86553805]]), 'currentState': array([30.36544913,  2.14417799, 41.19488374,  0.41210261,  0.76113569,
       -0.50084318]), 'targetState': array([31.85977432,  2.99245384, 39.        ]), 'previousTarget': array([31.85977432,  2.99245384, 39.        ])}
episode index:18938
target thresh 85.70261159450861
target distance 37.25641247084438
model initialize at round 18938
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.40266464, -23.80037457,  79.81490037]), 'distance': 27.5, 'localFrame': array([[ 0.35567883,  0.76288043, -0.53991297],
       [-0.90633425,  0.42256151,  0.        ],
       [ 0.22814644,  0.48934162,  0.84172084]]), 'currentState': array([ -3.23547756, -43.12564524,  69.52140058,   0.35567883,
         0.76288043,  -0.53991297]), 'targetState': array([28.24921798, -6.55604175, 89.        ]), 'previousTarget': array([ 12.86610949, -24.2956229 ,  80.42932882])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6277915315796863
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([28.24921798, -6.55604175, 89.        ]), 'distance': 2.1363917881999748, 'localFrame': array([[ 0.59922751,  0.6844801 , -0.4152269 ],
       [-0.7524092 ,  0.65869598,  0.        ],
       [ 0.27350829,  0.31242054,  0.90971788]]), 'currentState': array([26.20917864, -6.30371846, 89.58201572,  0.59922751,  0.6844801 ,
       -0.4152269 ]), 'targetState': array([28.24921798, -6.55604175, 89.        ]), 'previousTarget': array([28.24921798, -6.55604175, 89.        ])}
episode index:18939
target thresh 85.7040412618646
target distance 47.03025039051565
model initialize at round 18939
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.29251519,  3.98313093, 24.67995225]), 'distance': 27.499999999999996, 'localFrame': array([[-0.71898566,  0.25616454,  0.64609547],
       [-0.33562046, -0.94199729,  0.        ],
       [ 0.60862018, -0.21684286,  0.7632566 ]]), 'currentState': array([14.74071705, 26.65208023, 20.63388428, -0.71898566,  0.25616454,
        0.64609547]), 'targetState': array([-16.34372385, -20.22084792,  29.        ]), 'previousTarget': array([ 0.3585632 ,  4.39697364, 23.76553466])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6277930029465814
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.34372385, -20.22084792,  29.        ]), 'distance': 3.860192826754643, 'localFrame': array([[ 0.71995607,  0.10970326,  0.68529443],
       [-0.15063622,  0.98858926,  0.        ],
       [-0.67747471, -0.10323016,  0.72826612]]), 'currentState': array([-13.85776146, -17.27162917,  29.15227742,   0.71995607,
         0.10970326,   0.68529443]), 'targetState': array([-16.34372385, -20.22084792,  29.        ]), 'previousTarget': array([-16.34372385, -20.22084792,  29.        ])}
episode index:18940
target thresh 85.705470786261
target distance 70.52660825698021
model initialize at round 18940
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.09769578, -4.5073902 ,  9.67975782]), 'distance': 27.500000000000004, 'localFrame': array([[-0.03468357, -0.96065628, -0.27556588],
       [ 0.99934889, -0.03608053,  0.        ],
       [-0.00994256, -0.27538646,  0.96128219]]), 'currentState': array([-24.71968475, -16.36907092,  12.73069038,  -0.03468357,
        -0.96065628,  -0.27556588]), 'targetState': array([45.73974302, 17.57486581,  4.        ]), 'previousTarget': array([-0.03258474, -3.60127722, 10.49007926])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6277902344415939
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([45.73974302, 17.57486581,  4.        ]), 'distance': 1.6973523254692133, 'localFrame': array([[-0.04031355,  0.99643217, -0.07414684],
       [-0.99918258, -0.04042483,  0.        ],
       [-0.00299737,  0.07408623,  0.99724733]]), 'currentState': array([ 4.52091421e+01,  1.60282499e+01,  3.54453663e+00, -4.03135545e-02,
        9.96432167e-01, -7.41468372e-02]), 'targetState': array([45.73974302, 17.57486581,  4.        ]), 'previousTarget': array([45.73974302, 17.57486581,  4.        ])}
episode index:18941
target thresh 85.70690016771212
target distance 59.0
model initialize at round 18941
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.74389725, -18.34032835,  49.67323993]), 'distance': 27.5, 'localFrame': array([[-0.98241962, -0.05934248,  0.17700326],
       [ 0.06029452, -0.99818063,  0.        ],
       [ 0.17668123,  0.01067233,  0.98421026]]), 'currentState': array([ 7.30517296e+00, -1.81234843e+01,  7.71280282e+01, -9.82419623e-01,
       -5.93424850e-02,  1.77003261e-01]), 'targetState': array([  3.88586304, -18.59838887,  17.        ]), 'previousTarget': array([  6.48324545, -18.27314191,  48.59390655])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.627790677649894
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  3.88586304, -18.59838887,  17.        ]), 'distance': 3.7272340916124547, 'localFrame': array([[ 0.41019702,  0.56316821, -0.7173423 ],
       [-0.80831248,  0.58875371,  0.        ],
       [ 0.42233794,  0.57983673,  0.69672091]]), 'currentState': array([  4.88347712, -16.40312198,  19.84215471,   0.41019702,
         0.56316821,  -0.7173423 ]), 'targetState': array([  3.88586304, -18.59838887,  17.        ]), 'previousTarget': array([  3.88586304, -18.59838887,  17.        ])}
episode index:18942
target thresh 85.70832940623222
target distance 67.45612530037295
model initialize at round 18942
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.23661787, -9.15688622, 21.52095136]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.73776402,  0.66811557, -0.09657037],
       [-0.6712529 ,  0.7412284 ,  0.        ],
       [ 0.0715807 ,  0.06482314,  0.99532616]]), 'currentState': array([ 17.52523761, -33.71677527,  10.54634495,   0.73776402,
         0.66811557,  -0.09657037]), 'targetState': array([32.85344326, 32.19706921, 40.        ]), 'previousTarget': array([ 22.55668513, -10.71491594,  20.91560183])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6277894748514717
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([32.85344326, 32.19706921, 40.        ]), 'distance': 2.2292607804286746, 'localFrame': array([[-0.22578249,  0.6506815 ,  0.72500748],
       [-0.9447405 , -0.32781915,  0.        ],
       [ 0.23767134, -0.68494393,  0.68874099]]), 'currentState': array([33.5775505 , 30.32257551, 39.03483385, -0.22578249,  0.6506815 ,
        0.72500748]), 'targetState': array([32.85344326, 32.19706921, 40.        ]), 'previousTarget': array([32.85344326, 32.19706921, 40.        ])}
episode index:18943
target thresh 85.70975850183564
target distance 40.78602297228787
model initialize at round 18943
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.92090793, -24.16357815,  56.3720526 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.6979086 ,  0.25082438,  0.67082838],
       [-0.33821483, -0.94106893,  0.        ],
       [ 0.63129575, -0.22688411,  0.74161262]]), 'currentState': array([ 21.54625629, -29.27346993,  50.92887116,  -0.6979086 ,
         0.25082438,   0.67082838]), 'targetState': array([-17.69915469, -21.69654174,  59.        ]), 'previousTarget': array([ -3.37497489, -24.23119634,  55.83917164])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6277969840190686
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-17.69915469, -21.69654174,  59.        ]), 'distance': 3.703141768996988, 'localFrame': array([[-0.64315171,  0.7649344 ,  0.03508906],
       [-0.76540575, -0.64354801,  0.        ],
       [ 0.02258149, -0.02685737,  0.99938419]]), 'currentState': array([-1.54351091e+01, -2.36190960e+01,  5.67884074e+01, -6.43151707e-01,
        7.64934403e-01,  3.50890589e-02]), 'targetState': array([-17.69915469, -21.69654174,  59.        ]), 'previousTarget': array([-17.69915469, -21.69654174,  59.        ])}
episode index:18944
target thresh 85.71118745453663
target distance 16.755479051584427
model initialize at round 18944
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.00095469,  13.7470326 ,  41.        ]), 'distance': 20.239498935573547, 'localFrame': array([[-0.68476686, -0.50388887,  0.52648871],
       [ 0.59268344, -0.80543549,  0.        ],
       [ 0.42405269,  0.31204114,  0.85018212]]), 'currentState': array([-15.63964085,  -4.52834107,  34.37782114,  -0.68476686,
        -0.50388887,   0.52648871]), 'targetState': array([-10.00095469,  13.7470326 ,  41.        ]), 'previousTarget': array([-10.00095469,  13.7470326 ,  41.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6278106334193165
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.00095469,  13.7470326 ,  41.        ]), 'distance': 2.313733541480407, 'localFrame': array([[ 0.37890866,  0.77210406, -0.51017993],
       [-0.89772475,  0.44055678,  0.        ],
       [ 0.22476323,  0.45800115,  0.8600677 ]]), 'currentState': array([-8.93017523, 11.69693496, 41.06240118,  0.37890866,  0.77210406,
       -0.51017993]), 'targetState': array([-10.00095469,  13.7470326 ,  41.        ]), 'previousTarget': array([-10.00095469,  13.7470326 ,  41.        ])}
episode index:18945
target thresh 85.71261626434949
target distance 11.0
model initialize at round 18945
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.82869387,  3.21575858, 63.        ]), 'distance': 10.599007867447975, 'localFrame': array([[ 0.42079748,  0.38940257,  0.81932601],
       [-0.67919696,  0.73395605,  0.        ],
       [-0.60134929, -0.55648374,  0.5733279 ]]), 'currentState': array([ 0.25930044,  1.86412124, 53.11205407,  0.42079748,  0.38940257,
        0.81932601]), 'targetState': array([ 3.82869387,  3.21575858, 63.        ]), 'previousTarget': array([ 3.82869387,  3.21575858, 63.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6278276913427031
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.82869387,  3.21575858, 63.        ]), 'distance': 2.2067593950355735, 'localFrame': array([[ 0.05387677, -0.10217126,  0.99330676],
       [ 0.8845525 ,  0.46644064,  0.        ],
       [-0.46331864,  0.87863198,  0.11550616]]), 'currentState': array([ 3.20488908e+00,  1.77900991e+00,  6.14455201e+01,  5.38767654e-02,
       -1.02171260e-01,  9.93306764e-01]), 'targetState': array([ 3.82869387,  3.21575858, 63.        ]), 'previousTarget': array([ 3.82869387,  3.21575858, 63.        ])}
episode index:18946
target thresh 85.71404493128853
target distance 43.0
model initialize at round 18946
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.99966164,  0.89225285, 77.52728032]), 'distance': 27.5, 'localFrame': array([[ 0.35976598, -0.84607569,  0.39335019],
       [ 0.92025893,  0.39130998,  0.        ],
       [-0.15392185,  0.36198402,  0.91938873]]), 'currentState': array([-10.66686524,   3.92049187,  51.0201461 ,   0.35976598,
        -0.84607569,   0.39335019]), 'targetState': array([ 0.14363711, -0.98963043, 94.        ]), 'previousTarget': array([-4.54254006,  1.46447221, 77.22664804])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6278360225006417
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.14363711, -0.98963043, 94.        ]), 'distance': 2.994410689070671, 'localFrame': array([[ 0.78478774, -0.11560528,  0.6088872 ],
       [ 0.14573499,  0.98932366,  0.        ],
       [-0.60238652,  0.08873617,  0.79325682]]), 'currentState': array([-0.22771607,  0.63548437, 91.51251249,  0.78478774, -0.11560528,
        0.6088872 ]), 'targetState': array([ 0.14363711, -0.98963043, 94.        ]), 'previousTarget': array([ 0.14363711, -0.98963043, 94.        ])}
episode index:18947
target thresh 85.71547345536801
target distance 78.0
model initialize at round 18947
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.14659585, -10.06147626,  66.99395843]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.66162462, -0.64558807, -0.38140386],
       [ 0.69837962,  0.71572753,  0.        ],
       [ 0.27298124, -0.26636468,  0.92440851]]), 'currentState': array([-47.38875541,   0.51472872,  85.62462729,   0.66162462,
        -0.64558807,  -0.38140386]), 'targetState': array([ 23.52518009, -42.9833212 ,   9.        ]), 'previousTarget': array([-30.84896234,  -9.0817456 ,  68.3183514 ])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6278274751474999
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 23.52518009, -42.9833212 ,   9.        ]), 'distance': 2.32252448856539, 'localFrame': array([[ 0.82260609,  0.18238483, -0.53856754],
       [-0.21645935,  0.97629163,  0.        ],
       [ 0.52579898,  0.11657798,  0.84258234]]), 'currentState': array([ 21.61292808, -42.78060691,  10.30242819,   0.82260609,
         0.18238483,  -0.53856754]), 'targetState': array([ 23.52518009, -42.9833212 ,   9.        ]), 'previousTarget': array([ 23.52518009, -42.9833212 ,   9.        ])}
episode index:18948
target thresh 85.7169018366022
target distance 24.764516060357465
model initialize at round 18948
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.98760617, -4.39508554, 83.88710653]), 'distance': 27.5, 'localFrame': array([[-0.23137144,  0.73240024,  0.64035705],
       [-0.95355007, -0.30123455,  0.        ],
       [ 0.19289767, -0.61061251,  0.76807737]]), 'currentState': array([ -4.29129974, -24.76491526,  65.71004279,  -0.23137144,
         0.73240024,   0.64035705]), 'targetState': array([-0.42183569, -0.90667229, 87.        ]), 'previousTarget': array([-1.12916569, -5.64082059, 82.60316823])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278383826511309
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.42183569, -0.90667229, 87.        ]), 'distance': 1.80886901554929, 'localFrame': array([[-0.06543254,  0.77652733,  0.62667686],
       [-0.99646867, -0.08396546,  0.        ],
       [ 0.05261921, -0.62446385,  0.77927923]]), 'currentState': array([-9.95207904e-01, -2.31323474e+00,  8.79822594e+01, -6.54325422e-02,
        7.76527333e-01,  6.26676857e-01]), 'targetState': array([-0.42183569, -0.90667229, 87.        ]), 'previousTarget': array([-0.42183569, -0.90667229, 87.        ])}
episode index:18949
target thresh 85.71833007500544
target distance 21.226803678374413
model initialize at round 18949
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.25912843, -21.09083477,  71.        ]), 'distance': 23.68903630239485, 'localFrame': array([[ 0.41148077, -0.07866035,  0.90801769],
       [ 0.18776409,  0.98221416,  0.        ],
       [-0.89186783,  0.17049311,  0.41893182]]), 'currentState': array([ 1.81264779,  0.89849784, 67.4664294 ,  0.41148077, -0.07866035,
        0.90801769]), 'targetState': array([ -6.25912843, -21.09083477,  71.        ]), 'previousTarget': array([ -6.25912843, -21.09083477,  71.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.627851095444277
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.25912843, -21.09083477,  71.        ]), 'distance': 2.7901477044415635, 'localFrame': array([[ 0.3077413 , -0.70524204,  0.63869316],
       [ 0.9165397 ,  0.39994372,  0.        ],
       [-0.25544132,  0.58538764,  0.76946153]]), 'currentState': array([ -4.9768074 , -19.66960819,  68.97005124,   0.3077413 ,
        -0.70524204,   0.63869316]), 'targetState': array([ -6.25912843, -21.09083477,  71.        ]), 'previousTarget': array([ -6.25912843, -21.09083477,  71.        ])}
episode index:18950
target thresh 85.71975817059196
target distance 43.99759864627101
model initialize at round 18950
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.92614912,   3.61749617,  74.23626112]), 'distance': 27.499999999999996, 'localFrame': array([[-0.91734324,  0.03161218,  0.39684008],
       [-0.03444014, -0.99940676,  0.        ],
       [ 0.39660466, -0.01366723,  0.91788777]]), 'currentState': array([ 5.94958113e+00,  1.66085731e+01,  7.84113570e+01, -9.17343240e-01,
        3.16121824e-02,  3.96840081e-01]), 'targetState': array([-36.43305301,  -6.45233663,  71.        ]), 'previousTarget': array([-16.37171152,   4.43680377,  74.19175125])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278566071467839
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-36.43305301,  -6.45233663,  71.        ]), 'distance': 3.714665831545563, 'localFrame': array([[-0.74879356,  0.36647253, -0.55227356],
       [-0.43959316, -0.898197  ,  0.        ],
       [-0.49605045,  0.24277568,  0.83366295]]), 'currentState': array([-33.71080508,  -5.97924521,  73.48279942,  -0.74879356,
         0.36647253,  -0.55227356]), 'targetState': array([-36.43305301,  -6.45233663,  71.        ]), 'previousTarget': array([-36.43305301,  -6.45233663,  71.        ])}
episode index:18951
target thresh 85.72118612337609
target distance 33.45123398730861
model initialize at round 18951
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.14790887, -6.98506699, 70.5234075 ]), 'distance': 27.500000000000007, 'localFrame': array([[-0.74313392,  0.20260758, -0.63773204],
       [-0.26303849, -0.96478534,  0.        ],
       [-0.61527452,  0.16774807,  0.7702583 ]]), 'currentState': array([  5.59605754, -32.60518694,  75.36102108,  -0.74313392,
         0.20260758,  -0.63773204]), 'targetState': array([-5.90146158,  1.08293641, 69.        ]), 'previousTarget': array([-2.85241559, -7.19082309, 70.97870357])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6278662035619947
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.90146158,  1.08293641, 69.        ]), 'distance': 2.5030448455386676, 'localFrame': array([[ 0.21759818,  0.94793377,  0.23253516],
       [-0.97465091,  0.2237311 ,  0.        ],
       [-0.05202535, -0.2266406 ,  0.97258799]]), 'currentState': array([-5.97814407, -0.96928824, 67.56901178,  0.21759818,  0.94793377,
        0.23253516]), 'targetState': array([-5.90146158,  1.08293641, 69.        ]), 'previousTarget': array([-5.90146158,  1.08293641, 69.        ])}
episode index:18952
target thresh 85.72261393337206
target distance 64.0
model initialize at round 18952
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.48321214, -10.63522661,  45.06502263]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.41227464, -0.89785298,  0.15456277],
       [ 0.90877374,  0.41728921,  0.        ],
       [-0.06449738,  0.14046259,  0.98798297]]), 'currentState': array([-18.65002048, -15.35533891,  20.38156802,   0.41227464,
        -0.89785298,   0.15456277]), 'targetState': array([10.58339342, -2.99863032, 85.        ]), 'previousTarget': array([-7.99184435, -9.69958547, 45.58668417])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627833076025269
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([10.58339342, -2.99863032, 85.        ]), 'distance': 16.503447080455715, 'localFrame': array([[-0.46433579,  0.72144113,  0.51372655],
       [-0.84088558, -0.54121294,  0.        ],
       [ 0.27803545, -0.43198525,  0.85795398]]), 'currentState': array([  9.76208064, -12.55325002,  71.56875086,  -0.46433579,
         0.72144113,   0.51372655]), 'targetState': array([10.58339342, -2.99863032, 85.        ]), 'previousTarget': array([10.58339342, -2.99863032, 85.        ])}
episode index:18953
target thresh 85.72404160059418
target distance 30.0
model initialize at round 18953
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.0785024 , 16.64505747, 75.49134778]), 'distance': 27.5, 'localFrame': array([[ 0.45915302,  0.52692209,  0.71521438],
       [-0.75392505,  0.65696044,  0.        ],
       [-0.46986755, -0.53921804,  0.69890514]]), 'currentState': array([ 2.89465602,  1.66656095, 52.53213488,  0.45915302,  0.52692209,
        0.71521438]), 'targetState': array([ 5.60247725, 20.23887963, 81.        ]), 'previousTarget': array([ 4.97490509, 15.64996082, 73.96699057])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278439803560396
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.60247725, 20.23887963, 81.        ]), 'distance': 1.9897597091508783, 'localFrame': array([[ 0.41786824,  0.88950562,  0.18484018],
       [-0.9051018 ,  0.42519494,  0.        ],
       [-0.07859311, -0.16729918,  0.98276859]]), 'currentState': array([ 5.22221788, 19.91624566, 79.07374617,  0.41786824,  0.88950562,
        0.18484018]), 'targetState': array([ 5.60247725, 20.23887963, 81.        ]), 'previousTarget': array([ 5.60247725, 20.23887963, 81.        ])}
episode index:18954
target thresh 85.72546912505669
target distance 57.0
model initialize at round 18954
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.83270043,   4.39126469,  35.79545848]), 'distance': 27.5, 'localFrame': array([[ 0.28535004,  0.89362999,  0.34641131],
       [-0.9526132 ,  0.3041843 ,  0.        ],
       [-0.10537288, -0.32999599,  0.93808273]]), 'currentState': array([-32.39322629, -11.39944281,  15.91132284,   0.28535004,
         0.89362999,   0.34641131]), 'targetState': array([-2.07325351, 33.93672966, 73.        ]), 'previousTarget': array([-21.97192924,   2.99118575,  35.60347611])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6278437525224619
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.07325351, 33.93672966, 73.        ]), 'distance': 3.5128727395494677, 'localFrame': array([[ 0.61651113,  0.54687466,  0.56642929],
       [-0.66359397,  0.74809294,  0.        ],
       [-0.42374175, -0.37587906,  0.82411034]]), 'currentState': array([-2.0524599 , 31.66905614, 70.31718444,  0.61651113,  0.54687466,
        0.56642929]), 'targetState': array([-2.07325351, 33.93672966, 73.        ]), 'previousTarget': array([-2.07325351, 33.93672966, 73.        ])}
episode index:18955
target thresh 85.72689650677393
target distance 27.0
model initialize at round 18955
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.36806296, -8.21069117, 54.14674973]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.7544636 , -0.43957115, -0.48740319],
       [ 0.50341585,  0.86404426,  0.        ],
       [ 0.42113793, -0.24536649,  0.87317703]]), 'currentState': array([19.9996578 , -0.10345919, 79.64118453,  0.7544636 , -0.43957115,
       -0.48740319]), 'targetState': array([26.65451633, -8.57535766, 53.        ]), 'previousTarget': array([26.06409954, -7.81399265, 55.0694548 ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.627853347590815
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([26.65451633, -8.57535766, 53.        ]), 'distance': 2.6779472630851964, 'localFrame': array([[ 0.75657601, -0.32800894, -0.56568796],
       [ 0.39777011,  0.91748512,  0.        ],
       [ 0.51901029, -0.22501376,  0.82461939]]), 'currentState': array([26.79767999, -8.56744833, 55.67410605,  0.75657601, -0.32800894,
       -0.56568796]), 'targetState': array([26.65451633, -8.57535766, 53.        ]), 'previousTarget': array([26.65451633, -8.57535766, 53.        ])}
episode index:18956
target thresh 85.7283237457601
target distance 25.0
model initialize at round 18956
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.68723474, -28.20948659,  55.18506679]), 'distance': 27.5, 'localFrame': array([[ 0.73643751,  0.50428645, -0.45094897],
       [-0.56499535,  0.82509409,  0.        ],
       [ 0.37207533,  0.25478407,  0.89254973]]), 'currentState': array([ -5.11212095, -44.92083524,  72.88131838,   0.73643751,
         0.50428645,  -0.45094897]), 'targetState': array([ 12.88405426, -21.42431203,  48.        ]), 'previousTarget': array([  6.74775905, -28.9092394 ,  55.77615202])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6278620916397707
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.88405426, -21.42431203,  48.        ]), 'distance': 3.168616832574402, 'localFrame': array([[ 0.72857762, -0.43846049, -0.5262386 ],
       [ 0.51563146,  0.85681048,  0.        ],
       [ 0.45088674, -0.27134518,  0.85033696]]), 'currentState': array([ 12.94663278, -22.96647494,  50.76730015,   0.72857762,
        -0.43846049,  -0.5262386 ]), 'targetState': array([ 12.88405426, -21.42431203,  48.        ]), 'previousTarget': array([ 12.88405426, -21.42431203,  48.        ])}
episode index:18957
target thresh 85.72975084202952
target distance 56.15969044342189
model initialize at round 18957
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.25183238, -14.02765874,  59.15352584]), 'distance': 27.5, 'localFrame': array([[ 0.02346491,  0.80833601, -0.58825359],
       [-0.99957893,  0.02901643,  0.        ],
       [ 0.01706902,  0.5880059 ,  0.80867652]]), 'currentState': array([-2.25025415e+01, -1.98140609e+01,  5.65132557e+01,  2.34649053e-02,
        8.08336014e-01, -5.88253590e-01]), 'targetState': array([33.09571225, -7.78934084, 62.        ]), 'previousTarget': array([  3.67117871, -14.56092513,  59.90422395])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6278668320364875
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.09571225, -7.78934084, 62.        ]), 'distance': 2.3480321927932053, 'localFrame': array([[ 0.9874236 , -0.14574625,  0.06125893],
       [ 0.14602049,  0.98928157,  0.        ],
       [-0.06060233,  0.00894506,  0.99812191]]), 'currentState': array([31.20135505, -7.22056188, 60.73463189,  0.9874236 , -0.14574625,
        0.06125893]), 'targetState': array([33.09571225, -7.78934084, 62.        ]), 'previousTarget': array([33.09571225, -7.78934084, 62.        ])}
episode index:18958
target thresh 85.73117779559645
target distance 15.0
model initialize at round 18958
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.45059533, 12.1022649 , 75.        ]), 'distance': 14.37608404291661, 'localFrame': array([[ 0.31991955,  0.5708131 , -0.75619038],
       [-0.87233383,  0.48891072,  0.        ],
       [ 0.36970959,  0.65965045,  0.65435167]]), 'currentState': array([29.46379279, 11.36984588, 89.21556861,  0.31991955,  0.5708131 ,
       -0.75619038]), 'targetState': array([27.45059533, 12.1022649 , 75.        ]), 'previousTarget': array([27.45059533, 12.1022649 , 75.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278828771082671
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.45059533, 12.1022649 , 75.        ]), 'distance': 2.714088987165914, 'localFrame': array([[ 0.15348737,  0.39933955, -0.90386368],
       [-0.93342763,  0.35876575,  0.        ],
       [ 0.32427533,  0.84369133,  0.42782059]]), 'currentState': array([26.43742799, 13.01919991, 77.34499492,  0.15348737,  0.39933955,
       -0.90386368]), 'targetState': array([27.45059533, 12.1022649 , 75.        ]), 'previousTarget': array([27.45059533, 12.1022649 , 75.        ])}
episode index:18959
target thresh 85.73260460647516
target distance 34.0
model initialize at round 18959
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.6235297 , -18.59124154,  64.23184394]), 'distance': 27.5, 'localFrame': array([[ 0.96164052, -0.24917544,  0.11471321],
       [ 0.25083126,  0.96803082,  0.        ],
       [-0.11104593,  0.02877366,  0.99339865]]), 'currentState': array([-30.70436176, -24.04228908,  85.08325294,   0.96164052,
        -0.24917544,   0.11471321]), 'targetState': array([ -1.96524147, -14.87070361,  50.        ]), 'previousTarget': array([-14.25153251, -18.48461444,  63.82549687])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6278872373588031
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ -1.96524147, -14.87070361,  50.        ]), 'distance': 2.84646782657854, 'localFrame': array([[ 0.81791676, -0.55630684,  0.14674765],
       [ 0.56239535,  0.82686847,  0.        ],
       [-0.121341  ,  0.0825302 ,  0.98917396]]), 'currentState': array([ -4.72969553, -15.4931932 ,  50.26959141,   0.81791676,
        -0.55630684,   0.14674765]), 'targetState': array([ -1.96524147, -14.87070361,  50.        ]), 'previousTarget': array([ -1.96524147, -14.87070361,  50.        ])}
episode index:18960
target thresh 85.73403127467991
target distance 72.0
model initialize at round 18960
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.94418714, 10.43868188, 48.30379977]), 'distance': 27.499999999999996, 'localFrame': array([[-0.83967862,  0.4339557 , -0.32653066],
       [-0.45912171, -0.88837337,  0.        ],
       [-0.29008115,  0.14991732,  0.94518661]]), 'currentState': array([ 2.04904361,  4.77647606, 75.19187737, -0.83967862,  0.4339557 ,
       -0.32653066]), 'targetState': array([-0.91738873, 19.97894887,  3.        ]), 'previousTarget': array([ 1.82564856,  9.70221396, 48.23025047])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6278873394838937
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.91738873, 19.97894887,  3.        ]), 'distance': 2.923273818831829, 'localFrame': array([[-0.12823905, -0.88506067, -0.44746214],
       [ 0.98966546, -0.14339554,  0.        ],
       [-0.06416408, -0.44283782,  0.89430288]]), 'currentState': array([-2.51627087, 19.60538162,  5.41858496, -0.12823905, -0.88506067,
       -0.44746214]), 'targetState': array([-0.91738873, 19.97894887,  3.        ]), 'previousTarget': array([-0.91738873, 19.97894887,  3.        ])}
episode index:18961
target thresh 85.73545780022498
target distance 59.0
model initialize at round 18961
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.29224707,  9.68653346, 44.90289383]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69222428,  0.56706525,  0.44638835],
       [-0.63370621, -0.77357381,  0.        ],
       [ 0.34531433, -0.28287907,  0.89483934]]), 'currentState': array([ 9.23297229,  5.32904907, 19.26452878, -0.69222428,  0.56706525,
        0.44638835]), 'targetState': array([-11.24952368,  15.31170196,  78.        ]), 'previousTarget': array([ 1.41645364,  9.34395413, 44.38580816])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278902224738487
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.24952368,  15.31170196,  78.        ]), 'distance': 1.8095385957951944, 'localFrame': array([[-0.55470635,  0.57760309,  0.59889527],
       [-0.72125793, -0.69266658,  0.        ],
       [ 0.41483474, -0.43195796,  0.80082736]]), 'currentState': array([-10.33558315,  15.15375595,  76.44623177,  -0.55470635,
         0.57760309,   0.59889527]), 'targetState': array([-11.24952368,  15.31170196,  78.        ]), 'previousTarget': array([-11.24952368,  15.31170196,  78.        ])}
episode index:18962
target thresh 85.73688418312462
target distance 51.0
model initialize at round 18962
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.14447143, -22.08319056,  60.75053511]), 'distance': 27.500000000000004, 'localFrame': array([[-0.78479411, -0.53195722, -0.31799328],
       [ 0.56108128, -0.82776071,  0.        ],
       [-0.26322234, -0.17842008,  0.94809297]]), 'currentState': array([  7.49643915, -14.74552367,  82.14610851,  -0.78479411,
        -0.53195722,  -0.31799328]), 'targetState': array([-29.8931505 , -32.28621305,  31.        ]), 'previousTarget': array([ -6.85316548, -21.67497981,  61.0003783 ])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278923888787477
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-29.8931505 , -32.28621305,  31.        ]), 'distance': 3.9346584051014815, 'localFrame': array([[-0.62097985, -0.77354895, -0.12651503],
       [ 0.779815  , -0.62601004,  0.        ],
       [-0.07919968, -0.09865832,  0.99196469]]), 'currentState': array([-27.1230535 , -30.67536909,  33.28326107,  -0.62097985,
        -0.77354895,  -0.12651503]), 'targetState': array([-29.8931505 , -32.28621305,  31.        ]), 'previousTarget': array([-29.8931505 , -32.28621305,  31.        ])}
episode index:18963
target thresh 85.7383104233931
target distance 12.0
model initialize at round 18963
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.3220551 ,  1.97389982, 38.        ]), 'distance': 15.153305569201397, 'localFrame': array([[ 0.51428359, -0.51011898, -0.68941353],
       [ 0.70422628,  0.70997559,  0.        ],
       [ 0.48946678, -0.48550313,  0.72436799]]), 'currentState': array([-1.93420495, 12.67792079, 48.60413022,  0.51428359, -0.51011898,
       -0.68941353]), 'targetState': array([-0.3220551 ,  1.97389982, 38.        ]), 'previousTarget': array([-0.3220551 ,  1.97389982, 38.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.627908428372474
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.3220551 ,  1.97389982, 38.        ]), 'distance': 2.969301542825962, 'localFrame': array([[ 0.07440652, -0.98721652, -0.1409511 ],
       [ 0.99717172,  0.07515685,  0.        ],
       [ 0.01059344, -0.14055245,  0.99001656]]), 'currentState': array([-1.00218305,  4.24037749, 39.79367127,  0.07440652, -0.98721652,
       -0.1409511 ]), 'targetState': array([-0.3220551 ,  1.97389982, 38.        ]), 'previousTarget': array([-0.3220551 ,  1.97389982, 38.        ])}
episode index:18964
target thresh 85.73973652104469
target distance 70.0
model initialize at round 18964
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.16164806, -10.1998793 ,  49.06336293]), 'distance': 27.5, 'localFrame': array([[ 0.44604709, -0.3177354 , -0.83671154],
       [ 0.58018619,  0.81448388,  0.        ],
       [ 0.68148806, -0.48544848,  0.54764386]]), 'currentState': array([18.08113437, -6.09672492, 69.22806014,  0.44604709, -0.3177354 ,
       -0.83671154]), 'targetState': array([-43.64405018, -19.97991201,   1.        ]), 'previousTarget': array([ 0.17478756, -9.76736825, 50.62673192])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278753195705561
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 46, 'trapConfig': [], 'currentTarget': array([-43.64405018, -19.97991201,   1.        ]), 'distance': 9.256440699725138, 'localFrame': array([[-0.24029505,  0.27771508, -0.93012506],
       [-0.75621625, -0.65432177,  0.        ],
       [-0.60860107,  0.70337569,  0.36724294]]), 'currentState': array([-43.54635643, -17.74519501,   9.98210389,  -0.24029505,
         0.27771508,  -0.93012506]), 'targetState': array([-43.64405018, -19.97991201,   1.        ]), 'previousTarget': array([-43.64405018, -19.97991201,   1.        ])}
episode index:18965
target thresh 85.74116247609365
target distance 82.0
model initialize at round 18965
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.03878708,  5.35324506, 37.92148111]), 'distance': 27.5, 'localFrame': array([[ 0.46497283, -0.76895885,  0.43875113],
       [ 0.85572165,  0.51743642,  0.        ],
       [-0.22702582,  0.37544884,  0.89860862]]), 'currentState': array([38.76600549, 15.63667254, 13.06790816,  0.46497283, -0.76895885,
        0.43875113]), 'targetState': array([ 19.88570259, -18.26359308,  95.        ]), 'previousTarget': array([32.08509046,  6.22875403, 37.8071256 ])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6278693753439323
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 19.88570259, -18.26359308,  95.        ]), 'distance': 3.259853771940081, 'localFrame': array([[-0.54396995,  0.73784754,  0.39959692],
       [-0.80490327, -0.59340604,  0.        ],
       [ 0.23712323, -0.32163687,  0.91669095]]), 'currentState': array([ 21.16976033, -18.94485221,  92.08217061,  -0.54396995,
         0.73784754,   0.39959692]), 'targetState': array([ 19.88570259, -18.26359308,  95.        ]), 'previousTarget': array([ 19.88570259, -18.26359308,  95.        ])}
episode index:18966
target thresh 85.74258828855423
target distance 35.25244047500381
model initialize at round 18966
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.47267877,  14.33563193,  19.94496793]), 'distance': 27.499999999999996, 'localFrame': array([[-0.89128934,  0.26395963,  0.368685  ],
       [-0.28396362, -0.95883506,  0.        ],
       [ 0.3535081 , -0.10469313,  0.9295544 ]]), 'currentState': array([ -8.99443531, -10.65399089,  19.80829765,  -0.89128934,
         0.26395963,   0.368685  ]), 'targetState': array([-25.09454358,  24.3980303 ,  20.        ]), 'previousTarget': array([-19.62695039,  13.65434533,  20.        ])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278748814331563
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-25.09454358,  24.3980303 ,  20.        ]), 'distance': 3.590920284406368, 'localFrame': array([[-0.47372591,  0.7655386 ,  0.4353555 ],
       [-0.85035409, -0.52621091,  0.        ],
       [ 0.22908881, -0.37020633,  0.90025862]]), 'currentState': array([-25.22465513,  21.77539261,  17.55060191,  -0.47372591,
         0.7655386 ,   0.4353555 ]), 'targetState': array([-25.09454358,  24.3980303 ,  20.        ]), 'previousTarget': array([-25.09454358,  24.3980303 ,  20.        ])}
episode index:18967
target thresh 85.74401395844069
target distance 43.00254925955002
model initialize at round 18967
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.82336301, 11.93278838, 51.33600806]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.40480463, -0.8663183 ,  0.29261888],
       [ 0.90597353,  0.42333433,  0.        ],
       [-0.12387562,  0.26510496,  0.95622915]]), 'currentState': array([-28.32669769,  23.2302092 ,  40.28077351,   0.40480463,
        -0.8663183 ,   0.29261888]), 'targetState': array([13.84804639,  2.05708803, 61.        ]), 'previousTarget': array([-6.92948467, 12.83918297, 50.37026546])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6278784947997007
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.84804639,  2.05708803, 61.        ]), 'distance': 2.2856760538884577, 'localFrame': array([[ 0.80305788,  0.1802567 , -0.56798376],
       [-0.21901336,  0.97572186,  0.        ],
       [ 0.55419417,  0.12439603,  0.82303976]]), 'currentState': array([12.11532993,  0.79405565, 60.20831958,  0.80305788,  0.1802567 ,
       -0.56798376]), 'targetState': array([13.84804639,  2.05708803, 61.        ]), 'previousTarget': array([13.84804639,  2.05708803, 61.        ])}
episode index:18968
target thresh 85.74543948576729
target distance 19.46587474472619
model initialize at round 18968
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.59886628, -23.5972298 ,  51.99709093]), 'distance': 27.500000000000004, 'localFrame': array([[-0.82739799,  0.2357624 ,  0.50973392],
       [-0.2740365 , -0.96171929,  0.        ],
       [ 0.49022094, -0.1396857 ,  0.86033211]]), 'currentState': array([-3.40793512, -4.37266544, 44.53008692, -0.82739799,  0.2357624 ,
        0.50973392]), 'targetState': array([-21.60595329, -23.6047195 ,  52.        ]), 'previousTarget': array([-21.47035394, -23.4793359 ,  51.94427197])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.627891192744662
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.60595329, -23.6047195 ,  52.        ]), 'distance': 2.4880354548086587, 'localFrame': array([[-0.71220299, -0.01629343,  0.70178446],
       [ 0.02287153, -0.99973841,  0.        ],
       [ 0.70160088,  0.01605088,  0.71238934]]), 'currentState': array([-1.98960122e+01, -2.19433779e+01,  5.12884060e+01, -7.12202987e-01,
       -1.62934344e-02,  7.01784461e-01]), 'targetState': array([-21.60595329, -23.6047195 ,  52.        ]), 'previousTarget': array([-21.60595329, -23.6047195 ,  52.        ])}
episode index:18969
target thresh 85.7468648705483
target distance 56.90331629645007
model initialize at round 18969
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.13795011,  32.39901743,  75.40168118]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69232793,  0.38110134,  0.6127347 ],
       [-0.48223055,  0.87604435,  0.        ],
       [-0.53678277, -0.29547939,  0.79028867]]), 'currentState': array([-33.31135021,  29.12569563,  59.46820409,   0.69232793,
         0.38110134,   0.6127347 ]), 'targetState': array([ 23.0936468 ,  37.45241618, 100.        ]), 'previousTarget': array([-11.87246686,  31.74152047,  74.19171905])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6278816846804036
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([ 23.0936468 ,  37.45241618, 100.        ]), 'distance': 1.8822232788915627, 'localFrame': array([[-0.18214721, -0.47659958,  0.86004374],
       [ 0.93410547, -0.35699717,  0.        ],
       [ 0.30703318,  0.80337156,  0.51022031]]), 'currentState': array([23.46482863, 38.75019828, 98.68822635, -0.18214721, -0.47659958,
        0.86004374]), 'targetState': array([ 23.0936468 ,  37.45241618, 100.        ]), 'previousTarget': array([ 23.0936468 ,  37.45241618, 100.        ])}
episode index:18970
target thresh 85.74829011279793
target distance 79.0
model initialize at round 18970
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.83851233, -8.76137393, 67.91165756]), 'distance': 27.5, 'localFrame': array([[-0.82271563, -0.39687888, -0.40697193],
       [ 0.43448787, -0.90067769,  0.        ],
       [-0.36655054, -0.17682436,  0.91344067]]), 'currentState': array([ 33.90261129, -23.1241782 ,  89.09357829,  -0.82271563,
        -0.39687888,  -0.40697193]), 'targetState': array([-3.20174343, 29.82865802, 11.        ]), 'previousTarget': array([24.48937006, -7.8058407 , 68.63759283])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6278749351861871
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-3.20174343, 29.82865802, 11.        ]), 'distance': 1.8790719013180104, 'localFrame': array([[-0.24910839,  0.94172177, -0.22606442],
       [-0.96674861, -0.25572861,  0.        ],
       [-0.05781114,  0.21854746,  0.97411235]]), 'currentState': array([-3.05911065, 27.98786356, 10.65065367, -0.24910839,  0.94172177,
       -0.22606442]), 'targetState': array([-3.20174343, 29.82865802, 11.        ]), 'previousTarget': array([-3.20174343, 29.82865802, 11.        ])}
episode index:18971
target thresh 85.74971521253048
target distance 32.40652126379071
model initialize at round 18971
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.40847773,  -8.6086361 ,  45.19855568]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.67680239, -0.06384971,  0.73339057],
       [ 0.09392322,  0.99557944,  0.        ],
       [-0.73014858,  0.0688824 ,  0.67980752]]), 'currentState': array([-35.14914789,  -3.95074333,  30.45352412,   0.67680239,
        -0.06384971,   0.73339057]), 'targetState': array([ -3.46114307, -10.44128769,  51.        ]), 'previousTarget': array([-13.50636345,  -8.16069625,  44.18054393])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278828387506091
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.46114307, -10.44128769,  51.        ]), 'distance': 2.717224630705885, 'localFrame': array([[ 0.76847215, -0.13867762, -0.62467518],
       [ 0.17759039,  0.98410449,  0.        ],
       [ 0.61474565, -0.11093631,  0.7808847 ]]), 'currentState': array([-5.88261114, -9.35695516, 51.58653649,  0.76847215, -0.13867762,
       -0.62467518]), 'targetState': array([ -3.46114307, -10.44128769,  51.        ]), 'previousTarget': array([ -3.46114307, -10.44128769,  51.        ])}
episode index:18972
target thresh 85.75114016976018
target distance 37.0
model initialize at round 18972
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.83140823,  -6.79403355,  17.66818566]), 'distance': 27.5, 'localFrame': array([[-0.47198687, -0.70129431, -0.53424216],
       [ 0.82960865, -0.55834531,  0.        ],
       [-0.29829161, -0.44321192,  0.84533148]]), 'currentState': array([-3.11046566, -7.24913327, 41.49629131, -0.47198687, -0.70129431,
       -0.53424216]), 'targetState': array([-24.12613197,  -6.5520803 ,   5.        ]), 'previousTarget': array([-16.21995225,  -6.20116249,  18.34602992])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278860836906354
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([-24.12613197,  -6.5520803 ,   5.        ]), 'distance': 3.048452151148268, 'localFrame': array([[-0.86967089,  0.15976033, -0.46706442],
       [-0.18067872, -0.98354217,  0.        ],
       [-0.45937756,  0.0843886 ,  0.88422329]]), 'currentState': array([-22.46434676,  -7.11385101,   7.49317952,  -0.86967089,
         0.15976033,  -0.46706442]), 'targetState': array([-24.12613197,  -6.5520803 ,   5.        ]), 'previousTarget': array([-24.12613197,  -6.5520803 ,   5.        ])}
episode index:18973
target thresh 85.75256498450128
target distance 26.0
model initialize at round 18973
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.57811909, 20.53678286,  9.62219419]), 'distance': 27.500000000000004, 'localFrame': array([[-0.50751711,  0.43084455,  0.74618989],
       [-0.64717304, -0.76234314,  0.        ],
       [ 0.56885274, -0.48291398,  0.66573317]]), 'currentState': array([ 4.78931322,  3.49584668, 29.33547361, -0.50751711,  0.43084455,
        0.74618989]), 'targetState': array([16.97633526, 27.12570812,  2.        ]), 'previousTarget': array([14.21925782, 21.18068282,  8.55376596])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278931700525334
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([16.97633526, 27.12570812,  2.        ]), 'distance': 2.728625076860625, 'localFrame': array([[ 0.05039738,  0.9687515 ,  0.2428593 ],
       [-0.99864954,  0.05195277,  0.        ],
       [-0.01261721, -0.24253133,  0.97006152]]), 'currentState': array([14.87719006, 25.4927746 ,  2.6103379 ,  0.05039738,  0.9687515 ,
        0.2428593 ]), 'targetState': array([16.97633526, 27.12570812,  2.        ]), 'previousTarget': array([16.97633526, 27.12570812,  2.        ])}
episode index:18974
target thresh 85.75398965676803
target distance 43.0
model initialize at round 18974
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.06616083, -9.06852235, 73.83424127]), 'distance': 27.500000000000004, 'localFrame': array([[-0.77608922,  0.00871997,  0.63056283],
       [-0.01123507, -0.99993688,  0.        ],
       [ 0.63052303, -0.00708442,  0.77613821]]), 'currentState': array([-3.46329882e+00, -9.01602158e+00,  4.67222346e+01, -7.76089219e-01,
        8.71996853e-03,  6.30562833e-01]), 'targetState': array([-10.64088337,  -9.09788992,  89.        ]), 'previousTarget': array([-7.40927053, -9.36061124, 72.95515476])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6279019037080587
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.64088337,  -9.09788992,  89.        ]), 'distance': 2.9887777219697442, 'localFrame': array([[-0.4759804 ,  0.20226528,  0.85588049],
       [-0.39109741, -0.92034929,  0.        ],
       [ 0.787709  , -0.33473265,  0.51717365]]), 'currentState': array([-8.84051703, -8.28455702, 86.75724213, -0.4759804 ,  0.20226528,
        0.85588049]), 'targetState': array([-10.64088337,  -9.09788992,  89.        ]), 'previousTarget': array([-10.64088337,  -9.09788992,  89.        ])}
episode index:18975
target thresh 85.75541418657467
target distance 16.604657217223902
model initialize at round 18975
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.21302552, 12.98321433, 21.        ]), 'distance': 18.406605384336498, 'localFrame': array([[ 0.04457465, -0.80689501, -0.58901064],
       [ 0.99847763,  0.05515809,  0.        ],
       [ 0.0324887 , -0.58811395,  0.80812528]]), 'currentState': array([15.42050635, 28.05039146, 10.42927406,  0.04457465, -0.80689501,
       -0.58901064]), 'targetState': array([15.21302552, 12.98321433, 21.        ]), 'previousTarget': array([15.21302552, 12.98321433, 21.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6279127917697019
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.21302552, 12.98321433, 21.        ]), 'distance': 2.9688173225344303, 'localFrame': array([[ 0.8414229 ,  0.50353258, -0.19611848],
       [-0.51350473,  0.85808676,  0.        ],
       [ 0.16828667,  0.10070776,  0.98058021]]), 'currentState': array([14.05789534, 10.4923833 , 19.87070318,  0.8414229 ,  0.50353258,
       -0.19611848]), 'targetState': array([15.21302552, 12.98321433, 21.        ]), 'previousTarget': array([15.21302552, 12.98321433, 21.        ])}
episode index:18976
target thresh 85.75683857393545
target distance 44.0
model initialize at round 18976
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.91539619, 17.96037493, 37.50179587]), 'distance': 27.5, 'localFrame': array([[ 0.29340536,  0.40464937, -0.8661248 ],
       [-0.80957758,  0.5870129 ,  0.        ],
       [ 0.50842643,  0.70119522,  0.49982779]]), 'currentState': array([-4.70574808, -0.75729857, 57.63321516,  0.29340536,  0.40464937,
       -0.8661248 ]), 'targetState': array([-3.03198422, 38.88196332, 15.        ]), 'previousTarget': array([-4.58794862, 17.50722264, 38.62998961])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6279167721092302
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.03198422, 38.88196332, 15.        ]), 'distance': 3.402619606034914, 'localFrame': array([[-0.55822513,  0.19782747, -0.80575989],
       [-0.33403128, -0.94256199,  0.        ],
       [-0.75947864,  0.26914901,  0.59224235]]), 'currentState': array([-3.17872622, 36.89292686, 17.75681355, -0.55822513,  0.19782747,
       -0.80575989]), 'targetState': array([-3.03198422, 38.88196332, 15.        ]), 'previousTarget': array([-3.03198422, 38.88196332, 15.        ])}
episode index:18977
target thresh 85.75826281886462
target distance 49.0
model initialize at round 18977
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.36955768, -4.37814376, 29.46588969]), 'distance': 27.5, 'localFrame': array([[-0.06263823,  0.53872962,  0.84014692],
       [-0.99330839, -0.11549222,  0.        ],
       [ 0.09703043, -0.83452498,  0.54235888]]), 'currentState': array([ 12.52399619, -10.91924701,  52.88501688,  -0.06263823,
         0.53872962,   0.84014692]), 'targetState': array([39.88629055,  3.01393859,  3.        ]), 'previousTarget': array([25.71967271, -5.14476002, 28.77674097])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6279178884468981
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([39.88629055,  3.01393859,  3.        ]), 'distance': 3.234660824565577, 'localFrame': array([[ 0.42774251,  0.25447375, -0.86734045],
       [-0.51128373,  0.85941198,  0.        ],
       [ 0.74540277,  0.44345706,  0.49771532]]), 'currentState': array([41.68404157,  1.20716163,  4.99165232,  0.42774251,  0.25447375,
       -0.86734045]), 'targetState': array([39.88629055,  3.01393859,  3.        ]), 'previousTarget': array([39.88629055,  3.01393859,  3.        ])}
episode index:18978
target thresh 85.75968692137643
target distance 36.663734236372356
model initialize at round 18978
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-0.05808238,  5.2205027 , 37.53941463]), 'distance': 27.500000000000004, 'localFrame': array([[-0.81630821, -0.57739114, -0.01613642],
       [ 0.57746633, -0.8164145 ,  0.        ],
       [-0.01317401, -0.00931824,  0.9998698 ]]), 'currentState': array([ 7.16821051e+00,  2.60310726e+01,  5.40000000e+01, -8.16308207e-01,
       -5.77391139e-01, -1.61364191e-02]), 'targetState': array([ -5.56295846, -10.63266162,  25.        ]), 'previousTarget': array([-0.05808238,  5.2205027 , 37.53941463])}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6279136331517795
{'scaleFactor': 20, 'timeStep': 61, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ -5.56295846, -10.63266162,  25.        ]), 'distance': 2.4733329059055205, 'localFrame': array([[ 0.50125863,  0.53071864, -0.6834307 ],
       [-0.72699649,  0.68664117,  0.        ],
       [ 0.46927165,  0.49685172,  0.7300154 ]]), 'currentState': array([ -6.28329621, -12.91752496,  25.61472654,   0.50125863,
         0.53071864,  -0.6834307 ]), 'targetState': array([ -5.56295846, -10.63266162,  25.        ]), 'previousTarget': array([ -5.56295846, -10.63266162,  25.        ])}
episode index:18979
target thresh 85.76111088148511
target distance 81.0
model initialize at round 18979
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.76638953, 15.10467755, 71.30586844]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.04505936, -0.1134167 , -0.99252522],
       [ 0.9293425 ,  0.36921879,  0.        ],
       [ 0.36645896, -0.92239587,  0.12203972]]), 'currentState': array([ 3.02181120e+01,  1.28132288e+01,  9.83462351e+01,  4.50593571e-02,
       -1.13416697e-01, -9.92525217e-01]), 'targetState': array([17.15514494, 19.53716976, 19.        ]), 'previousTarget': array([25.39331885, 14.73701448, 72.92011322])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279124262199557
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.15514494, 19.53716976, 19.        ]), 'distance': 3.1734210234471987, 'localFrame': array([[-0.10536802, -0.99284365, -0.05620564],
       [ 0.9944156 , -0.10553485,  0.        ],
       [-0.00593165, -0.05589177,  0.99841921]]), 'currentState': array([18.91800125, 18.80003651, 21.53368767, -0.10536802, -0.99284365,
       -0.05620564]), 'targetState': array([17.15514494, 19.53716976, 19.        ]), 'previousTarget': array([17.15514494, 19.53716976, 19.        ])}
episode index:18980
target thresh 85.76253469920488
target distance 61.3648203579955
model initialize at round 18980
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.11828528,  10.55463654,  33.46515551]), 'distance': 27.5, 'localFrame': array([[ 0.83781708, -0.13339936, -0.52940263],
       [ 0.15724182,  0.98756013,  0.        ],
       [ 0.52281693, -0.08324423,  0.84837071]]), 'currentState': array([-29.95632966,   8.19794632,  14.56679562,   0.83781708,
        -0.13339936,  -0.52940263]), 'targetState': array([30.33263004, 15.36006364, 72.        ]), 'previousTarget': array([-10.8024622 ,  10.29928399,  34.46114384])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278793451164195
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([20.06302504,  5.50886859, 52.72595594]), 'distance': 27.5, 'localFrame': array([[-0.31130861,  0.77425559,  0.55101292],
       [-0.92781143, -0.37304953,  0.        ],
       [ 0.20555511, -0.51123609,  0.83449671]]), 'currentState': array([ 8.27528263, -5.79861107, 30.60266431, -0.31130861,  0.77425559,
        0.55101292]), 'targetState': array([30.33263004, 15.36006364, 72.        ]), 'previousTarget': array([19.62323493,  4.6207566 , 51.31949623])}
episode index:18981
target thresh 85.76395837455001
target distance 52.60652800980611
model initialize at round 18981
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.84906602, -10.12124891,  38.68694878]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.82402358, -0.47019797,  0.31606805],
       [ 0.49560438,  0.86854839,  0.        ],
       [-0.27452039,  0.15664471,  0.94873652]]), 'currentState': array([-23.60506074,   1.4956558 ,  33.86064486,   0.82402358,
        -0.47019797,   0.31606805]), 'targetState': array([ 27.76946517, -22.90975347,  44.        ]), 'previousTarget': array([-0.49126596, -9.07157695, 38.62790414])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278818659100105
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 27.76946517, -22.90975347,  44.        ]), 'distance': 1.959345948588094, 'localFrame': array([[ 0.90433791, -0.41929581,  0.07977449],
       [ 0.4206364 ,  0.90722931,  0.        ],
       [-0.07237376,  0.03355606,  0.99681294]]), 'currentState': array([ 25.87317475, -22.46683549,  43.78333703,   0.90433791,
        -0.41929581,   0.07977449]), 'targetState': array([ 27.76946517, -22.90975347,  44.        ]), 'previousTarget': array([ 27.76946517, -22.90975347,  44.        ])}
episode index:18982
target thresh 85.76538190753472
target distance 45.0
model initialize at round 18982
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.87630885, -17.51669013,  36.44786176]), 'distance': 27.499999999999996, 'localFrame': array([[-0.36579298, -0.7412396 , -0.56281379],
       [ 0.89675078, -0.44253591,  0.        ],
       [-0.24906531, -0.5047037 ,  0.82658372]]), 'currentState': array([ 3.5690685 , -2.91529148, 59.51539997, -0.36579298, -0.7412396 ,
       -0.56281379]), 'targetState': array([  9.8079608 , -30.45987369,  16.        ]), 'previousTarget': array([  6.75304614, -16.50502843,  37.9510301 ])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6278836780669442
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  9.8079608 , -30.45987369,  16.        ]), 'distance': 2.756010308997672, 'localFrame': array([[ 0.59249791,  0.39480423, -0.7021936 ],
       [-0.55451118,  0.83217627,  0.        ],
       [ 0.58434885,  0.3893742 ,  0.71198606]]), 'currentState': array([  7.82890662, -32.35466318,  16.29784249,   0.59249791,
         0.39480423,  -0.7021936 ]), 'targetState': array([  9.8079608 , -30.45987369,  16.        ]), 'previousTarget': array([  9.8079608 , -30.45987369,  16.        ])}
episode index:18983
target thresh 85.76680529817324
target distance 36.38331154818612
model initialize at round 18983
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.291362  ,  -6.25414782,  32.30046167]), 'distance': 27.5, 'localFrame': array([[ 0.56587917,  0.76829424, -0.29917342],
       [-0.80517212,  0.59304119,  0.        ],
       [ 0.17742216,  0.24088609,  0.95419876]]), 'currentState': array([ -2.89005347, -19.70182132,  37.57197347,   0.56587917,
         0.76829424,  -0.29917342]), 'targetState': array([-40.94277982,   2.16535928,  29.        ]), 'previousTarget': array([-27.40298484,  -6.26904362,  32.34928707])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278903592515805
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.94277982,   2.16535928,  29.        ]), 'distance': 3.990311815042019, 'localFrame': array([[-0.79172601, -0.24646512, -0.55894979],
       [ 0.29723188, -0.95480532,  0.        ],
       [-0.53368824, -0.1661377 ,  0.8292015 ]]), 'currentState': array([-38.24461601,  -0.12503743,  30.84298218,  -0.79172601,
        -0.24646512,  -0.55894979]), 'targetState': array([-40.94277982,   2.16535928,  29.        ]), 'previousTarget': array([-40.94277982,   2.16535928,  29.        ])}
episode index:18984
target thresh 85.76822854647983
target distance 81.0
model initialize at round 18984
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.31391152, -25.40227499,  36.59426993]), 'distance': 27.5, 'localFrame': array([[-0.33834142,  0.6543668 ,  0.67626117],
       [-0.88828642, -0.45928993,  0.        ],
       [ 0.31059995, -0.60071361,  0.73666195]]), 'currentState': array([-23.12855363, -38.12257432,  12.91656054,  -0.33834142,
         0.6543668 ,   0.67626117]), 'targetState': array([-3.46210146,  4.9003932 , 93.        ]), 'previousTarget': array([-17.48666948, -26.7281102 ,  35.55702457])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278572862803269
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 21, 'trapConfig': [], 'currentTarget': array([-3.37380025,  2.45417534, 86.5025546 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.48247789,  0.51120182,  0.71125789],
       [-0.72724308,  0.68638   ,  0.        ],
       [-0.48819319, -0.51725738,  0.70293116]]), 'currentState': array([-3.02406678, -7.23452839, 60.76820583,  0.48247789,  0.51120182,
        0.71125789]), 'targetState': array([-3.46210146,  4.9003932 , 93.        ]), 'previousTarget': array([-3.43654859,  1.88593005, 84.79140737])}
episode index:18985
target thresh 85.7696516524687
target distance 78.0
model initialize at round 18985
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.08253542, 21.45593883, 22.73773593]), 'distance': 27.5, 'localFrame': array([[-0.18549927, -0.78991408,  0.58448762],
       [ 0.97351678, -0.22861557,  0.        ],
       [ 0.13362297,  0.5690085 ,  0.81140263]]), 'currentState': array([ 2.0482499 , 36.45604106,  0.51926604, -0.18549927, -0.78991408,
        0.58448762]), 'targetState': array([-19.33115872, -15.85264339,  78.        ]), 'previousTarget': array([-3.25729442, 22.69459507, 22.0389656 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278242167930057
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 38, 'trapConfig': [], 'currentTarget': array([-19.33115872, -15.85264339,  78.        ]), 'distance': 8.007818935463526, 'localFrame': array([[ 0.48846105,  0.01761946,  0.87240779],
       [-0.03604793,  0.99935006,  0.        ],
       [-0.87184078, -0.0314485 ,  0.48877873]]), 'currentState': array([-1.71621163e+01, -1.36564502e+01,  7.06110113e+01,  4.88461051e-01,
        1.76194613e-02,  8.72407792e-01]), 'targetState': array([-19.33115872, -15.85264339,  78.        ]), 'previousTarget': array([-19.33115872, -15.85264339,  78.        ])}
episode index:18986
target thresh 85.77107461615408
target distance 15.675710560488108
model initialize at round 18986
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.11602009, 12.05662715, 94.        ]), 'distance': 14.442071278639682, 'localFrame': array([[-0.97863925, -0.14995047, -0.14064168],
       [ 0.15145586, -0.98846402,  0.        ],
       [-0.13901924, -0.02130101,  0.99006056]]), 'currentState': array([ 6.81647735,  8.53470625, 92.56646894, -0.97863925, -0.14995047,
       -0.14064168]), 'targetState': array([-7.11602009, 12.05662715, 94.        ]), 'previousTarget': array([-7.11602009, 12.05662715, 94.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278402404476701
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.11602009, 12.05662715, 94.        ]), 'distance': 2.866706446399347, 'localFrame': array([[-0.70666688,  0.15457661,  0.69045491],
       [-0.21368795, -0.97690197,  0.        ],
       [ 0.67450677, -0.14754189,  0.72337543]]), 'currentState': array([-4.7865767 , 10.94350251, 95.24605494, -0.70666688,  0.15457661,
        0.69045491]), 'targetState': array([-7.11602009, 12.05662715, 94.        ]), 'previousTarget': array([-7.11602009, 12.05662715, 94.        ])}
episode index:18987
target thresh 85.77249743755021
target distance 33.3922096397527
model initialize at round 18987
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.52969283, -11.68282669,   1.63207955]), 'distance': 27.499999999999996, 'localFrame': array([[-0.1337516 ,  0.98590678,  0.10049048],
       [-0.99092281, -0.1344321 ,  0.        ],
       [ 0.01350915, -0.09957831,  0.99493802]]), 'currentState': array([ 23.1227074 , -33.56878488,   3.02013636,  -0.1337516 ,
         0.98590678,   0.10049048]), 'targetState': array([-1.02626963, -1.71661604,  1.        ]), 'previousTarget': array([  6.97228783, -12.8094126 ,   1.33219714])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6278403449026287
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([-1.02626963, -1.71661604,  1.        ]), 'distance': 1.980531092789639, 'localFrame': array([[-0.89538578,  0.18643919, -0.40438191],
       [-0.20384997, -0.97900214,  0.        ],
       [-0.39589076,  0.08243324,  0.91459022]]), 'currentState': array([ 0.59806934, -2.8084017 ,  0.69663487, -0.89538578,  0.18643919,
       -0.40438191]), 'targetState': array([-1.02626963, -1.71661604,  1.        ]), 'previousTarget': array([-1.02626963, -1.71661604,  1.        ])}
episode index:18988
target thresh 85.7739201166713
target distance 65.32005445550072
model initialize at round 18988
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.60736201, 21.50252002, 40.28213728]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.30575386,  0.61036107,  0.7307352 ],
       [-0.894091  ,  0.44788534,  0.        ],
       [-0.32728558, -0.65334377,  0.68266102]]), 'currentState': array([-31.23233169,  17.01054586,  56.66605789,   0.30575386,
         0.61036107,   0.7307352 ]), 'targetState': array([34.32139151, 30.62747272,  7.        ]), 'previousTarget': array([-9.17902338, 21.05309846, 38.96598552])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278072815319983
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 54, 'trapConfig': [], 'currentTarget': array([34.32139151, 30.62747272,  7.        ]), 'distance': 21.843152794799806, 'localFrame': array([[ 0.99595517, -0.02720047,  0.08563542],
       [ 0.02730076,  0.99962726,  0.        ],
       [-0.0856035 ,  0.00233791,  0.99632654]]), 'currentState': array([23.82301275, 22.41455174, 24.3047767 ,  0.99595517, -0.02720047,
        0.08563542]), 'targetState': array([34.32139151, 30.62747272,  7.        ]), 'previousTarget': array([34.32139151, 30.62747272,  7.        ])}
episode index:18989
target thresh 85.77534265353162
target distance 54.0
model initialize at round 18989
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.47974925, -8.64022017, 26.93780624]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.40116015, -0.60990018,  0.68344152],
       [ 0.83547367,  0.54953048,  0.        ],
       [-0.37557195,  0.57099739,  0.73000527]]), 'currentState': array([-7.92237143, -7.60559174,  1.93438271,  0.40116015, -0.60990018,
        0.68344152]), 'targetState': array([16.27673763, -9.80141889, 55.        ]), 'previousTarget': array([ 3.27062447, -7.78251948, 26.03621241])}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.6277961869347577
{'scaleFactor': 20, 'timeStep': 88, 'trapCount': 36, 'trapConfig': [], 'currentTarget': array([16.27673763, -9.80141889, 55.        ]), 'distance': 2.837360923678994, 'localFrame': array([[ 0.2462405 ,  0.05890715,  0.96741695],
       [-0.23266122,  0.97255784,  0.        ],
       [-0.94086895, -0.22508041,  0.25318854]]), 'currentState': array([ 16.79640165, -11.44335957,  52.74509488,   0.2462405 ,
         0.05890715,   0.96741695]), 'targetState': array([16.27673763, -9.80141889, 55.        ]), 'previousTarget': array([16.27673763, -9.80141889, 55.        ])}
episode index:18990
target thresh 85.77676504814534
target distance 38.0
model initialize at round 18990
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.03377064, -16.36014061,  64.11173912]), 'distance': 27.5, 'localFrame': array([[ 0.0361705 , -0.35135711, -0.93554256],
       [ 0.9947429 ,  0.10240391,  0.        ],
       [ 0.09580322, -0.93062432,  0.35321399]]), 'currentState': array([ 2.05401361e+01, -2.48176184e+01,  8.26113475e+01,  3.61704951e-02,
       -3.51357107e-01, -9.35542558e-01]), 'targetState': array([-16.08458386,  -8.07998528,  46.        ]), 'previousTarget': array([  1.60534602, -15.97291449,  64.83119611])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6278013043139911
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.08458386,  -8.07998528,  46.        ]), 'distance': 2.8002564912989953, 'localFrame': array([[-0.30779971, -0.06109772, -0.94948744],
       [ 0.19469963, -0.98086291,  0.        ],
       [-0.93131702, -0.18486486,  0.31380502]]), 'currentState': array([-14.82469749,  -7.86433831,  48.49150941,  -0.30779971,
        -0.06109772,  -0.94948744]), 'targetState': array([-16.08458386,  -8.07998528,  46.        ]), 'previousTarget': array([-16.08458386,  -8.07998528,  46.        ])}
episode index:18991
target thresh 85.77818730052672
target distance 32.86464280559106
model initialize at round 18991
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([17.29898486, 27.19205364, 50.6747853 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69974846,  0.68566263, -0.20054635],
       [-0.69988127,  0.7142592 ,  0.        ],
       [ 0.14324208,  0.14035863,  0.97968422]]), 'currentState': array([31.43558502,  5.98364782, 61.        ,  0.69974846,  0.68566263,
       -0.20054635]), 'targetState': array([ 9.52944464, 38.84829063, 45.        ]), 'previousTarget': array([17.29898486, 27.19205364, 50.6747853 ])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278092034323086
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 9.52944464, 38.84829063, 45.        ]), 'distance': 2.236170822063377, 'localFrame': array([[-0.79098261,  0.59497747, -0.14264754],
       [-0.60112484, -0.79915513,  0.        ],
       [-0.11399751,  0.08574898,  0.98977355]]), 'currentState': array([11.01645583, 37.27027413, 45.54692013, -0.79098261,  0.59497747,
       -0.14264754]), 'targetState': array([ 9.52944464, 38.84829063, 45.        ]), 'previousTarget': array([ 9.52944464, 38.84829063, 45.        ])}
episode index:18992
target thresh 85.77960941068999
target distance 25.154477867911204
model initialize at round 18992
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.63920662, -10.28873985,  56.22148727]), 'distance': 27.5, 'localFrame': array([[-0.83676065,  0.25909451,  0.48239159],
       [-0.29578496, -0.95525455,  0.        ],
       [ 0.46080676, -0.14268417,  0.87595568]]), 'currentState': array([ 20.93276078, -17.99700112,  38.20506487,  -0.83676065,
         0.25909451,   0.48239159]), 'targetState': array([-2.40715388, -8.67211682, 60.        ]), 'previousTarget': array([  2.80757204, -10.60332324,  55.43922276])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278192122636761
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.40715388, -8.67211682, 60.        ]), 'distance': 2.394508039555801, 'localFrame': array([[-0.51550323,  0.64024748,  0.5695082 ],
       [-0.77890347, -0.62714383,  0.        ],
       [ 0.35716355, -0.44359192,  0.82198565]]), 'currentState': array([ -1.39253808, -10.72383772,  60.70332418,  -0.51550323,
         0.64024748,   0.5695082 ]), 'targetState': array([-2.40715388, -8.67211682, 60.        ]), 'previousTarget': array([-2.40715388, -8.67211682, 60.        ])}
episode index:18993
target thresh 85.78103137864933
target distance 69.0
model initialize at round 18993
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.62038103, -24.48599863,  66.74822069]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69157282,  0.36274493, -0.6246144 ],
       [-0.4645018 , -0.88557218,  0.        ],
       [-0.55314114,  0.29013451,  0.78093332]]), 'currentState': array([-12.5191182 , -30.54635102,  92.8690308 ,  -0.69157282,
         0.36274493,  -0.6246144 ]), 'targetState': array([-28.37187289, -14.79989287,  25.        ]), 'previousTarget': array([-18.10787426, -25.4233938 ,  67.99236436])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6278116926342148
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-28.37187289, -14.79989287,  25.        ]), 'distance': 2.1304087895417787, 'localFrame': array([[-0.25948096, -0.71997557, -0.64366514],
       [ 0.94076669, -0.33905463,  0.        ],
       [-0.21823764, -0.60553872,  0.76530725]]), 'currentState': array([-27.36196502, -13.79107243,  26.58145788,  -0.25948096,
        -0.71997557,  -0.64366514]), 'targetState': array([-28.37187289, -14.79989287,  25.        ]), 'previousTarget': array([-28.37187289, -14.79989287,  25.        ])}
episode index:18994
target thresh 85.782453204419
target distance 30.032285997746776
model initialize at round 18994
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.43885603, -12.33037304,  70.69506894]), 'distance': 27.5, 'localFrame': array([[ 0.55040158, -0.64339195, -0.53207602],
       [ 0.7598849 ,  0.65005764,  0.        ],
       [ 0.34588008, -0.40431654,  0.84669658]]), 'currentState': array([-0.96337345, 11.09350374, 65.40808095,  0.55040158, -0.64339195,
       -0.53207602]), 'targetState': array([ 15.7467859, -18.1118396,  72.       ]), 'previousTarget': array([ 12.10305382, -11.72226064,  70.93621502])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278225745541314
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.7467859, -18.1118396,  72.       ]), 'distance': 4.476089715344712, 'localFrame': array([[ 0.76280152, -0.59198968, -0.26015774],
       [ 0.61310113,  0.79000443,  0.        ],
       [ 0.20552576, -0.159503  ,  0.96556613]]), 'currentState': array([ 13.92917135, -15.25694811,  69.07062276,   0.76280152,
        -0.59198968,  -0.26015774]), 'targetState': array([ 15.7467859, -18.1118396,  72.       ]), 'previousTarget': array([ 15.7467859, -18.1118396,  72.       ])}
episode index:18995
target thresh 85.78387488801317
target distance 48.0
model initialize at round 18995
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.41378264, -12.46236253,  78.66395536]), 'distance': 27.499999999999996, 'localFrame': array([[-0.11174055, -0.23114936,  0.96648022],
       [ 0.90032093, -0.43522664,  0.        ],
       [ 0.42063794,  0.87014237,  0.25674107]]), 'currentState': array([14.95059547, -9.94907956, 53.82778499, -0.11174055, -0.23114936,
        0.96648022]), 'targetState': array([ -6.49716377, -14.62145215, 100.        ]), 'previousTarget': array([  3.64669581, -12.57575493,  77.06639338])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6278233530879033
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.49716377, -14.62145215, 100.        ]), 'distance': 2.9819066317778553, 'localFrame': array([[ 0.62765028, -0.01012632,  0.77842956],
       [ 0.0161316 ,  0.99986988,  0.        ],
       [-0.77832827,  0.01255731,  0.62773196]]), 'currentState': array([-6.32943336e+00, -1.38900384e+01,  9.71140569e+01,  6.27650281e-01,
       -1.01263195e-02,  7.78429562e-01]), 'targetState': array([ -6.49716377, -14.62145215, 100.        ]), 'previousTarget': array([ -6.49716377, -14.62145215, 100.        ])}
episode index:18996
target thresh 85.78529642944612
target distance 13.0
model initialize at round 18996
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.52598001, -13.24822648,  59.        ]), 'distance': 16.402260625835034, 'localFrame': array([[ 0.42202275, -0.46497559,  0.77826378],
       [ 0.74048069,  0.67207764,  0.        ],
       [-0.52305368,  0.5762893 ,  0.62793749]]), 'currentState': array([-4.61074503, -6.5735098 , 47.12554985,  0.42202275, -0.46497559,
        0.77826378]), 'targetState': array([  4.52598001, -13.24822648,  59.        ]), 'previousTarget': array([  4.52598001, -13.24822648,  59.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6278388777150202
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.52598001, -13.24822648,  59.        ]), 'distance': 2.964314051472041, 'localFrame': array([[ 0.32353697, -0.91633546,  0.2359092 ],
       [ 0.94295013,  0.332934  ,  0.        ],
       [-0.07854219,  0.22245061,  0.9717751 ]]), 'currentState': array([  1.79108691, -12.22840395,  58.48281575,   0.32353697,
        -0.91633546,   0.2359092 ]), 'targetState': array([  4.52598001, -13.24822648,  59.        ]), 'previousTarget': array([  4.52598001, -13.24822648,  59.        ])}
episode index:18997
target thresh 85.78671782873202
target distance 33.0
model initialize at round 18997
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.46434938, -9.08619555, 52.36997385]), 'distance': 27.5, 'localFrame': array([[-0.56931878,  0.42430654,  0.70415913],
       [-0.59757933, -0.80180979,  0.        ],
       [ 0.56460169, -0.42079094,  0.71004219]]), 'currentState': array([ 20.0991094 , -10.41161648,  27.04419373,  -0.56931878,
         0.42430654,   0.70415913]), 'targetState': array([ 6.68027969, -8.73921411, 59.        ]), 'previousTarget': array([10.28414685, -9.1545827 , 51.02945554])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6278373574038691
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ 6.68027969, -8.73921411, 59.        ]), 'distance': 2.4946358963498345, 'localFrame': array([[ 0.63516911,  0.75194422,  0.17646557],
       [-0.76393276,  0.64529586,  0.        ],
       [-0.1138725 , -0.13480783,  0.98430681]]), 'currentState': array([ 4.53046283, -9.34963216, 57.8914489 ,  0.63516911,  0.75194422,
        0.17646557]), 'targetState': array([ 6.68027969, -8.73921411, 59.        ]), 'previousTarget': array([ 6.68027969, -8.73921411, 59.        ])}
episode index:18998
target thresh 85.78813908588512
target distance 56.0
model initialize at round 18998
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([13.01169551, -4.49627276, 55.27471027]), 'distance': 27.499999999999996, 'localFrame': array([[-0.69492729, -0.11219738, -0.71027306],
       [ 0.15938798, -0.98721602,  0.        ],
       [-0.70119294, -0.11320899,  0.70392627]]), 'currentState': array([10.87940407,  1.62436628, 82.        , -0.69492729, -0.11219738,
       -0.71027306]), 'targetState': array([ 15.34739383, -11.20078135,  26.        ]), 'previousTarget': array([13.01169551, -4.49627276, 55.27471027])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278406002970985
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 15.34739383, -11.20078135,  26.        ]), 'distance': 3.4679147617509636, 'localFrame': array([[-0.39068812, -0.03011678, -0.92003031],
       [ 0.07685848, -0.99704201,  0.        ],
       [-0.91730887, -0.07071213,  0.3918472 ]]), 'currentState': array([14.35445467, -9.12094125, 28.59128729, -0.39068812, -0.03011678,
       -0.92003031]), 'targetState': array([ 15.34739383, -11.20078135,  26.        ]), 'previousTarget': array([ 15.34739383, -11.20078135,  26.        ])}
episode index:18999
target thresh 85.7895602009196
target distance 36.285836985532214
model initialize at round 18999
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.19477059, -23.72176369,  64.05647154]), 'distance': 27.499999999999996, 'localFrame': array([[-0.66712259,  0.4742315 , -0.57450146],
       [-0.57938845, -0.81505155,  0.        ],
       [-0.4682483 ,  0.33285951,  0.81850356]]), 'currentState': array([  9.0363114 , -36.48499251,  79.0069055 ,  -0.66712259,
         0.4742315 ,  -0.57450146]), 'targetState': array([-25.70328312, -13.42911899,  52.        ]), 'previousTarget': array([ -8.86490803, -24.13358891,  64.99334786])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278427650949024
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-25.70328312, -13.42911899,  52.        ]), 'distance': 2.587595766521235, 'localFrame': array([[-0.85329255, -0.38963293, -0.34652274],
       [ 0.41536851, -0.90965323,  0.        ],
       [-0.31521553, -0.14393464,  0.93804157]]), 'currentState': array([-24.00914372, -13.93785313,  53.88857965,  -0.85329255,
        -0.38963293,  -0.34652274]), 'targetState': array([-25.70328312, -13.42911899,  52.        ]), 'previousTarget': array([-25.70328312, -13.42911899,  52.        ])}
episode index:19000
target thresh 85.79098117384967
target distance 19.91899172588124
model initialize at round 19000
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.59103139, -13.2977462 ,  48.        ]), 'distance': 21.09423478797854, 'localFrame': array([[ 0.49971293, -0.67555256,  0.54213994],
       [ 0.80395339,  0.59469231,  0.        ],
       [-0.32240645,  0.43585525,  0.84028821]]), 'currentState': array([-29.71010232, -21.4674594 ,  51.56141182,   0.49971293,
        -0.67555256,   0.54213994]), 'targetState': array([-10.59103139, -13.2977462 ,  48.        ]), 'previousTarget': array([-10.59103139, -13.2977462 ,  48.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6278563718580528
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.59103139, -13.2977462 ,  48.        ]), 'distance': 3.1219842393087895, 'localFrame': array([[ 0.8379172 ,  0.28714058,  0.46416059],
       [-0.32417752,  0.94599627,  0.        ],
       [-0.43909419, -0.15047043,  0.88575106]]), 'currentState': array([-12.56081662, -13.15971161,  50.41819731,   0.8379172 ,
         0.28714058,   0.46416059]), 'targetState': array([-10.59103139, -13.2977462 ,  48.        ]), 'previousTarget': array([-10.59103139, -13.2977462 ,  48.        ])}
episode index:19001
target thresh 85.79240200468956
target distance 54.0
model initialize at round 19001
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.41978685, -8.48014487, 28.09838145]), 'distance': 27.499999999999996, 'localFrame': array([[-0.19177915, -0.68928352, -0.69864797],
       [ 0.96340555, -0.26804805,  0.        ],
       [-0.18727123, -0.67308133,  0.71546559]]), 'currentState': array([27.16878482, -7.55291614, 55.55425952, -0.19177915, -0.68928352,
       -0.69864797]), 'targetState': array([29.5633717 , -9.32775718,  3.        ]), 'previousTarget': array([28.87070279, -8.16146799, 29.53585989])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6278571483673796
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([29.5633717 , -9.32775718,  3.        ]), 'distance': 3.374832207085735, 'localFrame': array([[-0.64033825,  0.3680169 , -0.67418877],
       [-0.4982904 , -0.86701019,  0.        ],
       [-0.58452853,  0.33594179,  0.73855908]]), 'currentState': array([30.92849057, -8.15259847,  5.85393498, -0.64033825,  0.3680169 ,
       -0.67418877]), 'targetState': array([29.5633717 , -9.32775718,  3.        ]), 'previousTarget': array([29.5633717 , -9.32775718,  3.        ])}
episode index:19002
target thresh 85.79382269345346
target distance 64.0
model initialize at round 19002
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.61868057,  7.25266178, 37.60957285]), 'distance': 27.5, 'localFrame': array([[-0.89222633, -0.32070036,  0.31793624],
       [ 0.33825152, -0.94105574,  0.        ],
       [ 0.29919573,  0.10754242,  0.9481121 ]]), 'currentState': array([23.46210081, 15.62287732, 13.76403387, -0.89222633, -0.32070036,
        0.31793624]), 'targetState': array([-5.74829263, -6.92510879, 78.        ]), 'previousTarget': array([13.70372342,  7.92546108, 37.53169232])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6278565922968947
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-5.74829263, -6.92510879, 78.        ]), 'distance': 1.9738175333184271, 'localFrame': array([[ 0.25754237, -0.25126394,  0.93302645],
       [ 0.69832826,  0.71577765,  0.        ],
       [-0.66783948,  0.65155874,  0.35980778]]), 'currentState': array([-7.06659892, -6.71251161, 76.54644345,  0.25754237, -0.25126394,
        0.93302645]), 'targetState': array([-5.74829263, -6.92510879, 78.        ]), 'previousTarget': array([-5.74829263, -6.92510879, 78.        ])}
episode index:19003
target thresh 85.7952432401556
target distance 73.0
model initialize at round 19003
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.13144051, 23.3889646 , 63.77866368]), 'distance': 27.499999999999996, 'localFrame': array([[-0.02811983, -0.92582851, -0.37689633],
       [ 0.99953907, -0.03035862,  0.        ],
       [-0.01144205, -0.37672261,  0.92625545]]), 'currentState': array([-1.60034229e+01,  2.96830812e+01,  8.90357820e+01, -2.81198347e-02,
       -9.25828512e-01, -3.76896328e-01]), 'targetState': array([ 9.65160886, 11.48244078, 16.        ]), 'previousTarget': array([-6.77294525, 24.54159981, 63.81790565])}
done in step count: 96
reward sum = 0.38104711810454966
running average episode reward sum: 0.6278436050587242
{'scaleFactor': 20, 'timeStep': 97, 'trapCount': 37, 'trapConfig': [], 'currentTarget': array([ 9.65160886, 11.48244078, 16.        ]), 'distance': 2.333069420720914, 'localFrame': array([[ 0.72106967,  0.19674177, -0.66434268],
       [-0.263225  ,  0.96473447,  0.        ],
       [ 0.64091429,  0.1748716 ,  0.74742812]]), 'currentState': array([ 8.97902552, 11.85273619, 18.20311731,  0.72106967,  0.19674177,
       -0.66434268]), 'targetState': array([ 9.65160886, 11.48244078, 16.        ]), 'previousTarget': array([ 9.65160886, 11.48244078, 16.        ])}
episode index:19004
target thresh 85.79666364481018
target distance 51.0
model initialize at round 19004
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.82757961,  1.18910332, 50.24035806]), 'distance': 27.499999999999996, 'localFrame': array([[-0.89248394, -0.26951969,  0.36170645],
       [ 0.28909363, -0.95730083,  0.        ],
       [ 0.34626188,  0.10456703,  0.93229204]]), 'currentState': array([ 5.79465216,  6.57061133, 23.34388995, -0.89248394, -0.26951969,
        0.36170645]), 'targetState': array([ 2.16305374, -3.36469888, 73.        ]), 'previousTarget': array([ 4.37961161,  1.69283101, 48.81032591])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6278494907082022
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.16305374, -3.36469888, 73.        ]), 'distance': 2.6743406810615036, 'localFrame': array([[ 0.92258936,  0.2523422 ,  0.29180865],
       [-0.26382472,  0.96457064,  0.        ],
       [-0.28147005, -0.07698633,  0.95647672]]), 'currentState': array([ 2.66057621, -2.78234116, 70.4376907 ,  0.92258936,  0.2523422 ,
        0.29180865]), 'targetState': array([ 2.16305374, -3.36469888, 73.        ]), 'previousTarget': array([ 2.16305374, -3.36469888, 73.        ])}
episode index:19005
target thresh 85.79808390743138
target distance 22.68255097588123
model initialize at round 19005
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.23418055,  -8.01736959,  30.99530032]), 'distance': 27.5, 'localFrame': array([[ 0.09903552,  0.65726407, -0.74712509],
       [-0.98883772,  0.14899652,  0.        ],
       [ 0.11131904,  0.73878547,  0.66468346]]), 'currentState': array([  6.44931908, -29.87300273,  30.50322284,   0.09903552,
         0.65726407,  -0.74712509]), 'targetState': array([-10.39351945,  -7.80863326,  31.        ]), 'previousTarget': array([-10.23899199,  -8.02787788,  31.00966578])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6278582124167645
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.39351945,  -7.80863326,  31.        ]), 'distance': 3.1204102334674464, 'localFrame': array([[ 0.56963988, -0.44854669,  0.68870623],
       [ 0.61865053,  0.78566629,  0.        ],
       [-0.54109327,  0.42606848,  0.7250405 ]]), 'currentState': array([-8.39942854, -7.78860552, 28.5999666 ,  0.56963988, -0.44854669,
        0.68870623]), 'targetState': array([-10.39351945,  -7.80863326,  31.        ]), 'previousTarget': array([-10.39351945,  -7.80863326,  31.        ])}
episode index:19006
target thresh 85.79950402803343
target distance 52.0
model initialize at round 19006
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.99856315, -35.65454491,  50.37990873]), 'distance': 27.500000000000004, 'localFrame': array([[-0.74313868, -0.06575699, -0.66589859],
       [ 0.0881411 , -0.996108  ,  0.        ],
       [-0.66330691, -0.05869303,  0.74604227]]), 'currentState': array([ 2.69941755e+01, -2.90916199e+01,  7.17649128e+01, -7.43138675e-01,
       -6.57569854e-02, -6.65898587e-01]), 'targetState': array([-10.97709936, -44.67105651,  21.        ]), 'previousTarget': array([ 12.00758675, -35.9503544 ,  51.56981406])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278603754906927
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.97709936, -44.67105651,  21.        ]), 'distance': 2.458214541588331, 'localFrame': array([[-0.01747276, -0.61793534, -0.78603474],
       [ 0.99960047, -0.02826474,  0.        ],
       [-0.02221707, -0.7857207 ,  0.61818232]]), 'currentState': array([-9.98871718e+00, -4.52409423e+01,  2.31774181e+01, -1.74727621e-02,
       -6.17935339e-01, -7.86034744e-01]), 'targetState': array([-10.97709936, -44.67105651,  21.        ]), 'previousTarget': array([-10.97709936, -44.67105651,  21.        ])}
episode index:19007
target thresh 85.80092400663051
target distance 71.0
model initialize at round 19007
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.13960564, -0.71079873, 62.71504058]), 'distance': 27.5, 'localFrame': array([[ 0.58953144, -0.71665443, -0.37263805],
       [ 0.77227626,  0.63528685,  0.        ],
       [ 0.23673205, -0.28777952,  0.92797677]]), 'currentState': array([ 3.53637767,  2.03795894, 87.49526317,  0.58953144, -0.71665443,
       -0.37263805]), 'targetState': array([36.54546815, -5.78176078, 17.        ]), 'previousTarget': array([14.84500752,  0.46034249, 63.28441923])}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6278564204771022
{'scaleFactor': 20, 'timeStep': 60, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([36.54546815, -5.78176078, 17.        ]), 'distance': 2.229591772359579, 'localFrame': array([[ 0.70535659,  0.50637975, -0.49603592],
       [-0.5831839 ,  0.81234016,  0.        ],
       [ 0.4029499 ,  0.28928016,  0.868302  ]]), 'currentState': array([37.38825102, -6.33972497, 18.98732797,  0.70535659,  0.50637975,
       -0.49603592]), 'targetState': array([36.54546815, -5.78176078, 17.        ]), 'previousTarget': array([36.54546815, -5.78176078, 17.        ])}
episode index:19008
target thresh 85.80234384323683
target distance 59.0
model initialize at round 19008
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.39733215,  -5.78609456,  40.78224369]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55949094,  0.78483942, -0.26645257],
       [-0.81427715,  0.5804763 ,  0.        ],
       [ 0.1546694 ,  0.21696624,  0.96384803]]), 'currentState': array([-37.06922779, -20.11115932,  21.70026818,   0.55949094,
         0.78483942,  -0.26645257]), 'targetState': array([ 5.41797403, 24.40585088, 81.        ]), 'previousTarget': array([-24.69129426,  -6.48593169,  40.75862495])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6278555398847014
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.41797403, 24.40585088, 81.        ]), 'distance': 1.6647296677960917, 'localFrame': array([[ 0.52180485,  0.84411068,  0.12327555],
       [-0.85059864,  0.52581552,  0.        ],
       [-0.0648202 , -0.10485801,  0.99237248]]), 'currentState': array([ 4.71475383, 22.89696653, 80.99138506,  0.52180485,  0.84411068,
        0.12327555]), 'targetState': array([ 5.41797403, 24.40585088, 81.        ]), 'previousTarget': array([ 5.41797403, 24.40585088, 81.        ])}
episode index:19009
target thresh 85.80376353786659
target distance 49.0
model initialize at round 19009
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.79797026, 17.08372837, 74.41249155]), 'distance': 27.500000000000004, 'localFrame': array([[-0.33104024,  0.75128641, -0.57094754],
       [-0.91510197, -0.40322249,  0.        ],
       [-0.23021889,  0.52247522,  0.82098655]]), 'currentState': array([-0.33758517,  0.79060487, 95.01823988, -0.33104024,  0.75128641,
       -0.57094754]), 'targetState': array([18.62096202, 38.75899603, 47.        ]), 'previousTarget': array([ 8.46378958, 16.40830003, 75.24464776])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278587799449847
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.62096202, 38.75899603, 47.        ]), 'distance': 3.1340600674843957, 'localFrame': array([[-0.4627192 ,  0.67527623, -0.57436308],
       [-0.82491532, -0.56525632,  0.        ],
       [-0.32466236,  0.47380091,  0.81860066]]), 'currentState': array([19.59840987, 35.97275468, 48.05061283, -0.4627192 ,  0.67527623,
       -0.57436308]), 'targetState': array([18.62096202, 38.75899603, 47.        ]), 'previousTarget': array([18.62096202, 38.75899603, 47.        ])}
episode index:19010
target thresh 85.80518309053399
target distance 64.0
model initialize at round 19010
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.62504975,  0.35813055, 71.97147218]), 'distance': 27.500000000000004, 'localFrame': array([[-0.75447542, -0.25929646, -0.60293631],
       [ 0.32501871, -0.94570759,  0.        ],
       [-0.57020144, -0.19596558,  0.79778933]]), 'currentState': array([ 6.85944988, 11.58187702, 94.295983  , -0.75447542, -0.25929646,
       -0.60293631]), 'targetState': array([-25.18775598, -19.73770374,  32.        ]), 'previousTarget': array([-3.8960723 ,  0.73109907, 73.5802467 ])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6278598973953253
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.18775598, -19.73770374,  32.        ]), 'distance': 4.128439600542357, 'localFrame': array([[-0.80982165, -0.06523329, -0.58303818],
       [ 0.08029258, -0.99677134,  0.        ],
       [-0.58115575, -0.04681364,  0.81244476]]), 'currentState': array([-22.81787295, -18.14049533,  34.97936122,  -0.80982165,
        -0.06523329,  -0.58303818]), 'targetState': array([-25.18775598, -19.73770374,  32.        ]), 'previousTarget': array([-25.18775598, -19.73770374,  32.        ])}
episode index:19011
target thresh 85.80660250125321
target distance 38.0
model initialize at round 19011
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.7121351 , -10.61952958,  25.64484059]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69556733, -0.46220417,  0.55004854],
       [ 0.55344998,  0.83288242,  0.        ],
       [-0.45812575,  0.30442435,  0.83513269]]), 'currentState': array([ 3.61224272,  7.64493972,  6.35104135,  0.69556733, -0.46220417,
        0.55004854]), 'targetState': array([ 17.09863072, -27.04878607,  43.        ]), 'previousTarget': array([  9.93654951, -10.18300312,  24.5252211 ])}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.6278479483502778
{'scaleFactor': 20, 'timeStep': 92, 'trapCount': 57, 'trapConfig': [], 'currentTarget': array([ 17.09863072, -27.04878607,  43.        ]), 'distance': 3.1305051343071613, 'localFrame': array([[ 0.55996856,  0.44448958,  0.69918827],
       [-0.62171801,  0.78324116,  0.        ],
       [-0.54763303, -0.43469793,  0.7149376 ]]), 'currentState': array([ 15.40285567, -25.32276299,  41.01373372,   0.55996856,
         0.44448958,   0.69918827]), 'targetState': array([ 17.09863072, -27.04878607,  43.        ]), 'previousTarget': array([ 17.09863072, -27.04878607,  43.        ])}
episode index:19012
target thresh 85.80802177003847
target distance 38.0
model initialize at round 19012
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.84764957,  15.72813706,  22.45584331]), 'distance': 27.500000000000004, 'localFrame': array([[-0.1021752 , -0.72685042, -0.67915293],
       [ 0.99026374, -0.13920387,  0.        ],
       [-0.09454072, -0.67254052,  0.7339968 ]]), 'currentState': array([-30.47584983,  31.73649215,  38.4477764 ,  -0.1021752 ,
        -0.72685042,  -0.67915293]), 'targetState': array([ 5.14293032, -4.74871222,  2.        ]), 'previousTarget': array([-14.66919684,  16.33402645,  23.56504139])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6278464287614762
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([ 5.14293032, -4.74871222,  2.        ]), 'distance': 2.614552447547159, 'localFrame': array([[ 0.5566355 , -0.52741439,  0.64186524],
       [ 0.68779653,  0.72590353,  0.        ],
       [-0.46593224,  0.44147268,  0.76681746]]), 'currentState': array([ 2.78656696, -3.7688715 ,  2.56863732,  0.5566355 , -0.52741439,
        0.64186524]), 'targetState': array([ 5.14293032, -4.74871222,  2.        ]), 'previousTarget': array([ 5.14293032, -4.74871222,  2.        ])}
episode index:19013
target thresh 85.80944089690394
target distance 52.99853714396874
model initialize at round 19013
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.33374986, -18.34576168,  52.03424511]), 'distance': 27.5, 'localFrame': array([[-0.85506347, -0.25661864, -0.45057001],
       [ 0.28745024, -0.95779557,  0.        ],
       [-0.43155396, -0.12951646,  0.8927411 ]]), 'currentState': array([-12.67647719, -42.45770879,  39.54405993,  -0.85506347,
        -0.25661864,  -0.45057001]), 'targetState': array([-3.13028876, 10.54520234, 67.        ]), 'previousTarget': array([ -7.67441289, -18.0123945 ,  52.99022619])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278496686193235
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.13028876, 10.54520234, 67.        ]), 'distance': 2.1340816829330893, 'localFrame': array([[-0.68259645,  0.51269856,  0.52077085],
       [-0.60056303, -0.79957742,  0.        ],
       [ 0.41639661, -0.31275572,  0.8536965 ]]), 'currentState': array([-3.27205907,  9.3261843 , 65.25409026, -0.68259645,  0.51269856,
        0.52077085]), 'targetState': array([-3.13028876, 10.54520234, 67.        ]), 'previousTarget': array([-3.13028876, 10.54520234, 67.        ])}
episode index:19014
target thresh 85.8108598818638
target distance 44.55992312292519
model initialize at round 19014
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.02288051, -2.58771262, 39.18238903]), 'distance': 27.5, 'localFrame': array([[-0.70705188, -0.08688988,  0.70180324],
       [ 0.12197283, -0.99253344,  0.        ],
       [ 0.69656318,  0.08560092,  0.71237084]]), 'currentState': array([40.75469749, -4.71656037, 33.08911665, -0.70705188, -0.08688988,
        0.70180324]), 'targetState': array([-2.72537417, -1.25392808, 43.        ]), 'previousTarget': array([15.17460202, -2.24629901, 38.58123703])}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6278457156247689
{'scaleFactor': 20, 'timeStep': 60, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([-2.72537417, -1.25392808, 43.        ]), 'distance': 2.666616535438932, 'localFrame': array([[-0.85480594, -0.11265258,  0.506573  ],
       [ 0.13065757, -0.99142756,  0.        ],
       [ 0.50223043,  0.0661876 ,  0.86219708]]), 'currentState': array([-0.60861707,  0.08945538, 42.09142749, -0.85480594, -0.11265258,
        0.506573  ]), 'targetState': array([-2.72537417, -1.25392808, 43.        ]), 'previousTarget': array([-2.72537417, -1.25392808, 43.        ])}
episode index:19015
target thresh 85.81227872493228
target distance 75.0
model initialize at round 19015
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.22410481, -4.05820207, 65.2683798 ]), 'distance': 27.5, 'localFrame': array([[-0.30148562, -0.73682715, -0.60513815],
       [ 0.92552216, -0.37869346,  0.        ],
       [-0.22916186, -0.56006876,  0.79612048]]), 'currentState': array([-2.30254371, -2.00188214, 92.67590201, -0.30148562, -0.73682715,
       -0.60513815]), 'targetState': array([-4.81347505, -7.60463398, 18.        ]), 'previousTarget': array([-3.01880711, -2.90714481, 65.65185723])}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.6278350795502928
{'scaleFactor': 20, 'timeStep': 86, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([-4.81347505, -7.60463398, 18.        ]), 'distance': 2.2517790268356563, 'localFrame': array([[-0.85402941,  0.25010051,  0.45616171],
       [-0.28104437, -0.95969478,  0.        ],
       [ 0.43777601, -0.12820168,  0.8898969 ]]), 'currentState': array([-2.84288978, -6.51820722, 17.91645739, -0.85402941,  0.25010051,
        0.45616171]), 'targetState': array([-4.81347505, -7.60463398, 18.        ]), 'previousTarget': array([-4.81347505, -7.60463398, 18.        ])}
episode index:19016
target thresh 85.81369742612355
target distance 65.0
model initialize at round 19016
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.91417368, -3.10498748, 65.74059094]), 'distance': 27.500000000000004, 'localFrame': array([[-0.24436045,  0.32478353, -0.91367589],
       [-0.79908642, -0.60121618,  0.        ],
       [-0.54931672,  0.730106  ,  0.40644357]]), 'currentState': array([-1.18700355, -1.86300841, 93.15818331, -0.24436045,  0.32478353,
       -0.91367589]), 'targetState': array([-5.16565077, -4.72398689, 30.        ]), 'previousTarget': array([-2.84484025, -2.93794973, 67.58310641])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278020651379486
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-5.16565077, -4.72398689, 30.        ]), 'distance': 11.77660324243133, 'localFrame': array([[-0.87426924, -0.26169497,  0.40886309],
       [ 0.28675893, -0.95800278,  0.        ],
       [ 0.39169198,  0.11724514,  0.91259573]]), 'currentState': array([-6.78515494, -2.9827795 , 41.53402735, -0.87426924, -0.26169497,
        0.40886309]), 'targetState': array([-5.16565077, -4.72398689, 30.        ]), 'previousTarget': array([-5.16565077, -4.72398689, 30.        ])}
episode index:19017
target thresh 85.8151159854518
target distance 59.629033978006376
model initialize at round 19017
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.38766834, -6.21820195, 64.51499798]), 'distance': 27.5, 'localFrame': array([[ 0.95095631, -0.28747982,  0.11418162],
       [ 0.28937234,  0.95721661,  0.        ],
       [-0.10929655,  0.033041  ,  0.99345989]]), 'currentState': array([-12.16557351, -25.8585618 ,  80.46313346,   0.95095631,
        -0.28747982,   0.11418162]), 'targetState': array([21.26209346, 35.05600351, 31.        ]), 'previousTarget': array([-2.1499843 , -5.1934405 , 64.07487351])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6277975369546921
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([21.26209346, 35.05600351, 31.        ]), 'distance': 1.581039755398845, 'localFrame': array([[-0.48846538,  0.82666497, -0.27933242],
       [-0.86093495, -0.50871506,  0.        ],
       [-0.14210061,  0.24048704,  0.96019446]]), 'currentState': array([20.72473025, 33.6983708 , 31.60643296, -0.48846538,  0.82666497,
       -0.27933242]), 'targetState': array([21.26209346, 35.05600351, 31.        ]), 'previousTarget': array([21.26209346, 35.05600351, 31.        ])}
episode index:19018
target thresh 85.8165344029312
target distance 31.026858166822773
model initialize at round 19018
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.09340499, -3.65950013, 86.78358398]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.64636243, -0.12931409,  0.751993  ],
       [ 0.19617682,  0.98056854,  0.        ],
       [-0.73738068,  0.1475236 ,  0.65917109]]), 'currentState': array([-19.07819217,  15.18714312,  76.17363382,   0.64636243,
        -0.12931409,   0.751993  ]), 'targetState': array([  9.45889151, -16.4781483 ,  94.        ]), 'previousTarget': array([-2.90880535, -3.59675265, 86.11178534])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278030317668641
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.45889151, -16.4781483 ,  94.        ]), 'distance': 3.1069745577763372, 'localFrame': array([[ 0.42936282, -0.28569178,  0.85675421],
       [ 0.55396153,  0.83254227,  0.        ],
       [-0.71328409,  0.47460887,  0.51572495]]), 'currentState': array([ 10.47718727, -14.08885493,  92.29481911,   0.42936282,
        -0.28569178,   0.85675421]), 'targetState': array([  9.45889151, -16.4781483 ,  94.        ]), 'previousTarget': array([  9.45889151, -16.4781483 ,  94.        ])}
episode index:19019
target thresh 85.81795267857594
target distance 13.97221874506955
model initialize at round 19019
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.05592727,  4.45851172, 42.        ]), 'distance': 20.353279933073914, 'localFrame': array([[ 0.95799369, -0.2243903 , -0.17859755],
       [ 0.22805695,  0.97364779,  0.        ],
       [ 0.17389111, -0.04073041,  0.98392221]]), 'currentState': array([-2.5831616 , 16.32266608, 52.66542441,  0.95799369, -0.2243903 ,
       -0.17859755]), 'targetState': array([10.05592727,  4.45851172, 42.        ]), 'previousTarget': array([10.05592727,  4.45851172, 42.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6278166270265881
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.05592727,  4.45851172, 42.        ]), 'distance': 3.273809432869328, 'localFrame': array([[ 0.39153049, -0.83874018,  0.37844257],
       [ 0.90613412,  0.4229905 ,  0.        ],
       [-0.16007761,  0.34291973,  0.92562477]]), 'currentState': array([ 9.54144424,  7.3511393 , 43.44424413,  0.39153049, -0.83874018,
        0.37844257]), 'targetState': array([10.05592727,  4.45851172, 42.        ]), 'previousTarget': array([10.05592727,  4.45851172, 42.        ])}
episode index:19020
target thresh 85.81937081240021
target distance 58.77319751051556
model initialize at round 19020
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.89936114, -16.97463425,  27.75529527]), 'distance': 27.499999999999996, 'localFrame': array([[-0.72700208, -0.63498515, -0.26126967],
       [ 0.65783447, -0.75316254,  0.        ],
       [-0.19677853, -0.1718722 ,  0.96526585]]), 'currentState': array([43.46916873, -0.06789738, 30.02590062, -0.72700208, -0.63498515,
       -0.26126967]), 'targetState': array([-13.77438388, -44.9362476 ,  24.        ]), 'previousTarget': array([ 23.30746459, -16.3784713 ,  28.41651893])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.627816401421618
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.77438388, -44.9362476 ,  24.        ]), 'distance': 3.9139644400426077, 'localFrame': array([[ 0.61733762,  0.12801208, -0.77621336],
       [-0.2030422 ,  0.97916999,  0.        ],
       [ 0.76004482,  0.15760406,  0.63047032]]), 'currentState': array([-11.72620122, -42.66998283,  26.44706138,   0.61733762,
         0.12801208,  -0.77621336]), 'targetState': array([-13.77438388, -44.9362476 ,  24.        ]), 'previousTarget': array([-13.77438388, -44.9362476 ,  24.        ])}
episode index:19021
target thresh 85.82078880441819
target distance 41.68707448246228
model initialize at round 19021
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.92706713, -17.34356062,  22.24323508]), 'distance': 27.5, 'localFrame': array([[ 0.11577386,  0.76355301,  0.63528199],
       [-0.98869939,  0.14991172,  0.        ],
       [-0.09523621, -0.62810292,  0.77228025]]), 'currentState': array([  9.27885801, -39.25150744,   6.28434984,   0.11577386,
         0.76355301,   0.63528199]), 'targetState': array([17.93388335,  1.54137212, 36.        ]), 'previousTarget': array([ 13.32716968, -18.44645842,  21.13633411])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278218943754731
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.93388335,  1.54137212, 36.        ]), 'distance': 3.25636204294781, 'localFrame': array([[ 0.06599341, -0.59607758,  0.80021021],
       [ 0.99392711,  0.11004044,  0.        ],
       [-0.08805548,  0.79535063,  0.59971961]]), 'currentState': array([15.20191069,  0.62237406, 34.48485588,  0.06599341, -0.59607758,
        0.80021021]), 'targetState': array([17.93388335,  1.54137212, 36.        ]), 'previousTarget': array([17.93388335,  1.54137212, 36.        ])}
episode index:19022
target thresh 85.82220665464405
target distance 31.99645242807363
model initialize at round 19022
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.33580787, 30.86369245, 34.36100886]), 'distance': 27.5, 'localFrame': array([[-0.1422167 ,  0.63974968,  0.75531104],
       [-0.97617087, -0.21700331,  0.        ],
       [ 0.16390499, -0.73731263,  0.65536649]]), 'currentState': array([-11.91655009,   9.64504628,  37.25500254,  -0.1422167 ,
         0.63974968,   0.75531104]), 'targetState': array([13.44937506, 40.84255514, 33.        ]), 'previousTarget': array([ 5.73521361, 30.68531497, 33.95234684])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278297795389606
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.44937506, 40.84255514, 33.        ]), 'distance': 3.86158134224183, 'localFrame': array([[ 0.76992173, -0.38962288, -0.50538553],
       [ 0.45153059,  0.89225564,  0.        ],
       [ 0.45093309, -0.22819703,  0.86289366]]), 'currentState': array([10.89585843, 39.22094919, 35.40036611,  0.76992173, -0.38962288,
       -0.50538553]), 'targetState': array([13.44937506, 40.84255514, 33.        ]), 'previousTarget': array([13.44937506, 40.84255514, 33.        ])}
episode index:19023
target thresh 85.823624363092
target distance 26.132425400332075
model initialize at round 19023
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.34835049,  4.01071085, 19.81024682]), 'distance': 27.500000000000004, 'localFrame': array([[ 7.19159316e-04, -6.74890932e-01, -7.37917145e-01],
       [ 9.99999432e-01,  1.06559278e-03,  0.00000000e+00],
       [ 7.86319184e-04, -7.37916726e-01,  6.74891316e-01]]), 'currentState': array([ 1.05780396e+01,  2.53745795e+01,  3.69818397e+01,  7.19159316e-04,
       -6.74890932e-01, -7.37917145e-01]), 'targetState': array([ 7.98344685,  0.51436987, 17.        ]), 'previousTarget': array([ 8.62019956,  5.32384292, 20.8648894 ])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6278389154099657
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.98344685,  0.51436987, 17.        ]), 'distance': 3.5524246754054616, 'localFrame': array([[-0.5318128 , -0.80996622,  0.24724457],
       [ 0.8359189 , -0.54885298,  0.        ],
       [ 0.13570092,  0.20667641,  0.96895311]]), 'currentState': array([ 7.80216295,  3.45185315, 15.01051534, -0.5318128 , -0.80996622,
        0.24724457]), 'targetState': array([ 7.98344685,  0.51436987, 17.        ]), 'previousTarget': array([ 7.98344685,  0.51436987, 17.        ])}
episode index:19024
target thresh 85.82504192977616
target distance 34.8387834234276
model initialize at round 19024
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.39914612, -10.20672758,  90.30373967]), 'distance': 27.5, 'localFrame': array([[-0.66709498,  0.26992119, -0.69435354],
       [-0.37508119, -0.92699196,  0.        ],
       [-0.64366016,  0.26043895,  0.71963404]]), 'currentState': array([ 22.24184894, -32.14595326,  85.41199726,  -0.66709498,
         0.26992119,  -0.69435354]), 'targetState': array([-2.33313116,  1.88586823, 93.        ]), 'previousTarget': array([  6.79268853, -10.82785885,  90.8104183 ])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6278447951186636
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.33313116,  1.88586823, 93.        ]), 'distance': 3.759186110040425, 'localFrame': array([[-0.87522242,  0.41073978, -0.25549666],
       [-0.42484026, -0.90526833,  0.        ],
       [-0.23129303,  0.10854527,  0.96680994]]), 'currentState': array([ 0.46702899,  0.10343979, 91.23547961, -0.87522242,  0.41073978,
       -0.25549666]), 'targetState': array([-2.33313116,  1.88586823, 93.        ]), 'previousTarget': array([-2.33313116,  1.88586823, 93.        ])}
episode index:19025
target thresh 85.82645935471076
target distance 57.0
model initialize at round 19025
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.38263468,   9.67190412,  56.53690461]), 'distance': 27.5, 'localFrame': array([[ 0.29340771, -0.71447433,  0.63516797],
       [ 0.9250367 ,  0.37987775,  0.        ],
       [-0.24128618,  0.58755368,  0.77237403]]), 'currentState': array([-15.73565276,  21.48494074,  32.08790873,   0.29340771,
        -0.71447433,   0.63516797]), 'targetState': array([-5.78079222, -5.53013935, 88.        ]), 'previousTarget': array([-12.20621843,  10.38743728,  55.34224134])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6278448991256059
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-5.78079222, -5.53013935, 88.        ]), 'distance': 2.132192599933945, 'localFrame': array([[-0.13816356, -0.38332652,  0.91322046],
       [ 0.9407575 , -0.33908012,  0.        ],
       [ 0.30965491,  0.85911899,  0.40746582]]), 'currentState': array([-5.98200698, -5.84076238, 85.90017352, -0.13816356, -0.38332652,
        0.91322046]), 'targetState': array([-5.78079222, -5.53013935, 88.        ]), 'previousTarget': array([-5.78079222, -5.53013935, 88.        ])}
episode index:19026
target thresh 85.82787663790994
target distance 14.471095291111393
model initialize at round 19026
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.29059969, -16.44964298,  74.        ]), 'distance': 15.393217715408507, 'localFrame': array([[-0.74371121, -0.63583313, -0.20642157],
       [ 0.64982839, -0.76008096,  0.        ],
       [-0.1568971 , -0.13413859,  0.97846315]]), 'currentState': array([ 9.36815729, -2.9850321 , 68.53430569, -0.74371121, -0.63583313,
       -0.20642157]), 'targetState': array([  4.29059969, -16.44964298,  74.        ]), 'previousTarget': array([  4.29059969, -16.44964298,  74.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6278594330603241
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.29059969, -16.44964298,  74.        ]), 'distance': 1.9336954533667454, 'localFrame': array([[ 0.05361189, -0.96982293,  0.2378429 ],
       [ 0.99847555,  0.05519581,  0.        ],
       [-0.01312793,  0.23748032,  0.97130364]]), 'currentState': array([ 4.61223938e+00, -1.70387272e+01,  7.21865211e+01,  5.36118869e-02,
       -9.69822932e-01,  2.37842901e-01]), 'targetState': array([  4.29059969, -16.44964298,  74.        ]), 'previousTarget': array([  4.29059969, -16.44964298,  74.        ])}
episode index:19027
target thresh 85.8292937793879
target distance 23.795190790607954
model initialize at round 19027
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-37.39549255, -25.55439334,  15.66797979]), 'distance': 27.5, 'localFrame': array([[-0.77234193, -0.33700761,  0.53843645],
       [ 0.39993018, -0.91654561,  0.        ],
       [ 0.49350157,  0.21533699,  0.84266612]]), 'currentState': array([-20.67912249,  -3.95363235,  12.47109124,  -0.77234193,
        -0.33700761,   0.53843645]), 'targetState': array([-39.13160959, -27.79779003,  16.        ]), 'previousTarget': array([-36.81981226, -24.98201247,  15.40833053])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278264364535835
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([-39.13160959, -27.79779003,  16.        ]), 'distance': 13.267321246311452, 'localFrame': array([[-0.24511806, -0.47110926,  0.84733299],
       [ 0.88710791, -0.46156208,  0.        ],
       [ 0.39109677,  0.7516758 ,  0.53106196]]), 'currentState': array([-32.27592915, -19.45339626,   8.29334371,  -0.24511806,
        -0.47110926,   0.84733299]), 'targetState': array([-39.13160959, -27.79779003,  16.        ]), 'previousTarget': array([-39.13160959, -27.79779003,  16.        ])}
episode index:19028
target thresh 85.83071077915878
target distance 75.0
model initialize at round 19028
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.62808359,  9.70679008, 32.63262765]), 'distance': 27.5, 'localFrame': array([[-0.0888157 ,  0.94132955,  0.32559246],
       [-0.99557841, -0.09393415,  0.        ],
       [ 0.03058425, -0.32415282,  0.94551021]]), 'currentState': array([ 3.46182858, 17.82253348,  8.37199324, -0.0888157 ,  0.94132955,
        0.32559246]), 'targetState': array([-27.15979942,  -6.80773791,  82.        ]), 'previousTarget': array([-6.39519435,  8.97587875, 31.43967861])}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6278221971454715
{'scaleFactor': 20, 'timeStep': 61, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.15979942,  -6.80773791,  82.        ]), 'distance': 2.480918878110129, 'localFrame': array([[-0.23214693,  0.29619942,  0.92648459],
       [-0.78706801, -0.61686623,  0.        ],
       [ 0.57151706, -0.72920638,  0.37633269]]), 'currentState': array([-25.95326475,  -6.91231506,  79.83475263,  -0.23214693,
         0.29619942,   0.92648459]), 'targetState': array([-27.15979942,  -6.80773791,  82.        ]), 'previousTarget': array([-27.15979942,  -6.80773791,  82.        ])}
episode index:19029
target thresh 85.8321276372368
target distance 41.0
model initialize at round 19029
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.71277244, -21.65950094,  47.7453805 ]), 'distance': 27.5, 'localFrame': array([[-0.03241797,  0.94626266, -0.32177017],
       [-0.99941368, -0.03423887,  0.        ],
       [-0.01101705,  0.32158151,  0.9468178 ]]), 'currentState': array([ 5.23691550e-01, -6.73351411e+00,  6.96793383e+01, -3.24179737e-02,
        9.46262665e-01, -3.21770174e-01]), 'targetState': array([-12.56733859, -33.73517453,  30.        ]), 'previousTarget': array([ -6.72936702, -22.11904105,  48.50006471])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278292660113255
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.56733859, -33.73517453,  30.        ]), 'distance': 3.5924348307717935, 'localFrame': array([[-0.2316028 , -0.58966104, -0.77373122],
       [ 0.93077822, -0.36558434,  0.        ],
       [-0.28286402, -0.72017216,  0.63351401]]), 'currentState': array([-12.39504031, -31.62218617,  32.9002037 ,  -0.2316028 ,
        -0.58966104,  -0.77373122]), 'targetState': array([-12.56733859, -33.73517453,  30.        ]), 'previousTarget': array([-12.56733859, -33.73517453,  30.        ])}
episode index:19030
target thresh 85.83354435363606
target distance 49.87256913562261
model initialize at round 19030
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.96926857,  -0.4220154 ,  22.33835992]), 'distance': 27.5, 'localFrame': array([[ 0.77392972, -0.39937564, -0.49145894],
       [ 0.45857768,  0.88865433,  0.        ],
       [ 0.43673711, -0.2253721 ,  0.87090075]]), 'currentState': array([-21.35562697,  23.95180741,  34.99714233,   0.77392972,
        -0.39937564,  -0.49145894]), 'targetState': array([-18.50848452, -26.10432916,   9.        ]), 'previousTarget': array([-20.77304196,  -0.34325652,  22.94652358])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6278328698131246
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.50848452, -26.10432916,   9.        ]), 'distance': 3.230239412519153, 'localFrame': array([[ 0.43307176, -0.83109547,  0.34889709],
       [ 0.88682246,  0.46211029,  0.        ],
       [-0.16122893,  0.30940977,  0.93716104]]), 'currentState': array([-18.24238896, -23.97671143,   6.58403588,   0.43307176,
        -0.83109547,   0.34889709]), 'targetState': array([-18.50848452, -26.10432916,   9.        ]), 'previousTarget': array([-18.50848452, -26.10432916,   9.        ])}
episode index:19031
target thresh 85.83496092837079
target distance 39.580328916882706
model initialize at round 19031
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.95856058, -11.53543529,  77.6905269 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.58554816,  0.76266275,  0.2747342 ],
       [-0.79318431,  0.60898165,  0.        ],
       [-0.16730809, -0.21791486,  0.96152021]]), 'currentState': array([ -5.19232935, -38.57121979,  79.30279137,   0.58554816,
         0.76266275,   0.2747342 ]), 'targetState': array([-1.19999197e+01,  4.38909286e-02,  7.70000000e+01]), 'previousTarget': array([-10.14398088, -12.34824691,  77.3130883 ])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6278403419797908
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.19999197e+01,  4.38909286e-02,  7.70000000e+01]), 'distance': 3.6213166157512546, 'localFrame': array([[-0.32261741,  0.75351185, -0.57283321],
       [-0.91928466, -0.39359333,  0.        ],
       [-0.22546333,  0.52659679,  0.81967195]]), 'currentState': array([-10.3755555 ,  -2.92749577,  78.28305727,  -0.32261741,
         0.75351185,  -0.57283321]), 'targetState': array([-1.19999197e+01,  4.38909286e-02,  7.70000000e+01]), 'previousTarget': array([-1.19999197e+01,  4.38909286e-02,  7.70000000e+01])}
episode index:19032
target thresh 85.8363773614551
target distance 34.63954823263549
model initialize at round 19032
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.07000478, -8.23257219, 96.86256232]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.64305869,  0.19694402, -0.74005984],
       [-0.2928357 ,  0.95616278,  0.        ],
       [ 0.70761767,  0.21671594,  0.67254103]]), 'currentState': array([-22.35968707, -10.11206879,  96.28854354,   0.64305869,
         0.19694402,  -0.74005984]), 'targetState': array([11.63751309, -7.78256314, 97.        ]), 'previousTarget': array([ 4.44036191, -8.20031426, 97.20777266])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278512006683565
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.63751309, -7.78256314, 97.        ]), 'distance': 2.553850591187531, 'localFrame': array([[ 0.6131188 ,  0.50225871, -0.6097717 ],
       [-0.63370327,  0.77357622,  0.        ],
       [ 0.47170488,  0.38641432,  0.79257711]]), 'currentState': array([ 9.17321978, -8.42295161, 96.80172265,  0.6131188 ,  0.50225871,
       -0.6097717 ]), 'targetState': array([11.63751309, -7.78256314, 97.        ]), 'previousTarget': array([11.63751309, -7.78256314, 97.        ])}
episode index:19033
target thresh 85.8377936529032
target distance 28.688604366449027
model initialize at round 19033
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.86129544e+00,  5.33404325e-02,  6.58354640e+01]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.24907642, -0.59146892, -0.76689338],
       [ 0.92161482,  0.38810581,  0.        ],
       [ 0.29763577, -0.7067803 ,  0.64177453]]), 'currentState': array([ -1.57595271, -20.30879026,  84.17696998,   0.24907642,
        -0.59146892,  -0.76689338]), 'targetState': array([-4.83759045,  8.75201226, 58.        ]), 'previousTarget': array([-3.81097229, -0.32061289, 66.85485753])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278590797352227
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-4.83759045,  8.75201226, 58.        ]), 'distance': 1.2878085043923182, 'localFrame': array([[-0.30520238,  0.81614105, -0.4906784 ],
       [-0.93664969, -0.35026754,  0.        ],
       [-0.17186871,  0.45959377,  0.87134075]]), 'currentState': array([-4.33194744,  7.62472938, 57.63666931, -0.30520238,  0.81614105,
       -0.4906784 ]), 'targetState': array([-4.83759045,  8.75201226, 58.        ]), 'previousTarget': array([-4.83759045,  8.75201226, 58.        ])}
episode index:19034
target thresh 85.83920980272926
target distance 46.370971371531034
model initialize at round 19034
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.99355111, -28.85219887,  89.03331263]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.21593125, -0.88458204, -0.41338639],
       [ 0.97147493,  0.23714228,  0.        ],
       [ 0.09803139, -0.40159451,  0.91055571]]), 'currentState': array([-0.51029698, -1.95124187, 90.54862413,  0.21593125, -0.88458204,
       -0.41338639]), 'targetState': array([  8.74670401, -47.19634699,  88.        ]), 'previousTarget': array([  4.83008786, -27.69128004,  89.6825239 ])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278661448066496
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.74670401, -47.19634699,  88.        ]), 'distance': 2.0961738080289827, 'localFrame': array([[ 0.52563827, -0.72967186,  0.43735957],
       [ 0.81138956,  0.58450576,  0.        ],
       [-0.25563919,  0.35486899,  0.89928672]]), 'currentState': array([  7.3808    , -45.63353846,  87.70694644,   0.52563827,
        -0.72967186,   0.43735957]), 'targetState': array([  8.74670401, -47.19634699,  88.        ]), 'previousTarget': array([  8.74670401, -47.19634699,  88.        ])}
episode index:19035
target thresh 85.84062581094739
target distance 62.0
model initialize at round 19035
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.87943784,   3.24888243,  64.18493456]), 'distance': 27.500000000000004, 'localFrame': array([[-0.5908708 ,  0.02596042,  0.80634841],
       [-0.04389352, -0.99903622,  0.        ],
       [ 0.80557126, -0.03539347,  0.59144082]]), 'currentState': array([-1.61265615e+01, -4.29475478e+00,  3.92303620e+01, -5.90870799e-01,
        2.59604187e-02,  8.06348408e-01]), 'targetState': array([-37.44165816,  14.07559002, 100.        ]), 'previousTarget': array([-24.06334064,   2.50926287,  62.83227207])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6278630840301035
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-37.44165816,  14.07559002, 100.        ]), 'distance': 2.6248419902826394, 'localFrame': array([[ 0.44822142, -0.50277472,  0.73913134],
       [ 0.74644238,  0.66545005,  0.        ],
       [-0.49185499,  0.55171896,  0.67356133]]), 'currentState': array([-36.64367728,  14.0605243 ,  97.49944106,   0.44822142,
        -0.50277472,   0.73913134]), 'targetState': array([-37.44165816,  14.07559002, 100.        ]), 'previousTarget': array([-37.44165816,  14.07559002, 100.        ])}
episode index:19036
target thresh 85.84204167757179
target distance 16.001548933293762
model initialize at round 19036
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.01281405, -42.57753007,  27.        ]), 'distance': 21.389365641855697, 'localFrame': array([[ 0.1055887 , -0.98947962, -0.09889949],
       [ 0.99435451,  0.1061089 ,  0.        ],
       [ 0.01049412, -0.09834116,  0.99509743]]), 'currentState': array([ -9.07357937, -28.29104723,  42.62155838,   0.1055887 ,
        -0.98947962,  -0.09889949]), 'targetState': array([ -6.01281405, -42.57753007,  27.        ]), 'previousTarget': array([ -6.01281405, -42.57753007,  27.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6278771343095713
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.01281405, -42.57753007,  27.        ]), 'distance': 2.3213495838369624, 'localFrame': array([[-0.21977186,  0.27772186, -0.93518495],
       [-0.78417123, -0.6205445 ,  0.        ],
       [-0.58032388,  0.73334514,  0.35415971]]), 'currentState': array([ -6.59807362, -40.82133603,  28.4006847 ,  -0.21977186,
         0.27772186,  -0.93518495]), 'targetState': array([ -6.01281405, -42.57753007,  27.        ]), 'previousTarget': array([ -6.01281405, -42.57753007,  27.        ])}
episode index:19037
target thresh 85.84345740261658
target distance 30.711741828860415
model initialize at round 19037
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.68914332, -11.34975408,  16.52626317]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.68838955,  0.16589678, -0.70611478],
       [-0.23428526,  0.9721679 ,  0.        ],
       [ 0.68646212,  0.16543228,  0.70809739]]), 'currentState': array([-12.77339947, -26.01740828,  30.6770382 ,   0.68838955,
         0.16589678,  -0.70611478]), 'targetState': array([16.8133749 , -2.51205582,  8.        ]), 'previousTarget': array([  4.77572582, -11.50374189,  17.40694213])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6278834004593781
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.8133749 , -2.51205582,  8.        ]), 'distance': 4.346628880990721, 'localFrame': array([[ 0.10703065,  0.82491454, -0.55503192],
       [-0.99168759,  0.12866905,  0.        ],
       [ 0.07141543,  0.55041827,  0.83182905]]), 'currentState': array([14.67933443, -5.26235239, 10.60286816,  0.10703065,  0.82491454,
       -0.55503192]), 'targetState': array([16.8133749 , -2.51205582,  8.        ]), 'previousTarget': array([16.8133749 , -2.51205582,  8.        ])}
episode index:19038
target thresh 85.84487298609598
target distance 37.230907612408146
model initialize at round 19038
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.45910583, -3.8003302 , 35.76379911]), 'distance': 27.5, 'localFrame': array([[ 0.87763209, -0.43197832, -0.20774178],
       [ 0.44161268,  0.8972058 ,  0.        ],
       [ 0.18638713, -0.0917414 ,  0.9781837 ]]), 'currentState': array([ 5.08208251,  4.42195972, 53.46054785,  0.87763209, -0.43197832,
       -0.20774178]), 'targetState': array([ 40.6247012 , -10.65990867,  21.        ]), 'previousTarget': array([23.26877239, -3.38399667, 35.91743708])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.627888500356195
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 40.6247012 , -10.65990867,  21.        ]), 'distance': 3.46726610540335, 'localFrame': array([[ 0.81074823, -0.47400722,  0.34351778],
       [ 0.50472145,  0.86328226,  0.        ],
       [-0.2965528 ,  0.17338079,  0.93914617]]), 'currentState': array([37.68478405, -9.91789738, 22.6817374 ,  0.81074823, -0.47400722,
        0.34351778]), 'targetState': array([ 40.6247012 , -10.65990867,  21.        ]), 'previousTarget': array([ 40.6247012 , -10.65990867,  21.        ])}
episode index:19039
target thresh 85.84628842802408
target distance 35.0
model initialize at round 19039
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.79350946, -20.17871871,  69.80821553]), 'distance': 27.5, 'localFrame': array([[ 0.79876494,  0.09324712, -0.59437324],
       [-0.1159517 ,  0.99325485,  0.        ],
       [ 0.5903641 ,  0.06891859,  0.80418931]]), 'currentState': array([-2.02280395, -5.700988  , 89.98295347,  0.79876494,  0.09324712,
       -0.59437324]), 'targetState': array([ 17.88096034, -30.08772602,  56.        ]), 'previousTarget': array([  8.79856621, -19.349021  ,  70.96253707])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6278863547827487
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([ 17.88096034, -30.08772602,  56.        ]), 'distance': 1.9904419983437718, 'localFrame': array([[-0.12435346, -0.90112552, -0.41534205],
       [ 0.99061216, -0.13670243,  0.        ],
       [-0.05677827, -0.41144288,  0.90966532]]), 'currentState': array([ 17.90818253, -28.10106827,  55.88037861,  -0.12435346,
        -0.90112552,  -0.41534205]), 'targetState': array([ 17.88096034, -30.08772602,  56.        ]), 'previousTarget': array([ 17.88096034, -30.08772602,  56.        ])}
episode index:19040
target thresh 85.84770372841508
target distance 41.0
model initialize at round 19040
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.79190467,  0.75044102, 38.52707324]), 'distance': 27.5, 'localFrame': array([[ 0.42840036,  0.34735607,  0.8341564 ],
       [-0.6298068 ,  0.77675182,  0.        ],
       [-0.6479325 , -0.52535737,  0.55152798]]), 'currentState': array([ -5.77377248, -15.03931357,  20.55748634,   0.42840036,
         0.34735607,   0.8341564 ]), 'targetState': array([24.00233808, 19.61855669, 60.        ]), 'previousTarget': array([ 7.49148632, -0.18737775, 37.21122128])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278892258630611
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.00233808, 19.61855669, 60.        ]), 'distance': 3.5865222115802404, 'localFrame': array([[ 0.88743482,  0.34394633, -0.30685561],
       [-0.36138076,  0.93241833,  0.        ],
       [ 0.2861178 ,  0.11089171,  0.95175608]]), 'currentState': array([21.60507871, 18.30669154, 62.32277827,  0.88743482,  0.34394633,
       -0.30685561]), 'targetState': array([24.00233808, 19.61855669, 60.        ]), 'previousTarget': array([24.00233808, 19.61855669, 60.        ])}
episode index:19041
target thresh 85.84911888728313
target distance 63.19087056333792
model initialize at round 19041
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.09664114,  17.44538865,  57.43704987]), 'distance': 27.5, 'localFrame': array([[ 0.65687219, -0.64626624,  0.38840554],
       [ 0.70132857,  0.71283815,  0.        ],
       [-0.27687029,  0.27239991,  0.92148854]]), 'currentState': array([-37.30485615,  23.61999349,  51.84654712,   0.65687219,
        -0.64626624,   0.38840554]), 'targetState': array([24.35838536,  9.09225288, 65.        ]), 'previousTarget': array([-12.64799939,  18.20825236,  57.38682744])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.627892458709401
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.35838536,  9.09225288, 65.        ]), 'distance': 2.688386721855723, 'localFrame': array([[ 0.94690495, -0.32141438,  0.00798795],
       [ 0.32142464,  0.94693516,  0.        ],
       [-0.00756407,  0.00256752,  0.9999681 ]]), 'currentState': array([ 2.24108470e+01,  1.08509854e+01,  6.44157249e+01,  9.46904952e-01,
       -3.21414382e-01,  7.98795093e-03]), 'targetState': array([24.35838536,  9.09225288, 65.        ]), 'previousTarget': array([24.35838536,  9.09225288, 65.        ])}
episode index:19042
target thresh 85.85053390464235
target distance 40.12742722725077
model initialize at round 19042
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.27714279, -26.4276575 ,  65.92119683]), 'distance': 27.5, 'localFrame': array([[ 0.78136824, -0.02345038,  0.62362949],
       [ 0.02999844,  0.99954995,  0.        ],
       [-0.62334883,  0.01870791,  0.78172006]]), 'currentState': array([-5.47788346e+00, -4.74602162e+01,  5.11319799e+01,  7.81368245e-01,
       -2.34503799e-02,  6.23629494e-01]), 'targetState': array([13.56357729, -6.40541732, 80.        ]), 'previousTarget': array([  3.66140354, -26.10175026,  65.27466052])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.627895329167643
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([13.56357729, -6.40541732, 80.        ]), 'distance': 2.7986444755899456, 'localFrame': array([[ 0.06914853,  0.87364852,  0.48161887],
       [-0.99688235,  0.07890238,  0.        ],
       [-0.03800088, -0.48011735,  0.87638077]]), 'currentState': array([ 1.32672795e+01, -9.17455722e+00,  7.97234449e+01,  6.91485317e-02,
        8.73648523e-01,  4.81618873e-01]), 'targetState': array([13.56357729, -6.40541732, 80.        ]), 'previousTarget': array([13.56357729, -6.40541732, 80.        ])}
episode index:19043
target thresh 85.85194878050692
target distance 71.05201720173298
model initialize at round 19043
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.39812904, 21.72935196, 94.42631792]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.73027282, -0.02091555, -0.68283538],
       [ 0.02862899,  0.99959011,  0.        ],
       [ 0.68255549, -0.01954889,  0.73057227]]), 'currentState': array([-3.65265801e+01,  2.56824768e+01,  9.65873240e+01,  7.30272818e-01,
       -2.09155452e-02, -6.82835377e-01]), 'targetState': array([33.61456679, 15.46159433, 91.        ]), 'previousTarget': array([-10.309402  ,  21.33885187,  95.32736175])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6278964427624341
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.61456679, 15.46159433, 91.        ]), 'distance': 4.15434719175724, 'localFrame': array([[ 0.78766727,  0.27482862,  0.55140684],
       [-0.32943731,  0.94417745,  0.        ],
       [-0.5206259 , -0.18165398,  0.83423648]]), 'currentState': array([31.19040957, 17.64489726, 88.42798704,  0.78766727,  0.27482862,
        0.55140684]), 'targetState': array([33.61456679, 15.46159433, 91.        ]), 'previousTarget': array([33.61456679, 15.46159433, 91.        ])}
episode index:19044
target thresh 85.85336351489097
target distance 38.091984058106114
model initialize at round 19044
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.16503979,  -4.45887291,  71.73460083]), 'distance': 27.500000000000004, 'localFrame': array([[-0.98142775,  0.16263328, -0.1017349 ],
       [-0.1634815 , -0.9865464 ,  0.        ],
       [-0.1003662 ,  0.01663177,  0.99481154]]), 'currentState': array([-4.54202734, -7.37731061, 89.6907719 , -0.98142775,  0.16263328,
       -0.1017349 ]), 'targetState': array([-40.93949654,  -2.22657201,  58.        ]), 'previousTarget': array([-24.05605608,  -4.55329886,  71.74007334])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.627902705595281
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.93949654,  -2.22657201,  58.        ]), 'distance': 3.5757898494938756, 'localFrame': array([[-0.94960511,  0.03660119, -0.31130449],
       [-0.03851499, -0.99925802,  0.        ],
       [-0.31107351,  0.01198989,  0.95031022]]), 'currentState': array([-4.01807448e+01, -4.40697786e+00,  6.07306408e+01, -9.49605112e-01,
        3.66011851e-02, -3.11304488e-01]), 'targetState': array([-40.93949654,  -2.22657201,  58.        ]), 'previousTarget': array([-40.93949654,  -2.22657201,  58.        ])}
episode index:19045
target thresh 85.85477810780866
target distance 57.0
model initialize at round 19045
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.39252201, 22.99091423, 51.09240199]), 'distance': 27.5, 'localFrame': array([[ 0.93425577, -0.05729004,  0.35197161],
       [ 0.06120661,  0.99812512,  0.        ],
       [-0.3513117 ,  0.02154299,  0.93601068]]), 'currentState': array([-12.21742223,  24.77933515,  30.92506388,   0.93425577,
        -0.05729004,   0.35197161]), 'targetState': array([40.44998351, 19.7179825 , 88.        ]), 'previousTarget': array([ 5.00536118, 23.44577129, 50.8456407 ])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6278999465930959
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([40.44998351, 19.7179825 , 88.        ]), 'distance': 3.883555256806956, 'localFrame': array([[ 0.98952722, -0.13005099,  0.0626309 ],
       [ 0.13030681,  0.99147372,  0.        ],
       [-0.06209689,  0.00816123,  0.99803676]]), 'currentState': array([ 3.96099051e+01,  2.21170649e+01,  8.50639017e+01,  9.89527216e-01,
       -1.30050986e-01,  6.26309015e-02]), 'targetState': array([40.44998351, 19.7179825 , 88.        ]), 'previousTarget': array([40.44998351, 19.7179825 , 88.        ])}
episode index:19046
target thresh 85.85619255927412
target distance 29.0
model initialize at round 19046
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.44226821,  0.26108354,  6.47467012]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.96368303,  0.12921983, -0.23370336],
       [-0.13290011,  0.99112944,  0.        ],
       [ 0.23163028,  0.0310592 ,  0.97230794]]), 'currentState': array([32.37009356, -0.37764397, 32.79940397,  0.96368303,  0.12921983,
       -0.23370336]), 'targetState': array([23.99816392,  0.29686404,  5.        ]), 'previousTarget': array([24.54742191,  0.23744643,  7.27601109])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6279107941709222
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.99816392,  0.29686404,  5.        ]), 'distance': 2.213571124017145, 'localFrame': array([[-0.84705389,  0.51790273, -0.1194842 ],
       [-0.5216397 , -0.85316588,  0.        ],
       [-0.10193984,  0.0623277 ,  0.9928361 ]]), 'currentState': array([25.60252261,  1.76069841,  5.4279244 , -0.84705389,  0.51790273,
       -0.1194842 ]), 'targetState': array([23.99816392,  0.29686404,  5.        ]), 'previousTarget': array([23.99816392,  0.29686404,  5.        ])}
episode index:19047
target thresh 85.8576068693015
target distance 66.0
model initialize at round 19047
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.96936078, 16.00339926, 42.42432721]), 'distance': 27.500000000000004, 'localFrame': array([[-0.05975616, -0.9980009 ,  0.020577  ],
       [ 0.99821225, -0.05976881,  0.        ],
       [ 0.00122986,  0.02054021,  0.99978827]]), 'currentState': array([-2.55281658e+00,  2.41965571e+01,  1.62114379e+01, -5.97561594e-02,
       -9.98000896e-01,  2.05769968e-02]), 'targetState': array([-6.16206963,  3.32097845, 83.        ]), 'previousTarget': array([-4.14694349, 16.97287205, 42.99664411])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6279057014970868
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([-6.16206963,  3.32097845, 83.        ]), 'distance': 2.41129063613286, 'localFrame': array([[-0.15801095, -0.82218214,  0.5468538 ],
       [ 0.98202881, -0.18873105,  0.        ],
       [ 0.10320829,  0.53702618,  0.83722812]]), 'currentState': array([-5.84004433,  3.10448014, 80.62013641, -0.15801095, -0.82218214,
        0.5468538 ]), 'targetState': array([-6.16206963,  3.32097845, 83.        ]), 'previousTarget': array([-6.16206963,  3.32097845, 83.        ])}
episode index:19048
target thresh 85.85902103790497
target distance 27.984830856323242
model initialize at round 19048
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.86430695,  10.63569654,  28.68803989]), 'distance': 27.5, 'localFrame': array([[-0.86003967, -0.12625267,  0.49436023],
       [ 0.14524202, -0.98939616,  0.        ],
       [ 0.48911811,  0.07180188,  0.86925713]]), 'currentState': array([-28.10681855,  27.51538541,  15.49630394,  -0.86003967,
        -0.12625267,   0.49436023]), 'targetState': array([-0.,  0., 37.]), 'previousTarget': array([-10.66506656,  10.98758941,  27.96958786])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6279115702918735
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 37.]), 'distance': 3.5463633807235104, 'localFrame': array([[ 0.02204517, -0.74187325,  0.67017766],
       [ 0.99955879,  0.02970243,  0.        ],
       [-0.01990591,  0.66988197,  0.74220071]]), 'currentState': array([-1.50526645e-01,  2.93715072e+00,  3.89817115e+01,  2.20451677e-02,
       -7.41873245e-01,  6.70177662e-01]), 'targetState': array([-0.,  0., 37.]), 'previousTarget': array([ 0.,  0., 37.])}
episode index:19049
target thresh 85.86043506509864
target distance 66.0
model initialize at round 19049
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.12915276,   1.76162789,  59.69577639]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.31492803,  0.59833453, -0.73676056],
       [-0.88490901,  0.46576394,  0.        ],
       [ 0.3431565 ,  0.65196605,  0.67615374]]), 'currentState': array([-27.76916962,   0.15307722,  80.731279  ,   0.31492803,
         0.59833453,  -0.73676056]), 'targetState': array([26.51338659,  5.10297284, 16.        ]), 'previousTarget': array([-11.33198956,   1.4000898 ,  60.99596686])}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.6278992238067326
{'scaleFactor': 20, 'timeStep': 94, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([26.51338659,  5.10297284, 16.        ]), 'distance': 3.233505665433792, 'localFrame': array([[ 0.49309598, -0.37286166,  0.78602197],
       [ 0.60314223,  0.79763365,  0.        ],
       [-0.62695757,  0.47408305,  0.61819856]]), 'currentState': array([28.19733344,  3.43219855, 13.80263905,  0.49309598, -0.37286166,
        0.78602197]), 'targetState': array([26.51338659,  5.10297284, 16.        ]), 'previousTarget': array([26.51338659,  5.10297284, 16.        ])}
episode index:19050
target thresh 85.86184895089667
target distance 28.024993838645024
model initialize at round 19050
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.22665019,  17.57998079,  65.        ]), 'distance': 27.349078554170685, 'localFrame': array([[-0.3517879 ,  0.75702742, -0.55059492],
       [-0.90686667, -0.42141766,  0.        ],
       [-0.23203042,  0.49931618,  0.83477257]]), 'currentState': array([-7.11406906, -9.067071  , 65.73694011, -0.3517879 ,  0.75702742,
       -0.55059492]), 'targetState': array([-13.22665019,  17.57998079,  65.        ]), 'previousTarget': array([-12.85235629,  16.14675893,  65.05114084])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6279114099980525
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.22665019,  17.57998079,  65.        ]), 'distance': 2.382014892839164, 'localFrame': array([[-0.63307603,  0.77353793,  0.02921998],
       [-0.77386837, -0.63334647,  0.        ],
       [ 0.01850637, -0.02261242,  0.99957301]]), 'currentState': array([-1.27976676e+01,  1.52779276e+01,  6.45635139e+01, -6.33076033e-01,
        7.73537930e-01,  2.92199771e-02]), 'targetState': array([-13.22665019,  17.57998079,  65.        ]), 'previousTarget': array([-13.22665019,  17.57998079,  65.        ])}
episode index:19051
target thresh 85.86326269531317
target distance 22.556515689684446
model initialize at round 19051
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.56647163,  16.35375782,  31.63815107]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.6849939 ,  0.65013304,  0.32880144],
       [-0.68840923,  0.7253225 ,  0.        ],
       [-0.23848708, -0.22634995,  0.94439907]]), 'currentState': array([-30.95329993,  -3.16096244,  47.3150715 ,   0.6849939 ,
         0.65013304,   0.32880144]), 'targetState': array([-18.37661134,  18.39293766,  30.        ]), 'previousTarget': array([-19.77187032,  16.03571219,  31.67204936])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.627922254127354
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.37661134,  18.39293766,  30.        ]), 'distance': 1.9531795827357703, 'localFrame': array([[ 0.39005755, -0.12742089, -0.91193148],
       [ 0.31052324,  0.95056579,  0.        ],
       [ 0.86685086, -0.28317592,  0.41034251]]), 'currentState': array([-18.3375314 ,  17.69939065,  31.82547961,   0.39005755,
        -0.12742089,  -0.91193148]), 'targetState': array([-18.37661134,  18.39293766,  30.        ]), 'previousTarget': array([-18.37661134,  18.39293766,  30.        ])}
episode index:19052
target thresh 85.86467629836233
target distance 36.32527291572213
model initialize at round 19052
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.72559088, 12.17349343, 60.75931909]), 'distance': 27.500000000000004, 'localFrame': array([[-0.66838599, -0.61865746,  0.41294445],
       [ 0.67927888, -0.73388024,  0.        ],
       [ 0.30305177,  0.28050444,  0.91075621]]), 'currentState': array([ 9.89054963, 35.45198423, 47.05949912, -0.66838599, -0.61865746,
        0.41294445]), 'targetState': array([ 1.99578725, -0.12974298, 68.        ]), 'previousTarget': array([ 5.45255075, 12.98436798, 60.41860259])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6279305339723484
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.99578725, -0.12974298, 68.        ]), 'distance': 3.031358655466002, 'localFrame': array([[ 0.49647242, -0.65246779,  0.57253901],
       [ 0.79581134,  0.60554464,  0.        ],
       [-0.34669793,  0.45563303,  0.81987748]]), 'currentState': array([ 3.77564509,  1.98314461, 66.75222301,  0.49647242, -0.65246779,
        0.57253901]), 'targetState': array([ 1.99578725, -0.12974298, 68.        ]), 'previousTarget': array([ 1.99578725, -0.12974298, 68.        ])}
episode index:19053
target thresh 85.86608976005822
target distance 57.0
model initialize at round 19053
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.62138849, -19.82236718,  28.08569134]), 'distance': 27.5, 'localFrame': array([[ 0.49622829,  0.57809896,  0.6477338 ],
       [-0.75879276,  0.65133213,  0.        ],
       [-0.42188984, -0.49149572,  0.76186674]]), 'currentState': array([-29.46167331, -28.77037761,   7.4636525 ,   0.49622829,
         0.57809896,   0.6477338 ]), 'targetState': array([13.19713309, -4.67286618, 63.        ]), 'previousTarget': array([-13.9837545 , -20.69323419,  26.74471857])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278975786593447
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([13.19713309, -4.67286618, 63.        ]), 'distance': 22.969369806449382, 'localFrame': array([[-0.83356821,  0.23701859, -0.4989852 ],
       [-0.27350071, -0.9618718 ,  0.        ],
       [-0.47995979,  0.13647281,  0.86661051]]), 'currentState': array([  8.9525765 , -10.83688956,  41.28409559,  -0.83356821,
         0.23701859,  -0.4989852 ]), 'targetState': array([13.19713309, -4.67286618, 63.        ]), 'previousTarget': array([13.19713309, -4.67286618, 63.        ])}
episode index:19054
target thresh 85.867503080415
target distance 39.92006674624459
model initialize at round 19054
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.54870773,  3.8259433 , 26.04996328]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.66192781, -0.29399383,  0.68950649],
       [ 0.40591211,  0.91391212,  0.        ],
       [-0.63014833,  0.27987904,  0.7242795 ]]), 'currentState': array([  5.20677079, -16.66083466,  32.03320251,   0.66192781,
        -0.29399383,   0.68950649]), 'targetState': array([40.08403894, 24.54118625, 20.        ]), 'previousTarget': array([22.36275206,  4.70801865, 25.46504205])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6278980136059417
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([40.08403894, 24.54118625, 20.        ]), 'distance': 3.5326900733233297, 'localFrame': array([[-0.09916231,  0.91020944,  0.4021015 ],
       [-0.99411786, -0.10830367,  0.        ],
       [ 0.04354907, -0.39973629,  0.9155951 ]]), 'currentState': array([37.20638053, 22.64025515, 19.23485804, -0.09916231,  0.91020944,
        0.4021015 ]), 'targetState': array([40.08403894, 24.54118625, 20.        ]), 'previousTarget': array([40.08403894, 24.54118625, 20.        ])}
episode index:19055
target thresh 85.86891625944683
target distance 65.0
model initialize at round 19055
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.55933444, 16.41348457, 68.6602417 ]), 'distance': 27.5, 'localFrame': array([[ 0.75024062,  0.34256305, -0.5654994 ],
       [-0.41535445,  0.90965965,  0.        ],
       [ 0.51441199,  0.23488269,  0.82474871]]), 'currentState': array([-2.95254847, 23.88943046, 94.5442016 ,  0.75024062,  0.34256305,
       -0.5654994 ]), 'targetState': array([10.79187406,  5.24742358, 30.        ]), 'previousTarget': array([ 1.79315407, 15.68690356, 69.06659458])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6278958693347584
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([10.79187406,  5.24742358, 30.        ]), 'distance': 2.9936093689350365, 'localFrame': array([[ 0.10135073, -0.82524728, -0.55560324],
       [ 0.99254279,  0.12189672,  0.        ],
       [ 0.06772621, -0.55145999,  0.83144756]]), 'currentState': array([10.94049882,  6.01473142, 32.88978311,  0.10135073, -0.82524728,
       -0.55560324]), 'targetState': array([10.79187406,  5.24742358, 30.        ]), 'previousTarget': array([10.79187406,  5.24742358, 30.        ])}
episode index:19056
target thresh 85.87032929716784
target distance 48.994473184836515
model initialize at round 19056
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.80557621, 29.53099589, 84.43849524]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.37702732, -0.72833375, -0.57217161],
       [ 0.88806685,  0.45971434,  0.        ],
       [ 0.26303549, -0.50812664,  0.82013392]]), 'currentState': array([-20.18757093,  25.7455499 ,  95.26729918,   0.37702732,
        -0.72833375,  -0.57217161]), 'targetState': array([28.89788124, 33.18000091, 74.        ]), 'previousTarget': array([ 4.8475437 , 30.35496201, 84.79932881])}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.627882716203499
{'scaleFactor': 20, 'timeStep': 98, 'trapCount': 60, 'trapConfig': [], 'currentTarget': array([28.89788124, 33.18000091, 74.        ]), 'distance': 2.346108593671689, 'localFrame': array([[ 0.51829182,  0.40167733, -0.75500259],
       [-0.61257278,  0.79041419,  0.        ],
       [ 0.59676476,  0.46249404,  0.65572181]]), 'currentState': array([26.99744716, 31.84225587, 73.67904195,  0.51829182,  0.40167733,
       -0.75500259]), 'targetState': array([28.89788124, 33.18000091, 74.        ]), 'previousTarget': array([28.89788124, 33.18000091, 74.        ])}
episode index:19057
target thresh 85.87174219359211
target distance 48.965387935799185
model initialize at round 19057
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  9.65343749, -14.58084562,  72.21162808]), 'distance': 27.5, 'localFrame': array([[ 0.69161223,  0.02243934, -0.72192036],
       [-0.03242791,  0.99947408,  0.        ],
       [ 0.72154068,  0.02341037,  0.69197615]]), 'currentState': array([-1.76533051e+01, -1.62591763e+01,  7.50000000e+01,  6.91612226e-01,
        2.24393413e-02, -7.21920359e-01]), 'targetState': array([ 31.31208288, -13.24965907,  70.        ]), 'previousTarget': array([  9.65343749, -14.58084562,  72.21162808])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278497703164068
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 72, 'trapConfig': [], 'currentTarget': array([ 31.31208288, -13.24965907,  70.        ]), 'distance': 9.001457867145117, 'localFrame': array([[ 0.59244601,  0.4616715 ,  0.66020236],
       [-0.61467051,  0.78878398,  0.        ],
       [-0.52075704, -0.40580692,  0.75108778]]), 'currentState': array([ 22.4899135 , -14.42139021,  68.64995657,   0.59244601,
         0.4616715 ,   0.66020236]), 'targetState': array([ 31.31208288, -13.24965907,  70.        ]), 'previousTarget': array([ 31.31208288, -13.24965907,  70.        ])}
episode index:19058
target thresh 85.87315494873383
target distance 34.0
model initialize at round 19058
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 30.302696  , -32.90326147,  71.93539726]), 'distance': 27.5, 'localFrame': array([[-0.43014778, -0.63179472,  0.644832  ],
       [ 0.82660553, -0.56278175,  0.        ],
       [ 0.36289968,  0.5330217 ,  0.76432434]]), 'currentState': array([ 32.2127069 , -35.61925255,  44.63658317,  -0.43014778,
        -0.63179472,   0.644832  ]), 'targetState': array([ 29.8783751 , -32.29988702,  78.        ]), 'previousTarget': array([ 30.39264195, -32.61721023,  71.38593671])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278606136970214
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 29.8783751 , -32.29988702,  78.        ]), 'distance': 1.6786529983032652, 'localFrame': array([[-0.88030903,  0.1453722 ,  0.45157827],
       [-0.16293103, -0.98663746,  0.        ],
       [ 0.44554404, -0.07357611,  0.89223151]]), 'currentState': array([ 29.83765764, -31.99063657,  76.35058127,  -0.88030903,
         0.1453722 ,   0.45157827]), 'targetState': array([ 29.8783751 , -32.29988702,  78.        ]), 'previousTarget': array([ 29.8783751 , -32.29988702,  78.        ])}
episode index:19059
target thresh 85.87456756260708
target distance 63.0
model initialize at round 19059
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.58962244, -8.80459301, 35.4452303 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.43001567, -0.5929427 , -0.68081236],
       [ 0.80952466,  0.58708588,  0.        ],
       [ 0.39969532, -0.5511344 ,  0.73245787]]), 'currentState': array([-7.14985694, -6.86177869, 62.0805226 ,  0.43001567, -0.5929427 ,
       -0.68081236]), 'targetState': array([  8.14048722, -11.39001614,   0.        ]), 'previousTarget': array([-0.77150594, -7.86469891, 36.40605737])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6278624196480858
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  8.14048722, -11.39001614,   0.        ]), 'distance': 1.857517845096555, 'localFrame': array([[ 0.59673495, -0.0265968 , -0.80199751],
       [ 0.04452633,  0.99900821,  0.        ],
       [ 0.8012021 , -0.03571001,  0.59732737]]), 'currentState': array([  8.09061877, -10.39503205,   1.56776667,   0.59673495,
        -0.0265968 ,  -0.80199751]), 'targetState': array([  8.14048722, -11.39001614,   0.        ]), 'previousTarget': array([  8.14048722, -11.39001614,   0.        ])}
episode index:19060
target thresh 85.87598003522601
target distance 38.0
model initialize at round 19060
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.33015065,   0.30997342,  56.74677091]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.0722132 , -0.79942537,  0.59640954],
       [ 0.99594492,  0.08996509,  0.        ],
       [-0.05365604,  0.59399105,  0.8026803 ]]), 'currentState': array([-26.14447452,   4.26813249,  33.9187446 ,   0.0722132 ,
        -0.79942537,   0.59640954]), 'targetState': array([-2.08047004, -2.16139871, 71.        ]), 'previousTarget': array([-11.21954216,   0.90828553,  56.09205525])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278694749072379
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.08047004, -2.16139871, 71.        ]), 'distance': 3.7099025793094023, 'localFrame': array([[-0.10264432,  0.69134473,  0.7151969 ],
       [-0.98915719, -0.14686069,  0.        ],
       [ 0.10503431, -0.70744215,  0.69892303]]), 'currentState': array([-0.64792312, -4.52491818, 68.52513391, -0.10264432,  0.69134473,
        0.7151969 ]), 'targetState': array([-2.08047004, -2.16139871, 71.        ]), 'previousTarget': array([-2.08047004, -2.16139871, 71.        ])}
episode index:19061
target thresh 85.87739236660474
target distance 47.83448622907583
model initialize at round 19061
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.76366864, -2.08313417, 15.95836273]), 'distance': 27.500000000000004, 'localFrame': array([[-0.10869595,  0.69120464,  0.71443777],
       [-0.98786001, -0.15534673,  0.        ],
       [ 0.11098557, -0.7057645 ,  0.69969898]]), 'currentState': array([ 13.25430851, -24.20990936,   2.66988747,  -0.10869595,
         0.69120464,   0.71443777]), 'targetState': array([-6.97908096, 22.96284888, 31.        ]), 'previousTarget': array([ 3.7444288 , -2.89174832, 14.784964  ])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6278692470150956
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-6.97908096, 22.96284888, 31.        ]), 'distance': 2.0145189746542025, 'localFrame': array([[-0.28175644, -0.87256135,  0.39906139],
       [ 0.95161775, -0.30728433,  0.        ],
       [ 0.12262531,  0.37975391,  0.91692421]]), 'currentState': array([-4.98328273, 22.81952339, 31.23352494, -0.28175644, -0.87256135,
        0.39906139]), 'targetState': array([-6.97908096, 22.96284888, 31.        ]), 'previousTarget': array([-6.97908096, 22.96284888, 31.        ])}
episode index:19062
target thresh 85.87880455675739
target distance 29.383379512602055
model initialize at round 19062
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.15659661, -16.25596515,  33.03955962]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.6679081 , -0.60353887, -0.4354763 ],
       [ 0.67044961,  0.74195507,  0.        ],
       [ 0.32310385, -0.29196491,  0.9002002 ]]), 'currentState': array([ 1.66881489, -0.29084066, 41.26555718,  0.6679081 , -0.60353887,
       -0.4354763 ]), 'targetState': array([-29.38337951, -24.09599569,  29.        ]), 'previousTarget': array([-20.11950329, -16.49910502,  33.09858883])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6278755053609655
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-29.38337951, -24.09599569,  29.        ]), 'distance': 2.8704023343620504, 'localFrame': array([[-0.80426155, -0.58348605,  0.11272706],
       [ 0.58722905, -0.80942081,  0.        ],
       [ 0.09124363,  0.06619661,  0.99362599]]), 'currentState': array([-28.09472622, -24.41201234,  31.54533215,  -0.80426155,
        -0.58348605,   0.11272706]), 'targetState': array([-29.38337951, -24.09599569,  29.        ]), 'previousTarget': array([-29.38337951, -24.09599569,  29.        ])}
episode index:19063
target thresh 85.8802166056981
target distance 79.0
model initialize at round 19063
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.12965049, -8.65887374, 76.04329515]), 'distance': 27.499999999999996, 'localFrame': array([[-0.28424005,  0.55833707, -0.77940189],
       [-0.89116587, -0.45367763,  0.        ],
       [-0.3535972 ,  0.69457637,  0.62652429]]), 'currentState': array([-8.66935084,  3.06840561, 99.3091176 , -0.28424005,  0.55833707,
       -0.77940189]), 'targetState': array([ 20.94671071, -36.40378154,  21.        ]), 'previousTarget': array([ 0.73558749, -9.76108348, 76.45099421])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278425702211542
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([ 20.94671071, -36.40378154,  21.        ]), 'distance': 12.567286569878565, 'localFrame': array([[ 0.23244773,  0.8333506 ,  0.5014926 ],
       [-0.96323078,  0.2686754 ,  0.        ],
       [-0.13473873, -0.4830531 ,  0.86516194]]), 'currentState': array([ 18.31698551, -42.47799853,  31.68293617,   0.23244773,
         0.8333506 ,   0.5014926 ]), 'targetState': array([ 20.94671071, -36.40378154,  21.        ]), 'previousTarget': array([ 20.94671071, -36.40378154,  21.        ])}
episode index:19064
target thresh 85.88162851344097
target distance 34.083683013916016
model initialize at round 19064
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.63706475, 16.70491278, 52.31716853]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23872643, -0.18914316,  0.95248861],
       [ 0.62100847, -0.78380386,  0.        ],
       [ 0.74656425,  0.59150349,  0.3045742 ]]), 'currentState': array([30.37498553, 34.66620476, 38.68045924, -0.23872643, -0.18914316,
        0.95248861]), 'targetState': array([-0.,  0., 65.]), 'previousTarget': array([15.11652055, 16.67843527, 51.29854093])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6278472850369097
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 65.]), 'distance': 2.8821616587273984, 'localFrame': array([[-0.11628206, -0.89302309, -0.43472778],
       [ 0.99162876, -0.12912167,  0.        ],
       [-0.05613278, -0.43108857,  0.90056191]]), 'currentState': array([ 1.49330018,  2.06914314, 66.33998398, -0.11628206, -0.89302309,
       -0.43472778]), 'targetState': array([-0.,  0., 65.]), 'previousTarget': array([ 0.,  0., 65.])}
episode index:19065
target thresh 85.88304028000012
target distance 50.0
model initialize at round 19065
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.79724931, -7.31853664, 36.3330096 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.47820845,  0.72141058,  0.50088267],
       [-0.83350424,  0.55251307,  0.        ],
       [-0.27674422, -0.41748783,  0.86551519]]), 'currentState': array([-15.6553051 , -20.76094875,  14.46133173,   0.47820845,
         0.72141058,   0.50088267]), 'targetState': array([ 6.2221614 ,  9.07109186, 63.        ]), 'previousTarget': array([-6.20531862, -8.29907579, 34.89510011])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278494419903101
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.2221614 ,  9.07109186, 63.        ]), 'distance': 2.9931619472485935, 'localFrame': array([[ 0.65222635,  0.17609769,  0.73728583],
       [-0.26066113,  0.96543036,  0.        ],
       [-0.71179812, -0.19218176,  0.67558094]]), 'currentState': array([ 7.33248782,  7.95298451, 60.45519555,  0.65222635,  0.17609769,
        0.73728583]), 'targetState': array([ 6.2221614 ,  9.07109186, 63.        ]), 'previousTarget': array([ 6.2221614 ,  9.07109186, 63.        ])}
episode index:19066
target thresh 85.88445190538967
target distance 14.51523146223893
model initialize at round 19066
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.85757765, -18.1883476 ,   5.        ]), 'distance': 17.583014901182395, 'localFrame': array([[ 0.69604129,  0.12545425,  0.70695668],
       [-0.17738145,  0.98414218,  0.        ],
       [-0.69574589, -0.125401  ,  0.70725685]]), 'currentState': array([ 10.64874018, -14.83250742,  16.10978678,   0.69604129,
         0.12545425,   0.70695668]), 'targetState': array([ 23.85757765, -18.1883476 ,   5.        ]), 'previousTarget': array([ 23.85757765, -18.1883476 ,   5.        ])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6278537795255953
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([ 23.85757765, -18.1883476 ,   5.        ]), 'distance': 3.5934577576189124, 'localFrame': array([[ 0.90032978, -0.18129018, -0.39565157],
       [ 0.19739769,  0.98032349,  0.        ],
       [ 0.38786653, -0.07810071,  0.91840069]]), 'currentState': array([ 22.34203913, -16.4725531 ,   7.76986116,   0.90032978,
        -0.18129018,  -0.39565157]), 'targetState': array([ 23.85757765, -18.1883476 ,   5.        ]), 'previousTarget': array([ 23.85757765, -18.1883476 ,   5.        ])}
episode index:19067
target thresh 85.88586338962374
target distance 42.0
model initialize at round 19067
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.31730163,  8.73870957, 29.19438186]), 'distance': 27.5, 'localFrame': array([[-0.79961568,  0.11041651, -0.59027363],
       [-0.13678899, -0.99060021,  0.        ],
       [-0.58472519,  0.08074293,  0.80720322]]), 'currentState': array([-5.1380219 , -7.78187533, 51.07064787, -0.79961568,  0.11041651,
       -0.59027363]), 'targetState': array([-9.22941622, 23.23398107, 10.        ]), 'previousTarget': array([-6.62707032,  8.92121693, 29.82766612])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6278588732195555
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.22941622, 23.23398107, 10.        ]), 'distance': 2.9175718604066465, 'localFrame': array([[ 0.60035726,  0.46833824, -0.64825185],
       [-0.61508045,  0.78846436,  0.        ],
       [ 0.51112348,  0.39872703,  0.76142599]]), 'currentState': array([-9.01543568, 23.31991562, 12.90844514,  0.60035726,  0.46833824,
       -0.64825185]), 'targetState': array([-9.22941622, 23.23398107, 10.        ]), 'previousTarget': array([-9.22941622, 23.23398107, 10.        ])}
episode index:19068
target thresh 85.88727473271646
target distance 49.6439982828816
model initialize at round 19068
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.23490182, -14.25031384,  70.14978856]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.65653446, -0.40153403,  0.63853968],
       [ 0.52175133,  0.85309762,  0.        ],
       [-0.54473669,  0.33315893,  0.7695889 ]]), 'currentState': array([-28.89106019,   5.04514364,  51.72071163,   0.65653446,
        -0.40153403,   0.63853968]), 'targetState': array([-11.81486954, -44.45682015,  99.        ]), 'previousTarget': array([-23.35761405, -13.89217993,  69.4475308 ])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278610292259192
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.81486954, -44.45682015,  99.        ]), 'distance': 2.090205677069397, 'localFrame': array([[ 0.80387457, -0.59470915,  0.01032976],
       [ 0.59474088,  0.80391746,  0.        ],
       [-0.00830428,  0.00614353,  0.99994665]]), 'currentState': array([-1.25732131e+01, -4.25161211e+01,  9.91660181e+01,  8.03874569e-01,
       -5.94709151e-01,  1.03297649e-02]), 'targetState': array([-11.81486954, -44.45682015,  99.        ]), 'previousTarget': array([-11.81486954, -44.45682015,  99.        ])}
episode index:19069
target thresh 85.8886859346819
target distance 34.33165051490789
model initialize at round 19069
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.3454531 ,  24.16523509,  83.63314032]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.98517458,  0.10652918, -0.13447151],
       [-0.10750561,  0.99420448,  0.        ],
       [ 0.13369218,  0.01445644,  0.99091746]]), 'currentState': array([-35.25982929,  -1.40177356,  85.7027329 ,   0.98517458,
         0.10652918,  -0.13447151]), 'targetState': array([-22.31239658,  31.98682477,  83.        ]), 'previousTarget': array([-26.18999684,  22.87696796,  83.79604592])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6278697210588725
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.31239658,  31.98682477,  83.        ]), 'distance': 2.610693246986629, 'localFrame': array([[ 0.03784857,  0.96993304, -0.24041086],
       [-0.99923952,  0.03899216,  0.        ],
       [ 0.00937414,  0.24022803,  0.97067122]]), 'currentState': array([-2.30555465e+01,  2.95651703e+01,  8.36316939e+01,  3.78485717e-02,
        9.69933042e-01, -2.40410858e-01]), 'targetState': array([-22.31239658,  31.98682477,  83.        ]), 'previousTarget': array([-22.31239658,  31.98682477,  83.        ])}
episode index:19070
target thresh 85.89009699553422
target distance 38.5539751617021
model initialize at round 19070
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.18813904, -12.63115161,  62.28873449]), 'distance': 27.500000000000004, 'localFrame': array([[-0.44648422, -0.17306004, -0.87789639],
       [ 0.36140713, -0.93240811,  0.        ],
       [-0.81855771, -0.31727802,  0.47885064]]), 'currentState': array([ 2.38243982,  9.49357664, 78.61995145, -0.44648422, -0.17306004,
       -0.87789639]), 'targetState': array([  2.05383123, -27.924573  ,  51.        ]), 'previousTarget': array([  2.38814355, -11.34459811,  63.47132805])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6278775838682867
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.05383123, -27.924573  ,  51.        ]), 'distance': 3.5065838158424962, 'localFrame': array([[-0.41872352, -0.90631354,  0.0571522 ],
       [ 0.90779736, -0.41940905,  0.        ],
       [ 0.02397015,  0.05188261,  0.99836548]]), 'currentState': array([  1.86953763, -25.14320635,  53.12747867,  -0.41872352,
        -0.90631354,   0.0571522 ]), 'targetState': array([  2.05383123, -27.924573  ,  51.        ]), 'previousTarget': array([  2.05383123, -27.924573  ,  51.        ])}
episode index:19071
target thresh 85.8915079152875
target distance 69.94213418345286
model initialize at round 19071
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.93376967, -13.71440316,  21.16203658]), 'distance': 27.500000000000004, 'localFrame': array([[-0.66784925,  0.49835716,  0.55282684],
       [-0.59805528, -0.80145485,  0.        ],
       [ 0.44306576, -0.33062101,  0.83329616]]), 'currentState': array([  7.57482978, -40.94703234,  19.98875526,  -0.66784925,
         0.49835716,   0.55282684]), 'targetState': array([-1.77000756, 28.94593362, 23.        ]), 'previousTarget': array([  4.90062695, -13.86731845,  20.55150439])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6278716725602727
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-1.77000756, 28.94593362, 23.        ]), 'distance': 2.5778106641933314, 'localFrame': array([[ 0.16329558,  0.42725166, -0.88926406],
       [-0.93409948,  0.35701282,  0.        ],
       [ 0.31747867,  0.8306611 ,  0.45739417]]), 'currentState': array([-3.40980639, 27.06671317, 23.65168866,  0.16329558,  0.42725166,
       -0.88926406]), 'targetState': array([-1.77000756, 28.94593362, 23.        ]), 'previousTarget': array([-1.77000756, 28.94593362, 23.        ])}
episode index:19072
target thresh 85.89291869395586
target distance 23.86992384337372
model initialize at round 19072
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.7182057 , 13.18099142, 59.        ]), 'distance': 24.034309078265217, 'localFrame': array([[-0.52853068,  0.60789921, -0.59254863],
       [-0.75465305, -0.65612405,  0.        ],
       [-0.38878541,  0.44716863,  0.80553468]]), 'currentState': array([18.98187188, 17.14012342, 59.52878082, -0.52853068,  0.60789921,
       -0.59254863]), 'targetState': array([-4.7182057 , 13.18099142, 59.        ]), 'previousTarget': array([-4.7182057 , 13.18099142, 59.        ])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6278760075655007
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 21, 'trapConfig': [], 'currentTarget': array([-4.7182057 , 13.18099142, 59.        ]), 'distance': 3.838489165372304, 'localFrame': array([[-0.83487721, -0.3457408 , -0.42830287],
       [ 0.38261102, -0.92390952,  0.        ],
       [-0.3957131 , -0.1638734 ,  0.90363524]]), 'currentState': array([-1.93516026, 13.40730157, 61.63390221, -0.83487721, -0.3457408 ,
       -0.42830287]), 'targetState': array([-4.7182057 , 13.18099142, 59.        ]), 'previousTarget': array([-4.7182057 , 13.18099142, 59.        ])}
episode index:19073
target thresh 85.89432933155341
target distance 32.03233231657402
model initialize at round 19073
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.45556004,  2.90686861, 46.1158178 ]), 'distance': 27.5, 'localFrame': array([[ 0.38770247,  0.46476672,  0.79603938],
       [-0.7678987 ,  0.6405713 ,  0.        ],
       [-0.50991998, -0.6112776 ,  0.60524483]]), 'currentState': array([-26.77490303,   4.52253382,  31.62969106,   0.38770247,
         0.46476672,   0.79603938]), 'targetState': array([ 4.40685357,  2.3621265 , 51.        ]), 'previousTarget': array([-4.66509396,  2.98581217, 45.05254365])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278859703908143
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.40685357,  2.3621265 , 51.        ]), 'distance': 2.3950910642051584, 'localFrame': array([[ 0.46396187,  0.76336067, -0.44946621],
       [-0.85454265,  0.51938123,  0.        ],
       [ 0.23344431,  0.38408805,  0.89329733]]), 'currentState': array([ 2.07105554,  1.95037374, 50.66688073,  0.46396187,  0.76336067,
       -0.44946621]), 'targetState': array([ 4.40685357,  2.3621265 , 51.        ]), 'previousTarget': array([ 4.40685357,  2.3621265 , 51.        ])}
episode index:19074
target thresh 85.89573982809425
target distance 27.0
model initialize at round 19074
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.10909783,  0.49570703, 23.02481985]), 'distance': 27.499999999999996, 'localFrame': array([[-0.96132255, -0.27301671, -0.03634332],
       [ 0.2731972 , -0.96195805,  0.        ],
       [-0.03496075, -0.00992889,  0.99933936]]), 'currentState': array([ 13.53625338, -10.49179882,   0.07271445,  -0.96132255,
        -0.27301671,  -0.03634332]), 'targetState': array([ 0.84887044,  2.87739795, 28.        ]), 'previousTarget': array([ 3.23486744,  0.72042593, 23.45216667])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6278910605279344
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 0.84887044,  2.87739795, 28.        ]), 'distance': 3.73185585409898, 'localFrame': array([[-0.15828478,  0.65839469, -0.73584126],
       [-0.97229673, -0.23375002,  0.        ],
       [-0.17200291,  0.71545605,  0.67715407]]), 'currentState': array([ 0.19749745,  5.04300164, 30.96860608, -0.15828478,  0.65839469,
       -0.73584126]), 'targetState': array([ 0.84887044,  2.87739795, 28.        ]), 'previousTarget': array([ 0.84887044,  2.87739795, 28.        ])}
episode index:19075
target thresh 85.8971501835925
target distance 29.861248016357422
model initialize at round 19075
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.00369927, -3.40441171, 88.93784503]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.77699533,  0.14843963, -0.6117548 ],
       [-0.18764947,  0.98223606,  0.        ],
       [ 0.60088763,  0.11479546,  0.79104745]]), 'currentState': array([-29.03474833, -16.46422194,  96.37169572,   0.77699533,
         0.14843963,  -0.6117548 ]), 'targetState': array([-0.,  0., 87.]), 'previousTarget': array([-6.88150326, -3.7465261 , 89.53494214])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6279005927573166
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 87.]), 'distance': 2.5046647075489505, 'localFrame': array([[ 0.98942211,  0.10878965,  0.09596196],
       [-0.10929405,  0.99400946,  0.        ],
       [-0.0953871 , -0.01048807,  0.995385  ]]), 'currentState': array([-1.81580827, -1.69444371, 86.67589179,  0.98942211,  0.10878965,
        0.09596196]), 'targetState': array([-0.,  0., 87.]), 'previousTarget': array([ 0.,  0., 87.])}
episode index:19076
target thresh 85.89856039806224
target distance 51.28933770550592
model initialize at round 19076
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.17936824,  -3.42586045,  32.27882457]), 'distance': 27.5, 'localFrame': array([[-0.70579537, -0.20221993,  0.67894035],
       [ 0.27543141, -0.96132073,  0.        ],
       [ 0.65267943,  0.1870015 ,  0.73419344]]), 'currentState': array([ 5.27466067, -4.08734876, 13.9091448 , -0.70579537, -0.20221993,
        0.67894035]), 'targetState': array([-44.93251045,  -2.46363654,  59.        ]), 'previousTarget': array([-14.11509627,  -2.7444524 ,  31.36070662])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6278925956639186
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([-44.93251045,  -2.46363654,  59.        ]), 'distance': 2.4048440039110375, 'localFrame': array([[-0.83539364,  0.40209723,  0.37474697],
       [-0.43370237, -0.90105619,  0.        ],
       [ 0.33766807, -0.16252865,  0.92712713]]), 'currentState': array([-42.64704712,  -1.86074358,  59.44322923,  -0.83539364,
         0.40209723,   0.37474697]), 'targetState': array([-44.93251045,  -2.46363654,  59.        ]), 'previousTarget': array([-44.93251045,  -2.46363654,  59.        ])}
episode index:19077
target thresh 85.89997047151758
target distance 35.17255796380153
model initialize at round 19077
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.60879602,  9.77542643, 20.97106306]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.6197165 ,  0.06120357,  0.78243567],
       [-0.09828245,  0.99515856,  0.        ],
       [-0.77864756, -0.07689969,  0.62273142]]), 'currentState': array([-0.31168893, 34.3107721 ,  8.58479967,  0.6197165 ,  0.06120357,
        0.78243567]), 'targetState': array([ 0.98252139, -0.18614973, 26.        ]), 'previousTarget': array([ 0.36987906, 10.81993611, 20.05457996])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278596837970739
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([ 0.98252139, -0.18614973, 26.        ]), 'distance': 11.892910563613803, 'localFrame': array([[ 0.23187613, -0.65180377,  0.72207016],
       [ 0.94215825,  0.33516838,  0.        ],
       [-0.24201508,  0.68030435,  0.69181984]]), 'currentState': array([ 3.78551666,  3.83112181, 36.83725374,  0.23187613, -0.65180377,
        0.72207016]), 'targetState': array([ 0.98252139, -0.18614973, 26.        ]), 'previousTarget': array([ 0.98252139, -0.18614973, 26.        ])}
episode index:19078
target thresh 85.90138040397264
target distance 17.0
model initialize at round 19078
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.17366221, -35.66971691,  97.        ]), 'distance': 20.376383921014792, 'localFrame': array([[-0.14512155, -0.86024509,  0.48879251],
       [ 0.9860672 , -0.16634748,  0.        ],
       [ 0.0813094 ,  0.48198226,  0.87240007]]), 'currentState': array([ 24.45891828, -24.33342571,  80.22313712,  -0.14512155,
        -0.86024509,   0.48879251]), 'targetState': array([ 22.17366221, -35.66971691,  97.        ]), 'previousTarget': array([ 22.17366221, -35.66971691,  97.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6278741773444931
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.17366221, -35.66971691,  97.        ]), 'distance': 2.661396517781322, 'localFrame': array([[ 0.23337746, -0.49521115,  0.83683981],
       [ 0.90458145,  0.42630082,  0.        ],
       [-0.3567455 ,  0.75698978,  0.54744783]]), 'currentState': array([ 22.87344081, -34.32912966,  94.80998466,   0.23337746,
        -0.49521115,   0.83683981]), 'targetState': array([ 22.17366221, -35.66971691,  97.        ]), 'previousTarget': array([ 22.17366221, -35.66971691,  97.        ])}
episode index:19079
target thresh 85.9027901954415
target distance 37.79840810956986
model initialize at round 19079
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.33286769,  0.77734235, 18.44345202]), 'distance': 27.499999999999996, 'localFrame': array([[-0.88031233,  0.18072977,  0.43861936],
       [-0.20110739, -0.9795692 ,  0.        ],
       [ 0.42965802, -0.0882096 ,  0.89867294]]), 'currentState': array([ 43.79988676, -11.85423189,   2.45400116,  -0.88031233,
         0.18072977,   0.43861936]), 'targetState': array([ 7.36582987, 13.06692582, 34.        ]), 'previousTarget': array([26.59241486, -0.19605578, 17.72284045])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278763313057732
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.36582987, 13.06692582, 34.        ]), 'distance': 3.19804079133623, 'localFrame': array([[ 0.69040332,  0.28360653,  0.66551528],
       [-0.37997391,  0.9249972 ,  0.        ],
       [-0.61559977, -0.25287844,  0.74638423]]), 'currentState': array([ 8.66738097, 12.03169655, 31.26838326,  0.69040332,  0.28360653,
        0.66551528]), 'targetState': array([ 7.36582987, 13.06692582, 34.        ]), 'previousTarget': array([ 7.36582987, 13.06692582, 34.        ])}
episode index:19080
target thresh 85.90419984593825
target distance 67.0
model initialize at round 19080
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.55481871,   8.00437926,  28.87185077]), 'distance': 27.500000000000004, 'localFrame': array([[-0.97139139, -0.14114661, -0.19098796],
       [ 0.1437935 , -0.98960771,  0.        ],
       [-0.18900316, -0.02746283,  0.98159238]]), 'currentState': array([-5.26912565, 17.5683706 ,  4.81866025, -0.97139139, -0.14114661,
       -0.19098796]), 'targetState': array([-31.59041524,  -9.54178522,  73.        ]), 'previousTarget': array([-13.69758892,   7.95033649,  29.79679598])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6278761032812246
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.59041524,  -9.54178522,  73.        ]), 'distance': 2.0301606116957776, 'localFrame': array([[ 0.53535469,  0.03108205,  0.84405525],
       [-0.05796118,  0.99831884,  0.        ],
       [-0.84263625, -0.04892244,  0.53625622]]), 'currentState': array([-3.06183411e+01, -9.29565522e+00,  7.12347680e+01,  5.35354689e-01,
        3.10820461e-02,  8.44055249e-01]), 'targetState': array([-31.59041524,  -9.54178522,  73.        ]), 'previousTarget': array([-31.59041524,  -9.54178522,  73.        ])}
episode index:19081
target thresh 85.905609355477
target distance 36.0
model initialize at round 19081
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.63033621,  14.43137203,  66.41660453]), 'distance': 27.5, 'localFrame': array([[-0.7742927 ,  0.05603864, -0.63034156],
       [-0.07218516, -0.99739125,  0.        ],
       [-0.62869716,  0.04550131,  0.77631792]]), 'currentState': array([-3.13875548e+00,  1.72503303e+01,  8.65750994e+01, -7.74292702e-01,
        5.60386372e-02, -6.30341560e-01]), 'targetState': array([-34.85482583,  12.41535808,  52.        ]), 'previousTarget': array([-20.5422008 ,  14.80343214,  67.76573768])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278827505500603
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.85482583,  12.41535808,  52.        ]), 'distance': 3.047886432633137, 'localFrame': array([[-0.57287139, -0.09530197, -0.81408593],
       [ 0.16410311, -0.98644319,  0.        ],
       [-0.80304953, -0.13359403,  0.58074443]]), 'currentState': array([-32.18925429,  13.33513387,  50.84312811,  -0.57287139,
        -0.09530197,  -0.81408593]), 'targetState': array([-34.85482583,  12.41535808,  52.        ]), 'previousTarget': array([-34.85482583,  12.41535808,  52.        ])}
episode index:19082
target thresh 85.90701872407185
target distance 45.214155028218855
model initialize at round 19082
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.84876142,  9.69540839, 31.99472953]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.38377612, -0.35978756, -0.85045212],
       [ 0.68393855,  0.72953962,  0.        ],
       [ 0.62043852, -0.58165699,  0.52605247]]), 'currentState': array([ -2.53657107, -14.4385378 ,  41.2531406 ,   0.38377612,
        -0.35978756,  -0.85045212]), 'targetState': array([14.95308885, 30.53530962, 24.        ]), 'previousTarget': array([ 6.40639579,  9.11496986, 33.00130623])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6278867103543074
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.95308885, 30.53530962, 24.        ]), 'distance': 4.071325130274989, 'localFrame': array([[ 0.12481579,  0.8262821 ,  0.54925306],
       [-0.98878249,  0.14936263,  0.        ],
       [-0.08203788, -0.5430918 ,  0.83565608]]), 'currentState': array([16.73519619, 28.08551645, 26.71998073,  0.12481579,  0.8262821 ,
        0.54925306]), 'targetState': array([14.95308885, 30.53530962, 24.        ]), 'previousTarget': array([14.95308885, 30.53530962, 24.        ])}
episode index:19083
target thresh 85.90842795173688
target distance 73.0
model initialize at round 19083
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.15893851, -13.83561866,  29.75934807]), 'distance': 27.500000000000004, 'localFrame': array([[-0.11252721,  0.89356796,  0.43459629],
       [-0.99216387, -0.12494341,  0.        ],
       [ 0.05429994, -0.43119074,  0.90062537]]), 'currentState': array([-28.10536724, -13.25417548,   2.94020941,  -0.11252721,
         0.89356796,   0.43459629]), 'targetState': array([-44.59630138, -14.83812332,  76.        ]), 'previousTarget': array([-34.23316282, -15.00735303,  29.83215832])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278538091433268
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 39, 'trapConfig': [], 'currentTarget': array([-44.59630138, -14.83812332,  76.        ]), 'distance': 9.623002835609862, 'localFrame': array([[ 0.28559254, -0.10701156,  0.95235783],
       [ 0.35087723,  0.93642147,  0.        ],
       [-0.89180832,  0.33416067,  0.30498291]]), 'currentState': array([-47.26424885, -11.39134634,  67.42072449,   0.28559254,
        -0.10701156,   0.95235783]), 'targetState': array([-44.59630138, -14.83812332,  76.        ]), 'previousTarget': array([-44.59630138, -14.83812332,  76.        ])}
episode index:19084
target thresh 85.9098370384862
target distance 26.0
model initialize at round 19084
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.00046143, -11.34257795,  51.98571979]), 'distance': 27.5, 'localFrame': array([[-0.47592463,  0.32981544, -0.81530211],
       [-0.56959419, -0.82192607,  0.        ],
       [-0.67011805,  0.46439135,  0.57903581]]), 'currentState': array([14.06997992, -4.63223739, 73.26862947, -0.47592463,  0.32981544,
       -0.81530211]), 'targetState': array([ -4.25493864, -12.28395283,  49.        ]), 'previousTarget': array([ -1.41187609, -11.19409995,  53.03955124])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278646375400943
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.25493864, -12.28395283,  49.        ]), 'distance': 3.1133309551446406, 'localFrame': array([[-0.44544618,  0.55089089, -0.70575983],
       [-0.77759894, -0.62876059,  0.        ],
       [-0.44375397,  0.54879809,  0.70845118]]), 'currentState': array([ -4.59727898, -13.19686091,  51.9567265 ,  -0.44544618,
         0.55089089,  -0.70575983]), 'targetState': array([ -4.25493864, -12.28395283,  49.        ]), 'previousTarget': array([ -4.25493864, -12.28395283,  49.        ])}
episode index:19085
target thresh 85.91124598433389
target distance 48.0
model initialize at round 19085
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.22135165, -11.90300696,  33.89250768]), 'distance': 27.5, 'localFrame': array([[-0.58804576,  0.1007841 , -0.80252399],
       [-0.16892515, -0.98562888,  0.        ],
       [-0.79099082,  0.13556649,  0.59661985]]), 'currentState': array([ 20.57250489, -19.78772428,  58.87929363,  -0.58804576,
         0.1007841 ,  -0.80252399]), 'targetState': array([ 4.90437671, -4.99470611, 12.        ]), 'previousTarget': array([ 12.97425748, -12.64340196,  35.20075597])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6278704971091945
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.90437671, -4.99470611, 12.        ]), 'distance': 4.4712888823515335, 'localFrame': array([[-0.60247949, -0.13610657, -0.78644356],
       [ 0.22035764, -0.97541915,  0.        ],
       [-0.7671121 , -0.17329885,  0.61766215]]), 'currentState': array([ 6.4073217 , -2.02730016, 14.98798972, -0.60247949, -0.13610657,
       -0.78644356]), 'targetState': array([ 4.90437671, -4.99470611, 12.        ]), 'previousTarget': array([ 4.90437671, -4.99470611, 12.        ])}
episode index:19086
target thresh 85.91265478929402
target distance 49.349545914156444
model initialize at round 19086
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.44450293, 21.70830918, 82.23051531]), 'distance': 27.5, 'localFrame': array([[ 0.54430313, -0.62108365, -0.56390531],
       [ 0.75206343,  0.65909074,  0.        ],
       [ 0.37166477, -0.42409256,  0.82583945]]), 'currentState': array([-6.73293315, 41.69029903, 99.70777175,  0.54430313, -0.62108365,
       -0.56390531]), 'targetState': array([10.3953196 , -5.99477525, 58.        ]), 'previousTarget': array([-0.18453941, 23.16869853, 82.82020606])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278737233149241
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.3953196 , -5.99477525, 58.        ]), 'distance': 2.731573802275507, 'localFrame': array([[ 0.57554179, -0.7759063 ,  0.25830421],
       [ 0.80316273,  0.59575971,  0.        ],
       [-0.15388724,  0.20746032,  0.96606363]]), 'currentState': array([ 8.54889091, -3.98490005, 57.88775833,  0.57554179, -0.7759063 ,
        0.25830421]), 'targetState': array([10.3953196 , -5.99477525, 58.        ]), 'previousTarget': array([10.3953196 , -5.99477525, 58.        ])}
episode index:19087
target thresh 85.91406345338072
target distance 43.0
model initialize at round 19087
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.93311051, -22.80163164,  54.61753148]), 'distance': 27.5, 'localFrame': array([[ 0.50743215, -0.10205554,  0.85562683],
       [ 0.19717326,  0.98036866,  0.        ],
       [-0.83882973,  0.16870673,  0.5175932 ]]), 'currentState': array([15.68665046, -6.65672158, 35.12514175,  0.50743215, -0.10205554,
        0.85562683]), 'targetState': array([ -7.41482491, -41.34029961,  77.        ]), 'previousTarget': array([  4.19662371, -23.07002117,  54.01785503])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278762304044958
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ -7.41482491, -41.34029961,  77.        ]), 'distance': 2.642328176119749, 'localFrame': array([[ 0.2620604 , -0.40423194,  0.87631095],
       [ 0.83909783,  0.54398055,  0.        ],
       [-0.47669612,  0.73531062,  0.4817459 ]]), 'currentState': array([ -5.22989407, -41.49065669,  75.52170093,   0.2620604 ,
        -0.40423194,   0.87631095]), 'targetState': array([ -7.41482491, -41.34029961,  77.        ]), 'previousTarget': array([ -7.41482491, -41.34029961,  77.        ])}
episode index:19088
target thresh 85.91547197660805
target distance 4.0
model initialize at round 19088
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.17831256,  5.9973498 , 68.        ]), 'distance': 5.372799600387378, 'localFrame': array([[ 0.56603298, -0.82031632, -0.08177904],
       [ 0.82307321,  0.56793529,  0.        ],
       [ 0.0464452 , -0.06731014,  0.99665048]]), 'currentState': array([-0.15847898,  8.49052077, 72.75927313,  0.56603298, -0.82031632,
       -0.08177904]), 'targetState': array([-0.17831256,  5.9973498 , 68.        ]), 'previousTarget': array([-0.17831256,  5.9973498 , 68.        ])}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6278946820661646
{'scaleFactor': 20, 'timeStep': 3, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.17831256,  5.9973498 , 68.        ]), 'distance': 2.195586775445645, 'localFrame': array([[ 0.78727703, -0.18948828, -0.58676151],
       [ 0.23400554,  0.97223526,  0.        ],
       [ 0.57047023, -0.13730544,  0.8097598 ]]), 'currentState': array([ 0.40022587,  6.37900159, 70.08332342,  0.78727703, -0.18948828,
       -0.58676151]), 'targetState': array([-0.17831256,  5.9973498 , 68.        ]), 'previousTarget': array([-0.17831256,  5.9973498 , 68.        ])}
episode index:19089
target thresh 85.9168803589901
target distance 41.15109590366147
model initialize at round 19089
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.83513067,  9.94380546, 85.07048988]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.77511837,  0.41859011, -0.47325874],
       [-0.47517193,  0.87989297,  0.        ],
       [ 0.41641704,  0.22487927,  0.88092347]]), 'currentState': array([ 7.41910811,  9.53717595, 82.96185065,  0.77511837,  0.41859011,
       -0.47325874]), 'targetState': array([46.92039875, 10.12305197, 86.        ]), 'previousTarget': array([33.23224102,  9.87111171, 85.33473666])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6279042071152037
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([46.92039875, 10.12305197, 86.        ]), 'distance': 2.444315427695173, 'localFrame': array([[ 0.73265454,  0.29255883,  0.61451334],
       [-0.37084098,  0.92869638,  0.        ],
       [-0.57069632, -0.22788673,  0.78890643]]), 'currentState': array([45.07141827, 10.86676238, 84.58478131,  0.73265454,  0.29255883,
        0.61451334]), 'targetState': array([46.92039875, 10.12305197, 86.        ]), 'previousTarget': array([46.92039875, 10.12305197, 86.        ])}
episode index:19090
target thresh 85.91828860054093
target distance 75.0
model initialize at round 19090
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.22670653, -0.38549612, 40.03951591]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.0552938 , -0.69534568,  0.71654517],
       [ 0.99685321,  0.07926964,  0.        ],
       [-0.05680028,  0.71429035,  0.6975407 ]]), 'currentState': array([ 1.21844461,  0.22883281, 12.58438637,  0.0552938 , -0.69534568,
        0.71654517]), 'targetState': array([-2.64592097, -1.41389612, 86.        ]), 'previousTarget': array([-0.66587703,  0.03896631, 38.46337357])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278713170514504
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-2.64592097, -1.41389612, 86.        ]), 'distance': 12.092436413306636, 'localFrame': array([[-0.86240991, -0.06278301,  0.50230215],
       [ 0.07260736, -0.9973606 ,  0.        ],
       [ 0.50097637,  0.03647083,  0.86469217]]), 'currentState': array([-1.70696962e+00, -1.69879535e+00,  7.39474392e+01, -8.62409907e-01,
       -6.27830120e-02,  5.02302147e-01]), 'targetState': array([-2.64592097, -1.41389612, 86.        ]), 'previousTarget': array([-2.64592097, -1.41389612, 86.        ])}
episode index:19091
target thresh 85.91969670127469
target distance 55.699403179842875
model initialize at round 19091
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.61170215,  -5.78492333,  41.94305624]), 'distance': 27.5, 'localFrame': array([[ 0.1314308 ,  0.2480351 ,  0.95979401],
       [-0.88361388,  0.4682163 ,  0.        ],
       [-0.4493912 , -0.84808731,  0.2807053 ]]), 'currentState': array([-45.48965829,   4.1475637 ,  28.56580928,   0.1314308 ,
         0.2480351 ,   0.95979401]), 'targetState': array([ 10.82609309, -21.419517  ,  63.        ]), 'previousTarget': array([-23.23977493,  -6.25554137,  40.98232694])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6278720891175007
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.82609309, -21.419517  ,  63.        ]), 'distance': 2.824387859369449, 'localFrame': array([[ 0.83295054, -0.54741357, -0.08081944],
       [ 0.54921017,  0.83568427,  0.        ],
       [ 0.06753954, -0.04438686,  0.99672876]]), 'currentState': array([  8.08049525, -21.50092608,  62.3425566 ,   0.83295054,
        -0.54741357,  -0.08081944]), 'targetState': array([ 10.82609309, -21.419517  ,  63.        ]), 'previousTarget': array([ 10.82609309, -21.419517  ,  63.        ])}
episode index:19092
target thresh 85.92110466120538
target distance 37.40756107788663
model initialize at round 19092
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.1873181 , 16.88838295, 38.86636955]), 'distance': 27.5, 'localFrame': array([[-0.97827063,  0.08797438, -0.18774206],
       [-0.08956703, -0.9959808 ,  0.        ],
       [-0.18698749,  0.0168155 ,  0.98221837]]), 'currentState': array([28.64033321, 36.79727873, 29.42263286, -0.97827063,  0.08797438,
       -0.18774206]), 'targetState': array([-1.98321092, -0.25860096, 47.        ]), 'previousTarget': array([13.48333754, 17.60949084, 38.40212184])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6278712115786338
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([-1.98321092, -0.25860096, 47.        ]), 'distance': 4.002249884886525, 'localFrame': array([[ 0.28811882, -0.82588406,  0.48466799],
       [ 0.94419317,  0.32939226,  0.        ],
       [-0.15964589,  0.4576202 ,  0.8746982 ]]), 'currentState': array([ 0.3022478 ,  2.03243018, 44.64503526,  0.28811882, -0.82588406,
        0.48466799]), 'targetState': array([-1.98321092, -0.25860096, 47.        ]), 'previousTarget': array([-1.98321092, -0.25860096, 47.        ])}
episode index:19093
target thresh 85.92251248034714
target distance 57.0
model initialize at round 19093
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.84889823, -1.26479104, 31.28109072]), 'distance': 27.5, 'localFrame': array([[-0.84425696,  0.11177304,  0.52415357],
       [-0.13124697, -0.9913497 ,  0.        ],
       [ 0.51961949, -0.06879357,  0.85162376]]), 'currentState': array([35.81022064, -2.7955983 ,  8.25794018, -0.84425696,  0.11177304,
        0.52415357]), 'targetState': array([-0.41309848,  0.91068636, 64.        ]), 'previousTarget': array([21.81199629, -1.83474614, 29.97178725])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6278700140744733
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.41309848,  0.91068636, 64.        ]), 'distance': 2.3462018480485347, 'localFrame': array([[ 0.2904127 , -0.5835679 ,  0.75835939],
       [ 0.89526707,  0.44552987,  0.        ],
       [-0.33787176,  0.67893419,  0.65183666]]), 'currentState': array([-0.72044058,  1.45758129, 61.73922361,  0.2904127 , -0.5835679 ,
        0.75835939]), 'targetState': array([-0.41309848,  0.91068636, 64.        ]), 'previousTarget': array([-0.41309848,  0.91068636, 64.        ])}
episode index:19094
target thresh 85.92392015871401
target distance 59.02171215653928
model initialize at round 19094
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.80795712, 20.25711992, 65.92793134]), 'distance': 27.500000000000004, 'localFrame': array([[-0.81617063, -0.00832697, -0.57775096],
       [ 0.01020196, -0.99994796,  0.        ],
       [-0.57772089, -0.00589419,  0.81621311]]), 'currentState': array([-1.33745608e+01,  3.37687142e+01,  8.02707980e+01, -8.16170628e-01,
       -8.32697070e-03, -5.77750956e-01]), 'targetState': array([47.17176613, -8.87831514, 35.        ]), 'previousTarget': array([ 7.08514901, 19.51923426, 66.24247536])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278371326911752
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 42, 'trapConfig': [], 'currentTarget': array([47.17176613, -8.87831514, 35.        ]), 'distance': 17.594487261698454, 'localFrame': array([[ 0.78380754,  0.04092083,  0.61965412],
       [-0.05213674,  0.99863996,  0.        ],
       [-0.61881136, -0.03230675,  0.784875  ]]), 'currentState': array([31.01575482, -2.15707214, 36.83689221,  0.78380754,  0.04092083,
        0.61965412]), 'targetState': array([47.17176613, -8.87831514, 35.        ]), 'previousTarget': array([47.17176613, -8.87831514, 35.        ])}
episode index:19095
target thresh 85.92532769632008
target distance 28.444621307170774
model initialize at round 19095
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.09932063, -3.00964522, 62.36570776]), 'distance': 27.500000000000004, 'localFrame': array([[-0.29548403,  0.89149109,  0.34341349],
       [-0.94921846, -0.31461773,  0.        ],
       [ 0.10804397, -0.32597443,  0.93918431]]), 'currentState': array([ -9.09528185, -24.4487648 ,  45.40580304,  -0.29548403,
         0.89149109,   0.34341349]), 'targetState': array([-5.28067453,  2.84859203, 67.        ]), 'previousTarget': array([-6.11307091, -4.29916283, 61.22040603])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6278475187139625
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.28067453,  2.84859203, 67.        ]), 'distance': 2.288705148183585, 'localFrame': array([[ 0.1711938 ,  0.96474717,  0.19988891],
       [-0.98461818,  0.1747199 ,  0.        ],
       [-0.03492457, -0.19681425,  0.97981857]]), 'currentState': array([-5.05997439,  0.59865946, 67.35674381,  0.1711938 ,  0.96474717,
        0.19988891]), 'targetState': array([-5.28067453,  2.84859203, 67.        ]), 'previousTarget': array([-5.28067453,  2.84859203, 67.        ])}
episode index:19096
target thresh 85.92673509317943
target distance 64.0
model initialize at round 19096
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.46354245, 27.39413633, 28.21495068]), 'distance': 27.499999999999996, 'localFrame': array([[-0.08293723, -0.13857456,  0.9868731 ],
       [ 0.85805927, -0.51355066,  0.        ],
       [ 0.50680933,  0.84679561,  0.16149765]]), 'currentState': array([-6.43364148, 40.43920005,  4.52160926, -0.08293723, -0.13857456,
        0.9868731 ]), 'targetState': array([ 6.67231025,  6.03989039, 67.        ]), 'previousTarget': array([-1.78195383, 28.35877776, 26.66692547])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6278482918240516
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.67231025,  6.03989039, 67.        ]), 'distance': 2.819479934795854, 'localFrame': array([[-0.0055409 , -0.91566616,  0.40190146],
       [ 0.99998169, -0.00605111,  0.        ],
       [ 0.00243195,  0.4018941 ,  0.91568292]]), 'currentState': array([ 7.58567162e+00,  6.28370766e+00,  6.43437261e+01, -5.54089792e-03,
       -9.15666160e-01,  4.01901459e-01]), 'targetState': array([ 6.67231025,  6.03989039, 67.        ]), 'previousTarget': array([ 6.67231025,  6.03989039, 67.        ])}
episode index:19097
target thresh 85.92814234930614
target distance 69.0
model initialize at round 19097
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.99643807, -4.96174794, 24.9316438 ]), 'distance': 27.5, 'localFrame': array([[-0.61872126, -0.6237338 ,  0.47764019],
       [ 0.70995374, -0.70424831,  0.        ],
       [ 0.33637729,  0.33910244,  0.87855555]]), 'currentState': array([  7.56022137, -16.64357812,   2.38520761,  -0.61872126,
        -0.6237338 ,   0.47764019]), 'targetState': array([-24.56649657,  18.90733314,  71.        ]), 'previousTarget': array([-2.62386402, -3.78205061, 24.71921031])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278154167433193
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 37, 'trapConfig': [], 'currentTarget': array([-24.56649657,  18.90733314,  71.        ]), 'distance': 12.459416886335667, 'localFrame': array([[-0.67336039,  0.59620655,  0.43717678],
       [-0.66291164, -0.74869764,  0.        ],
       [ 0.32731323, -0.28980958,  0.89937559]]), 'currentState': array([-19.2332244 ,  14.12825792,  60.80423042,  -0.67336039,
         0.59620655,   0.43717678]), 'targetState': array([-24.56649657,  18.90733314,  71.        ]), 'previousTarget': array([-24.56649657,  18.90733314,  71.        ])}
episode index:19098
target thresh 85.92954946471426
target distance 35.0
model initialize at round 19098
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.00638398, 15.79972223, 32.53769449]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.48014392, -0.34023095, -0.80852008],
       [ 0.57816331,  0.81592107,  0.        ],
       [ 0.65968856, -0.46745664,  0.58846859]]), 'currentState': array([-5.30840726,  3.55082317, 57.14922228,  0.48014392, -0.34023095,
       -0.80852008]), 'targetState': array([-6.24851085, 20.04884316, 24.        ]), 'previousTarget': array([-6.15719186, 15.34519423, 34.08032646])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6278249414541145
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.24851085, 20.04884316, 24.        ]), 'distance': 2.2247504312297393, 'localFrame': array([[-0.87043841,  0.35170845, -0.34443888],
       [-0.3746327 , -0.92717331,  0.        ],
       [-0.31935453,  0.12903807,  0.93880874]]), 'currentState': array([-6.37329629, 21.19664875, 25.90170592, -0.87043841,  0.35170845,
       -0.34443888]), 'targetState': array([-6.24851085, 20.04884316, 24.        ]), 'previousTarget': array([-6.24851085, 20.04884316, 24.        ])}
episode index:19099
target thresh 85.93095643941788
target distance 34.0
model initialize at round 19099
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.40813421,  0.29459006, 46.66078689]), 'distance': 27.499999999999996, 'localFrame': array([[-0.78831948, -0.51148592, -0.34195694],
       [ 0.54429863, -0.83889153,  0.        ],
       [-0.28686478, -0.18612669,  0.93971562]]), 'currentState': array([-2.08838319,  1.50738878, 74.08260088, -0.78831948, -0.51148592,
       -0.34195694]), 'targetState': array([-0., -0., 40.]), 'previousTarget': array([-0.06086697,  0.38030438, 46.54745474])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6278344651675578
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 40.]), 'distance': 2.309288720686026, 'localFrame': array([[-0.17272222, -0.55560794, -0.81330612],
       [ 0.95492188, -0.29685721,  0.        ],
       [-0.24143579, -0.77664381,  0.58183602]]), 'currentState': array([-0.56213962,  1.09093713, 41.95618752, -0.17272222, -0.55560794,
       -0.81330612]), 'targetState': array([-0., -0., 40.]), 'previousTarget': array([ 0.,  0., 40.])}
episode index:19100
target thresh 85.93236327343106
target distance 51.0
model initialize at round 19100
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.90128936, -7.03062898, 33.76603369]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.59630987, -0.46035206, -0.65764011],
       [ 0.61108769,  0.79156291,  0.        ],
       [ 0.52056352, -0.40187577,  0.75333225]]), 'currentState': array([26.69130435, 10.77018963, 52.79539241,  0.59630987, -0.46035206,
       -0.65764011]), 'targetState': array([  3.68988574, -35.81039993,   3.        ]), 'previousTarget': array([17.87954445, -5.8076182 , 34.93312779])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.627834569306924
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.68988574, -35.81039993,   3.        ]), 'distance': 2.918555918526184, 'localFrame': array([[-0.64474233,  0.38957094, -0.65767911],
       [-0.51715368, -0.85589256,  0.        ],
       [-0.56290265,  0.34012117,  0.75329821]]), 'currentState': array([  1.24784813, -35.48409142,   4.56459059,  -0.64474233,
         0.38957094,  -0.65767911]), 'targetState': array([  3.68988574, -35.81039993,   3.        ]), 'previousTarget': array([  3.68988574, -35.81039993,   3.        ])}
episode index:19101
target thresh 85.93376996676788
target distance 77.0
model initialize at round 19101
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.12949696, 15.19159422, 42.53783376]), 'distance': 27.5, 'localFrame': array([[ 0.14091037,  0.81223521,  0.56605498],
       [-0.98528291,  0.1709315 ,  0.        ],
       [-0.09675663, -0.5577243 ,  0.82436749]]), 'currentState': array([-17.07544371,  19.0242474 ,  17.60297825,   0.14091037,
         0.81223521,   0.56605498]), 'targetState': array([16.46145534,  7.28151689, 94.        ]), 'previousTarget': array([-5.79742928, 14.08476449, 42.09281261])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6278330574985878
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.46145534,  7.28151689, 94.        ]), 'distance': 2.1063747939061943, 'localFrame': array([[-0.69842627, -0.09634916,  0.70916682],
       [ 0.13665758, -0.99061835,  0.        ],
       [ 0.70251366,  0.09691302,  0.70504072]]), 'currentState': array([17.0109507 ,  8.07404925, 92.12736494, -0.69842627, -0.09634916,
        0.70916682]), 'targetState': array([16.46145534,  7.28151689, 94.        ]), 'previousTarget': array([16.46145534,  7.28151689, 94.        ])}
episode index:19102
target thresh 85.93517651944241
target distance 63.0
model initialize at round 19102
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.64070045, 12.41508338, 51.59527149]), 'distance': 27.5, 'localFrame': array([[-0.12464721, -0.25091584, -0.95995016],
       [ 0.89558164, -0.4448972 ,  0.        ],
       [-0.42707914, -0.85971374,  0.28017082]]), 'currentState': array([-0.17259729, 22.4430859 , 77.19742072, -0.12464721, -0.25091584,
       -0.95995016]), 'targetState': array([-1.29151531, -1.52708487, 16.        ]), 'previousTarget': array([-0.63670681, 12.38002979, 53.24119898])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6278277056391953
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-1.29151531, -1.52708487, 16.        ]), 'distance': 3.1860826454912203, 'localFrame': array([[-0.77900214,  0.37120407, -0.50533475],
       [-0.43017036, -0.90274773,  0.        ],
       [-0.4561898 ,  0.21738003,  0.8629234 ]]), 'currentState': array([ 1.68780333, -2.16737326, 16.92995367, -0.77900214,  0.37120407,
       -0.50533475]), 'targetState': array([-1.29151531, -1.52708487, 16.        ]), 'previousTarget': array([-1.29151531, -1.52708487, 16.        ])}
episode index:19103
target thresh 85.93658293146869
target distance 25.0
model initialize at round 19103
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.47288768,   7.13675755,  78.47283883]), 'distance': 27.5, 'localFrame': array([[ 0.90897902,  0.12493965, -0.39767729],
       [-0.13617025,  0.99068545,  0.        ],
       [ 0.39397311,  0.05415181,  0.91752535]]), 'currentState': array([-27.45227993,  18.2041361 ,  59.0197631 ,   0.90897902,
         0.12493965,  -0.39767729]), 'targetState': array([-6.93269683,  3.99220674, 84.        ]), 'previousTarget': array([-12.20993657,   7.20490411,  78.05979727])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278298593270581
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-6.93269683,  3.99220674, 84.        ]), 'distance': 2.520881116972622, 'localFrame': array([[-0.61350413,  0.48057339,  0.6266274 ],
       [-0.61665813, -0.78723107,  0.        ],
       [ 0.49330056, -0.38641488,  0.779319  ]]), 'currentState': array([-7.29144668,  5.02668428, 81.72931809, -0.61350413,  0.48057339,
        0.6266274 ]), 'targetState': array([-6.93269683,  3.99220674, 84.        ]), 'previousTarget': array([-6.93269683,  3.99220674, 84.        ])}
episode index:19104
target thresh 85.93798920286079
target distance 37.974534822142466
model initialize at round 19104
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.14799569, -4.4599594 , 23.70541539]), 'distance': 27.500000000000004, 'localFrame': array([[-0.71472018, -0.19996493, -0.6702157 ],
       [ 0.2694341 , -0.96301883,  0.        ],
       [-0.64543034, -0.18057897,  0.74216637]]), 'currentState': array([ 5.60261988, 19.18732521, 36.01334454, -0.71472018, -0.19996493,
       -0.6702157 ]), 'targetState': array([ -5.37423851, -19.26441176,  16.        ]), 'previousTarget': array([-0.51231785, -4.42336847, 24.20712908])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.62783650101394
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.37423851, -19.26441176,  16.        ]), 'distance': 3.514674263280967, 'localFrame': array([[-9.99835946e-01, -1.80460500e-02, -1.55584806e-03],
       [ 1.80460718e-02, -9.99837156e-01,  0.00000000e+00],
       [-1.55559470e-03, -2.80769457e-05,  9.99998790e-01]]), 'currentState': array([-3.81173852e+00, -1.67862879e+01,  1.40582401e+01, -9.99835946e-01,
       -1.80460500e-02, -1.55584806e-03]), 'targetState': array([ -5.37423851, -19.26441176,  16.        ]), 'previousTarget': array([ -5.37423851, -19.26441176,  16.        ])}
episode index:19105
target thresh 85.9393953336328
target distance 69.84163524353285
model initialize at round 19105
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.24948   ,  1.28068769, 68.14411108]), 'distance': 27.500000000000004, 'localFrame': array([[-0.68407185,  0.19454483,  0.70299218],
       [-0.27354541, -0.96185909,  0.        ],
       [ 0.67617942, -0.19230028,  0.71119758]]), 'currentState': array([ 35.58297738, -13.13565533,  66.15238982,  -0.68407185,
         0.19454483,   0.70299218]), 'targetState': array([-32.92319347,  29.19012387,  72.        ]), 'previousTarget': array([13.40280207,  1.48998251, 67.3568961 ])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6278295399628873
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-32.92319347,  29.19012387,  72.        ]), 'distance': 2.579604905438411, 'localFrame': array([[-0.42847612, -0.44820036, -0.78455379],
       [ 0.72283297, -0.69102279,  0.        ],
       [-0.54214455, -0.56710135,  0.62006076]]), 'currentState': array([-32.07322227,  28.21198654,  74.23050617,  -0.42847612,
        -0.44820036,  -0.78455379]), 'targetState': array([-32.92319347,  29.19012387,  72.        ]), 'previousTarget': array([-32.92319347,  29.19012387,  72.        ])}
episode index:19106
target thresh 85.94080132379875
target distance 38.08204144389697
model initialize at round 19106
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([0.87320813, 2.90696573, 7.10405789]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.16757654, -0.66371033, -0.72897647],
       [ 0.96957306,  0.24480212,  0.        ],
       [ 0.17845498, -0.70679595,  0.68453875]]), 'currentState': array([13.90712977, 27.02465696,  9.2728955 ,  0.16757654, -0.66371033,
       -0.72897647]), 'targetState': array([-5.76177644, -9.37026853,  6.        ]), 'previousTarget': array([1.45234404, 4.44144732, 7.45073271])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278361809712738
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.76177644, -9.37026853,  6.        ]), 'distance': 2.9881420148738806, 'localFrame': array([[ 0.47639909, -0.79393213,  0.37777729],
       [ 0.85747395,  0.51452737,  0.        ],
       [-0.19437675,  0.32393418,  0.9258965 ]]), 'currentState': array([-6.73788942, -6.65598384,  5.21970851,  0.47639909, -0.79393213,
        0.37777729]), 'targetState': array([-5.76177644, -9.37026853,  6.        ]), 'previousTarget': array([-5.76177644, -9.37026853,  6.        ])}
episode index:19107
target thresh 85.94220717337274
target distance 50.72624357260585
model initialize at round 19107
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.49745158, 10.08710241, 82.26942957]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.72316377, -0.3594493 ,  0.58977145],
       [ 0.44509975,  0.89548099,  0.        ],
       [-0.52812912,  0.26250713,  0.80757021]]), 'currentState': array([-27.8311908 ,  17.96751688,  81.44284214,   0.72316377,
        -0.3594493 ,   0.58977145]), 'targetState': array([21.77734144,  3.12208259, 83.        ]), 'previousTarget': array([-2.57943395, 10.18495668, 81.55951632])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6278372939316776
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([21.77734144,  3.12208259, 83.        ]), 'distance': 2.875194041204991, 'localFrame': array([[ 0.27486596,  0.12847666,  0.95286014],
       [-0.42344257,  0.90592295,  0.        ],
       [-0.86321787, -0.40348155,  0.30340987]]), 'currentState': array([19.09329913,  3.22654543, 81.97445374,  0.27486596,  0.12847666,
        0.95286014]), 'targetState': array([21.77734144,  3.12208259, 83.        ]), 'previousTarget': array([21.77734144,  3.12208259, 83.        ])}
episode index:19108
target thresh 85.94361288236877
target distance 55.815135648196105
model initialize at round 19108
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.77608639, -4.08009887, 15.05373642]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.25450197,  0.12377098, -0.95911912],
       [-0.43734918,  0.89929178,  0.        ],
       [ 0.86252794,  0.41946996,  0.28300266]]), 'currentState': array([-27.90474132,  14.34161279,  18.47832254,   0.25450197,
         0.12377098,  -0.95911912]), 'targetState': array([ 27.80589836, -36.64467242,   9.        ]), 'previousTarget': array([-7.73077302, -4.85539437, 16.00353731])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6278339482776153
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 27.80589836, -36.64467242,   9.        ]), 'distance': 3.4672179824464884, 'localFrame': array([[ 0.60753882, -0.64438981, -0.46439031],
       [ 0.72760559,  0.68599571,  0.        ],
       [ 0.31856976, -0.33789298,  0.88563065]]), 'currentState': array([ 25.26642478, -34.3021212 ,   9.2917675 ,   0.60753882,
        -0.64438981,  -0.46439031]), 'targetState': array([ 27.80589836, -36.64467242,   9.        ]), 'previousTarget': array([ 27.80589836, -36.64467242,   9.        ])}
episode index:19109
target thresh 85.94501845080093
target distance 27.846993233716248
model initialize at round 19109
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.37646023, -5.14430698, 91.69977833]), 'distance': 27.5, 'localFrame': array([[ 0.07533928,  0.74516968,  0.66260558],
       [-0.99492789,  0.10059071,  0.        ],
       [-0.06665197, -0.65924477,  0.74896853]]), 'currentState': array([-1.90112310e+00, -1.45922386e+01,  9.69941499e+01,  7.53392769e-02,
        7.45169675e-01,  6.62605575e-01]), 'targetState': array([26.71749988, -3.89553594, 91.        ]), 'previousTarget': array([23.76845279, -5.17319336, 91.52950907])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278447635477971
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.71749988, -3.89553594, 91.        ]), 'distance': 2.931085073288888, 'localFrame': array([[ 0.62271511,  0.77209292,  0.12687956],
       [-0.77838371,  0.62778882,  0.        ],
       [-0.07965357, -0.09876098,  0.99191813]]), 'currentState': array([23.97751802, -4.31462236, 91.95295628,  0.62271511,  0.77209292,
        0.12687956]), 'targetState': array([26.71749988, -3.89553594, 91.        ]), 'previousTarget': array([26.71749988, -3.89553594, 91.        ])}
episode index:19110
target thresh 85.9464238786833
target distance 41.0
model initialize at round 19110
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.67664218, 15.91271089, 57.09197216]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.11014632, -0.82569162, -0.55326408],
       [ 0.99121939,  0.13222754,  0.        ],
       [ 0.07315675, -0.54840608,  0.83300592]]), 'currentState': array([-3.62543111, 20.00461521, 83.54551996,  0.11014632, -0.82569162,
       -0.55326408]), 'targetState': array([ 6.03379646, 13.73292759, 43.        ]), 'previousTarget': array([ 2.20714638, 16.54742384, 57.81343433])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278518012721861
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 6.03379646, 13.73292759, 43.        ]), 'distance': 2.900072615750879, 'localFrame': array([[-0.40715535, -0.04805494, -0.91209388],
       [ 0.11721249, -0.99310686,  0.        ],
       [-0.90580669, -0.10690879,  0.40998141]]), 'currentState': array([ 7.07379066, 14.01531293, 45.69241374, -0.40715535, -0.04805494,
       -0.91209388]), 'targetState': array([ 6.03379646, 13.73292759, 43.        ]), 'previousTarget': array([ 6.03379646, 13.73292759, 43.        ])}
episode index:19111
target thresh 85.94782916602989
target distance 41.0
model initialize at round 19111
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.28306296,  3.63735388, 62.14249555]), 'distance': 27.500000000000004, 'localFrame': array([[-0.82428945, -0.3969446 , -0.40371015],
       [ 0.43387285, -0.90097411,  0.        ],
       [-0.3637324 , -0.17515887,  0.91488694]]), 'currentState': array([ 0.57346626, -0.81328089, 89.00453257, -0.82428945, -0.3969446 ,
       -0.40371015]), 'targetState': array([-5.31347291,  5.98055228, 48.        ]), 'previousTarget': array([-2.79838861,  4.04145092, 62.16605315])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278572665070323
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-5.31347291,  5.98055228, 48.        ]), 'distance': 2.950538984722064, 'localFrame': array([[-0.90705312, -0.13466262, -0.39889927],
       [ 0.14685211, -0.98915846,  0.        ],
       [-0.39457459, -0.0585792 ,  0.91699475]]), 'currentState': array([-5.50160726,  7.18543086, 50.68673657, -0.90705312, -0.13466262,
       -0.39889927]), 'targetState': array([-5.31347291,  5.98055228, 48.        ]), 'previousTarget': array([-5.31347291,  5.98055228, 48.        ])}
episode index:19112
target thresh 85.94923431285477
target distance 62.0
model initialize at round 19112
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.84667153,  5.58298273, 37.88726746]), 'distance': 27.499999999999996, 'localFrame': array([[-0.7140984 ,  0.59711522, -0.36539964],
       [-0.64147257, -0.76714597,  0.        ],
       [-0.28031486,  0.23439385,  0.93085074]]), 'currentState': array([ 5.80507239,  1.17102932, 10.7604148 , -0.7140984 ,  0.59711522,
       -0.36539964]), 'targetState': array([ 3.57079649, 11.45641359, 74.        ]), 'previousTarget': array([ 5.50367559,  5.1152641 , 39.01910246])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278594176341219
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.57079649, 11.45641359, 74.        ]), 'distance': 2.136627280253855, 'localFrame': array([[ 0.17183547, -0.46009864,  0.87108083],
       [ 0.93679793,  0.34987088,  0.        ],
       [-0.30476582,  0.81602672,  0.49113968]]), 'currentState': array([ 3.71498183, 10.45389669, 72.11868488,  0.17183547, -0.46009864,
        0.87108083]), 'targetState': array([ 3.57079649, 11.45641359, 74.        ]), 'previousTarget': array([ 3.57079649, 11.45641359, 74.        ])}
episode index:19113
target thresh 85.950639319172
target distance 28.0
model initialize at round 19113
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.47758306,  4.08799074, 93.56393891]), 'distance': 27.500000000000004, 'localFrame': array([[-0.08991857, -0.46686497,  0.87974528],
       [ 0.98195303, -0.18912495,  0.        ],
       [ 0.16638179,  0.86386854,  0.47544532]]), 'currentState': array([-1.8731869 , 11.44030034, 67.1694807 , -0.08991857, -0.46686497,
        0.87974528]), 'targetState': array([ 0.51641998,  3.96652372, 94.        ]), 'previousTarget': array([ 0.32112407,  4.5632325 , 92.10478847])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6278706703167498
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 0.51641998,  3.96652372, 94.        ]), 'distance': 2.1621483736882934, 'localFrame': array([[-0.47180429,  0.0899191 ,  0.87710619],
       [-0.18721582, -0.98231881,  0.        ],
       [ 0.8615979 , -0.16420815,  0.48029651]]), 'currentState': array([ 1.50226281e+00,  5.02159770e+00,  9.23907087e+01, -4.71804290e-01,
        8.99191025e-02,  8.77106189e-01]), 'targetState': array([ 0.51641998,  3.96652372, 94.        ]), 'previousTarget': array([ 0.51641998,  3.96652372, 94.        ])}
episode index:19114
target thresh 85.95204418499563
target distance 44.05549264282585
model initialize at round 19114
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.7075804 ,  12.60114672,  92.68513283]), 'distance': 27.500000000000004, 'localFrame': array([[-0.07899005,  0.80461137,  0.58852453],
       [-0.99521572, -0.09770199,  0.        ],
       [ 0.05750002, -0.58570886,  0.80847936]]), 'currentState': array([-3.79944059e+01,  9.84903837e+00,  9.06590393e+01, -7.89900457e-02,
        8.04611368e-01,  5.88524527e-01]), 'targetState': array([ 7.00066018, 14.38717335, 94.        ]), 'previousTarget': array([-9.91309724, 12.09724003, 92.46432248])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6278781080606938
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.00066018, 14.38717335, 94.        ]), 'distance': 2.288160690158541, 'localFrame': array([[ 0.65396151,  0.42162139, -0.62814787],
       [-0.54186435,  0.84046596,  0.        ],
       [ 0.5279369 ,  0.34037094,  0.77809398]]), 'currentState': array([ 4.77421963, 14.85665288, 94.24131054,  0.65396151,  0.42162139,
       -0.62814787]), 'targetState': array([ 7.00066018, 14.38717335, 94.        ]), 'previousTarget': array([ 7.00066018, 14.38717335, 94.        ])}
episode index:19115
target thresh 85.95344891033967
target distance 33.81265559758357
model initialize at round 19115
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.38665113, 23.21171097, 54.87875991]), 'distance': 27.500000000000004, 'localFrame': array([[-0.08303864,  0.66479162, -0.74239927],
       [-0.99228895, -0.12394609,  0.        ],
       [-0.09201749,  0.7366746 ,  0.6699577 ]]), 'currentState': array([-3.54885764, -1.32328009, 67.11023883, -0.08303864,  0.66479162,
       -0.74239927]), 'targetState': array([-0.70098752, 30.99207345, 51.        ]), 'previousTarget': array([-1.64809398, 21.64873547, 55.69755311])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6278876210215727
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.70098752, 30.99207345, 51.        ]), 'distance': 1.8682430678986353, 'localFrame': array([[-0.14334713,  0.29838396, -0.94361995],
       [-0.90137825, -0.43303262,  0.        ],
       [-0.40861822,  0.8505585 ,  0.3310308 ]]), 'currentState': array([-1.065238  , 30.09072932, 52.59537849, -0.14334713,  0.29838396,
       -0.94361995]), 'targetState': array([-0.70098752, 30.99207345, 51.        ]), 'previousTarget': array([-0.70098752, 30.99207345, 51.        ])}
episode index:19116
target thresh 85.95485349521823
target distance 41.339428714272735
model initialize at round 19116
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.77826463,  11.06877149,  81.19435856]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.48398379,  0.56755549,  0.6660634 ],
       [-0.76090521,  0.64886305,  0.        ],
       [-0.43218393, -0.50681111,  0.74589513]]), 'currentState': array([-25.49789533, -14.02366339,  73.00735583,   0.48398379,
         0.56755549,   0.6660634 ]), 'targetState': array([-13.2469632 ,  25.79763489,  86.        ]), 'previousTarget': array([-18.13735112,   9.51149608,  80.48454053])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6278877222932252
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-13.2469632 ,  25.79763489,  86.        ]), 'distance': 3.1940984200056453, 'localFrame': array([[ 0.65614898,  0.24013806,  0.71540354],
       [-0.34368699,  0.93908426,  0.        ],
       [-0.67182421, -0.24587489,  0.69871151]]), 'currentState': array([-15.17711046,  23.72325491,  84.52563778,   0.65614898,
         0.24013806,   0.71540354]), 'targetState': array([-13.2469632 ,  25.79763489,  86.        ]), 'previousTarget': array([-13.2469632 ,  25.79763489,  86.        ])}
episode index:19117
target thresh 85.95625793964533
target distance 18.0
model initialize at round 19117
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01]), 'distance': 17.734468429360984, 'localFrame': array([[-0.49757798,  0.55022993, -0.67056929],
       [-0.74170281, -0.67072867,  0.        ],
       [-0.44977005,  0.49736313,  0.7418469 ]]), 'currentState': array([ 2.51728131, -2.45379061, 84.2139161 , -0.49757798,  0.55022993,
       -0.67056929]), 'targetState': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01]), 'previousTarget': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278548795417714
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01]), 'distance': 13.827728820041283, 'localFrame': array([[ 0.42509849,  0.15291086, -0.89213763],
       [-0.3384753 ,  0.94097528,  0.        ],
       [ 0.83947945,  0.30196655,  0.45176372]]), 'currentState': array([-4.12579257, -1.77925652, 80.35684133,  0.42509849,  0.15291086,
       -0.89213763]), 'targetState': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01]), 'previousTarget': array([-9.99203119e-01, -3.99139899e-02,  6.70000000e+01])}
episode index:19118
target thresh 85.95766224363499
target distance 29.790378156846288
model initialize at round 19118
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.33174162, -11.23367111,  52.1401589 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.17775106, -0.77879529,  0.60156667],
       [ 0.97492897,  0.22251631,  0.        ],
       [-0.1338584 ,  0.58648477,  0.7988226 ]]), 'currentState': array([ 0.37337505, 15.56128722, 54.53428753,  0.17775106, -0.77879529,
        0.60156667]), 'targetState': array([ -5.66573491, -12.80232197,  52.        ]), 'previousTarget': array([ -5.20437405, -10.06912102,  52.18349555])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6278670247101955
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.66573491, -12.80232197,  52.        ]), 'distance': 2.819297155649407, 'localFrame': array([[ 0.47342581, -0.58577501, -0.65782645],
       [ 0.77774654,  0.62857801,  0.        ],
       [ 0.41349524, -0.51162225,  0.75316954]]), 'currentState': array([ -5.21937794, -10.1549635 ,  52.8606364 ,   0.47342581,
        -0.58577501,  -0.65782645]), 'targetState': array([ -5.66573491, -12.80232197,  52.        ]), 'previousTarget': array([ -5.66573491, -12.80232197,  52.        ])}
episode index:19119
target thresh 85.95906640720128
target distance 45.92487702485927
model initialize at round 19119
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.04386773, -10.16854566,  40.51669226]), 'distance': 27.5, 'localFrame': array([[ 0.4189482 ,  0.56944781,  0.70725639],
       [-0.80549127,  0.59260763,  0.        ],
       [-0.41912553, -0.56968885,  0.70695714]]), 'currentState': array([ 10.64475002, -37.26920857,  41.31415817,   0.4189482 ,
         0.56944781,   0.70725639]), 'targetState': array([ 3.06287472,  7.3904532 , 40.        ]), 'previousTarget': array([  6.20088398, -11.40968051,  40.        ])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278695279541469
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([ 3.06287472,  7.3904532 , 40.        ]), 'distance': 3.150551118861193, 'localFrame': array([[-0.66194493,  0.7491189 ,  0.02549083],
       [-0.7493624 , -0.6621601 ,  0.        ],
       [ 0.01687901, -0.01910187,  0.99967506]]), 'currentState': array([ 4.59361112e+00,  4.76045849e+00,  3.91839447e+01, -6.61944935e-01,
        7.49118896e-01,  2.54908316e-02]), 'targetState': array([ 3.06287472,  7.3904532 , 40.        ]), 'previousTarget': array([ 3.06287472,  7.3904532 , 40.        ])}
episode index:19120
target thresh 85.96047043035823
target distance 13.174526008099052
model initialize at round 19120
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.67150657, -8.97491387, 17.        ]), 'distance': 13.342993480894114, 'localFrame': array([[ 0.70278761, -0.59134733,  0.39547175],
       [ 0.64383386,  0.76516531,  0.        ],
       [-0.30260126,  0.2546181 ,  0.91847814]]), 'currentState': array([ 3.58611198,  3.42513219, 19.47931789,  0.70278761, -0.59134733,
        0.39547175]), 'targetState': array([-0.67150657, -8.97491387, 17.        ]), 'previousTarget': array([-0.67150657, -8.97491387, 17.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6278844669071059
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.67150657, -8.97491387, 17.        ]), 'distance': 3.1343132436846046, 'localFrame': array([[-0.5386763 , -0.57989954,  0.61118276],
       [ 0.73266865, -0.68058552,  0.        ],
       [ 0.41596214,  0.44779445,  0.7914895 ]]), 'currentState': array([ 0.65518315, -8.3700223 , 14.22548741, -0.5386763 , -0.57989954,
        0.61118276]), 'targetState': array([-0.67150657, -8.97491387, 17.        ]), 'previousTarget': array([-0.67150657, -8.97491387, 17.        ])}
episode index:19121
target thresh 85.96187431311988
target distance 63.88774245341012
model initialize at round 19121
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([31.11752418,  5.72481223, 51.21412108]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.35845865, -0.77462541, -0.52102099],
       [ 0.90754025,  0.41996512,  0.        ],
       [ 0.21881064, -0.47284752,  0.85354386]]), 'currentState': array([36.48307585, 31.7913964 , 44.28634761,  0.35845865, -0.77462541,
       -0.52102099]), 'targetState': array([ 23.5383735 , -31.09573882,  61.        ]), 'previousTarget': array([30.31669344,  6.51440108, 51.58094377])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6278849010154187
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.5383735 , -31.09573882,  61.        ]), 'distance': 1.9266813272074763, 'localFrame': array([[ 0.62148747, -0.78322362,  0.01772261],
       [ 0.78334665,  0.6215851 ,  0.        ],
       [-0.01101611,  0.01388295,  0.99984294]]), 'currentState': array([ 2.38647612e+01, -2.92448781e+01,  6.05758694e+01,  6.21487471e-01,
       -7.83223616e-01,  1.77226141e-02]), 'targetState': array([ 23.5383735 , -31.09573882,  61.        ]), 'previousTarget': array([ 23.5383735 , -31.09573882,  61.        ])}
episode index:19122
target thresh 85.96327805550028
target distance 19.0
model initialize at round 19122
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.79720284,  4.21877156, 76.        ]), 'distance': 19.919100074271558, 'localFrame': array([[ 0.01606987, -0.70845233,  0.70557569],
       [ 0.99974284,  0.02267723,  0.        ],
       [-0.0160005 ,  0.70539424,  0.70863457]]), 'currentState': array([-8.64841269e+00,  1.27481048e+01,  5.80948598e+01,  1.60698697e-02,
       -7.08452332e-01,  7.05575689e-01]), 'targetState': array([-6.79720284,  4.21877156, 76.        ]), 'previousTarget': array([-6.79720284,  4.21877156, 76.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6278993598960333
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.79720284,  4.21877156, 76.        ]), 'distance': 3.0111359992925983, 'localFrame': array([[-0.63163565, -0.58358774,  0.51035454],
       [ 0.67861873, -0.73449072,  0.        ],
       [ 0.37485068,  0.34633615,  0.85996409]]), 'currentState': array([-5.81300446,  4.58148171, 73.17745949, -0.63163565, -0.58358774,
        0.51035454]), 'targetState': array([-6.79720284,  4.21877156, 76.        ]), 'previousTarget': array([-6.79720284,  4.21877156, 76.        ])}
episode index:19123
target thresh 85.96468165751347
target distance 25.0
model initialize at round 19123
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.33945993, -0.28635259, 45.95466326]), 'distance': 27.5, 'localFrame': array([[ 0.20802246,  0.41915095, -0.88376419],
       [-0.8957508 ,  0.44455652,  0.        ],
       [ 0.39288313,  0.79163248,  0.46793254]]), 'currentState': array([ 4.7895669 ,  1.77644942, 68.54217474,  0.20802246,  0.41915095,
       -0.88376419]), 'targetState': array([20.9966776 , -0.37353711, 45.        ]), 'previousTarget': array([19.30808492, -0.18477098, 47.43815181])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6279114995631921
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.9966776 , -0.37353711, 45.        ]), 'distance': 2.6216158662081464, 'localFrame': array([[ 0.31353558,  0.63675823, -0.70443907],
       [-0.89714019,  0.44174595,  0.        ],
       [ 0.3111831 ,  0.63198059,  0.70976447]]), 'currentState': array([19.46836406, -0.93855131, 47.05374936,  0.31353558,  0.63675823,
       -0.70443907]), 'targetState': array([20.9966776 , -0.37353711, 45.        ]), 'previousTarget': array([20.9966776 , -0.37353711, 45.        ])}
episode index:19124
target thresh 85.96608511917346
target distance 70.0
model initialize at round 19124
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.20739266,  8.94707907, 67.78046425]), 'distance': 27.499999999999996, 'localFrame': array([[-0.75082908,  0.13144633, -0.64728475],
       [-0.17244555, -0.98501905,  0.        ],
       [-0.63758781,  0.11162138,  0.76224829]]), 'currentState': array([ 8.46089128, 14.53707607, 94.50903848, -0.75082908,  0.13144633,
       -0.64728475]), 'targetState': array([-0., -0., 25.]), 'previousTarget': array([ 6.20487641,  8.48299237, 68.27677318])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6279061497586411
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 25.]), 'distance': 3.175665725702574, 'localFrame': array([[ 0.71184837, -0.22832932,  0.66418192],
       [ 0.30542828,  0.95221508,  0.        ],
       [-0.63244404,  0.20285994,  0.74757098]]), 'currentState': array([ 0.06987084, -1.20274311, 22.06173863,  0.71184837, -0.22832932,
        0.66418192]), 'targetState': array([-0., -0., 25.]), 'previousTarget': array([ 0.,  0., 25.])}
episode index:19125
target thresh 85.96748844049431
target distance 51.0
model initialize at round 19125
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.25297138,  25.83914871,  32.67524867]), 'distance': 27.5, 'localFrame': array([[ 0.57883446, -0.55934887, -0.59336288],
       [ 0.69489936,  0.719107  ,  0.        ],
       [ 0.4266914 , -0.41232748,  0.80493509]]), 'currentState': array([-0.83665292, 18.13645503, 55.41114339,  0.57883446, -0.55934887,
       -0.59336288]), 'targetState': array([-29.99387956,  34.87645609,   6.        ]), 'previousTarget': array([-14.5597329 ,  26.17912694,  33.80708531])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6279052719530243
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-29.99387956,  34.87645609,   6.        ]), 'distance': 2.3817244752963895, 'localFrame': array([[-0.41595335,  0.56325324, -0.7139528 ],
       [-0.80442474, -0.59405458,  0.        ],
       [-0.42412693,  0.57432129,  0.70019383]]), 'currentState': array([-29.96907828,  35.51990928,   8.29302516,  -0.41595335,
         0.56325324,  -0.7139528 ]), 'targetState': array([-29.99387956,  34.87645609,   6.        ]), 'previousTarget': array([-29.99387956,  34.87645609,   6.        ])}
episode index:19126
target thresh 85.96889162149004
target distance 45.0
model initialize at round 19126
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.97827184,  14.82038056,  46.83386003]), 'distance': 27.5, 'localFrame': array([[ 0.72779752,  0.30766259, -0.6129066 ],
       [-0.38936974,  0.92108154,  0.        ],
       [ 0.56453696,  0.23864728,  0.79015536]]), 'currentState': array([-29.0509671 ,  21.4172983 ,  69.52063866,   0.72779752,
         0.30766259,  -0.6129066 ]), 'targetState': array([-2.05494906,  8.76225909, 26.        ]), 'previousTarget': array([-15.5937079 ,  14.5119775 ,  48.10533532])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.627906722988138
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-2.05494906,  8.76225909, 26.        ]), 'distance': 2.900847663809365, 'localFrame': array([[ 0.47515335, -0.4489039 , -0.75677909],
       [ 0.68674316,  0.72690015,  0.        ],
       [ 0.55010283, -0.51971286,  0.65367072]]), 'currentState': array([-2.47603271,  6.76404781, 28.0602809 ,  0.47515335, -0.4489039 ,
       -0.75677909]), 'targetState': array([-2.05494906,  8.76225909, 26.        ]), 'previousTarget': array([-2.05494906,  8.76225909, 26.        ])}
episode index:19127
target thresh 85.97029466217468
target distance 79.0
model initialize at round 19127
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.23587228,  -0.6134572 ,  47.06150242]), 'distance': 27.5, 'localFrame': array([[ 0.05539126, -0.63824205,  0.76784041],
       [ 0.99625513,  0.08646223,  0.        ],
       [-0.06638919,  0.76496494,  0.64064117]]), 'currentState': array([-17.83616563,  11.3881115 ,  22.5532519 ,   0.05539126,
        -0.63824205,   0.76784041]), 'targetState': array([-28.57933248, -26.53717685, 100.        ]), 'previousTarget': array([-20.70507354,   0.11637747,  45.49858455])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6279048963321057
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.57933248, -26.53717685, 100.        ]), 'distance': 2.1037014682913315, 'localFrame': array([[-0.04305038,  0.02013345,  0.99887002],
       [-0.42363306, -0.90583389,  0.        ],
       [ 0.90481031, -0.42315436,  0.04752568]]), 'currentState': array([-2.83980421e+01, -2.69894762e+01,  9.79535106e+01, -4.30503756e-02,
        2.01334510e-02,  9.98870016e-01]), 'targetState': array([-28.57933248, -26.53717685, 100.        ]), 'previousTarget': array([-28.57933248, -26.53717685, 100.        ])}
episode index:19128
target thresh 85.97169756256228
target distance 45.63302742535198
model initialize at round 19128
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.70577031, -5.47152761, 25.87030016]), 'distance': 27.500000000000007, 'localFrame': array([[-0.46907848, -0.82911311,  0.30419866],
       [ 0.87036071, -0.49241469,  0.        ],
       [ 0.14979189,  0.26476257,  0.95260861]]), 'currentState': array([-1.46732514, 16.633924  , 19.75669372, -0.46907848, -0.82911311,
        0.30419866]), 'targetState': array([ 28.91880726, -27.63516938,  32.        ]), 'previousTarget': array([14.19243717, -4.61839636, 25.94733833])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6279081136560398
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.91880726, -27.63516938,  32.        ]), 'distance': 2.42586480568162, 'localFrame': array([[ 0.83885085, -0.21186863, -0.50143887],
       [ 0.24488017,  0.96955335,  0.        ],
       [ 0.48617174, -0.12279244,  0.86519307]]), 'currentState': array([ 26.72995289, -27.94167965,  31.00010598,   0.83885085,
        -0.21186863,  -0.50143887]), 'targetState': array([ 28.91880726, -27.63516938,  32.        ]), 'previousTarget': array([ 28.91880726, -27.63516938,  32.        ])}
episode index:19129
target thresh 85.97310032266685
target distance 57.273800678516295
model initialize at round 19129
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.83478022, 11.5157982 , 74.55154051]), 'distance': 27.500000000000004, 'localFrame': array([[ 1.56521353e-03,  9.94509470e-01,  1.04634912e-01],
       [-9.99998761e-01,  1.57385288e-03,  0.00000000e+00],
       [-1.64679958e-04, -1.04634783e-01,  9.94510701e-01]]), 'currentState': array([ 2.87035802e+01, -1.06730010e+01,  5.90530897e+01,  1.56521353e-03,
        9.94509470e-01,  1.04634912e-01]), 'targetState': array([16.46850466, 45.08645422, 98.        ]), 'previousTarget': array([23.83491096, 10.03115312, 73.51739142])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6279066002160404
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([16.46850466, 45.08645422, 98.        ]), 'distance': 2.7634599847844474, 'localFrame': array([[-0.68187917,  0.33214899,  0.65170381],
       [-0.43791769, -0.89901507,  0.        ],
       [ 0.58589154, -0.28539263,  0.75847357]]), 'currentState': array([17.99199497, 45.52732872, 95.73696707, -0.68187917,  0.33214899,
        0.65170381]), 'targetState': array([16.46850466, 45.08645422, 98.        ]), 'previousTarget': array([16.46850466, 45.08645422, 98.        ])}
episode index:19130
target thresh 85.97450294250241
target distance 28.22181552392398
model initialize at round 19130
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ -2.03419831, -32.27414761,  34.83447394]), 'distance': 27.5, 'localFrame': array([[ 0.89027576,  0.08652701,  0.44712654],
       [-0.09673543,  0.99531013,  0.        ],
       [-0.44502957, -0.04325298,  0.89447071]]), 'currentState': array([-21.46365929, -20.95975494,  19.        ,   0.89027576,
         0.08652701,   0.44712654]), 'targetState': array([  6.75815624, -37.39421512,  42.        ]), 'previousTarget': array([ -2.03419831, -32.27414761,  34.83447394])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.627910548838422
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  6.75815624, -37.39421512,  42.        ]), 'distance': 4.616405546774045, 'localFrame': array([[ 0.39159336,  0.74761705,  0.53639853],
       [-0.88583912,  0.46399251,  0.        ],
       [-0.2488849 , -0.4751628 ,  0.84396482]]), 'currentState': array([  3.89849859, -39.78715368,  39.27834548,   0.39159336,
         0.74761705,   0.53639853]), 'targetState': array([  6.75815624, -37.39421512,  42.        ]), 'previousTarget': array([  6.75815624, -37.39421512,  42.        ])}
episode index:19131
target thresh 85.97590542208302
target distance 9.0
model initialize at round 19131
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.39410907, -19.89348423,  24.        ]), 'distance': 10.97451517510525, 'localFrame': array([[ 0.33888003, -0.74096433,  0.57976908],
       [ 0.90940324,  0.41591556,  0.        ],
       [-0.24113498,  0.52724388,  0.81478084]]), 'currentState': array([  1.38588921, -19.61278422,  16.50135975,   0.33888003,
        -0.74096433,   0.57976908]), 'targetState': array([  9.39410907, -19.89348423,  24.        ]), 'previousTarget': array([  9.39410907, -19.89348423,  24.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6279269386356497
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.39410907, -19.89348423,  24.        ]), 'distance': 2.660298132957477, 'localFrame': array([[ 0.47606138, -0.19283782,  0.85800882],
       [ 0.3754375 ,  0.92684771,  0.        ],
       [-0.79524352,  0.32212869,  0.51363495]]), 'currentState': array([  7.07834357, -18.74956054,  24.63706744,   0.47606138,
        -0.19283782,   0.85800882]), 'targetState': array([  9.39410907, -19.89348423,  24.        ]), 'previousTarget': array([  9.39410907, -19.89348423,  24.        ])}
episode index:19132
target thresh 85.97730776142268
target distance 36.0
model initialize at round 19132
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([36.37938856,  2.55543842, 26.8185961 ]), 'distance': 27.5, 'localFrame': array([[-0.37602952, -0.08939421, -0.92228546],
       [ 0.23128598, -0.97288581,  0.        ],
       [-0.89727844, -0.2133117 ,  0.38650941]]), 'currentState': array([ 40.6977739 , -16.00765371,  46.64315558,  -0.37602952,
        -0.08939421,  -0.92228546]), 'targetState': array([33.15145261, 16.43110434, 12.        ]), 'previousTarget': array([37.01884019,  1.70609634, 28.04885839])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6279343664413869
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.15145261, 16.43110434, 12.        ]), 'distance': 3.279815026184603, 'localFrame': array([[-0.61350747,  0.09796302, -0.78358907],
       [-0.15767947, -0.98749035,  0.        ],
       [-0.77378664,  0.12355591,  0.62127946]]), 'currentState': array([32.93941744, 13.54918592, 13.55137807, -0.61350747,  0.09796302,
       -0.78358907]), 'targetState': array([33.15145261, 16.43110434, 12.        ]), 'previousTarget': array([33.15145261, 16.43110434, 12.        ])}
episode index:19133
target thresh 85.97870996053541
target distance 65.0
model initialize at round 19133
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 6.88684838,  1.06411613, 26.21184054]), 'distance': 27.5, 'localFrame': array([[ 0.44459532, -0.87668375, -0.18374056],
       [ 0.89186801,  0.45229576,  0.        ],
       [ 0.08310508, -0.16387233,  0.98297477]]), 'currentState': array([-1.26459324,  2.72044182,  0.        ,  0.44459532, -0.87668375,
       -0.18374056]), 'targetState': array([18.94931369, -1.38690689, 65.        ]), 'previousTarget': array([ 6.88684838,  1.06411613, 26.21184054])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6279344651800073
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([18.94931369, -1.38690689, 65.        ]), 'distance': 2.751547964210878, 'localFrame': array([[ 0.34483037,  0.08808189,  0.93452319],
       [-0.247489  ,  0.9688907 ,  0.        ],
       [-0.90545083, -0.23128421,  0.35590224]]), 'currentState': array([18.98767034, -1.09744809, 62.26398857,  0.34483037,  0.08808189,
        0.93452319]), 'targetState': array([18.94931369, -1.38690689, 65.        ]), 'previousTarget': array([18.94931369, -1.38690689, 65.        ])}
episode index:19134
target thresh 85.98011201943525
target distance 67.0
model initialize at round 19134
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.55060199, -5.24106797, 32.1629214 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.22015742, -0.95425855,  0.20229019],
       [ 0.97440374, -0.22480513,  0.        ],
       [ 0.04547587,  0.19711232,  0.97932562]]), 'currentState': array([21.95528362, -6.69069009,  7.1812915 , -0.22015742, -0.95425855,
        0.20229019]), 'targetState': array([-8.54897493, -2.8133659 , 74.        ]), 'previousTarget': array([10.2119226 , -4.13025568, 32.0896147 ])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279332669360541
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.54897493, -2.8133659 , 74.        ]), 'distance': 3.453988884807664, 'localFrame': array([[-0.60544497, -0.77084186, -0.19808892],
       [ 0.78642563, -0.61768498,  0.        ],
       [-0.12235655, -0.1557822 ,  0.98018405]]), 'currentState': array([-7.2212545 , -0.77223486, 71.55030986, -0.60544497, -0.77084186,
       -0.19808892]), 'targetState': array([-8.54897493, -2.8133659 , 74.        ]), 'previousTarget': array([-8.54897493, -2.8133659 , 74.        ])}
episode index:19135
target thresh 85.9815139381362
target distance 71.0
model initialize at round 19135
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.04773151, -31.24668278,  40.25107182]), 'distance': 27.5, 'localFrame': array([[-0.45775439,  0.42851398,  0.77899723],
       [-0.68340555, -0.73003894,  0.        ],
       [ 0.56869831, -0.53237103,  0.62702736]]), 'currentState': array([ 22.28182595, -40.94930544,  14.54920176,  -0.45775439,
         0.42851398,   0.77899723]), 'targetState': array([ 18.94709414, -14.73117863,  84.        ]), 'previousTarget': array([ 21.69427524, -31.36501207,  38.76824483])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6279263117412728
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([ 18.94709414, -14.73117863,  84.        ]), 'distance': 2.653372873803939, 'localFrame': array([[ 0.32657476,  0.80901691,  0.48871318],
       [-0.92729898,  0.37432153,  0.        ],
       [-0.18293586, -0.45318323,  0.87244452]]), 'currentState': array([ 18.74637528, -14.91838822,  81.36086148,   0.32657476,
         0.80901691,   0.48871318]), 'targetState': array([ 18.94709414, -14.73117863,  84.        ]), 'previousTarget': array([ 18.94709414, -14.73117863,  84.        ])}
episode index:19136
target thresh 85.98291571665229
target distance 21.0
model initialize at round 19136
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.99793055,  0.11141106, 28.        ]), 'distance': 22.66895340912783, 'localFrame': array([[-0.30508845, -0.7736951 , -0.55526294],
       [ 0.93028554, -0.36683621,  0.        ],
       [-0.20369055, -0.51655309,  0.83167486]]), 'currentState': array([-1.26077001e+00, -3.86043817e-03,  5.39799933e+00, -3.05088453e-01,
       -7.73695098e-01, -5.55262939e-01]), 'targetState': array([-2.99793055,  0.11141106, 28.        ]), 'previousTarget': array([-2.99793055,  0.11141106, 28.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6279388957147811
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.99793055,  0.11141106, 28.        ]), 'distance': 2.6974953884357986, 'localFrame': array([[-0.50471635, -0.44875293,  0.7374837 ],
       [ 0.66445993, -0.74732389,  0.        ],
       [ 0.55113919,  0.49002837,  0.67536493]]), 'currentState': array([-1.60695423, -0.8590489 , 25.90241241, -0.50471635, -0.44875293,
        0.7374837 ]), 'targetState': array([-2.99793055,  0.11141106, 28.        ]), 'previousTarget': array([-2.99793055,  0.11141106, 28.        ])}
episode index:19137
target thresh 85.98431735499754
target distance 29.49041757886919
model initialize at round 19137
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.31321998,  1.34425541, 71.45369653]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.33647034,  0.43149148,  0.8370202 ],
       [-0.78858474,  0.6149261 ,  0.        ],
       [-0.51470557, -0.66006136,  0.54717198]]), 'currentState': array([  9.67165858, -24.56672313,  67.57971983,   0.33647034,
         0.43149148,   0.8370202 ]), 'targetState': array([ 0.13452315,  4.99819002, 72.        ]), 'previousTarget': array([ 1.17670702,  1.42058014, 71.27211409])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6279488219370551
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 0.13452315,  4.99819002, 72.        ]), 'distance': 2.8848480737328694, 'localFrame': array([[-0.8612364 ,  0.18143976,  0.47471199],
       [-0.20614847, -0.97852072,  0.        ],
       [ 0.46451552, -0.09786115,  0.8801412 ]]), 'currentState': array([ 2.55318653,  4.30158843, 70.59033264, -0.8612364 ,  0.18143976,
        0.47471199]), 'targetState': array([ 0.13452315,  4.99819002, 72.        ]), 'previousTarget': array([ 0.13452315,  4.99819002, 72.        ])}
episode index:19138
target thresh 85.98571885318596
target distance 34.77153034866571
model initialize at round 19138
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.94830309,  21.01608794,  11.35585487]), 'distance': 27.499999999999996, 'localFrame': array([[-0.4556126 ,  0.53389237,  0.71230337],
       [-0.76066944, -0.64913943,  0.        ],
       [ 0.46238421, -0.54182741,  0.70187172]]), 'currentState': array([-2.7539298 , 38.31479153,  8.56239918, -0.4556126 ,  0.53389237,
        0.71230337]), 'targetState': array([-36.42268224,  10.83458435,  13.        ]), 'previousTarget': array([-23.13472565,  21.20214578,  10.70709719])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6279476231934008
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([-36.42268224,  10.83458435,  13.        ]), 'distance': 1.9936494388354822, 'localFrame': array([[-0.59191738, -0.65053048, -0.47586123],
       [ 0.73964231, -0.67300019,  0.        ],
       [-0.3202547 , -0.3519671 ,  0.87952037]]), 'currentState': array([-34.75905852,  11.78246333,  12.44455457,  -0.59191738,
        -0.65053048,  -0.47586123]), 'targetState': array([-36.42268224,  10.83458435,  13.        ]), 'previousTarget': array([-36.42268224,  10.83458435,  13.        ])}
episode index:19139
target thresh 85.98712021123157
target distance 23.41344515284501
model initialize at round 19139
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.5992983 ,   9.79766978,  50.45503986]), 'distance': 27.500000000000004, 'localFrame': array([[-0.8941612 ,  0.27441814, -0.35379433],
       [-0.29339391, -0.95599164,  0.        ],
       [-0.33822442,  0.1038011 ,  0.93532324]]), 'currentState': array([ 2.25112381, -3.00004714, 67.00287574, -0.8941612 ,  0.27441814,
       -0.35379433]), 'targetState': array([-19.3263065,  12.4697184,  47.       ]), 'previousTarget': array([-14.62646254,   9.38849673,  51.01465391])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.62795841557262
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.3263065,  12.4697184,  47.       ]), 'distance': 2.8499359832586246, 'localFrame': array([[-0.80612138, -0.20363228, -0.55560977],
       [ 0.24491424, -0.96954475,  0.        ],
       [-0.53868853, -0.13607674,  0.8314432 ]]), 'currentState': array([-19.51793475,  11.11083911,  49.49777116,  -0.80612138,
        -0.20363228,  -0.55560977]), 'targetState': array([-19.3263065,  12.4697184,  47.       ]), 'previousTarget': array([-19.3263065,  12.4697184,  47.       ])}
episode index:19140
target thresh 85.98852142914839
target distance 40.0
model initialize at round 19140
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.62980331, -18.80773692,  35.88523121]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.20971378,  0.95207058,  0.22266958],
       [-0.97658885,  0.21511445,  0.        ],
       [-0.04789944, -0.21745663,  0.97489397]]), 'currentState': array([  0.35980245, -37.34955324,  15.57813524,   0.20971378,
         0.95207058,   0.22266958]), 'targetState': array([ 0.89724701, -0.44152893, 56.        ]), 'previousTarget': array([  0.29197102, -19.9181706 ,  35.79460605])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6279634843736431
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.89724701, -0.44152893, 56.        ]), 'distance': 2.247033087354721, 'localFrame': array([[ 0.78678734,  0.35281533,  0.50644547],
       [-0.40916942,  0.91245843,  0.        ],
       [-0.46211044, -0.207222  ,  0.86227199]]), 'currentState': array([ 0.99435392, -1.51388639, 54.02774813,  0.78678734,  0.35281533,
        0.50644547]), 'targetState': array([ 0.89724701, -0.44152893, 56.        ]), 'previousTarget': array([ 0.89724701, -0.44152893, 56.        ])}
episode index:19141
target thresh 85.9899225069504
target distance 45.56017373242416
model initialize at round 19141
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.88359753, -4.34482413, 23.23548855]), 'distance': 27.500000000000004, 'localFrame': array([[-0.0581464 , -0.99231892,  0.10918861],
       [ 0.99828763, -0.05849614,  0.        ],
       [ 0.00638711,  0.10900164,  0.99402105]]), 'currentState': array([16.08697519, 22.32399092, 18.99884313, -0.0581464 , -0.99231892,
        0.10918861]), 'targetState': array([  7.48827041, -21.74685739,  26.        ]), 'previousTarget': array([11.26358162, -2.76699081, 22.66728811])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6279601378950139
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([  7.48827041, -21.74685739,  26.        ]), 'distance': 3.314664902716354, 'localFrame': array([[-0.50577937,  0.2931717 , -0.81131842],
       [-0.50148725, -0.86516503,  0.        ],
       [-0.70192433,  0.40686585,  0.58460449]]), 'currentState': array([  9.62569007, -19.85938697,  27.68993966,  -0.50577937,
         0.2931717 ,  -0.81131842]), 'targetState': array([  7.48827041, -21.74685739,  26.        ]), 'previousTarget': array([  7.48827041, -21.74685739,  26.        ])}
episode index:19142
target thresh 85.99132344465167
target distance 21.0
model initialize at round 19142
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.85543866,  5.74689652, 32.        ]), 'distance': 25.54533603039848, 'localFrame': array([[ 0.23781591,  0.79764859,  0.55425654],
       [-0.95831385,  0.28571764,  0.        ],
       [-0.15836087, -0.53115172,  0.83234589]]), 'currentState': array([-0.73995934, -1.77547591, 12.43082876,  0.23781591,  0.79764859,
        0.55425654]), 'targetState': array([13.85543866,  5.74689652, 32.        ]), 'previousTarget': array([13.85543866,  5.74689652, 32.        ])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6279704919923832
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.85543866,  5.74689652, 32.        ]), 'distance': 3.331521721432837, 'localFrame': array([[ 9.99263216e-01,  3.59329176e-02, -1.34852273e-02],
       [-3.59361852e-02,  9.99354087e-01,  0.00000000e+00],
       [ 1.34765170e-02,  4.84607626e-04,  9.99909070e-01]]), 'currentState': array([ 1.30736165e+01,  8.29401320e+00,  3.00000031e+01,  9.99263216e-01,
        3.59329176e-02, -1.34852273e-02]), 'targetState': array([13.85543866,  5.74689652, 32.        ]), 'previousTarget': array([13.85543866,  5.74689652, 32.        ])}
episode index:19143
target thresh 85.99272424226615
target distance 66.0
model initialize at round 19143
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.79023476,  -5.01193823,  42.01660678]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.28122629,  0.6303913 , -0.72354584],
       [-0.91324484,  0.40741117,  0.        ],
       [ 0.29478066,  0.6607745 ,  0.69027633]]), 'currentState': array([-14.06447012,  -6.85602579,  69.20057299,   0.28122629,
         0.6303913 ,  -0.72354584]), 'targetState': array([-22.86363589,  -2.50083064,   5.        ]), 'previousTarget': array([-17.80028436,  -5.30692375,  43.80104731])}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.6279573947376261
{'scaleFactor': 20, 'timeStep': 98, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-22.86363589,  -2.50083064,   5.        ]), 'distance': 2.409235728722696, 'localFrame': array([[ 0.49829239,  0.61902325, -0.60705429],
       [-0.77897839,  0.62705077,  0.        ],
       [ 0.38065386,  0.47288217,  0.79466036]]), 'currentState': array([-25.13578866,  -1.86638552,   4.51090088,   0.49829239,
         0.61902325,  -0.60705429]), 'targetState': array([-22.86363589,  -2.50083064,   5.        ]), 'previousTarget': array([-22.86363589,  -2.50083064,   5.        ])}
episode index:19144
target thresh 85.99412489980789
target distance 26.57470625739311
model initialize at round 19144
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.33331454, 23.73350384,  7.84469827]), 'distance': 27.5, 'localFrame': array([[ 0.59925625, -0.4020437 , -0.69228087],
       [ 0.55713417,  0.83042249,  0.        ],
       [ 0.57488561, -0.38569333,  0.72162815]]), 'currentState': array([-7.14852338, 44.94530832,  7.01121555,  0.59925625, -0.4020437 ,
       -0.69228087]), 'targetState': array([13.5906823 , 19.78113633,  8.        ]), 'previousTarget': array([ 9.46648913, 24.91569491,  8.        ])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6279677478966283
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.5906823 , 19.78113633,  8.        ]), 'distance': 2.765378441747817, 'localFrame': array([[-0.1271373 , -0.95633318, -0.26317856],
       [ 0.99127858, -0.13178303,  0.        ],
       [-0.03468247, -0.26088327,  0.96474714]]), 'currentState': array([14.06527222, 21.77016385,  6.13832014, -0.1271373 , -0.95633318,
       -0.26317856]), 'targetState': array([13.5906823 , 19.78113633,  8.        ]), 'previousTarget': array([13.5906823 , 19.78113633,  8.        ])}
episode index:19145
target thresh 85.99552541729086
target distance 37.0
model initialize at round 19145
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.26832283, -21.57220895,  39.43621676]), 'distance': 27.5, 'localFrame': array([[-0.91249237, -0.14247701, -0.38348138],
       [ 0.15427126, -0.98802853,  0.        ],
       [-0.37889055, -0.05916016,  0.92354861]]), 'currentState': array([-0.71822101, -6.89361006, 59.6199697 , -0.91249237, -0.14247701,
       -0.38348138]), 'targetState': array([-21.10165902, -32.79817048,  24.        ]), 'previousTarget': array([-11.38682421, -21.17818005,  40.65484626])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279349489961845
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([-21.10165902, -32.79817048,  24.        ]), 'distance': 10.284667327699838, 'localFrame': array([[-0.58426382,  0.32576286, -0.74331309],
       [-0.486981  , -0.87341256,  0.        ],
       [-0.64921899,  0.36197935,  0.66894368]]), 'currentState': array([-15.57112586, -29.87991552,  32.16525401,  -0.58426382,
         0.32576286,  -0.74331309]), 'targetState': array([-21.10165902, -32.79817048,  24.        ]), 'previousTarget': array([-21.10165902, -32.79817048,  24.        ])}
episode index:19146
target thresh 85.9969257947291
target distance 25.806609790064567
model initialize at round 19146
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.83788602, -2.54131431, 83.        ]), 'distance': 24.978053766840244, 'localFrame': array([[ 0.81323278,  0.39668749, -0.42578338],
       [-0.43841343,  0.89877342,  0.        ],
       [ 0.38268279,  0.18666915,  0.90482513]]), 'currentState': array([-4.79400447, -0.47720874, 86.59341192,  0.81323278,  0.39668749,
       -0.42578338]), 'targetState': array([19.83788602, -2.54131431, 83.        ]), 'previousTarget': array([19.83788602, -2.54131431, 83.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6279479842536141
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.83788602, -2.54131431, 83.        ]), 'distance': 2.288217184905638, 'localFrame': array([[ 0.89088275, -0.29775697,  0.34302874],
       [ 0.31699039,  0.94842875,  0.        ],
       [-0.32533832,  0.10873681,  0.93932491]]), 'currentState': array([17.91331916, -3.33931099, 83.94614036,  0.89088275, -0.29775697,
        0.34302874]), 'targetState': array([19.83788602, -2.54131431, 83.        ]), 'previousTarget': array([19.83788602, -2.54131431, 83.        ])}
episode index:19147
target thresh 85.99832603213659
target distance 71.0
model initialize at round 19147
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.59609827,  -4.43462924,  51.45718891]), 'distance': 27.5, 'localFrame': array([[-0.38730862, -0.80374307,  0.45165153],
       [ 0.9008609 , -0.43410786,  0.        ],
       [ 0.19606548,  0.40687521,  0.89219442]]), 'currentState': array([-0.37636088, -0.8323186 , 26.18209591, -0.38730862, -0.80374307,
        0.45165153]), 'targetState': array([-29.01088983, -10.92557877,  97.        ]), 'previousTarget': array([-10.41433724,  -3.2416685 ,  51.17919232])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.627941032649026
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([-29.01088983, -10.92557877,  97.        ]), 'distance': 3.154956148547241, 'localFrame': array([[ 0.56527357, -0.62612693,  0.53705759],
       [ 0.7422562 ,  0.67011621,  0.        ],
       [-0.359891  ,  0.39863432,  0.84354558]]), 'currentState': array([-26.93448813,  -9.27587377,  95.2909717 ,   0.56527357,
        -0.62612693,   0.53705759]), 'targetState': array([-29.01088983, -10.92557877,  97.        ]), 'previousTarget': array([-29.01088983, -10.92557877,  97.        ])}
episode index:19148
target thresh 85.99972612952733
target distance 59.0
model initialize at round 19148
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.40663358,  -5.2897372 ,  51.54505889]), 'distance': 27.5, 'localFrame': array([[-0.96133742, -0.21691305,  0.16964401],
       [ 0.22010336, -0.97547656,  0.        ],
       [ 0.16548375,  0.03733922,  0.98550541]]), 'currentState': array([-4.92286947, -9.63479924, 25.44211441, -0.96133742, -0.21691305,
        0.16964401]), 'targetState': array([-21.99822918,   0.27912858,  85.        ]), 'previousTarget': array([-11.43783923,  -5.19854959,  51.88096002])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6279362453072618
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.99822918,   0.27912858,  85.        ]), 'distance': 3.3973474184729398, 'localFrame': array([[ 0.07489215, -0.66933645,  0.73917514],
       [ 0.99379847,  0.11119625,  0.        ],
       [-0.0821935 ,  0.73459112,  0.67351327]]), 'currentState': array([-2.02735034e+01,  1.85007751e-01,  8.20745203e+01,  7.48921468e-02,
       -6.69336452e-01,  7.39175135e-01]), 'targetState': array([-21.99822918,   0.27912858,  85.        ]), 'previousTarget': array([-21.99822918,   0.27912858,  85.        ])}
episode index:19149
target thresh 86.00112608691535
target distance 30.0
model initialize at round 19149
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.01816481,   2.61032207,  41.85539902]), 'distance': 27.5, 'localFrame': array([[-0.37964947, -0.70882294, -0.59450511],
       [ 0.88151988, -0.4721469 ,  0.        ],
       [-0.28069374, -0.52406807,  0.80409183]]), 'currentState': array([ 0.14877991, -4.63942024, 61.18551428, -0.37964947, -0.70882294,
       -0.59450511]), 'targetState': array([-27.28052495,   6.30658058,  32.        ]), 'previousTarget': array([-18.00255186,   3.14716621,  42.18487581])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6279444824819614
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.28052495,   6.30658058,  32.        ]), 'distance': 3.2985323370515376, 'localFrame': array([[ 0.50881655, -0.18763194, -0.84017853],
       [ 0.34598656,  0.93823947,  0.        ],
       [ 0.78828866, -0.29069048,  0.5423099 ]]), 'currentState': array([-26.65433531,   8.28188595,  34.56639257,   0.50881655,
        -0.18763194,  -0.84017853]), 'targetState': array([-27.28052495,   6.30658058,  32.        ]), 'previousTarget': array([-27.28052495,   6.30658058,  32.        ])}
episode index:19150
target thresh 86.00252590431462
target distance 61.0
model initialize at round 19150
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.78511062, 16.26497972, 68.81315434]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.27232633,  0.96207942,  0.0155421 ],
       [-0.96219564,  0.27235923,  0.        ],
       [-0.00423303, -0.01495454,  0.99987921]]), 'currentState': array([3.36586248e+01, 7.01798189e+00, 8.75556334e+01, 2.72326328e-01,
       9.62079422e-01, 1.55420988e-02]), 'targetState': array([-25.04308068,  37.38775348,  26.        ]), 'previousTarget': array([14.85704131, 15.3997915 , 68.29063615])}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.6279332561850919
{'scaleFactor': 20, 'timeStep': 89, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-25.04308068,  37.38775348,  26.        ]), 'distance': 2.0632387302688273, 'localFrame': array([[ 0.46467889, -0.54539215, -0.69758221],
       [ 0.76118427,  0.64853567,  0.        ],
       [ 0.45240694, -0.5309886 ,  0.71650476]]), 'currentState': array([-25.96679536,  37.95099169,  27.75683464,   0.46467889,
        -0.54539215,  -0.69758221]), 'targetState': array([-25.04308068,  37.38775348,  26.        ]), 'previousTarget': array([-25.04308068,  37.38775348,  26.        ])}
episode index:19151
target thresh 86.00392558173915
target distance 40.620578510484
model initialize at round 19151
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.92989548,  23.98749969,  21.41945365]), 'distance': 27.5, 'localFrame': array([[-0.90248847,  0.39345296,  0.17524078],
       [-0.3996371 , -0.91667343,  0.        ],
       [ 0.16063857, -0.07003272,  0.98452561]]), 'currentState': array([-2.42546329, 24.72129728, 11.16101909, -0.90248847,  0.39345296,
        0.17524078]), 'targetState': array([-41.80420206,  23.58831681,  27.        ]), 'previousTarget': array([-26.55067533,  23.73194195,  20.61629095])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6279410824227284
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-41.80420206,  23.58831681,  27.        ]), 'distance': 2.408230010574572, 'localFrame': array([[-0.53294316, -0.82343274, -0.19475651],
       [ 0.83950796, -0.54334739,  0.        ],
       [-0.10582044, -0.16349964,  0.98085162]]), 'currentState': array([-39.89768556,  23.9969685 ,  25.58657494,  -0.53294316,
        -0.82343274,  -0.19475651]), 'targetState': array([-41.80420206,  23.58831681,  27.        ]), 'previousTarget': array([-41.80420206,  23.58831681,  27.        ])}
episode index:19152
target thresh 86.00532511920294
target distance 22.42003070980971
model initialize at round 19152
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.48184229, -23.2144531 ,  49.947095  ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.86496532, -0.4757274 , -0.15974493],
       [ 0.481916  , -0.87621742,  0.        ],
       [-0.13997129, -0.07698364,  0.98715832]]), 'currentState': array([ 2.08920878, -5.48227466, 45.62833295, -0.86496532, -0.4757274 ,
       -0.15974493]), 'targetState': array([-18.73383844, -23.43167295,  50.        ]), 'previousTarget': array([-17.13021373, -22.09430449,  49.64236786])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6279527524842683
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.73383844, -23.43167295,  50.        ]), 'distance': 3.3840311250940713, 'localFrame': array([[-0.70966978, -0.31276019,  0.63130806],
       [ 0.40328461, -0.9150746 ,  0.        ],
       [ 0.57769397,  0.25459683,  0.77553216]]), 'currentState': array([-16.83707982, -22.4573376 ,  47.37233107,  -0.70966978,
        -0.31276019,   0.63130806]), 'targetState': array([-18.73383844, -23.43167295,  50.        ]), 'previousTarget': array([-18.73383844, -23.43167295,  50.        ])}
episode index:19153
target thresh 86.00672451671997
target distance 81.0
model initialize at round 19153
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.45938446, -6.7637866 , 62.40247218]), 'distance': 27.5, 'localFrame': array([[-0.6427821 , -0.29285558, -0.70786071],
       [ 0.41460286, -0.91000246,  0.        ],
       [-0.64415498, -0.29348107,  0.70635205]]), 'currentState': array([ 2.62643926, -9.96126954, 89.69100614, -0.6427821 , -0.29285558,
       -0.70786071]), 'targetState': array([-0.78172371, -0.62362492, 10.        ]), 'previousTarget': array([ 2.29311296, -7.0453454 , 63.73823882])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6279476858031818
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-0.78172371, -0.62362492, 10.        ]), 'distance': 2.3928471488894276, 'localFrame': array([[-0.72954005, -0.09890209, -0.67674936],
       [ 0.13433887, -0.99093545,  0.        ],
       [-0.67061493, -0.09091375,  0.73621349]]), 'currentState': array([ 0.22800099,  0.41066388, 11.90694001, -0.72954005, -0.09890209,
       -0.67674936]), 'targetState': array([-0.78172371, -0.62362492, 10.        ]), 'previousTarget': array([-0.78172371, -0.62362492, 10.        ])}
episode index:19154
target thresh 86.00812377430425
target distance 80.08650598523786
model initialize at round 19154
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.74287809, 11.22906246, 11.7407887 ]), 'distance': 27.5, 'localFrame': array([[-0.36342964,  0.63798113, -0.6788954 ],
       [-0.86890592, -0.49497728,  0.        ],
       [-0.3360378 ,  0.58989623,  0.734235  ]]), 'currentState': array([44.15075836,  3.60368476, 12.59370987, -0.36342964,  0.63798113,
       -0.6788954 ]), 'targetState': array([-36.15488062,  26.79224902,  10.        ]), 'previousTarget': array([17.64998453, 10.43963662, 12.68733737])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6279382666190503
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([-36.15488062,  26.79224902,  10.        ]), 'distance': 3.8855073750281117, 'localFrame': array([[-0.21379142,  0.53929437,  0.81452735],
       [-0.92961729, -0.36852638,  0.        ],
       [ 0.30017482, -0.75719871,  0.58012515]]), 'currentState': array([-33.95900374,  24.43937031,   7.82301721,  -0.21379142,
         0.53929437,   0.81452735]), 'targetState': array([-36.15488062,  26.79224902,  10.        ]), 'previousTarget': array([-36.15488062,  26.79224902,  10.        ])}
episode index:19155
target thresh 86.00952289196977
target distance 44.0
model initialize at round 19155
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.33723393, -6.14539066, 17.03293675]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.78408247, -0.22536988, -0.57829327],
       [ 0.27624648,  0.96108682,  0.        ],
       [ 0.55579004, -0.15975148,  0.81582896]]), 'currentState': array([27.56353252, -3.68032308, 43.70514415,  0.78408247, -0.22536988,
       -0.57829327]), 'targetState': array([17.36110446, -7.71958884,  0.        ]), 'previousTarget': array([20.61957686, -6.1278714 , 17.09840952])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6279414776662026
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([17.36110446, -7.71958884,  0.        ]), 'distance': 2.9178457357093524, 'localFrame': array([[-0.54419207, -0.54475375, -0.63804259],
       [ 0.70747141, -0.70674196,  0.        ],
       [-0.45093147, -0.45139689,  0.77000108]]), 'currentState': array([16.45289205, -8.52887276,  2.65217523, -0.54419207, -0.54475375,
       -0.63804259]), 'targetState': array([17.36110446, -7.71958884,  0.        ]), 'previousTarget': array([17.36110446, -7.71958884,  0.        ])}
episode index:19156
target thresh 86.01092186973052
target distance 31.78838795445448
model initialize at round 19156
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 24.19414227, -11.17072481,   7.98091771]), 'distance': 27.5, 'localFrame': array([[ 0.90254137, -0.07817395, -0.42344764],
       [ 0.08629228,  0.99626986,  0.        ],
       [ 0.42186812, -0.03654026,  0.90592058]]), 'currentState': array([ 1.62109579, -0.84156044, 19.81375067,  0.90254137, -0.07817395,
       -0.42344764]), 'targetState': array([ 31.78838795, -14.64576359,   4.        ]), 'previousTarget': array([ 22.71557158, -10.46567356,   8.56660659])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6279509669594403
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 31.78838795, -14.64576359,   4.        ]), 'distance': 2.930069437136061, 'localFrame': array([[ 0.86342977, -0.44855782,  0.23083524],
       [ 0.46100834,  0.8873958 ,  0.        ],
       [-0.20484223,  0.10641697,  0.97299285]]), 'currentState': array([ 29.18264382, -13.31061406,   4.11304934,   0.86342977,
        -0.44855782,   0.23083524]), 'targetState': array([ 31.78838795, -14.64576359,   4.        ]), 'previousTarget': array([ 31.78838795, -14.64576359,   4.        ])}
episode index:19157
target thresh 86.0123207076005
target distance 54.607547995956054
model initialize at round 19157
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.83443923, -12.59879356,  32.8930004 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.19118887, -0.75698909,  0.62483144],
       [ 0.96955454,  0.24487545,  0.        ],
       [-0.15300588,  0.60580817,  0.78075967]]), 'currentState': array([18.68338886, 13.06371495, 23.4289028 ,  0.19118887, -0.75698909,
        0.62483144]), 'targetState': array([ 12.79195869, -40.00457215,  43.        ]), 'previousTarget': array([ 15.42428668, -11.12553335,  32.42305968])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279181894791731
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 67, 'trapConfig': [], 'currentTarget': array([ 12.79195869, -40.00457215,  43.        ]), 'distance': 14.128002372826368, 'localFrame': array([[-0.09611281, -0.78168826,  0.61621894],
       [ 0.99252562, -0.12203641,  0.        ],
       [ 0.07520115,  0.61161309,  0.78757489]]), 'currentState': array([ 17.96498427, -28.66045819,  36.35535285,  -0.09611281,
        -0.78168826,   0.61621894]), 'targetState': array([ 12.79195869, -40.00457215,  43.        ]), 'previousTarget': array([ 12.79195869, -40.00457215,  43.        ])}
episode index:19158
target thresh 86.01371940559368
target distance 32.18666413682507
model initialize at round 19158
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.58714858, -13.40390868,  75.3596104 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69064605,  0.33514288,  0.64084887],
       [-0.43657322, -0.89966873,  0.        ],
       [ 0.57655169, -0.27977746,  0.76766707]]), 'currentState': array([ 20.47741558, -34.89429903,  60.12291754,  -0.69064605,
         0.33514288,   0.64084887]), 'targetState': array([ 9.14844654, -4.03805968, 82.        ]), 'previousTarget': array([ 13.20749476, -14.82962576,  74.28854476])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6279176347504014
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([ 9.14844654, -4.03805968, 82.        ]), 'distance': 2.9063026930393008, 'localFrame': array([[-0.76312433,  0.23690809, -0.60126185],
       [-0.29648643, -0.95503706,  0.        ],
       [-0.57422736,  0.17826598,  0.79905205]]), 'currentState': array([ 9.5779123 , -6.70346047, 83.07600795, -0.76312433,  0.23690809,
       -0.60126185]), 'targetState': array([ 9.14844654, -4.03805968, 82.        ]), 'previousTarget': array([ 9.14844654, -4.03805968, 82.        ])}
episode index:19159
target thresh 86.01511796372404
target distance 45.74414293800251
model initialize at round 19159
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.39972375,   6.4523412 ,  36.24014321]), 'distance': 27.499999999999996, 'localFrame': array([[-0.44080523, -0.69341983, -0.56996464],
       [ 0.84391603, -0.53647529,  0.        ],
       [-0.30577195, -0.4810023 ,  0.82166922]]), 'currentState': array([ 0.587402  ,  7.95852953, 41.30733578, -0.44080523, -0.69341983,
       -0.56996464]), 'targetState': array([-43.65625207,   5.4892308 ,  33.        ]), 'previousTarget': array([-24.82889144,   6.83313613,  36.7042173 ])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6279254585356128
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-43.65625207,   5.4892308 ,  33.        ]), 'distance': 1.915263954258838, 'localFrame': array([[-0.74811545,  0.61790586,  0.24190001],
       [-0.63681862, -0.77101364,  0.        ],
       [ 0.18650821, -0.15404643,  0.97030118]]), 'currentState': array([-42.27123964,   6.20384471,  34.1132401 ,  -0.74811545,
         0.61790586,   0.24190001]), 'targetState': array([-43.65625207,   5.4892308 ,  33.        ]), 'previousTarget': array([-43.65625207,   5.4892308 ,  33.        ])}
episode index:19160
target thresh 86.01651638200558
target distance 22.028184977171627
model initialize at round 19160
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.7406865 ,  2.44336049, 75.        ]), 'distance': 25.715720181447605, 'localFrame': array([[-0.01176549, -0.35953547,  0.93305724],
       [ 0.99946499, -0.03270664,  0.        ],
       [ 0.03051717,  0.93255805,  0.35972792]]), 'currentState': array([ 1.94179592e+01,  5.13378901e+00,  6.06343614e+01, -1.17654912e-02,
       -3.59535468e-01,  9.33057244e-01]), 'targetState': array([-1.7406865 ,  2.44336049, 75.        ]), 'previousTarget': array([-1.7406865 ,  2.44336049, 75.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6279380267916659
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.7406865 ,  2.44336049, 75.        ]), 'distance': 2.5113469069998424, 'localFrame': array([[-0.41331538,  0.35131858,  0.84008669],
       [-0.64764895, -0.76193886,  0.        ],
       [ 0.6400947 , -0.54408127,  0.54245216]]), 'currentState': array([-1.07333443,  4.00654814, 73.15123044, -0.41331538,  0.35131858,
        0.84008669]), 'targetState': array([-1.7406865 ,  2.44336049, 75.        ]), 'previousTarget': array([-1.7406865 ,  2.44336049, 75.        ])}
episode index:19161
target thresh 86.0179146604523
target distance 57.0
model initialize at round 19161
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.74771614, -16.46991321,  66.91138047]), 'distance': 27.499999999999996, 'localFrame': array([[-0.47073584,  0.62700378, -0.62070446],
       [-0.7997042 , -0.60039419,  0.        ],
       [-0.37266736,  0.49637997,  0.78404462]]), 'currentState': array([  8.6895001 , -32.09974352,  88.44670122,  -0.47073584,
         0.62700378,  -0.62070446]), 'targetState': array([-9.50576198,  8.86794729, 32.        ]), 'previousTarget': array([  2.167374  , -17.8481811 ,  67.70029423])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6279394734670538
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.50576198,  8.86794729, 32.        ]), 'distance': 2.2933755734718635, 'localFrame': array([[ 0.42826568,  0.54030971, -0.72432998],
       [-0.78367829,  0.62116692,  0.        ],
       [ 0.44992982,  0.56764168,  0.68945347]]), 'currentState': array([-9.5712594 ,  9.16782611, 34.27274158,  0.42826568,  0.54030971,
       -0.72432998]), 'targetState': array([-9.50576198,  8.86794729, 32.        ]), 'previousTarget': array([-9.50576198,  8.86794729, 32.        ])}
episode index:19162
target thresh 86.01931279907815
target distance 30.56521777008535
model initialize at round 19162
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.24474291,  22.54744209,   6.44563435]), 'distance': 27.500000000000004, 'localFrame': array([[-0.93062043,  0.1015919 , -0.35160305],
       [-0.10852106, -0.99409415,  0.        ],
       [-0.34952654,  0.03815633,  0.93614918]]), 'currentState': array([ 6.77424296, 15.36820993, 22.66016051, -0.93062043,  0.1015919 ,
       -0.35160305]), 'targetState': array([-22.60025672,  25.40134635,   0.        ]), 'previousTarget': array([-12.93119818,  22.11685983,   7.59220519])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6279456954897468
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-22.60025672,  25.40134635,   0.        ]), 'distance': 2.020383543126553, 'localFrame': array([[-0.77909672,  0.57297446,  0.25437878],
       [-0.59246376, -0.80559711,  0.        ],
       [ 0.20492681, -0.15071021,  0.96710467]]), 'currentState': array([-21.01362663,  24.61605833,   0.97358994,  -0.77909672,
         0.57297446,   0.25437878]), 'targetState': array([-22.60025672,  25.40134635,   0.        ]), 'previousTarget': array([-22.60025672,  25.40134635,   0.        ])}
episode index:19163
target thresh 86.02071079789714
target distance 65.0
model initialize at round 19163
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 37.50967941, -16.27250054,  64.71755883]), 'distance': 27.5, 'localFrame': array([[ 0.51141372,  0.39947951, -0.76083647],
       [-0.61558427,  0.78807107,  0.        ],
       [ 0.59959321,  0.46835896,  0.64894366]]), 'currentState': array([ 35.89811111, -27.32754339,  89.84600636,   0.51141372,
         0.39947951,  -0.76083647]), 'targetState': array([39.99276121,  0.76095413, 26.        ]), 'previousTarget': array([ 36.61204943, -16.45920912,  65.83394974])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6279403547880161
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([39.99276121,  0.76095413, 26.        ]), 'distance': 2.6977096795246434, 'localFrame': array([[-0.48216392,  0.85140692, -0.20645633],
       [-0.87015367, -0.49278048,  0.        ],
       [-0.10173765,  0.17964873,  0.97845582]]), 'currentState': array([40.00237634, -1.93579708, 26.07125982, -0.48216392,  0.85140692,
       -0.20645633]), 'targetState': array([39.99276121,  0.76095413, 26.        ]), 'previousTarget': array([39.99276121,  0.76095413, 26.        ])}
episode index:19164
target thresh 86.02210865692324
target distance 63.0
model initialize at round 19164
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.61486247, 32.77354604, 59.61915964]), 'distance': 27.5, 'localFrame': array([[ 0.05821179, -0.39232914, -0.91798107],
       [ 0.98917093,  0.14676811,  0.        ],
       [ 0.13473035, -0.90804018,  0.39662421]]), 'currentState': array([ 1.80270756e+01,  3.08587252e+01,  8.45660002e+01,  5.82117868e-02,
       -3.92329135e-01, -9.17981066e-01]), 'targetState': array([-10.13698447,  35.58428791,  23.        ]), 'previousTarget': array([ 7.23750208, 33.39394595, 61.0219769 ])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.627941120310964
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-10.13698447,  35.58428791,  23.        ]), 'distance': 2.981574970108791, 'localFrame': array([[ 0.4318137 ,  0.59392924, -0.67881145],
       [-0.80882342,  0.5880516 ,  0.        ],
       [ 0.39917616,  0.5490386 ,  0.73431261]]), 'currentState': array([-9.98374945, 35.90320016, 25.96050727,  0.4318137 ,  0.59392924,
       -0.67881145]), 'targetState': array([-10.13698447,  35.58428791,  23.        ]), 'previousTarget': array([-10.13698447,  35.58428791,  23.        ])}
episode index:19165
target thresh 86.02350637617042
target distance 55.0
model initialize at round 19165
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.79943978,  4.13720788, 42.52652807]), 'distance': 27.499999999999996, 'localFrame': array([[-0.56052386,  0.52431169,  0.64102282],
       [-0.68312285, -0.73030348,  0.        ],
       [ 0.46814119, -0.43789733,  0.76752182]]), 'currentState': array([29.11597556,  7.78912757, 16.56985812, -0.56052386,  0.52431169,
        0.64102282]), 'targetState': array([11.99691955,  0.2718849 , 70.        ]), 'previousTarget': array([21.53969738,  4.163591  , 40.92034661])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6279465654872838
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.99691955,  0.2718849 , 70.        ]), 'distance': 3.221818044402492, 'localFrame': array([[-0.5063214 ,  0.3719974 ,  0.77798237],
       [-0.59208281, -0.80587713,  0.        ],
       [ 0.6269582 , -0.46062999,  0.62828611]]), 'currentState': array([13.37052763,  2.12502077, 67.7507335 , -0.5063214 ,  0.3719974 ,
        0.77798237]), 'targetState': array([11.99691955,  0.2718849 , 70.        ]), 'previousTarget': array([11.99691955,  0.2718849 , 70.        ])}
episode index:19166
target thresh 86.02490395565266
target distance 45.39838450683849
model initialize at round 19166
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.13182952,  16.91188232,  74.94806524]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.22830161,  0.26860452, -0.93580446],
       [-0.76195644,  0.64762828,  0.        ],
       [ 0.60605343,  0.71304223,  0.35251953]]), 'currentState': array([-38.66043804,  26.9138967 ,  91.52645953,   0.22830161,
         0.26860452,  -0.93580446]), 'targetState': array([ 5.54405635,  4.27357452, 54.        ]), 'previousTarget': array([-20.3263688 ,  17.08258831,  76.22428379])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6279473306063216
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 5.54405635,  4.27357452, 54.        ]), 'distance': 2.654606685289946, 'localFrame': array([[ 0.89904728, -0.41303092, -0.14532531],
       [ 0.41746273,  0.90869404,  0.        ],
       [ 0.13205624, -0.0606679 ,  0.98938393]]), 'currentState': array([ 3.41576347,  4.27804441, 55.58659576,  0.89904728, -0.41303092,
       -0.14532531]), 'targetState': array([ 5.54405635,  4.27357452, 54.        ]), 'previousTarget': array([ 5.54405635,  4.27357452, 54.        ])}
episode index:19167
target thresh 86.02630139538394
target distance 41.0
model initialize at round 19167
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.46809829,  0.2832667 , 70.71705387]), 'distance': 27.5, 'localFrame': array([[ 0.318088  ,  0.35373075, -0.8795991 ],
       [-0.74357596,  0.66865148,  0.        ],
       [ 0.58814524,  0.65404874,  0.4757157 ]]), 'currentState': array([-0.26500138,  0.30595824, 98.19071476,  0.318088  ,  0.35373075,
       -0.8795991 ]), 'targetState': array([-1.98119887,  0.27358915, 59.        ]), 'previousTarget': array([-1.32727469,  0.18328698, 72.53266034])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6279559734982789
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.98119887,  0.27358915, 59.        ]), 'distance': 3.595518844927571, 'localFrame': array([[-0.85324658, -0.00553927, -0.52147827],
       [ 0.00649185, -0.99997893,  0.        ],
       [-0.52146729, -0.00338536,  0.85326456]]), 'currentState': array([-2.26181541e+00,  2.66063382e+00,  6.16741406e+01, -8.53246579e-01,
       -5.53926908e-03, -5.21478275e-01]), 'targetState': array([-1.98119887,  0.27358915, 59.        ]), 'previousTarget': array([-1.98119887,  0.27358915, 59.        ])}
episode index:19168
target thresh 86.02769869537823
target distance 33.0
model initialize at round 19168
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.18196855, -1.62554243, 68.76145816]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.07204133,  0.80584412,  0.58772893],
       [-0.99602774,  0.08904348,  0.        ],
       [-0.05233343, -0.58539432,  0.80905791]]), 'currentState': array([ 1.09380563, -9.77107017, 42.51133566,  0.07204133,  0.80584412,
        0.58772893]), 'targetState': array([ 0.,  0., 74.]), 'previousTarget': array([ 0.31812489, -2.28166562, 67.08879071])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6279658827770153
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 74.]), 'distance': 3.2320944073450737, 'localFrame': array([[-0.51912937, -0.05286065,  0.85305946],
       [ 0.10130176, -0.99485575,  0.        ],
       [ 0.84867111,  0.08641643,  0.52181372]]), 'currentState': array([ 7.99544347e-01,  1.48348601e+00,  7.12420239e+01, -5.19129372e-01,
       -5.28606479e-02,  8.53059463e-01]), 'targetState': array([ 0.,  0., 74.]), 'previousTarget': array([ 0.,  0., 74.])}
episode index:19169
target thresh 86.02909585564953
target distance 54.0
model initialize at round 19169
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.7253158 , -5.34375973, 68.41686606]), 'distance': 27.500000000000004, 'localFrame': array([[-0.0982425 ,  0.27866217,  0.95535114],
       [-0.9431059 , -0.33249249,  0.        ],
       [ 0.31764708, -0.9009973 ,  0.29547283]]), 'currentState': array([ 23.63640163, -18.58464006,  44.33162139,  -0.0982425 ,
         0.27866217,   0.95535114]), 'targetState': array([21.64408583, 10.3698384 , 97.        ]), 'previousTarget': array([22.82156515, -6.46743347, 66.97245409])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.627966311551313
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([21.64408583, 10.3698384 , 97.        ]), 'distance': 2.2018510670782696, 'localFrame': array([[ 0.45360604, -0.3131448 ,  0.83437515],
       [ 0.56811778,  0.82294726,  0.        ],
       [-0.68664675,  0.47402336,  0.55119697]]), 'currentState': array([20.70793894, 11.26740874, 95.22063365,  0.45360604, -0.3131448 ,
        0.83437515]), 'targetState': array([21.64408583, 10.3698384 , 97.        ]), 'previousTarget': array([21.64408583, 10.3698384 , 97.        ])}
episode index:19170
target thresh 86.03049287621177
target distance 51.866542940019585
model initialize at round 19170
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.98855868, -20.02853576,  89.5221691 ]), 'distance': 27.5, 'localFrame': array([[-0.77911506, -0.07491709,  0.62238826],
       [ 0.09571516, -0.99540876,  0.        ],
       [ 0.61953073,  0.059572  ,  0.78270866]]), 'currentState': array([ 2.78222700e+01, -3.79497334e+01,  8.85037648e+01, -7.79115061e-01,
       -7.49170881e-02,  6.22388265e-01]), 'targetState': array([-23.2437383 ,   5.97734305,  91.        ]), 'previousTarget': array([  7.54054878, -19.69663257,  88.62588479])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6279670754807133
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-23.2437383 ,   5.97734305,  91.        ]), 'distance': 2.002357385323129, 'localFrame': array([[-0.80119208, -0.30198297,  0.51662126],
       [ 0.35269566, -0.93573809,  0.        ],
       [ 0.48342219,  0.18221008,  0.85621403]]), 'currentState': array([-21.50646168,   5.09312654,  90.5423252 ,  -0.80119208,
        -0.30198297,   0.51662126]), 'targetState': array([-23.2437383 ,   5.97734305,  91.        ]), 'previousTarget': array([-23.2437383 ,   5.97734305,  91.        ])}
episode index:19171
target thresh 86.03188975707894
target distance 34.17984654841187
model initialize at round 19171
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.35528286,  7.91964717, 26.05464217]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.96089077,  0.27626993,  0.019075  ],
       [-0.27632021,  0.96106563,  0.        ],
       [-0.01833233, -0.00527081,  0.99981806]]), 'currentState': array([-2.57064141e+01,  2.53491119e+01,  3.48862602e+01,  9.60890769e-01,
        2.76269932e-01,  1.90750016e-02]), 'targetState': array([ 6.91116216, -4.02937186, 20.        ]), 'previousTarget': array([-7.26283003,  8.01225036, 25.80564019])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279343210953867
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 79, 'trapConfig': [], 'currentTarget': array([ 6.91116216, -4.02937186, 20.        ]), 'distance': 20.156142716160062, 'localFrame': array([[ 0.98801114, -0.07481026, -0.13504599],
       [ 0.07550191,  0.99714566,  0.        ],
       [ 0.13466053, -0.01019623,  0.99083933]]), 'currentState': array([-5.37050111, 11.92741312, 19.09897297,  0.98801114, -0.07481026,
       -0.13504599]), 'targetState': array([ 6.91116216, -4.02937186, 20.        ]), 'previousTarget': array([ 6.91116216, -4.02937186, 20.        ])}
episode index:19172
target thresh 86.033286498265
target distance 64.87855918422095
model initialize at round 19172
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([16.78870222, -6.99884121, 64.30385996]), 'distance': 27.5, 'localFrame': array([[-0.02219315,  0.82239598, -0.56848247],
       [-0.99963608, -0.02697614,  0.        ],
       [-0.01533546,  0.56827558,  0.82269538]]), 'currentState': array([ 3.53581352e+01, -2.44704361e+01,  5.40000000e+01, -2.21931490e-02,
        8.22395980e-01, -5.68482467e-01]), 'targetState': array([-29.52042396,  36.57245643,  90.        ]), 'previousTarget': array([16.78870222, -6.99884121, 64.30385996])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6279306874343765
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-29.52042396,  36.57245643,  90.        ]), 'distance': 3.971384615591343, 'localFrame': array([[-0.4515534 ,  0.74971158,  0.48376862],
       [-0.85662141, -0.5159455 ,  0.        ],
       [ 0.24959824, -0.41440656,  0.87519593]]), 'currentState': array([-27.63649126,  33.66303155,  91.93854075,  -0.4515534 ,
         0.74971158,   0.48376862]), 'targetState': array([-29.52042396,  36.57245643,  90.        ]), 'previousTarget': array([-29.52042396,  36.57245643,  90.        ])}
episode index:19173
target thresh 86.03468309978395
target distance 40.662646559339635
model initialize at round 19173
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.01443705, -18.13409266,  76.63682118]), 'distance': 27.500000000000004, 'localFrame': array([[-0.57035358, -0.23535908,  0.786958  ],
       [ 0.38145312, -0.92438818,  0.        ],
       [ 0.72745468,  0.30018759,  0.61700657]]), 'currentState': array([ 22.7943147 , -39.39618002,  65.94623593,  -0.57035358,
        -0.23535908,   0.786958  ]), 'targetState': array([-4.34342249,  2.47682884, 87.        ]), 'previousTarget': array([  9.50610473, -17.52745047,  76.17694292])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.627936130882912
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.34342249,  2.47682884, 87.        ]), 'distance': 2.3411630318523238, 'localFrame': array([[-0.15069991,  0.39431619,  0.90653421],
       [-0.93410559, -0.35699683,  0.        ],
       [ 0.32362984, -0.84679868,  0.42213235]]), 'currentState': array([-3.73926122,  0.21773424, 87.11191547, -0.15069991,  0.39431619,
        0.90653421]), 'targetState': array([-4.34342249,  2.47682884, 87.        ]), 'previousTarget': array([-4.34342249,  2.47682884, 87.        ])}
episode index:19174
target thresh 86.0360795616497
target distance 37.0
model initialize at round 19174
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.31093519, -4.69845769, 28.84962707]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.8176308 , -0.51595233, -0.25548596],
       [ 0.53366313,  0.84569714,  0.        ],
       [ 0.21606375, -0.13634344,  0.96681277]]), 'currentState': array([ 1.17563794, -0.62413886, 55.72990316,  0.8176308 , -0.51595233,
       -0.25548596]), 'targetState': array([ 6.67237353, -6.03982048, 20.        ]), 'previousTarget': array([ 4.81869062, -4.36187005, 30.27914091])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6279451892640675
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.67237353, -6.03982048, 20.        ]), 'distance': 2.8581994151502474, 'localFrame': array([[-0.64382297,  0.51385908, -0.56695752],
       [-0.62380689, -0.78157851,  0.        ],
       [-0.44312181,  0.35367201,  0.82374703]]), 'currentState': array([ 8.28478998, -4.4074089 , 21.70430324, -0.64382297,  0.51385908,
       -0.56695752]), 'targetState': array([ 6.67237353, -6.03982048, 20.        ]), 'previousTarget': array([ 6.67237353, -6.03982048, 20.        ])}
episode index:19175
target thresh 86.03747588387627
target distance 43.81967891440145
model initialize at round 19175
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.14842986,  -3.28334558,  44.2387246 ]), 'distance': 27.5, 'localFrame': array([[ 0.75579751, -0.51905162, -0.3991936 ],
       [ 0.56611462,  0.82432653,  0.        ],
       [ 0.32906588, -0.22598934,  0.91686666]]), 'currentState': array([-37.52426704, -14.78878586,  53.03901324,   0.75579751,
        -0.51905162,  -0.3991936 ]), 'targetState': array([ 5.07949032,  6.18051601, 37.        ]), 'previousTarget': array([-14.92576318,  -2.76954862,  44.30457329])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6279510171314081
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.07949032,  6.18051601, 37.        ]), 'distance': 2.9094394697238677, 'localFrame': array([[-0.07963956,  0.94784579, -0.30861934],
       [-0.99648876, -0.08372662,  0.        ],
       [-0.02583965,  0.3075357 ,  0.95118563]]), 'currentState': array([ 3.89679099,  4.29376926, 38.87249754, -0.07963956,  0.94784579,
       -0.30861934]), 'targetState': array([ 5.07949032,  6.18051601, 37.        ]), 'previousTarget': array([ 5.07949032,  6.18051601, 37.        ])}
episode index:19176
target thresh 86.03887206647758
target distance 76.0
model initialize at round 19176
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.25154341, -2.38896779, 58.28076081]), 'distance': 27.5, 'localFrame': array([[ 0.89158837,  0.20975794, -0.4013375 ],
       [-0.22901082,  0.97342388,  0.        ],
       [ 0.3906715 ,  0.09191063,  0.91593024]]), 'currentState': array([ 27.44374592, -14.53276471,  82.59544021,   0.89158837,
         0.20975794,  -0.4013375 ]), 'targetState': array([14.5824128 , 22.72340725,  8.        ]), 'previousTarget': array([22.50371532, -2.51608236, 59.53487748])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6279430591100739
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([14.5824128 , 22.72340725,  8.        ]), 'distance': 2.5278457502060667, 'localFrame': array([[-3.96362945e-01,  9.18092593e-01,  1.55127461e-03],
       [-9.18093698e-01, -3.96363422e-01,  0.00000000e+00],
       [ 6.14868511e-04, -1.42421544e-03,  9.99998797e-01]]), 'currentState': array([ 1.60315482e+01,  2.09720601e+01,  6.89419978e+00, -3.96362945e-01,
        9.18092593e-01,  1.55127461e-03]), 'targetState': array([14.5824128 , 22.72340725,  8.        ]), 'previousTarget': array([14.5824128 , 22.72340725,  8.        ])}
episode index:19177
target thresh 86.04026810946762
target distance 44.0
model initialize at round 19177
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.32691233,  -4.30855614,  76.88881433]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.22452269, -0.65186409,  0.72433609],
       [ 0.94548826,  0.32565618,  0.        ],
       [-0.23588452,  0.68485128,  0.68944704]]), 'currentState': array([-27.85374086,   6.59338773,  51.68829024,   0.22452269,
        -0.65186409,   0.72433609]), 'targetState': array([-25.22960677, -12.14359675,  95.        ]), 'previousTarget': array([-26.83293569,  -3.44906626,  75.94553957])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6279103162245222
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 75, 'trapConfig': [], 'currentTarget': array([-25.22960677, -12.14359675,  95.        ]), 'distance': 14.333110387977834, 'localFrame': array([[-0.07628515, -0.79403416,  0.60306743],
       [ 0.9954167 , -0.09563256,  0.        ],
       [ 0.05767288,  0.60030339,  0.79769021]]), 'currentState': array([-2.88538915e+01, -9.04274032e+00,  8.14838133e+01, -7.62851536e-02,
       -7.94034162e-01,  6.03067430e-01]), 'targetState': array([-25.22960677, -12.14359675,  95.        ]), 'previousTarget': array([-25.22960677, -12.14359675,  95.        ])}
episode index:19178
target thresh 86.04166401286034
target distance 60.692448033250905
model initialize at round 19178
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.39944423, 23.84132389, 37.65467479]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.67733373, -0.32495598,  0.66001714],
       [ 0.43255341,  0.90160831,  0.        ],
       [-0.59507694,  0.28549267,  0.75125054]]), 'currentState': array([-24.61972209,  36.29754938,  41.45869721,   0.67733373,
        -0.32495598,   0.66001714]), 'targetState': array([35.60396994,  5.32515954, 32.        ]), 'previousTarget': array([-0.93148342, 24.6077712 , 36.81581542])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6279036384370293
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([35.60396994,  5.32515954, 32.        ]), 'distance': 2.994546740625962, 'localFrame': array([[ 0.2745732 , -0.85608351, -0.4378705 ],
       [ 0.95222166,  0.30540776,  0.        ],
       [ 0.13372905, -0.41694978,  0.89903805]]), 'currentState': array([32.79357387,  5.66776941, 32.97550119,  0.2745732 , -0.85608351,
       -0.4378705 ]), 'targetState': array([35.60396994,  5.32515954, 32.        ]), 'previousTarget': array([35.60396994,  5.32515954, 32.        ])}
episode index:19179
target thresh 86.04305977666971
target distance 45.0
model initialize at round 19179
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.1670115 ,  -9.78352735,  23.54411608]), 'distance': 27.5, 'localFrame': array([[-0.33376407, -0.51445682, -0.78989602],
       [ 0.83891486, -0.54426267,  0.        ],
       [-0.42991092, -0.66265551,  0.61324079]]), 'currentState': array([-36.1997787 , -17.58693059,  43.67473304,  -0.33376407,
        -0.51445682,  -0.78989602]), 'targetState': array([ 0.75396006, -0.65692025,  0.        ]), 'previousTarget': array([-18.53314414,  -9.65050341,  24.30869578])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.627903085074282
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 0.75396006, -0.65692025,  0.        ]), 'distance': 2.7712437839418005, 'localFrame': array([[ 0.38046995,  0.6266526 ,  0.68010965],
       [-0.85478613,  0.51898042,  0.        ],
       [-0.35296359, -0.58134829,  0.7331104 ]]), 'currentState': array([-0.88611094, -2.75650317,  0.76269967,  0.38046995,  0.6266526 ,
        0.68010965]), 'targetState': array([ 0.75396006, -0.65692025,  0.        ]), 'previousTarget': array([ 0.75396006, -0.65692025,  0.        ])}
episode index:19180
target thresh 86.04445540090965
target distance 24.0
model initialize at round 19180
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.17331937, -0.198199  , 70.62550161]), 'distance': 27.499999999999996, 'localFrame': array([[-0.97155234, -0.18695445,  0.14537568],
       [ 0.18896188, -0.98198442,  0.        ],
       [ 0.14275665,  0.02747046,  0.98937653]]), 'currentState': array([-10.58482889, -12.10425862,  48.12891934,  -0.97155234,
        -0.18695445,   0.14537568]), 'targetState': array([-0.,  0., 71.]), 'previousTarget': array([-0.25882104, -0.33713721, 70.31995327])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6279151884718925
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.11022302e-16, 0.00000000e+00, 7.10000000e+01]), 'distance': 2.27768314967957, 'localFrame': array([[ 0.52894633,  0.82283801,  0.20773394],
       [-0.84118822,  0.54074243,  0.        ],
       [-0.11233055, -0.17474334,  0.97818537]]), 'currentState': array([-0.72638669, -0.67237661, 68.94863153,  0.52894633,  0.82283801,
        0.20773394]), 'targetState': array([-0.,  0., 71.]), 'previousTarget': array([ 0.,  0., 71.])}
episode index:19181
target thresh 86.04585088559418
target distance 46.0
model initialize at round 19181
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.03435969, 10.98036382, 73.02355553]), 'distance': 27.5, 'localFrame': array([[-0.89939856,  0.22473755,  0.37493368],
       [-0.24242182, -0.97017094,  0.        ],
       [ 0.36374976, -0.09089211,  0.92705164]]), 'currentState': array([26.34554315, -0.86800996, 50.93457259, -0.89939856,  0.22473755,
        0.37493368]), 'targetState': array([ 2.75665714, 23.84115856, 97.        ]), 'previousTarget': array([16.07516164, 10.30976561, 72.67743931])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278824538671344
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 69, 'trapConfig': [], 'currentTarget': array([ 2.75665714, 23.84115856, 97.        ]), 'distance': 6.977089120481941, 'localFrame': array([[-0.81319347,  0.29384111,  0.50236817],
       [-0.33983671, -0.94048445,  0.        ],
       [ 0.47246945, -0.17072315,  0.86465382]]), 'currentState': array([ 3.40199772, 21.78476809, 90.36414539, -0.81319347,  0.29384111,
        0.50236817]), 'targetState': array([ 2.75665714, 23.84115856, 97.        ]), 'previousTarget': array([ 2.75665714, 23.84115856, 97.        ])}
episode index:19182
target thresh 86.0472462307372
target distance 57.0
model initialize at round 19182
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.3528393 ,  -4.08338206,  39.79532903]), 'distance': 27.500000000000004, 'localFrame': array([[-0.56750931,  0.41560153,  0.71078025],
       [-0.59083477, -0.80679259,  0.        ],
       [ 0.57345223, -0.41995368,  0.70341413]]), 'currentState': array([-1.43850636, -2.02623846, 16.16463927, -0.56750931,  0.41560153,
        0.71078025]), 'targetState': array([-34.31574385,  -6.88692413,  72.        ]), 'previousTarget': array([-14.16811951,  -4.01484231,  38.50245138])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6278800185838237
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.31574385,  -6.88692413,  72.        ]), 'distance': 2.7245891130743267, 'localFrame': array([[ 0.11384016,  0.35343723,  0.92850554],
       [-0.95184375,  0.30658356,  0.        ],
       [-0.28466453, -0.8837922 ,  0.37131853]]), 'currentState': array([-33.12832025,  -6.59960639,  69.56466439,   0.11384016,
         0.35343723,   0.92850554]), 'targetState': array([-34.31574385,  -6.88692413,  72.        ]), 'previousTarget': array([-34.31574385,  -6.88692413,  72.        ])}
episode index:19183
target thresh 86.04864143635268
target distance 52.578353776932786
model initialize at round 19183
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.31778739, 22.34458629, 53.58609363]), 'distance': 27.5, 'localFrame': array([[-0.33402559,  0.79295268,  0.50956153],
       [-0.92157269, -0.38820584,  0.        ],
       [ 0.19781476, -0.46959799,  0.86043422]]), 'currentState': array([-3.17591368, -4.40486739, 47.57534454, -0.33402559,  0.79295268,
        0.50956153]), 'targetState': array([-7.24698183, 46.43792905, 59.        ]), 'previousTarget': array([-5.33732023, 20.60083006, 53.1031781 ])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278866303054991
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.24698183, 46.43792905, 59.        ]), 'distance': 3.0873757286715566, 'localFrame': array([[-0.09197088,  0.65856051,  0.74688648],
       [-0.99038869, -0.13831215,  0.        ],
       [ 0.10330347, -0.73970792,  0.66495156]]), 'currentState': array([-6.00230748, 43.69296403, 58.33079026, -0.09197088,  0.65856051,
        0.74688648]), 'targetState': array([-7.24698183, 46.43792905, 59.        ]), 'previousTarget': array([-7.24698183, 46.43792905, 59.        ])}
episode index:19184
target thresh 86.05003650245457
target distance 86.0
model initialize at round 19184
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.14360095,  -1.49328008,  34.81886167]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.4339689 ,  0.37062164,  0.82116417],
       [-0.64942493,  0.76042571,  0.        ],
       [-0.62443435, -0.53328448,  0.57069204]]), 'currentState': array([-19.51641721,   3.84076549,   7.84842232,   0.4339689 ,
         0.37062164,   0.82116417]), 'targetState': array([-21.47331901, -12.8022096 ,  92.        ]), 'previousTarget': array([-20.2222131 ,  -1.46240667,  32.99986498])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278539023080894
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-21.47331901, -12.8022096 ,  92.        ]), 'distance': 18.217656857191866, 'localFrame': array([[-0.18536938, -0.77057704,  0.6097944 ],
       [ 0.97226379, -0.23388698,  0.        ],
       [ 0.14262297,  0.59288102,  0.79255964]]), 'currentState': array([-21.72016621,  -5.90748384,  75.13925136,  -0.18536938,
        -0.77057704,   0.6097944 ]), 'targetState': array([-21.47331901, -12.8022096 ,  92.        ]), 'previousTarget': array([-21.47331901, -12.8022096 ,  92.        ])}
episode index:19185
target thresh 86.05143142905683
target distance 42.02898258503999
model initialize at round 19185
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.94594475, 27.29095804, 82.60928433]), 'distance': 27.5, 'localFrame': array([[-0.89307397,  0.2910727 , -0.34306788],
       [-0.30987907, -0.95077598,  0.        ],
       [-0.3261807 ,  0.10630956,  0.93931061]]), 'currentState': array([20.29742022,  4.33204599, 67.65572553, -0.89307397,  0.2910727 ,
       -0.34306788]), 'targetState': array([15.99748122, 46.31501478, 95.        ]), 'previousTarget': array([18.49257453, 27.52508125, 83.37615755])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6278511659820011
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([15.99748122, 46.31501478, 95.        ]), 'distance': 3.0753891464892877, 'localFrame': array([[-0.05588148,  0.98694514,  0.15105147],
       [-0.99840089, -0.05653012,  0.        ],
       [ 0.00853896, -0.15080992,  0.9885259 ]]), 'currentState': array([ 1.51811165e+01,  4.43770648e+01,  9.27559151e+01, -5.58814845e-02,
        9.86945142e-01,  1.51051472e-01]), 'targetState': array([15.99748122, 46.31501478, 95.        ]), 'previousTarget': array([15.99748122, 46.31501478, 95.        ])}
episode index:19186
target thresh 86.05282621617341
target distance 8.0
model initialize at round 19186
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.36494843, -2.97771937, 97.        ]), 'distance': 7.740191199327576, 'localFrame': array([[-0.40358452, -0.59754171,  0.6928661 ],
       [ 0.82869181, -0.55970517,  0.        ],
       [ 0.38780074,  0.57417247,  0.72106627]]), 'currentState': array([-0.83426802, -1.58908259, 89.48041706, -0.40358452, -0.59754171,
        0.6928661 ]), 'targetState': array([ 0.36494843, -2.97771937, 97.        ]), 'previousTarget': array([ 0.36494843, -2.97771937, 97.        ])}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6278690138912114
{'scaleFactor': 20, 'timeStep': 4, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.36494843, -2.97771937, 97.        ]), 'distance': 2.873910328279832, 'localFrame': array([[-0.41081045, -0.73015511,  0.54599294],
       [ 0.87152545, -0.49035028,  0.        ],
       [ 0.26772779,  0.47584674,  0.83778978]]), 'currentState': array([-0.34706764, -2.36837409, 94.28318348, -0.41081045, -0.73015511,
        0.54599294]), 'targetState': array([ 0.36494843, -2.97771937, 97.        ]), 'previousTarget': array([ 0.36494843, -2.97771937, 97.        ])}
episode index:19187
target thresh 86.05422086381826
target distance 16.0
model initialize at round 19187
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.87477471, -14.3438029 ,  96.        ]), 'distance': 17.000831140303706, 'localFrame': array([[ 0.48623805,  0.70239584,  0.51981982],
       [-0.82221191,  0.5691815 ,  0.        ],
       [-0.29587183, -0.42740205,  0.85427592]]), 'currentState': array([-17.26056354, -20.68861339,  81.26051994,   0.48623805,
         0.70239584,   0.51981982]), 'targetState': array([-22.87477471, -14.3438029 ,  96.        ]), 'previousTarget': array([-22.87477471, -14.3438029 ,  96.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6278839007076379
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.87477471, -14.3438029 ,  96.        ]), 'distance': 2.28097949365557, 'localFrame': array([[-0.18141718, -0.13798956,  0.97367689],
       [ 0.60539608, -0.79592436,  0.        ],
       [ 0.77497316,  0.58946017,  0.22793269]]), 'currentState': array([-21.79295348, -13.18240029,  94.36180764,  -0.18141718,
        -0.13798956,   0.97367689]), 'targetState': array([-22.87477471, -14.3438029 ,  96.        ]), 'previousTarget': array([-22.87477471, -14.3438029 ,  96.        ])}
episode index:19188
target thresh 86.0556153720053
target distance 32.036995954095914
model initialize at round 19188
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.6152066 ,  6.39611614, 38.1661927 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55813941, -0.62512071, -0.54562303],
       [ 0.74594008,  0.66601306,  0.        ],
       [ 0.36339206, -0.40700209,  0.83803073]]), 'currentState': array([ 5.49999127, -7.04028028, 44.60048142,  0.55813941, -0.62512071,
       -0.54562303]), 'targetState': array([36.39726502, 10.91966569, 36.        ]), 'previousTarget': array([27.60069495,  6.07973131, 38.74575372])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6278913090793664
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([36.39726502, 10.91966569, 36.        ]), 'distance': 3.2717112050526196, 'localFrame': array([[ 0.63385466,  0.34287214, -0.6933015 ],
       [-0.47578333,  0.87956252,  0.        ],
       [ 0.60980201,  0.32986129,  0.72064765]]), 'currentState': array([35.84419348,  7.99715948, 34.63722214,  0.63385466,  0.34287214,
       -0.6933015 ]), 'targetState': array([36.39726502, 10.91966569, 36.        ]), 'previousTarget': array([36.39726502, 10.91966569, 36.        ])}
episode index:19189
target thresh 86.0570097407485
target distance 48.33405626630186
model initialize at round 19189
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.15939015,  5.385181  , 71.27351702]), 'distance': 27.5, 'localFrame': array([[-9.89178685e-01,  4.57175379e-03, -1.46644564e-01],
       [-4.62171803e-03, -9.99989320e-01,  0.00000000e+00],
       [-1.46642998e-01,  6.77749826e-04,  9.89189250e-01]]), 'currentState': array([ 3.04466190e+01,  2.47513385e+00,  5.18883570e+01, -9.89178685e-01,
        4.57175379e-03, -1.46644564e-01]), 'targetState': array([-16.42702235,   9.54740472,  99.        ]), 'previousTarget': array([12.09870369,  5.35126291, 71.85178404])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6278934498010699
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.42702235,   9.54740472,  99.        ]), 'distance': 3.100548166659729, 'localFrame': array([[-0.6444197 ,  0.75176123,  0.13992247],
       [-0.75923019, -0.65082219,  0.        ],
       [ 0.09106465, -0.10623336,  0.99016246]]), 'currentState': array([-13.93351488,   8.34942886,  97.59975956,  -0.6444197 ,
         0.75176123,   0.13992247]), 'targetState': array([-16.42702235,   9.54740472,  99.        ]), 'previousTarget': array([-16.42702235,   9.54740472,  99.        ])}
episode index:19190
target thresh 86.05840397006179
target distance 44.0
model initialize at round 19190
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.96321921, -4.74714797, 21.68056324]), 'distance': 27.5, 'localFrame': array([[ 0.83866529, -0.31071973,  0.44731843],
       [ 0.34741558,  0.93771126,  0.        ],
       [-0.41945553,  0.15540539,  0.89437477]]), 'currentState': array([  4.74331725, -11.72208514,   2.29924686,   0.83866529,
        -0.31071973,   0.44731843]), 'targetState': array([45.82532748,  4.00491716, 46.        ]), 'previousTarget': array([22.02221897, -4.11194929, 21.2781731 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278607316806071
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 65, 'trapConfig': [], 'currentTarget': array([41.79940142,  2.66874665, 42.4580266 ]), 'distance': 27.5, 'localFrame': array([[ 0.57734614, -0.51908987,  0.63025165],
       [ 0.66859344,  0.74362814,  0.        ],
       [-0.46867286,  0.42138211,  0.77639092]]), 'currentState': array([21.76524658, -3.98041846, 24.83215802,  0.57734614, -0.51908987,
        0.63025165]), 'targetState': array([45.82532748,  4.00491716, 46.        ]), 'previousTarget': array([41.79940142,  2.66874665, 42.4580266 ])}
episode index:19191
target thresh 86.05979805995914
target distance 68.0
model initialize at round 19191
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.08093821, -25.21919019,  48.54655565]), 'distance': 27.5, 'localFrame': array([[-0.32364314,  0.71543334,  0.6192013 ],
       [-0.91111045, -0.41216229,  0.        ],
       [ 0.25521143, -0.56416077,  0.78523229]]), 'currentState': array([-32.47791459, -34.05371895,  25.64423964,  -0.32364314,
         0.71543334,   0.6192013 ]), 'targetState': array([ 3.98161945, -8.07135098, 93.        ]), 'previousTarget': array([-19.27390489, -25.99127136,  48.02750077])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6278532874662779
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([ 3.98161945, -8.07135098, 93.        ]), 'distance': 2.979675392987682, 'localFrame': array([[ 0.55347873,  0.40318857,  0.72876627],
       [-0.58880056,  0.80827835,  0.        ],
       [-0.589046  , -0.42909799,  0.68476253]]), 'currentState': array([ 3.03893642, -9.3500096 , 90.47911796,  0.55347873,  0.40318857,
        0.72876627]), 'targetState': array([ 3.98161945, -8.07135098, 93.        ]), 'previousTarget': array([ 3.98161945, -8.07135098, 93.        ])}
episode index:19192
target thresh 86.06119201045446
target distance 79.0
model initialize at round 19192
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.24101524,  4.91560972, 60.36454443]), 'distance': 27.5, 'localFrame': array([[-0.27325954, -0.44116565, -0.85481115],
       [ 0.85012972, -0.52657331,  0.        ],
       [-0.45012074, -0.72670036,  0.51893921]]), 'currentState': array([ 6.2213068 , -9.65365742, 83.60384089, -0.27325954, -0.44116565,
       -0.85481115]), 'targetState': array([-0.39155392, 38.99803438,  6.        ]), 'previousTarget': array([ 5.09704244,  4.87960009, 61.63912169])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6278448483211574
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-0.39155392, 38.99803438,  6.        ]), 'distance': 1.56101016803658, 'localFrame': array([[ 0.38528487, -0.52289153, -0.7603552 ],
       [ 0.8050585 ,  0.59319542,  0.        ],
       [ 0.45103922, -0.61213042,  0.64950749]]), 'currentState': array([ 0.19288193, 39.16985075,  7.43724271,  0.38528487, -0.52289153,
       -0.7603552 ]), 'targetState': array([-0.39155392, 38.99803438,  6.        ]), 'previousTarget': array([-0.39155392, 38.99803438,  6.        ])}
episode index:19193
target thresh 86.06258582156168
target distance 83.0
model initialize at round 19193
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.21230053, -8.0663701 , 30.94277324]), 'distance': 27.500000000000004, 'localFrame': array([[-0.47216548, -0.75681326,  0.45198833],
       [ 0.84842267, -0.52931935,  0.        ],
       [ 0.23924617,  0.38347715,  0.89202385]]), 'currentState': array([ 1.59655504,  2.00319343,  5.80855754, -0.47216548, -0.75681326,
        0.45198833]), 'targetState': array([-14.1288919 , -30.92530378,  88.        ]), 'previousTarget': array([-3.15815458, -6.74623015, 30.00415965])}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.6278325979397902
{'scaleFactor': 20, 'timeStep': 94, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([-14.1288919 , -30.92530378,  88.        ]), 'distance': 2.292267951970812, 'localFrame': array([[ 0.39443501,  0.74364663,  0.5398247 ],
       [-0.88342426,  0.46857398,  0.        ],
       [-0.25294781, -0.47689424,  0.84177746]]), 'currentState': array([-14.78857884, -30.97525166,  85.80527662,   0.39443501,
         0.74364663,   0.5398247 ]), 'targetState': array([-14.1288919 , -30.92530378,  88.        ]), 'previousTarget': array([-14.1288919 , -30.92530378,  88.        ])}
episode index:19194
target thresh 86.06397949329478
target distance 31.077140636481836
model initialize at round 19194
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.61289286, 11.60015441, 68.71757152]), 'distance': 27.5, 'localFrame': array([[-0.64125279,  0.76651471,  0.03535623],
       [-0.76699425, -0.64165397,  0.        ],
       [ 0.02268646, -0.02711802,  0.99937477]]), 'currentState': array([ 7.45406267e+00, -1.46914806e+01,  7.39203341e+01, -6.41252791e-01,
        7.66514707e-01,  3.53562279e-02]), 'targetState': array([14.46232644, 15.22632963, 68.        ]), 'previousTarget': array([13.62307713, 10.83022634, 68.7072889 ])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6278416522764195
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([14.46232644, 15.22632963, 68.        ]), 'distance': 2.8532264497593145, 'localFrame': array([[ 0.42719524,  0.87219557, -0.23828369],
       [-0.89806376,  0.4398653 ,  0.        ],
       [ 0.10481273,  0.21399395,  0.97119559]]), 'currentState': array([12.10774333, 13.76233031, 68.6734579 ,  0.42719524,  0.87219557,
       -0.23828369]), 'targetState': array([14.46232644, 15.22632963, 68.        ]), 'previousTarget': array([14.46232644, 15.22632963, 68.        ])}
episode index:19195
target thresh 86.06537302566768
target distance 45.0
model initialize at round 19195
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.98412148, -5.96526803, 80.99775559]), 'distance': 27.5, 'localFrame': array([[-0.28483311, -0.27400853,  0.91858011],
       [ 0.6932803 , -0.72066804,  0.        ],
       [ 0.66199133,  0.6368335 ,  0.39523484]]), 'currentState': array([ 2.56292382e-02,  4.19531802e-01,  5.45437451e+01, -2.84833114e-01,
       -2.74008527e-01,  9.18580113e-01]), 'targetState': array([  6.52828214, -10.06883967,  98.        ]), 'previousTarget': array([ 4.26886193, -5.91146243, 79.63070565])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6278398354809478
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  6.52828214, -10.06883967,  98.        ]), 'distance': 2.26454382420899, 'localFrame': array([[-0.1038767 ,  0.5045777 ,  0.8570945 ],
       [-0.97945971, -0.20163998,  0.        ],
       [ 0.17282452, -0.83948952,  0.51515922]]), 'currentState': array([ 5.21450021, -8.88481708, 96.58570643, -0.1038767 ,  0.5045777 ,
        0.8570945 ]), 'targetState': array([  6.52828214, -10.06883967,  98.        ]), 'previousTarget': array([  6.52828214, -10.06883967,  98.        ])}
episode index:19196
target thresh 86.0667664186943
target distance 28.193213572783826
model initialize at round 19196
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 35.67445316, -19.28193602,  98.06892699]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.70441687, -0.00244234,  0.7097823 ],
       [ 0.00346716,  0.99999399,  0.        ],
       [-0.70977803,  0.00246093,  0.7044211 ]]), 'currentState': array([ 9.42451286e+00, -2.74258747e+01,  9.90000000e+01,  7.04416868e-01,
       -2.44234179e-03,  7.09782298e-01]), 'targetState': array([ 37.61772644, -18.67904328,  98.        ]), 'previousTarget': array([ 35.67445316, -19.28193602,  98.06892699])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278071303793444
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 97, 'trapConfig': [], 'currentTarget': array([ 37.61772644, -18.67904328,  98.        ]), 'distance': 24.600818737885234, 'localFrame': array([[ 0.89413141,  0.37341341,  0.24716684],
       [-0.38537032,  0.922762  ,  0.        ],
       [-0.22807616, -0.09525076,  0.96897294]]), 'currentState': array([ 14.37979082, -26.51049569,  99.96646486,   0.89413141,
         0.37341341,   0.24716684]), 'targetState': array([ 37.61772644, -18.67904328,  98.        ]), 'previousTarget': array([ 37.61772644, -18.67904328,  98.        ])}
episode index:19197
target thresh 86.06815967238857
target distance 72.0
model initialize at round 19197
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([30.66139565, 16.73235781, 38.28110917]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23337705,  0.56014064,  0.7948444 ],
       [-0.92308563, -0.38459448,  0.        ],
       [ 0.30569277, -0.73370944,  0.6068133 ]]), 'currentState': array([40.54210491, 13.54199879, 12.81656138, -0.23337705,  0.56014064,
        0.7948444 ]), 'targetState': array([13.30964989, 22.33502227, 83.        ]), 'previousTarget': array([31.10330171, 16.5482054 , 36.51119972])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.62780500670248
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([13.30964989, 22.33502227, 83.        ]), 'distance': 3.3177203204520347, 'localFrame': array([[-0.78328354,  0.1937113 ,  0.59071383],
       [-0.24007414, -0.97075455,  0.        ],
       [ 0.57343814, -0.14181512,  0.80688114]]), 'currentState': array([15.08941342, 22.56896223, 80.20984194, -0.78328354,  0.1937113 ,
        0.59071383]), 'targetState': array([13.30964989, 22.33502227, 83.        ]), 'previousTarget': array([13.30964989, 22.33502227, 83.        ])}
episode index:19198
target thresh 86.06955278676446
target distance 54.0
model initialize at round 19198
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.32749155,  -8.46103393,  50.40468583]), 'distance': 27.5, 'localFrame': array([[ 0.32355438, -0.5975547 ,  0.73364906],
       [ 0.87936658,  0.47614537,  0.        ],
       [-0.3493236 ,  0.64514647,  0.67952856]]), 'currentState': array([-43.50826488,   5.27549722,  28.42125769,   0.32355438,
        -0.5975547 ,   0.73364906]), 'targetState': array([-21.5502012 , -27.57877496,  81.        ]), 'previousTarget': array([-35.23289538,  -7.91511346,  49.01172185])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277723068219287
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 66, 'trapConfig': [], 'currentTarget': array([-21.5502012 , -27.57877496,  81.        ]), 'distance': 14.489489960098682, 'localFrame': array([[ 0.29305305, -0.62556965,  0.72303701],
       [ 0.90556056,  0.424217  ,  0.        ],
       [-0.30672459,  0.65475381,  0.69080929]]), 'currentState': array([-22.91835567, -21.92700251,  67.72856674,   0.29305305,
        -0.62556965,   0.72303701]), 'targetState': array([-21.5502012 , -27.57877496,  81.        ]), 'previousTarget': array([-21.5502012 , -27.57877496,  81.        ])}
episode index:19199
target thresh 86.07094576183587
target distance 45.053343107634305
model initialize at round 19199
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.78762977, -3.47372923, 36.65259337]), 'distance': 27.5, 'localFrame': array([[ 0.75148207,  0.22969597,  0.61847753],
       [-0.29230751,  0.95632438,  0.        ],
       [-0.59146514, -0.18078563,  0.78580248]]), 'currentState': array([ 6.88509185, 17.66341027, 19.18628333,  0.75148207,  0.22969597,
        0.61847753]), 'targetState': array([  2.46427403, -26.88730841,  56.        ]), 'previousTarget': array([ 4.12149771, -2.82607097, 35.70567865])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277762482483963
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.46427403, -26.88730841,  56.        ]), 'distance': 1.9980989701606968, 'localFrame': array([[ 0.042001  , -0.16534772,  0.98534057],
       [ 0.96921969,  0.24619748,  0.        ],
       [-0.24258837,  0.95501148,  0.17059881]]), 'currentState': array([ 2.81095858e+00, -2.62408826e+01,  5.41414137e+01,  4.20009969e-02,
       -1.65347724e-01,  9.85340574e-01]), 'targetState': array([  2.46427403, -26.88730841,  56.        ]), 'previousTarget': array([  2.46427403, -26.88730841,  56.        ])}
episode index:19200
target thresh 86.07233859761674
target distance 59.0
model initialize at round 19200
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.93486232, -28.89229253,  57.37892784]), 'distance': 27.5, 'localFrame': array([[-0.01231965,  0.72931456,  0.68406761],
       [-0.99985736, -0.01688968,  0.        ],
       [ 0.01155368, -0.68397003,  0.72941861]]), 'currentState': array([ 3.59659995e+00, -3.24432628e+01,  3.01171833e+01, -1.23196473e-02,
        7.29314564e-01,  6.84067608e-01]), 'targetState': array([  2.1915821 , -24.90375409,  88.        ]), 'previousTarget': array([  3.42741865, -29.64959883,  56.17911761])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6277809331233689
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.1915821 , -24.90375409,  88.        ]), 'distance': 2.7476155887385296, 'localFrame': array([[ 0.26858665,  0.13268626,  0.95407315],
       [-0.44291685,  0.89656269,  0.        ],
       [-0.85538639, -0.42257507,  0.29957375]]), 'currentState': array([  3.11258839, -24.56750524,  85.43327533,   0.26858665,
         0.13268626,   0.95407315]), 'targetState': array([  2.1915821 , -24.90375409,  88.        ]), 'previousTarget': array([  2.1915821 , -24.90375409,  88.        ])}
episode index:19201
target thresh 86.07373129412099
target distance 42.0
model initialize at round 19201
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.22065586,  1.73901077, 19.63978721]), 'distance': 27.5, 'localFrame': array([[ 0.16656578, -0.96304677, -0.21165244],
       [ 0.98537034,  0.1704268 ,  0.        ],
       [ 0.03607125, -0.20855603,  0.977345  ]]), 'currentState': array([-2.15119549, 11.03751263, 45.43717461,  0.16656578, -0.96304677,
       -0.21165244]), 'targetState': array([-5.55549569, -4.25869318,  3.        ]), 'previousTarget': array([-4.49484811,  2.41279224, 19.54121283])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6277863764332601
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.55549569, -4.25869318,  3.        ]), 'distance': 3.4854085121478438, 'localFrame': array([[-0.81978924,  0.26404376, -0.50815991],
       [-0.30657748, -0.95184571,  0.        ],
       [-0.48368983,  0.15579039,  0.86126274]]), 'currentState': array([-3.59387449, -2.92430574,  5.55333603, -0.81978924,  0.26404376,
       -0.50815991]), 'targetState': array([-5.55549569, -4.25869318,  3.        ]), 'previousTarget': array([-5.55549569, -4.25869318,  3.        ])}
episode index:19202
target thresh 86.07512385136256
target distance 76.0
model initialize at round 19202
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-6.95116519, -1.75036557, 43.06657008]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.33797882,  0.6983962 ,  0.63088277],
       [-0.90013642,  0.43560811,  0.        ],
       [-0.27481765, -0.56788055,  0.77587817]]), 'currentState': array([-11.79031563,  -2.23348784,  16.        ,   0.33797882,
         0.6983962 ,   0.63088277]), 'targetState': array([ 1.79749518, -0.87693276, 92.        ]), 'previousTarget': array([-6.95116519, -1.75036557, 43.06657008])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277536843342946
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 1.79749518, -0.87693276, 92.        ]), 'distance': 21.371874564053655, 'localFrame': array([[-0.91269546,  0.34067293, -0.22567446],
       [-0.34969407, -0.93686395,  0.        ],
       [-0.21142626,  0.07891702,  0.97420277]]), 'currentState': array([-0.87853039, -3.48660554, 70.95753062, -0.91269546,  0.34067293,
       -0.22567446]), 'targetState': array([ 1.79749518, -0.87693276, 92.        ]), 'previousTarget': array([ 1.79749518, -0.87693276, 92.        ])}
episode index:19203
target thresh 86.07651626935535
target distance 38.0
model initialize at round 19203
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.63555904, -16.15606997,  39.86290879]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.53425662,  0.60680511,  0.58852139],
       [-0.75054902,  0.66081478,  0.        ],
       [-0.38890363, -0.44171415,  0.80848165]]), 'currentState': array([-28.92868547,  -6.2227177 ,  19.27896367,   0.53425662,
         0.60680511,   0.58852139]), 'targetState': array([ -1.64628417, -23.94346985,  56.        ]), 'previousTarget': array([-13.86158262, -16.62637348,  39.05408151])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6277579959122439
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ -1.64628417, -23.94346985,  56.        ]), 'distance': 2.6480212239603036, 'localFrame': array([[ 0.66588954, -0.50955314,  0.54492819],
       [ 0.60770857,  0.79416012,  0.        ],
       [-0.43276023,  0.33115753,  0.83848272]]), 'currentState': array([ -3.84028767, -22.96295025,  54.8878191 ,   0.66588954,
        -0.50955314,   0.54492819]), 'targetState': array([ -1.64628417, -23.94346985,  56.        ]), 'previousTarget': array([ -1.64628417, -23.94346985,  56.        ])}
episode index:19204
target thresh 86.07790854811333
target distance 45.0
model initialize at round 19204
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.59432451, -29.72786531,  68.07751318]), 'distance': 27.500000000000004, 'localFrame': array([[-0.2794744 , -0.81621627,  0.50565309],
       [ 0.94607786, -0.32393932,  0.        ],
       [ 0.16380092,  0.4783872 ,  0.86273689]]), 'currentState': array([ 22.06790716, -37.75999111,  42.59927085,  -0.2794744 ,
        -0.81621627,   0.50565309]), 'targetState': array([ 33.18528599, -24.07772402,  86.        ]), 'previousTarget': array([ 28.50295915, -29.51901791,  66.76434466])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6277654046677708
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 33.18528599, -24.07772402,  86.        ]), 'distance': 2.768862090413779, 'localFrame': array([[ 0.33984415, -0.19763412,  0.91948176],
       [ 0.50271611,  0.86445157,  0.        ],
       [-0.79484745,  0.46223829,  0.39313267]]), 'currentState': array([ 31.98829987, -22.42898254,  84.12504052,   0.33984415,
        -0.19763412,   0.91948176]), 'targetState': array([ 33.18528599, -24.07772402,  86.        ]), 'previousTarget': array([ 33.18528599, -24.07772402,  86.        ])}
episode index:19205
target thresh 86.07930068765037
target distance 11.0
model initialize at round 19205
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.39605218,  13.45072857,  59.        ]), 'distance': 13.182017288663062, 'localFrame': array([[-0.76597708,  0.21204122,  0.60689179],
       [-0.26679084, -0.96375445,  0.        ],
       [ 0.58489466, -0.16191317,  0.79478447]]), 'currentState': array([-3.70216797,  7.23122431, 49.49867058, -0.76597708,  0.21204122,
        0.60689179]), 'targetState': array([-10.39605218,  13.45072857,  59.        ]), 'previousTarget': array([-10.39605218,  13.45072857,  59.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6277817388729531
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.39605218,  13.45072857,  59.        ]), 'distance': 2.7546668564387637, 'localFrame': array([[-0.50776701, -0.22594821,  0.83133631],
       [ 0.40655009, -0.91362849,  0.        ],
       [ 0.75953254,  0.33797985,  0.55576968]]), 'currentState': array([-8.95164431, 13.15510362, 56.6730962 , -0.50776701, -0.22594821,
        0.83133631]), 'targetState': array([-10.39605218,  13.45072857,  59.        ]), 'previousTarget': array([-10.39605218,  13.45072857,  59.        ])}
episode index:19206
target thresh 86.08069268798045
target distance 53.0
model initialize at round 19206
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.56691406,  24.50168471,  60.08599084]), 'distance': 27.500000000000004, 'localFrame': array([[-0.27501276,  0.65025205,  0.70819507],
       [-0.92101496, -0.3895272 ,  0.        ],
       [ 0.27586125, -0.65225825,  0.70601682]]), 'currentState': array([-17.93899646,  15.05430911,  35.1246647 ,  -0.27501276,
         0.65025205,   0.70819507]), 'targetState': array([-31.71332263,  34.68811278,  87.        ]), 'previousTarget': array([-23.73125956,  23.85649859,  58.8168704 ])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277856783718924
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.71332263,  34.68811278,  87.        ]), 'distance': 1.618135320255473, 'localFrame': array([[ 0.46216051,  0.24893596,  0.85113956],
       [-0.4742184 ,  0.88040724,  0.        ],
       [-0.74934943, -0.40362604,  0.52493948]]), 'currentState': array([-32.29227117,  33.94014943,  85.68709051,   0.46216051,
         0.24893596,   0.85113956]), 'targetState': array([-31.71332263,  34.68811278,  87.        ]), 'previousTarget': array([-31.71332263,  34.68811278,  87.        ])}
episode index:19207
target thresh 86.08208454911743
target distance 37.0
model initialize at round 19207
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.40407329, -10.09494464,  63.87300515]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.31320795, -0.65739706, -0.68536843],
       [ 0.90277429,  0.43011461,  0.        ],
       [ 0.29478698, -0.618733  ,  0.72819648]]), 'currentState': array([-33.59211203,   6.55635215,  81.33897062,   0.31320795,
        -0.65739706,  -0.68536843]), 'targetState': array([ -6.90869081, -27.13429548,  46.        ]), 'previousTarget': array([-21.15130116,  -9.42388986,  65.23443167])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6277892512342246
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.90869081, -27.13429548,  46.        ]), 'distance': 2.094526371361578, 'localFrame': array([[-0.04204206, -0.86597395, -0.49831876],
       [ 0.99882358, -0.04849176,  0.        ],
       [-0.02416436, -0.49773253,  0.86699389]]), 'currentState': array([-6.51090010e+00, -2.51277836e+01,  4.64502370e+01, -4.20420635e-02,
       -8.65973946e-01, -4.98318763e-01]), 'targetState': array([ -6.90869081, -27.13429548,  46.        ]), 'previousTarget': array([ -6.90869081, -27.13429548,  46.        ])}
episode index:19208
target thresh 86.08347627107526
target distance 52.0
model initialize at round 19208
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.5604155 ,   1.57987218,  73.89894897]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.33385236, -0.39429202,  0.85619881],
       [ 0.76317565,  0.64619109,  0.        ],
       [-0.55326804,  0.65343009,  0.51664649]]), 'currentState': array([-22.09648457,   6.05182694,  48.46893458,   0.33385236,
        -0.39429202,   0.85619881]), 'targetState': array([-40.90192076,  -2.83423324,  99.        ]), 'previousTarget': array([-31.16942538,   2.2628479 ,  72.41524186])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6277893571418705
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.90192076,  -2.83423324,  99.        ]), 'distance': 2.622395276731155, 'localFrame': array([[-0.44688955, -0.25498908,  0.85747904],
       [ 0.49558739, -0.86855808,  0.        ],
       [ 0.74477035,  0.4249558 ,  0.5145189 ]]), 'currentState': array([-41.90105417,  -2.64173257,  96.58305298,  -0.44688955,
        -0.25498908,   0.85747904]), 'targetState': array([-40.90192076,  -2.83423324,  99.        ]), 'previousTarget': array([-40.90192076,  -2.83423324,  99.        ])}
episode index:19209
target thresh 86.08486785386785
target distance 31.05496156989777
model initialize at round 19209
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.87665487,  2.96508015, 82.98161485]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.86002667, -0.0698046 , -0.50545173],
       [ 0.08089958,  0.99672226,  0.        ],
       [ 0.50379499, -0.04089083,  0.86285488]]), 'currentState': array([-1.25159888e+01,  1.85934896e+01,  7.97301036e+01,  8.60026667e-01,
       -6.98046015e-02, -5.05451729e-01]), 'targetState': array([16.89011229, -1.92979452, 84.        ]), 'previousTarget': array([ 8.38525392,  3.90459993, 82.90454112])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6277947977463739
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.89011229, -1.92979452, 84.        ]), 'distance': 3.047880797119499, 'localFrame': array([[ 0.82132   ,  0.35168359,  0.44916824],
       [-0.39362549,  0.91927089,  0.        ],
       [-0.41290729, -0.17680407,  0.89344719]]), 'currentState': array([17.66375131, -1.49275143, 81.0845149 ,  0.82132   ,  0.35168359,
        0.44916824]), 'targetState': array([16.89011229, -1.92979452, 84.        ]), 'previousTarget': array([16.89011229, -1.92979452, 84.        ])}
episode index:19210
target thresh 86.08625929750912
target distance 59.40215680544602
model initialize at round 19210
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.3915616 , 24.25987759, 27.32501361]), 'distance': 27.499999999999996, 'localFrame': array([[-0.87594946,  0.27009382, -0.39970223],
       [-0.29465475, -0.95560378,  0.        ],
       [-0.38195696,  0.11777416,  0.91664504]]), 'currentState': array([20.58769921, 42.88327443, 28.56401144, -0.87594946,  0.27009382,
       -0.39970223]), 'targetState': array([-37.50704763, -10.68744021,  25.        ]), 'previousTarget': array([ 1.27716709, 23.44400774, 27.6116368 ])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6277942509421054
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-37.50704763, -10.68744021,  25.        ]), 'distance': 2.3958228280888982, 'localFrame': array([[-0.92795519, -0.30110552,  0.21962386],
       [ 0.30864111, -0.95117857,  0.        ],
       [ 0.2089015 ,  0.06778495,  0.97558463]]), 'currentState': array([-35.51675595, -10.39627637,  26.30151054,  -0.92795519,
        -0.30110552,   0.21962386]), 'targetState': array([-37.50704763, -10.68744021,  25.        ]), 'previousTarget': array([-37.50704763, -10.68744021,  25.        ])}
episode index:19211
target thresh 86.08765060201297
target distance 42.359074398976055
model initialize at round 19211
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.71620276, -2.91830609, 40.33859118]), 'distance': 27.499999999999996, 'localFrame': array([[-0.85683151, -0.48340646,  0.17932642],
       [ 0.49137178, -0.87094993,  0.        ],
       [ 0.15618433,  0.08811594,  0.98378963]]), 'currentState': array([40.07593226, -2.85259063, 25.82760502, -0.85683151, -0.48340646,
        0.17932642]), 'targetState': array([-0.446492  , -2.96658809, 51.        ]), 'previousTarget': array([18.22999755, -2.85276131, 39.97727948])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6278012542974772
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.446492  , -2.96658809, 51.        ]), 'distance': 3.416117870265685, 'localFrame': array([[-0.65697762, -0.17903747,  0.73234281],
       [ 0.26292843, -0.96481534,  0.        ],
       [ 0.70657558,  0.19255375,  0.68093612]]), 'currentState': array([ 0.56546625, -0.98600863, 48.40710455, -0.65697762, -0.17903747,
        0.73234281]), 'targetState': array([-0.446492  , -2.96658809, 51.        ]), 'previousTarget': array([-0.446492  , -2.96658809, 51.        ])}
episode index:19212
target thresh 86.08904176739335
target distance 35.71964627237317
model initialize at round 19212
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.05530215, -23.21354824,  56.4042267 ]), 'distance': 27.5, 'localFrame': array([[ 0.66059452,  0.73879891,  0.13338311],
       [-0.74545992,  0.66655045,  0.        ],
       [-0.08890657, -0.09943176,  0.99106455]]), 'currentState': array([-2.71916093, -0.66089411, 50.98730578,  0.66059452,  0.73879891,
        0.13338311]), 'targetState': array([ 21.86264855, -38.18408829,  60.        ]), 'previousTarget': array([ 12.2980118 , -24.52553007,  56.55855988])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277685784397612
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 70, 'trapConfig': [], 'currentTarget': array([ 21.86264855, -38.18408829,  60.        ]), 'distance': 10.032285688657863, 'localFrame': array([[-0.0837011 , -0.37045201, -0.92507267],
       [ 0.97541234, -0.22038774,  0.        ],
       [-0.20387468, -0.9023273 ,  0.37979016]]), 'currentState': array([ 19.6812675 , -34.28029125,  68.98046221,  -0.0837011 ,
        -0.37045201,  -0.92507267]), 'targetState': array([ 21.86264855, -38.18408829,  60.        ]), 'previousTarget': array([ 21.86264855, -38.18408829,  60.        ])}
episode index:19213
target thresh 86.09043279366414
target distance 44.0
model initialize at round 19213
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.91251795, -7.01691913, 76.11180641]), 'distance': 27.5, 'localFrame': array([[-0.29360776,  0.63289389, -0.71640757],
       [-0.90713804, -0.4208332 ,  0.        ],
       [-0.30148809,  0.64988056,  0.69768201]]), 'currentState': array([-19.53746677, -17.52459993,  98.16740234,  -0.29360776,
         0.63289389,  -0.71640757]), 'targetState': array([ 5.17219182,  3.04112344, 55.        ]), 'previousTarget': array([-6.37179457, -7.61859168, 76.71931168])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6277732605441725
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 5.17219182,  3.04112344, 55.        ]), 'distance': 2.8946286526121066, 'localFrame': array([[ 0.34620307,  0.16641474, -0.92328195],
       [-0.43323301,  0.90128195,  0.        ],
       [ 0.83213736,  0.39999622,  0.38412294]]), 'currentState': array([ 5.03102554,  2.89844252, 57.88766156,  0.34620307,  0.16641474,
       -0.92328195]), 'targetState': array([ 5.17219182,  3.04112344, 55.        ]), 'previousTarget': array([ 5.17219182,  3.04112344, 55.        ])}
episode index:19214
target thresh 86.09182368083925
target distance 20.0
model initialize at round 19214
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.76775074, 13.48347338, 31.        ]), 'distance': 24.11379912364604, 'localFrame': array([[ 0.74603861, -0.17111614, -0.64354149],
       [ 0.22356106,  0.97468993,  0.        ],
       [ 0.62725341, -0.14387082,  0.76541123]]), 'currentState': array([-10.35281739,   0.39234307,  50.15031364,   0.74603861,
        -0.17111614,  -0.64354149]), 'targetState': array([-3.76775074, 13.48347338, 31.        ]), 'previousTarget': array([-3.76775074, 13.48347338, 31.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6277862580858043
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.76775074, 13.48347338, 31.        ]), 'distance': 2.536652491224932, 'localFrame': array([[ 0.63482331, -0.35163931, -0.68800375],
       [ 0.48454714,  0.87476515,  0.        ],
       [ 0.6018417 , -0.33337025,  0.72570713]]), 'currentState': array([-4.05861009, 13.05521788, 33.48326477,  0.63482331, -0.35163931,
       -0.68800375]), 'targetState': array([-3.76775074, 13.48347338, 31.        ]), 'previousTarget': array([-3.76775074, 13.48347338, 31.        ])}
episode index:19215
target thresh 86.09321442893261
target distance 35.0
model initialize at round 19215
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.93684758, 35.28258683, 80.06334849]), 'distance': 27.500000000000004, 'localFrame': array([[-0.06327466,  0.56598077,  0.82198667],
       [-0.99380875, -0.11110433,  0.        ],
       [ 0.09132628, -0.81689754,  0.56950673]]), 'currentState': array([11.83822605, 26.94853704, 54.78164386, -0.06327466,  0.56598077,
        0.82198667]), 'targetState': array([ 2.77030712, 37.89888387, 88.        ]), 'previousTarget': array([ 5.13055825, 34.8377668 , 78.47985614])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6277957263211361
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.77030712, 37.89888387, 88.        ]), 'distance': 1.8567860348040357, 'localFrame': array([[-0.24068919,  0.47871279,  0.8443357 ],
       [-0.89343001, -0.44920242,  0.        ],
       [ 0.37927764, -0.75435485,  0.53581454]]), 'currentState': array([ 3.90537853, 37.88321912, 86.53063895, -0.24068919,  0.47871279,
        0.8443357 ]), 'targetState': array([ 2.77030712, 37.89888387, 88.        ]), 'previousTarget': array([ 2.77030712, 37.89888387, 88.        ])}
episode index:19216
target thresh 86.0946050379581
target distance 43.18245352424342
model initialize at round 19216
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.11281825, -20.13866571,  66.1771504 ]), 'distance': 27.5, 'localFrame': array([[-0.44382069, -0.72926271, -0.52076779],
       [ 0.85423931, -0.51987998,  0.        ],
       [-0.27073675, -0.44486032,  0.85369837]]), 'currentState': array([-5.91985065, -1.53359715, 84.76686944, -0.44382069, -0.72926271,
       -0.52076779]), 'targetState': array([ 12.12772888, -43.33495347,  43.        ]), 'previousTarget': array([  1.84195649, -18.83037326,  67.40104401])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277996630421997
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.12772888, -43.33495347,  43.        ]), 'distance': 2.9194817045218313, 'localFrame': array([[ 0.08734993, -0.47921198, -0.87334178],
       [ 0.98379015,  0.17932357,  0.        ],
       [ 0.15661076, -0.85918504,  0.48710793]]), 'currentState': array([ 11.06822471, -41.00813335,  44.40951498,   0.08734993,
        -0.47921198,  -0.87334178]), 'targetState': array([ 12.12772888, -43.33495347,  43.        ]), 'previousTarget': array([ 12.12772888, -43.33495347,  43.        ])}
episode index:19217
target thresh 86.09599550792964
target distance 14.0
model initialize at round 19217
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 50.]), 'distance': 13.412545038474327, 'localFrame': array([[-0.0115783 ,  0.52922657, -0.84840155],
       [-0.99976077, -0.02187253,  0.        ],
       [-0.01855669,  0.84819858,  0.52935321]]), 'currentState': array([ 5.34989495e+00, -1.57660834e+00,  6.21979217e+01, -1.15782955e-02,
        5.29226569e-01, -8.48401545e-01]), 'targetState': array([ 0., -0., 50.]), 'previousTarget': array([ 0.,  0., 50.])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278154953704786
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 50.]), 'distance': 2.11213915229561, 'localFrame': array([[ 0.24936683, -0.49965001, -0.82955775],
       [ 0.89475514,  0.44655709,  0.        ],
       [ 0.37044489, -0.74225106,  0.55842094]]), 'currentState': array([-0.29779602, -1.2570223 , 51.6710309 ,  0.24936683, -0.49965001,
       -0.82955775]), 'targetState': array([ 0., -0., 50.]), 'previousTarget': array([ 0.,  0., 50.])}
episode index:19218
target thresh 86.09738583886116
target distance 68.0
model initialize at round 19218
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.68602985,   5.45738251,  74.20301465]), 'distance': 27.5, 'localFrame': array([[-0.43052056,  0.60201497, -0.6724805 ],
       [-0.81340751, -0.58169427,  0.        ],
       [-0.39117805,  0.54700069,  0.74011484]]), 'currentState': array([-29.96447447,  -6.55686877,  98.56697734,  -0.43052056,
         0.60201497,  -0.6724805 ]), 'targetState': array([-18.2749505 ,  26.26834948,  32.        ]), 'previousTarget': array([-25.82806998,   4.45422015,  75.68592295])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277828289728841
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([-18.2749505 ,  26.26834948,  32.        ]), 'distance': 15.863288561884318, 'localFrame': array([[ 0.9306846 , -0.1882364 , -0.31367695],
       [ 0.1982417 ,  0.98015316,  0.        ],
       [ 0.30745146, -0.06218385,  0.94952976]]), 'currentState': array([-21.54777139,  31.07497562,  46.75902817,   0.9306846 ,
        -0.1882364 ,  -0.31367695]), 'targetState': array([-18.2749505 ,  26.26834948,  32.        ]), 'previousTarget': array([-18.2749505 ,  26.26834948,  32.        ])}
episode index:19219
target thresh 86.09877603076652
target distance 41.6162396239926
model initialize at round 19219
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.81012457, -9.30415959, 78.27043847]), 'distance': 27.5, 'localFrame': array([[-0.77582887, -0.51515045,  0.36429325],
       [ 0.55316132, -0.83307416,  0.        ],
       [ 0.30348329,  0.20151294,  0.93128429]]), 'currentState': array([15.87368393, 14.90306034, 71.35247941, -0.77582887, -0.51515045,
        0.36429325]), 'targetState': array([ -2.75363583, -25.85377129,  83.        ]), 'previousTarget': array([ 5.5203166 , -8.19859054, 77.48490897])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6277886519460586
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -2.75363583, -25.85377129,  83.        ]), 'distance': 4.190319389576663, 'localFrame': array([[-0.65226225, -0.61071357, -0.44897982],
       [ 0.68347503, -0.72997389,  0.        ],
       [-0.32774355, -0.3068665 ,  0.89354189]]), 'currentState': array([ -1.12699079, -23.16755825,  80.22564205,  -0.65226225,
        -0.61071357,  -0.44897982]), 'targetState': array([ -2.75363583, -25.85377129,  83.        ]), 'previousTarget': array([ -2.75363583, -25.85377129,  83.        ])}
episode index:19220
target thresh 86.10016608365963
target distance 55.0
model initialize at round 19220
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.11001469, -1.71736766, 48.4867655 ]), 'distance': 27.5, 'localFrame': array([[ 0.04598446, -0.72105052,  0.69135488],
       [ 0.9979726 ,  0.06364496,  0.        ],
       [-0.04400125,  0.68995323,  0.72251535]]), 'currentState': array([ 3.33518486, -4.31451669, 21.11060588,  0.04598446, -0.72105052,
        0.69135488]), 'targetState': array([ 2.89194212,  0.79791651, 75.        ]), 'previousTarget': array([ 2.6405849 , -1.20993681, 47.42624721])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6277940894736433
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.89194212,  0.79791651, 75.        ]), 'distance': 3.132719842342689, 'localFrame': array([[-0.75904948,  0.17212456,  0.62786704],
       [-0.22114866, -0.97524011,  0.        ],
       [ 0.61232112, -0.13885195,  0.77832061]]), 'currentState': array([ 2.63645915,  2.76419295, 72.57463013, -0.75904948,  0.17212456,
        0.62786704]), 'targetState': array([ 2.89194212,  0.79791651, 75.        ]), 'previousTarget': array([ 2.89194212,  0.79791651, 75.        ])}
episode index:19221
target thresh 86.10155599755441
target distance 72.0
model initialize at round 19221
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.22847567,  1.25769397, 29.97487704]), 'distance': 27.5, 'localFrame': array([[ 0.20174799, -0.77925306, -0.59334848],
       [ 0.96808152,  0.25063553,  0.        ],
       [ 0.14871421, -0.5744097 ,  0.8049457 ]]), 'currentState': array([19.27729814, 12.41670582,  5.33319902,  0.20174799, -0.77925306,
       -0.59334848]), 'targetState': array([ 34.07894473, -20.9433886 ,  79.        ]), 'previousTarget': array([24.04421983,  1.56401275, 31.41922135])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277614292879461
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 34.07894473, -20.9433886 ,  79.        ]), 'distance': 11.811644273649343, 'localFrame': array([[-0.78913213,  0.48797632,  0.37302761],
       [-0.52593843, -0.85052264,  0.        ],
       [ 0.31726843, -0.19618955,  0.92782024]]), 'currentState': array([ 30.58921192, -18.84600341,  67.91227342,  -0.78913213,
         0.48797632,   0.37302761]), 'targetState': array([ 34.07894473, -20.9433886 ,  79.        ]), 'previousTarget': array([ 34.07894473, -20.9433886 ,  79.        ])}
episode index:19222
target thresh 86.10294577246475
target distance 38.13696025834479
model initialize at round 19222
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.24715619,  2.88722577, 47.80166526]), 'distance': 27.5, 'localFrame': array([[ 0.6757459 , -0.23201094, -0.69967021],
       [ 0.32473338,  0.9458056 ,  0.        ],
       [ 0.661752  , -0.22720627,  0.71446595]]), 'currentState': array([ -2.45592012, -11.60926068,  58.64069252,   0.6757459 ,
        -0.23201094,  -0.69967021]), 'targetState': array([35.05877065, 14.65887447, 39.        ]), 'previousTarget': array([17.76645213,  3.22372248, 48.52196207])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277653665644227
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([35.05877065, 14.65887447, 39.        ]), 'distance': 1.7920466214639361, 'localFrame': array([[ 0.97364481, -0.18943637, -0.12700255],
       [ 0.19098288,  0.98159337,  0.        ],
       [ 0.12466486, -0.02425531,  0.99190239]]), 'currentState': array([33.46587142, 14.76352576, 39.81434099,  0.97364481, -0.18943637,
       -0.12700255]), 'targetState': array([35.05877065, 14.65887447, 39.        ]), 'previousTarget': array([35.05877065, 14.65887447, 39.        ])}
episode index:19223
target thresh 86.10433540840455
target distance 25.41345154934404
model initialize at round 19223
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.9479698 , -21.23059061,  52.46738745]), 'distance': 27.5, 'localFrame': array([[-0.8867686 , -0.45556956, -0.07808862],
       [ 0.45696494, -0.88948471,  0.        ],
       [-0.06945863, -0.03568376,  0.99694642]]), 'currentState': array([-0.5940591 , -2.96800487, 45.52807172, -0.8867686 , -0.45556956,
       -0.07808862]), 'targetState': array([-24.22246134, -25.26405285,  54.        ]), 'previousTarget': array([-18.70723739, -20.37880696,  52.04682117])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6277756871666528
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.22246134, -25.26405285,  54.        ]), 'distance': 4.185143611230655, 'localFrame': array([[-0.15738004, -0.7369624 , -0.65735679],
       [ 0.97794911, -0.20884332,  0.        ],
       [-0.13728457, -0.64286149,  0.7535795 ]]), 'currentState': array([-21.2270286 , -22.74193305,  55.47706514,  -0.15738004,
        -0.7369624 ,  -0.65735679]), 'targetState': array([-24.22246134, -25.26405285,  54.        ]), 'previousTarget': array([-24.22246134, -25.26405285,  54.        ])}
episode index:19224
target thresh 86.1057249053877
target distance 39.0
model initialize at round 19224
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.84783474, -5.42807051, 36.15861025]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55445938,  0.05426694, -0.83043958],
       [-0.09740818,  0.99524452,  0.        ],
       [ 0.82649044,  0.0808916 ,  0.5571087 ]]), 'currentState': array([-2.52018427e+01, -1.64215842e+01,  5.44405977e+01,  5.54459383e-01,
        5.42669422e-02, -8.30439578e-01]), 'targetState': array([10.33829929,  6.09258301, 17.        ]), 'previousTarget': array([-8.63423722, -6.220783  , 37.52143704])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6277761246074277
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.33829929,  6.09258301, 17.        ]), 'distance': 2.3346336613020844, 'localFrame': array([[ 0.66192836, -0.35991891,  0.65750226],
       [ 0.47769285,  0.87852691,  0.        ],
       [-0.57763343,  0.31408413,  0.75345257]]), 'currentState': array([ 8.63641942,  6.09932881, 15.40185303,  0.66192836, -0.35991891,
        0.65750226]), 'targetState': array([10.33829929,  6.09258301, 17.        ]), 'previousTarget': array([10.33829929,  6.09258301, 17.        ])}
episode index:19225
target thresh 86.1071142634281
target distance 23.392471575675614
model initialize at round 19225
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.61762782, 24.00309727, 18.        ]), 'distance': 26.69199677660374, 'localFrame': array([[-0.08075606, -0.98940464, -0.12065208],
       [ 0.99668557, -0.08135033,  0.        ],
       [-0.00981509, -0.12025219,  0.99269485]]), 'currentState': array([ 8.36100734, 45.72840669, 28.6659326 , -0.08075606, -0.98940464,
       -0.12065208]), 'targetState': array([19.61762782, 24.00309727, 18.        ]), 'previousTarget': array([19.19187728, 24.83137327, 18.38948582])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6277882062796442
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.61762782, 24.00309727, 18.        ]), 'distance': 2.0337547182078506, 'localFrame': array([[ 0.51035465, -0.40930362,  0.75631255],
       [ 0.62564518,  0.78010776,  0.        ],
       [-0.59000529,  0.4731833 ,  0.65421046]]), 'currentState': array([19.31565673, 25.5879599 , 19.23821734,  0.51035465, -0.40930362,
        0.75631255]), 'targetState': array([19.61762782, 24.00309727, 18.        ]), 'previousTarget': array([19.61762782, 24.00309727, 18.        ])}
episode index:19226
target thresh 86.10850348253966
target distance 81.0
model initialize at round 19226
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.82785071, 30.11275552, 31.20779932]), 'distance': 27.5, 'localFrame': array([[-0.58176463, -0.1970118 ,  0.78913641],
       [ 0.32075225, -0.94716313,  0.        ],
       [ 0.74744091,  0.25311728,  0.61421798]]), 'currentState': array([ 8.09748257, 35.62512955,  5.78729267, -0.58176463, -0.1970118 ,
        0.78913641]), 'targetState': array([-19.71470161,  18.44804978,  85.        ]), 'previousTarget': array([-0.50653238, 30.52299535, 29.45791586])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277555548932459
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-19.82546765,  18.33414848,  84.0861276 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.55342331, -0.50326959,  0.66365832],
       [ 0.67278806, -0.73983527,  0.        ],
       [ 0.49099783,  0.44650139,  0.74803585]]), 'currentState': array([-23.10935134,  14.95731379,  56.99252881,  -0.55342331,
        -0.50326959,   0.66365832]), 'targetState': array([-19.71470161,  18.44804978,  85.        ]), 'previousTarget': array([-19.97334046,  18.19683827,  82.47560873])}
episode index:19227
target thresh 86.10989256273623
target distance 71.51890741577688
model initialize at round 19227
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.77638563, -11.59692111,  90.66542046]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.77829526,  0.62777615,  0.01239347],
       [-0.62782437,  0.77835504,  0.        ],
       [-0.00964652, -0.00778092,  0.9999232 ]]), 'currentState': array([-2.68614346e+01, -1.98678502e+01,  9.33879111e+01,  7.78295257e-01,
        6.27776150e-01,  1.23934736e-02]), 'targetState': array([43.92449175,  2.57663044, 86.        ]), 'previousTarget': array([ -1.64634497, -12.78388228,  90.46030104])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6277559933127992
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([43.92449175,  2.57663044, 86.        ]), 'distance': 3.1891012437689867, 'localFrame': array([[ 0.94623524,  0.10319402,  0.30657767],
       [-0.10841466,  0.99410576,  0.        ],
       [-0.30477062, -0.03323751,  0.95184565]]), 'currentState': array([42.82381602, -0.15353558, 87.22681424,  0.94623524,  0.10319402,
        0.30657767]), 'targetState': array([43.92449175,  2.57663044, 86.        ]), 'previousTarget': array([43.92449175,  2.57663044, 86.        ])}
episode index:19228
target thresh 86.11128150403174
target distance 31.0
model initialize at round 19228
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.8179824 , -33.61817368,  69.87071768]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.21389425, -0.82131474, -0.52885852],
       [ 0.9677213 ,  0.25202278,  0.        ],
       [ 0.1332844 , -0.51178766,  0.84871   ]]), 'currentState': array([ 19.37865777, -18.44030482,  92.65942626,   0.21389425,
        -0.82131474,  -0.52885852]), 'targetState': array([ 15.93358156, -38.86027507,  62.        ]), 'previousTarget': array([ 16.60882228, -32.79263555,  70.58712312])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6277637974298145
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 15.93358156, -38.86027507,  62.        ]), 'distance': 3.9736585421455963, 'localFrame': array([[ 0.07627762, -0.72241202, -0.68724275],
       [ 0.99447183,  0.10500371,  0.        ],
       [ 0.07216304, -0.68344355,  0.72642784]]), 'currentState': array([ 18.87307003, -36.31247673,  61.18877045,   0.07627762,
        -0.72241202,  -0.68724275]), 'targetState': array([ 15.93358156, -38.86027507,  62.        ]), 'previousTarget': array([ 15.93358156, -38.86027507,  62.        ])}
episode index:19229
target thresh 86.11267030644005
target distance 41.0
model initialize at round 19229
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.03473939, -2.82040051, 39.23234565]), 'distance': 27.499999999999996, 'localFrame': array([[-0.70313093,  0.12361695,  0.70023264],
       [-0.17315366, -0.98489482,  0.        ],
       [ 0.6896555 , -0.12124784,  0.71391474]]), 'currentState': array([ 24.81124253, -17.67840164,  19.31241151,  -0.70313093,
         0.12361695,   0.70023264]), 'targetState': array([ 1.34826283, 11.92401725, 59.        ]), 'previousTarget': array([13.82066126, -2.77609583, 38.17872266])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.627766291722671
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([ 1.34826283, 11.92401725, 59.        ]), 'distance': 2.722370050884916, 'localFrame': array([[-0.1922932 ,  0.43768998,  0.87832272],
       [-0.91553881, -0.40222964,  0.        ],
       [ 0.35328743, -0.80413854,  0.47806819]]), 'currentState': array([ 1.24871535,  9.99159768, 57.08502125, -0.1922932 ,  0.43768998,
        0.87832272]), 'targetState': array([ 1.34826283, 11.92401725, 59.        ]), 'previousTarget': array([ 1.34826283, 11.92401725, 59.        ])}
episode index:19230
target thresh 86.11405896997508
target distance 18.36868750943401
model initialize at round 19230
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.36590983,   4.59967339,  45.        ]), 'distance': 25.612557303642348, 'localFrame': array([[ 0.06507423, -0.64720358, -0.75953464],
       [ 0.99498318,  0.10004235,  0.        ],
       [ 0.07598563, -0.75572419,  0.65046685]]), 'currentState': array([-0.43295493, 21.49751034, 34.20152113,  0.06507423, -0.64720358,
       -0.75953464]), 'targetState': array([-16.36590983,   4.59967339,  45.        ]), 'previousTarget': array([-16.36590983,   4.59967339,  45.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.627778822507396
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.36590983,   4.59967339,  45.        ]), 'distance': 3.075794730479577, 'localFrame': array([[-0.2268988 , -0.65612652,  0.71973254],
       [ 0.94508484, -0.3268251 ,  0.        ],
       [ 0.23522666,  0.68020831,  0.69425145]]), 'currentState': array([-16.55861568,   5.65509606,  42.11738649,  -0.2268988 ,
        -0.65612652,   0.71973254]), 'targetState': array([-16.36590983,   4.59967339,  45.        ]), 'previousTarget': array([-16.36590983,   4.59967339,  45.        ])}
episode index:19231
target thresh 86.11544749465068
target distance 40.821702450259714
model initialize at round 19231
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.51724759, -15.14714073,  60.96180192]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.43678494,  0.61754514, -0.65410773],
       [-0.81642516,  0.57745126,  0.        ],
       [ 0.37771533,  0.53403001,  0.7564014 ]]), 'currentState': array([-27.80125611, -37.6621157 ,  64.92727616,   0.43678494,
         0.61754514,  -0.65410773]), 'targetState': array([-1.1016632 ,  1.66923282, 58.        ]), 'previousTarget': array([-12.9271038 , -16.4332663 ,  61.54762257])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6277854229891294
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.1016632 ,  1.66923282, 58.        ]), 'distance': 2.288548259267024, 'localFrame': array([[ 0.81390248,  0.49582895,  0.30284716],
       [-0.52026085,  0.85400741,  0.        ],
       [-0.25863372, -0.15755952,  0.95303914]]), 'currentState': array([-0.76053518, -0.14464179, 59.35312369,  0.81390248,  0.49582895,
        0.30284716]), 'targetState': array([-1.1016632 ,  1.66923282, 58.        ]), 'previousTarget': array([-1.1016632 ,  1.66923282, 58.        ])}
episode index:19232
target thresh 86.11683588048078
target distance 42.0
model initialize at round 19232
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.45381878, -5.85437753, 55.91331812]), 'distance': 27.5, 'localFrame': array([[-0.59817977,  0.70761843, -0.37610785],
       [-0.76369182, -0.64558098,  0.        ],
       [-0.24280807,  0.28723049,  0.92657589]]), 'currentState': array([11.16768664,  6.14296054, 80.20515479, -0.59817977,  0.70761843,
       -0.37610785]), 'targetState': array([  2.97771216, -14.70147034,  38.        ]), 'previousTarget': array([ 6.8022965 , -6.66746113, 55.48966077])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6277848772977632
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([  2.97771216, -14.70147034,  38.        ]), 'distance': 2.664557551925789, 'localFrame': array([[ 0.8264036 ,  0.56206744, -0.03372353],
       [-0.56238732,  0.82687393,  0.        ],
       [ 0.02788511,  0.01896569,  0.9994312 ]]), 'currentState': array([ 3.42356615e+00, -1.35085624e+01,  4.03405238e+01,  8.26403605e-01,
        5.62067438e-01, -3.37235346e-02]), 'targetState': array([  2.97771216, -14.70147034,  38.        ]), 'previousTarget': array([  2.97771216, -14.70147034,  38.        ])}
episode index:19233
target thresh 86.11822412747921
target distance 84.0
model initialize at round 19233
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.73531759, -23.81536901,  37.86219524]), 'distance': 27.500000000000004, 'localFrame': array([[-0.73516938,  0.01802194,  0.67764385],
       [-0.02450663, -0.99969967,  0.        ],
       [ 0.67744033, -0.01660677,  0.73539024]]), 'currentState': array([ 2.48970003e+01, -3.19620884e+01,  1.16857155e+01, -7.35169381e-01,
        1.80219350e-02,  6.77643853e-01]), 'targetState': array([18.01681445, -6.0327769 , 95.        ]), 'previousTarget': array([ 23.83119087, -23.42339901,  37.21203155])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6277804008601355
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([18.01681445, -6.0327769 , 95.        ]), 'distance': 2.542542246856283, 'localFrame': array([[-0.8310898 , -0.07428494,  0.55115469],
       [ 0.08902764, -0.99602916,  0.        ],
       [ 0.54896614,  0.049068  ,  0.83440309]]), 'currentState': array([ 1.73276322e+01, -4.71118116e+00,  9.29401618e+01, -8.31089800e-01,
       -7.42849390e-02,  5.51154689e-01]), 'targetState': array([18.01681445, -6.0327769 , 95.        ]), 'previousTarget': array([18.01681445, -6.0327769 , 95.        ])}
episode index:19234
target thresh 86.1196122356599
target distance 48.951992637737355
model initialize at round 19234
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.67970971,  1.37083842, 48.87955692]), 'distance': 27.5, 'localFrame': array([[-0.4546082 , -0.68388725, -0.57063965],
       [ 0.83278954, -0.55358973,  0.        ],
       [-0.31590025, -0.47522273,  0.82120058]]), 'currentState': array([39.44405484, 20.26794961, 62.33924866, -0.4546082 , -0.68388725,
       -0.57063965]), 'targetState': array([  1.77628885, -27.9436003 ,  28.        ]), 'previousTarget': array([25.20875757,  2.22743878, 50.18821642])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6277821945508101
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.77628885, -27.9436003 ,  28.        ]), 'distance': 2.8414144364876828, 'localFrame': array([[-0.58136625, -0.45072443,  0.67739263],
       [ 0.61271229, -0.79030605,  0.        ],
       [ 0.53534749,  0.41504678,  0.73562166]]), 'currentState': array([  3.83378114, -29.19158641,  29.51092426,  -0.58136625,
        -0.45072443,   0.67739263]), 'targetState': array([  1.77628885, -27.9436003 ,  28.        ]), 'previousTarget': array([  1.77628885, -27.9436003 ,  28.        ])}
episode index:19235
target thresh 86.12100020503671
target distance 49.0
model initialize at round 19235
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.3438875 , -19.41858824,  58.02765833]), 'distance': 27.5, 'localFrame': array([[-0.52853398,  0.2386758 , -0.81466907],
       [-0.41156242, -0.91138157,  0.        ],
       [-0.74247438,  0.33528718,  0.57992612]]), 'currentState': array([ 29.96178822, -36.1833599 ,  75.04956219,  -0.52853398,
         0.2386758 ,  -0.81466907]), 'targetState': array([-8.4789275 , 11.14036752, 27.        ]), 'previousTarget': array([ 17.6131818 , -19.70887577,  58.94444555])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6277810105142425
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.4789275 , 11.14036752, 27.        ]), 'distance': 3.227742752603077, 'localFrame': array([[-0.14956551,  0.5882648 , -0.79471673],
       [-0.96916591, -0.24640909,  0.        ],
       [-0.19582543,  0.77021236,  0.60698049]]), 'currentState': array([-6.64552367,  9.25444076, 28.87089121, -0.14956551,  0.5882648 ,
       -0.79471673]), 'targetState': array([-8.4789275 , 11.14036752, 27.        ]), 'previousTarget': array([-8.4789275 , 11.14036752, 27.        ])}
episode index:19236
target thresh 86.12238803562352
target distance 42.48577452659852
model initialize at round 19236
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.55848472,  7.53153592, 45.01661385]), 'distance': 27.5, 'localFrame': array([[-0.02425845,  0.70907741,  0.70471324],
       [-0.99941531, -0.03419129,  0.        ],
       [ 0.02409505, -0.7043012 ,  0.70949225]]), 'currentState': array([-8.79246525e-01, -1.70755051e+01,  3.63324699e+01, -2.42584522e-02,
        7.09077410e-01,  7.04713242e-01]), 'targetState': array([-15.53849084,  24.4858184 ,  51.        ]), 'previousTarget': array([-8.92455538,  6.35538711, 44.17214047])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6277876091666671
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.53849084,  24.4858184 ,  51.        ]), 'distance': 3.7025575234923913, 'localFrame': array([[-0.45425517,  0.13340701, -0.88082621],
       [-0.28178244, -0.95947832,  0.        ],
       [-0.84513365,  0.24820136,  0.47343975]]), 'currentState': array([-16.88234351,  21.88370749,  53.26539424,  -0.45425517,
         0.13340701,  -0.88082621]), 'targetState': array([-15.53849084,  24.4858184 ,  51.        ]), 'previousTarget': array([-15.53849084,  24.4858184 ,  51.        ])}
episode index:19237
target thresh 86.12377572743422
target distance 49.7213544859605
model initialize at round 19237
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.91277223, 14.73799517, 62.190024  ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.80104858,  0.44771023,  0.39733704],
       [-0.4878757 ,  0.87291311,  0.        ],
       [-0.34684071, -0.19385109,  0.91767275]]), 'currentState': array([-22.16787653,   1.48074795,  61.40405241,   0.80104858,
         0.44771023,   0.39733704]), 'targetState': array([26.72886944, 28.40013272, 63.        ]), 'previousTarget': array([ 0.97474816, 13.99425302, 61.44609294])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277915420123804
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.72886944, 28.40013272, 63.        ]), 'distance': 3.5238764082123692, 'localFrame': array([[ 0.6993511 , -0.48974838, -0.520629  ],
       [ 0.5736216 ,  0.81912042,  0.        ],
       [ 0.42645785, -0.29864404,  0.85378302]]), 'currentState': array([24.52442473, 29.04239651, 65.67313032,  0.6993511 , -0.48974838,
       -0.520629  ]), 'targetState': array([26.72886944, 28.40013272, 63.        ]), 'previousTarget': array([26.72886944, 28.40013272, 63.        ])}
episode index:19238
target thresh 86.12516328048267
target distance 53.484943427914594
model initialize at round 19238
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.0219926 , 14.28646944, 48.04276902]), 'distance': 27.500000000000004, 'localFrame': array([[-0.21468715,  0.1258643 ,  0.9685389 ],
       [-0.50575902, -0.8626748 ,  0.        ],
       [ 0.83553411, -0.48984728,  0.2488622 ]]), 'currentState': array([18.69367635, 29.43877602, 40.62068526, -0.21468715,  0.1258643 ,
        0.9685389 ]), 'targetState': array([-35.08086018,  -8.08289857,  59.        ]), 'previousTarget': array([-3.26945782, 13.7281296 , 47.10453921])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6277919783107354
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-35.08086018,  -8.08289857,  59.        ]), 'distance': 3.917562148470498, 'localFrame': array([[-0.73859259, -0.24416084,  0.62838401],
       [ 0.31387043, -0.94946583,  0.        ],
       [ 0.59662915,  0.19723116,  0.77790329]]), 'currentState': array([-32.3640074 ,  -5.28671908,  58.61609322,  -0.73859259,
        -0.24416084,   0.62838401]), 'targetState': array([-35.08086018,  -8.08289857,  59.        ]), 'previousTarget': array([-35.08086018,  -8.08289857,  59.        ])}
episode index:19239
target thresh 86.12655069478275
target distance 57.0
model initialize at round 19239
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-20.24760479,  -7.1072722 ,  74.07552049]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.64097506,  0.44204994,  0.6274893 ],
       [-0.56773137,  0.82321388,  0.        ],
       [-0.5165579 , -0.35624536,  0.77862519]]), 'currentState': array([-30.54343987, -12.49393177,  99.        ,   0.64097506,
         0.44204994,   0.6274893 ]), 'targetState': array([-6.99780878, -0.17513509, 42.        ]), 'previousTarget': array([-20.24760479,  -7.1072722 ,  74.07552049])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6277934267121004
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-6.99780878, -0.17513509, 42.        ]), 'distance': 3.0236239006848282, 'localFrame': array([[ 0.50513577,  0.74067942, -0.44298064],
       [-0.82616136,  0.56343358,  0.        ],
       [ 0.24959017,  0.36597349,  0.89653118]]), 'currentState': array([-7.97157899, -0.1234722 , 44.8620629 ,  0.50513577,  0.74067942,
       -0.44298064]), 'targetState': array([-6.99780878, -0.17513509, 42.        ]), 'previousTarget': array([-6.99780878, -0.17513509, 42.        ])}
episode index:19240
target thresh 86.12793797034833
target distance 59.0
model initialize at round 19240
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.06419952, -22.53794398,  55.86595775]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23366471,  0.51729485,  0.82329025],
       [-0.91133909, -0.4116565 ,  0.        ],
       [ 0.33891278, -0.75029658,  0.56762062]]), 'currentState': array([ -0.36381823, -26.78156423,  32.84262196,  -0.23366471,
         0.51729485,   0.82329025]), 'targetState': array([ 35.45495592, -16.2464181 ,  90.        ]), 'previousTarget': array([ 13.70269766, -22.76614133,  54.2187674 ])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6277962727787444
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 35.45495592, -16.2464181 ,  90.        ]), 'distance': 2.0896832805096524, 'localFrame': array([[ 0.65633229,  0.74883539, -0.0920515 ],
       [-0.75202832,  0.6591308 ,  0.        ],
       [ 0.06067398,  0.06922533,  0.99575425]]), 'currentState': array([ 33.6908112 , -17.25449354,  89.51178535,   0.65633229,
         0.74883539,  -0.0920515 ]), 'targetState': array([ 35.45495592, -16.2464181 ,  90.        ]), 'previousTarget': array([ 35.45495592, -16.2464181 ,  90.        ])}
episode index:19241
target thresh 86.1293251071933
target distance 32.0
model initialize at round 19241
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.53139173, -37.71876685,   8.14079384]), 'distance': 27.500000000000004, 'localFrame': array([[-0.25068029, -0.6987398 , -0.67001648],
       [ 0.94125865, -0.33768648,  0.        ],
       [-0.22625551, -0.63065881,  0.74234622]]), 'currentState': array([  6.16337884, -22.82595449,  31.13746001,  -0.25068029,
        -0.6987398 ,  -0.67001648]), 'targetState': array([  9.36966561, -42.9908056 ,   0.        ]), 'previousTarget': array([  8.8055662 , -36.95383228,   9.1082801 ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6278061527634039
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.36966561, -42.9908056 ,   0.        ]), 'distance': 3.257311207292947, 'localFrame': array([[ 0.59258961, -0.52287881, -0.61272775],
       [ 0.66162564,  0.74983433,  0.        ],
       [ 0.4594443 , -0.40539639,  0.79029406]]), 'currentState': array([  8.18431057, -41.60339283,   2.69816522,   0.59258961,
        -0.52287881,  -0.61272775]), 'targetState': array([  9.36966561, -42.9908056 ,   0.        ]), 'previousTarget': array([  9.36966561, -42.9908056 ,   0.        ])}
episode index:19242
target thresh 86.13071210533153
target distance 27.354252227052157
model initialize at round 19242
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.6603239 ,  10.89845203,  51.53809976]), 'distance': 27.500000000000004, 'localFrame': array([[-0.61830523,  0.64984023,  0.44204786],
       [-0.72446646, -0.68931005,  0.        ],
       [ 0.30470804, -0.32024885,  0.89699146]]), 'currentState': array([-10.18953439,  -5.92072441,  63.03571859,  -0.61830523,
         0.64984023,   0.44204786]), 'targetState': array([-35.95072783,  17.53696578,  47.        ]), 'previousTarget': array([-27.40479947,   9.91400978,  51.99866904])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.627816031721198
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-35.95072783,  17.53696578,  47.        ]), 'distance': 3.816592310262166, 'localFrame': array([[-0.16578746,  0.96493223, -0.20351982],
       [-0.98555916, -0.16933143,  0.        ],
       [-0.0344623 ,  0.20058082,  0.97907083]]), 'currentState': array([-33.27087288,  15.86357614,  49.14114956,  -0.16578746,
         0.96493223,  -0.20351982]), 'targetState': array([-35.95072783,  17.53696578,  47.        ]), 'previousTarget': array([-35.95072783,  17.53696578,  47.        ])}
episode index:19243
target thresh 86.13209896477684
target distance 41.0
model initialize at round 19243
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.62200798, -15.41055737,  41.01742173]), 'distance': 27.5, 'localFrame': array([[-0.60413135, -0.39390693, -0.69272118],
       [ 0.5461785 , -0.83766882,  0.        ],
       [-0.58027093, -0.37834942,  0.72120549]]), 'currentState': array([  0.40374701, -16.75959364,  62.57093115,  -0.60413135,
        -0.39390693,  -0.69272118]), 'targetState': array([-30.85450265, -14.28284518,  23.        ]), 'previousTarget': array([-15.48750095, -15.53870754,  42.44978947])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6278222339693071
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.85450265, -14.28284518,  23.        ]), 'distance': 2.3875204858461134, 'localFrame': array([[-0.61352195, -0.78802187, -0.05111111],
       [ 0.78905319, -0.61432489,  0.        ],
       [-0.03139883, -0.04032938,  0.99869297]]), 'currentState': array([-30.38641962, -13.12952015,  25.03739877,  -0.61352195,
        -0.78802187,  -0.05111111]), 'targetState': array([-30.85450265, -14.28284518,  23.        ]), 'previousTarget': array([-30.85450265, -14.28284518,  23.        ])}
episode index:19244
target thresh 86.13348568554319
target distance 26.50636131795116
model initialize at round 19244
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.86248367, -43.73049568,  36.        ]), 'distance': 26.254487880922508, 'localFrame': array([[ 0.87107013, -0.11917398,  0.47648125],
       [ 0.13555057,  0.99077043,  0.        ],
       [-0.47208353,  0.06458731,  0.87918463]]), 'currentState': array([-29.66258436, -35.68423664,  39.0839674 ,   0.87107013,
        -0.11917398,   0.47648125]), 'targetState': array([ -4.86248367, -43.73049568,  36.        ]), 'previousTarget': array([ -5.39883619, -43.55383401,  36.06070458])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6278347527315207
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.86248367, -43.73049568,  36.        ]), 'distance': 2.8986268576705245, 'localFrame': array([[ 0.81736819,  0.39601161, -0.41843045],
       [-0.43601664,  0.8999386 ,  0.        ],
       [ 0.37656172,  0.18244264,  0.90824884]]), 'currentState': array([ -7.30650961, -45.02078899,  35.12597595,   0.81736819,
         0.39601161,  -0.41843045]), 'targetState': array([ -4.86248367, -43.73049568,  36.        ]), 'previousTarget': array([ -4.86248367, -43.73049568,  36.        ])}
episode index:19245
target thresh 86.13487226764437
target distance 48.0
model initialize at round 19245
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-1.26467214, 23.13078935, 59.44723302]), 'distance': 27.499999999999996, 'localFrame': array([[-0.52261579,  0.76980393, -0.36643505],
       [-0.82735149, -0.56168453,  0.        ],
       [-0.2058209 ,  0.30317059,  0.93044363]]), 'currentState': array([-5.85936022,  5.44682455, 80.        , -0.52261579,  0.76980393,
       -0.36643505]), 'targetState': array([ 4.87131308, 46.74687486, 32.        ]), 'previousTarget': array([-1.26467214, 23.13078935, 59.44723302])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6278335665793023
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([ 4.87131308, 46.74687486, 32.        ]), 'distance': 3.0705851960792283, 'localFrame': array([[ 0.89561219,  0.39769652, -0.19928943],
       [-0.40583734,  0.91394532,  0.        ],
       [ 0.18213965,  0.0808791 ,  0.97994067]]), 'currentState': array([ 1.97595623, 46.50656709, 32.993808  ,  0.89561219,  0.39769652,
       -0.19928943]), 'targetState': array([ 4.87131308, 46.74687486, 32.        ]), 'previousTarget': array([ 4.87131308, 46.74687486, 32.        ])}
episode index:19246
target thresh 86.13625871109427
target distance 81.0
model initialize at round 19246
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.4742009 , -0.97839323, 30.75272509]), 'distance': 27.5, 'localFrame': array([[-0.75935457,  0.31851375,  0.56738842],
       [-0.38680386, -0.92216201,  0.        ],
       [ 0.52322405, -0.21946803,  0.82345029]]), 'currentState': array([17.21923925, -7.93069012,  9.30489687, -0.75935457,  0.31851375,
        0.56738842]), 'targetState': array([-41.28563322,  17.90241574,  89.        ]), 'previousTarget': array([ 2.65593208, -0.87867561, 29.44829442])}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.6278221874868642
{'scaleFactor': 20, 'timeStep': 90, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-41.28563322,  17.90241574,  89.        ]), 'distance': 2.4186430524820652, 'localFrame': array([[-0.08417008, -0.46529728,  0.88114349],
       [ 0.98402935, -0.17800626,  0.        ],
       [ 0.15684906,  0.86707105,  0.47284898]]), 'currentState': array([-4.03008654e+01,  1.86554292e+01,  8.69232147e+01, -8.41700799e-02,
       -4.65297277e-01,  8.81143485e-01]), 'targetState': array([-41.28563322,  17.90241574,  89.        ]), 'previousTarget': array([-41.28563322,  17.90241574,  89.        ])}
episode index:19247
target thresh 86.13764501590677
target distance 11.786837029581122
model initialize at round 19247
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.77954023, -6.32503835, 23.        ]), 'distance': 14.038895379080998, 'localFrame': array([[ 0.26241189,  0.56767719, -0.78030931],
       [-0.90771159,  0.41959466,  0.        ],
       [ 0.32741361,  0.7082958 ,  0.62539379]]), 'currentState': array([ 4.66160109,  0.41829877, 28.29239239,  0.26241189,  0.56767719,
       -0.78030931]), 'targetState': array([15.77954023, -6.32503835, 23.        ]), 'previousTarget': array([15.77954023, -6.32503835, 23.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6278360858693857
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.77954023, -6.32503835, 23.        ]), 'distance': 2.4590130268445356, 'localFrame': array([[ 0.45235461, -0.73431679,  0.50611674],
       [ 0.85141635,  0.52449041,  0.        ],
       [-0.26545338,  0.43091607,  0.86246498]]), 'currentState': array([15.29508036, -7.14672777, 20.73353354,  0.45235461, -0.73431679,
        0.50611674]), 'targetState': array([15.77954023, -6.32503835, 23.        ]), 'previousTarget': array([15.77954023, -6.32503835, 23.        ])}
episode index:19248
target thresh 86.13903118209572
target distance 19.0
model initialize at round 19248
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.70901164,  8.82185974, 65.        ]), 'distance': 20.106451367353245, 'localFrame': array([[-0.84639618,  0.27456484,  0.45631968],
       [-0.30856364, -0.9512037 ,  0.        ],
       [ 0.43405297, -0.14080366,  0.88981591]]), 'currentState': array([-0.24015233,  0.46583172, 47.26654352, -0.84639618,  0.27456484,
        0.45631968]), 'targetState': array([-4.70901164,  8.82185974, 65.        ]), 'previousTarget': array([-4.70901164,  8.82185974, 65.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6278504526411213
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.70901164,  8.82185974, 65.        ]), 'distance': 2.4520847719568097, 'localFrame': array([[-0.65186676,  0.10865163,  0.75050953],
       [-0.16440955, -0.98639216,  0.        ],
       [ 0.74029672, -0.12339093,  0.66085962]]), 'currentState': array([-3.67508162,  8.64025397, 62.78398381, -0.65186676,  0.10865163,
        0.75050953]), 'targetState': array([-4.70901164,  8.82185974, 65.        ]), 'previousTarget': array([-4.70901164,  8.82185974, 65.        ])}
episode index:19249
target thresh 86.14041720967496
target distance 18.0
model initialize at round 19249
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.08172759,   9.52391453,  38.        ]), 'distance': 22.72428190378785, 'localFrame': array([[ 0.5995346 , -0.1683276 , -0.7824475 ],
       [ 0.27031174,  0.96277285,  0.        ],
       [ 0.75331921, -0.21150475,  0.62271656]]), 'currentState': array([-16.330545  ,  24.89717862,  54.58308052,   0.5995346 ,
        -0.1683276 ,  -0.7824475 ]), 'targetState': array([-14.08172759,   9.52391453,  38.        ]), 'previousTarget': array([-14.08172759,   9.52391453,  38.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6278643481113352
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.08172759,   9.52391453,  38.        ]), 'distance': 4.253828190930819, 'localFrame': array([[ 0.57909887, -0.26058742, -0.77248864],
       [ 0.41035541,  0.91192568,  0.        ],
       [ 0.70445222, -0.31699489,  0.63502859]]), 'currentState': array([-16.07665072,  12.36138527,  40.46253849,   0.57909887,
        -0.26058742,  -0.77248864]), 'targetState': array([-14.08172759,   9.52391453,  38.        ]), 'previousTarget': array([-14.08172759,   9.52391453,  38.        ])}
episode index:19250
target thresh 86.1418030986584
target distance 59.0
model initialize at round 19250
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.5778068 , 13.45400038, 29.2434136 ]), 'distance': 27.5, 'localFrame': array([[ 0.21120772, -0.95258883, -0.21901101],
       [ 0.97629084,  0.21646292,  0.        ],
       [ 0.04740776, -0.21381844,  0.97572239]]), 'currentState': array([42.31388811, 20.68538805,  7.88146664,  0.21120772, -0.95258883,
       -0.21901101]), 'targetState': array([-1.97187531,  0.3342271 , 68.        ]), 'previousTarget': array([25.89608139, 14.17895314, 30.22555903])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.627864449886988
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-1.97187531,  0.3342271 , 68.        ]), 'distance': 2.197183322663072, 'localFrame': array([[ 0.35101589, -0.40368533,  0.84488224],
       [ 0.75461925,  0.65616292,  0.        ],
       [-0.5543804 ,  0.63756441,  0.53495233]]), 'currentState': array([-1.25250338,  1.4949951 , 66.27873993,  0.35101589, -0.40368533,
        0.84488224]), 'targetState': array([-1.97187531,  0.3342271 , 68.        ]), 'previousTarget': array([-1.97187531,  0.3342271 , 68.        ])}
episode index:19251
target thresh 86.14318884905985
target distance 36.0
model initialize at round 19251
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.35257452, -8.01778802, 78.46582445]), 'distance': 27.5, 'localFrame': array([[ 0.80294887,  0.17298701,  0.57039337],
       [-0.21060747,  0.97757071,  0.        ],
       [-0.55759986, -0.12012911,  0.82137166]]), 'currentState': array([-5.9391206 ,  2.55743401, 57.48578323,  0.80294887,  0.17298701,
        0.57039337]), 'targetState': array([ 17.57211363, -14.83983904,  92.        ]), 'previousTarget': array([ 7.62138953, -8.00336595, 77.26931519])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278672906383449
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 17.57211363, -14.83983904,  92.        ]), 'distance': 2.0912494122863765, 'localFrame': array([[ 0.00591835,  0.1529157 ,  0.98822151],
       [-0.99925187,  0.03867441,  0.        ],
       [-0.03821888, -0.98748219,  0.15303019]]), 'currentState': array([ 1.82998555e+01, -1.32489231e+01,  9.08542678e+01,  5.91835194e-03,
        1.52915705e-01,  9.88221514e-01]), 'targetState': array([ 17.57211363, -14.83983904,  92.        ]), 'previousTarget': array([ 17.57211363, -14.83983904,  92.        ])}
episode index:19252
target thresh 86.14457446089321
target distance 48.393154946072926
model initialize at round 19252
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.02688821,  11.59699281,  21.52683874]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.72069755,  0.17504493,  0.67078634],
       [-0.23602074,  0.97174802,  0.        ],
       [-0.6518353 , -0.15831949,  0.74165065]]), 'currentState': array([-39.64743613,  13.45232123,   3.42709399,   0.72069755,
         0.17504493,   0.67078634]), 'targetState': array([ 7.71544727,  9.19085814, 45.        ]), 'previousTarget': array([-20.17564255,  11.92800412,  20.21722104])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6278598696691263
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 7.71544727,  9.19085814, 45.        ]), 'distance': 2.991997557237836, 'localFrame': array([[ 0.60662348,  0.7053173 ,  0.36679076],
       [-0.7581583 ,  0.65207054,  0.        ],
       [-0.23917345, -0.27808546,  0.93030347]]), 'currentState': array([ 4.88438276,  9.79528603, 44.24383202,  0.60662348,  0.7053173 ,
        0.36679076]), 'targetState': array([ 7.71544727,  9.19085814, 45.        ]), 'previousTarget': array([ 7.71544727,  9.19085814, 45.        ])}
episode index:19253
target thresh 86.1459599341723
target distance 15.0
model initialize at round 19253
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.9434567 , -1.54000696,  5.        ]), 'distance': 14.592252236164597, 'localFrame': array([[ 0.62609737, -0.35251459, -0.69551099],
       [ 0.49061519,  0.87137635,  0.        ],
       [ 0.60605183, -0.34122825,  0.71851546]]), 'currentState': array([16.62087016, -0.86744935, 18.92108965,  0.62609737, -0.35251459,
       -0.69551099]), 'targetState': array([20.9434567 , -1.54000696,  5.        ]), 'previousTarget': array([20.9434567 , -1.54000696,  5.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6278737617634751
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([20.9434567 , -1.54000696,  5.        ]), 'distance': 3.869292738850477, 'localFrame': array([[ 0.4110193 ,  0.2530605 , -0.87579879],
       [-0.52428608,  0.8515422 ,  0.        ],
       [ 0.74577962,  0.45916911,  0.48267637]]), 'currentState': array([18.45624015, -0.03733401,  7.55482957,  0.4110193 ,  0.2530605 ,
       -0.87579879]), 'targetState': array([20.9434567 , -1.54000696,  5.        ]), 'previousTarget': array([20.9434567 , -1.54000696,  5.        ])}
episode index:19254
target thresh 86.147345268911
target distance 41.0
model initialize at round 19254
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.48699865, 13.64672399, 63.20411127]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.70754397, -0.65597386,  0.26283042],
       [ 0.67987694,  0.73332622,  0.        ],
       [-0.19274044,  0.17869234,  0.96484204]]), 'currentState': array([-16.01841338,  25.58804574,  85.6260679 ,   0.70754397,
        -0.65597386,   0.26283042]), 'targetState': array([ 3.06332765,  3.95171149, 45.        ]), 'previousTarget': array([-6.25153023, 14.65132857, 63.97494901])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278766015886242
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 3.06332765,  3.95171149, 45.        ]), 'distance': 2.0221932570364354, 'localFrame': array([[-0.38567171, -0.42676124, -0.818005  ],
       [ 0.74192106, -0.67048724,  0.        ],
       [-0.54846191, -0.60689514,  0.57521111]]), 'currentState': array([ 2.81359058,  3.64950381, 46.98382648, -0.38567171, -0.42676124,
       -0.818005  ]), 'targetState': array([ 3.06332765,  3.95171149, 45.        ]), 'previousTarget': array([ 3.06332765,  3.95171149, 45.        ])}
episode index:19255
target thresh 86.14873046512314
target distance 52.55535448000943
model initialize at round 19255
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.81678941, -3.95541072, 71.53447503]), 'distance': 27.5, 'localFrame': array([[-0.55420915,  0.6731663 , -0.48959101],
       [-0.77202201, -0.63559579,  0.        ],
       [-0.31118198,  0.37797504,  0.8719522 ]]), 'currentState': array([24.54815328, 13.44078196, 86.91658798, -0.55420915,  0.6731663 ,
       -0.48959101]), 'targetState': array([-21.34129604, -40.74983538,  39.        ]), 'previousTarget': array([10.29667369, -5.16853218, 71.49721308])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278439947854674
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 44, 'trapConfig': [], 'currentTarget': array([-21.34129604, -40.74983538,  39.        ]), 'distance': 16.528508549079227, 'localFrame': array([[-0.92250695, -0.24026643, -0.30208105],
       [ 0.25204123, -0.9677165 ,  0.        ],
       [-0.29232882, -0.07613688,  0.95328224]]), 'currentState': array([-14.21726572, -46.62422858,  52.70880342,  -0.92250695,
        -0.24026643,  -0.30208105]), 'targetState': array([-21.34129604, -40.74983538,  39.        ]), 'previousTarget': array([-21.34129604, -40.74983538,  39.        ])}
episode index:19256
target thresh 86.15011552282257
target distance 49.91085568316092
model initialize at round 19256
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.14492878,  4.12209399, 67.32136313]), 'distance': 27.499999999999996, 'localFrame': array([[-0.01869708, -0.71376946, -0.70013112],
       [ 0.99965709, -0.02618586,  0.        ],
       [-0.01833353, -0.69989104,  0.7140143 ]]), 'currentState': array([ 3.09618260e+01,  3.10534706e+01,  7.25796745e+01, -1.86970776e-02,
       -7.13769458e-01, -7.00131117e-01]), 'targetState': array([ 27.65177402, -18.01053563,  63.        ]), 'previousTarget': array([29.54682769,  5.13028336, 68.10007303])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6278468358614514
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 27.65177402, -18.01053563,  63.        ]), 'distance': 3.387639826451179, 'localFrame': array([[-0.10652038, -0.8876485 , -0.44803298],
       [ 0.9928765 , -0.11914804,  0.        ],
       [-0.05338225, -0.44484142,  0.89401703]]), 'currentState': array([ 25.90365965, -15.79898267,  61.12137464,  -0.10652038,
        -0.8876485 ,  -0.44803298]), 'targetState': array([ 27.65177402, -18.01053563,  63.        ]), 'previousTarget': array([ 27.65177402, -18.01053563,  63.        ])}
episode index:19257
target thresh 86.15150044202319
target distance 51.12350649148145
model initialize at round 19257
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.57318052, 12.22640427, 42.0795585 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.51391482,  0.3310783 ,  0.79137773],
       [-0.54157275,  0.84065389,  0.        ],
       [-0.66527477, -0.42858862,  0.61132748]]), 'currentState': array([-17.87679335,  30.58751303,  29.69764049,   0.51391482,
         0.3310783 ,   0.79137773]), 'targetState': array([ 27.2900692 , -20.27935213,  64.        ]), 'previousTarget': array([-2.30159856, 12.71260666, 40.76781978])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6278447167393243
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 27.2900692 , -20.27935213,  64.        ]), 'distance': 2.92795144952014, 'localFrame': array([[ 0.74890756, -0.16327683,  0.64224462],
       [ 0.21301615,  0.97704868,  0.        ],
       [-0.62750425,  0.13680848,  0.76649974]]), 'currentState': array([ 25.98790525, -17.81571985,  64.89876842,   0.74890756,
        -0.16327683,   0.64224462]), 'targetState': array([ 27.2900692 , -20.27935213,  64.        ]), 'previousTarget': array([ 27.2900692 , -20.27935213,  64.        ])}
episode index:19258
target thresh 86.15288522273879
target distance 27.0
model initialize at round 19258
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.90742076,  -1.55962443,  50.31787518]), 'distance': 27.5, 'localFrame': array([[-0.04903275, -0.9225807 , -0.38267563],
       [ 0.99859066, -0.05307248,  0.        ],
       [-0.02030955, -0.38213631,  0.92388277]]), 'currentState': array([-1.51573726e+01,  4.71572892e+00,  7.70817943e+01, -4.90327520e-02,
       -9.22580703e-01, -3.82675627e-01]), 'targetState': array([-15.91632908,  -1.63415683,  50.        ]), 'previousTarget': array([-15.88369618,  -1.42788969,  50.68741444])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6278501406270087
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([-15.91632908,  -1.63415683,  50.        ]), 'distance': 2.7961398450378288, 'localFrame': array([[-0.27703874, -0.55680065, -0.78308529],
       [ 0.89530121, -0.44546126,  0.        ],
       [-0.34883416, -0.70109721,  0.62191432]]), 'currentState': array([-16.99530714,  -0.06815086,  52.04983651,  -0.27703874,
        -0.55680065,  -0.78308529]), 'targetState': array([-15.91632908,  -1.63415683,  50.        ]), 'previousTarget': array([-15.91632908,  -1.63415683,  50.        ])}
episode index:19259
target thresh 86.15426986498325
target distance 46.09887626401785
model initialize at round 19259
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.74196152, -18.2413448 ,  56.34731435]), 'distance': 27.5, 'localFrame': array([[ 0.75144262, -0.56444102,  0.34167282],
       [ 0.60058484,  0.79956104,  0.        ],
       [-0.27318828,  0.20520352,  0.93981896]]), 'currentState': array([ 4.94751615,  1.68825557, 37.87099601,  0.75144262, -0.56444102,
        0.34167282]), 'targetState': array([ -4.64183083, -43.75446728,  80.        ]), 'previousTarget': array([ -0.20766965, -17.82327719,  56.37447869])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.62785333891077
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.64183083, -43.75446728,  80.        ]), 'distance': 3.1403824556688744, 'localFrame': array([[-0.62964563, -0.70717975,  0.32162582],
       [ 0.74686294, -0.66497801,  0.        ],
       [ 0.2138741 ,  0.24021041,  0.94686685]]), 'currentState': array([ -5.41852631, -41.9634327 ,  77.54013389,  -0.62964563,
        -0.70717975,   0.32162582]), 'targetState': array([ -4.64183083, -43.75446728,  80.        ]), 'previousTarget': array([ -4.64183083, -43.75446728,  80.        ])}
episode index:19260
target thresh 86.15565436877041
target distance 40.0
model initialize at round 19260
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.07042376, -2.50134272, 47.84007148]), 'distance': 27.5, 'localFrame': array([[ 0.68204939,  0.24401332,  0.68939548],
       [-0.33685577,  0.94155626,  0.        ],
       [-0.64910463, -0.23222684,  0.72438517]]), 'currentState': array([-20.83596112, -15.54428111,  29.46673237,   0.68204939,
         0.24401332,   0.68939548]), 'targetState': array([12.2281252 , 11.80986681, 68.        ]), 'previousTarget': array([-5.82485281, -3.39177898, 46.51330122])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6278583815875286
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.2281252 , 11.80986681, 68.        ]), 'distance': 3.8881555359150064, 'localFrame': array([[ 0.94745874,  0.30937096,  0.08131145],
       [-0.31039877,  0.95060644,  0.        ],
       [-0.07729519, -0.02523897,  0.99668874]]), 'currentState': array([ 9.75225813,  8.88659972, 67.33490958,  0.94745874,  0.30937096,
        0.08131145]), 'targetState': array([12.2281252 , 11.80986681, 68.        ]), 'previousTarget': array([12.2281252 , 11.80986681, 68.        ])}
episode index:19261
target thresh 86.15703873411411
target distance 21.0
model initialize at round 19261
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.13258707, -10.8469604 ,  28.        ]), 'distance': 27.33179318481931, 'localFrame': array([[ 0.26800956, -0.69492374, -0.66727196],
       [ 0.93301629,  0.35983414,  0.        ],
       [ 0.24010723, -0.62257561,  0.74481416]]), 'currentState': array([-8.29491741,  1.91527378, 48.0961295 ,  0.26800956, -0.69492374,
       -0.66727196]), 'targetState': array([  5.13258707, -10.8469604 ,  28.        ]), 'previousTarget': array([  4.33436561, -10.10307229,  29.13927715])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6278674030914198
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  5.13258707, -10.8469604 ,  28.        ]), 'distance': 1.8387847690302814, 'localFrame': array([[ 0.35587133,  0.69066214, -0.62955652],
       [-0.8889349 ,  0.45803357,  0.        ],
       [ 0.28835802,  0.55963476,  0.77695469]]), 'currentState': array([  5.90853069, -10.28836096,  29.57067107,   0.35587133,
         0.69066214,  -0.62955652]), 'targetState': array([  5.13258707, -10.8469604 ,  28.        ]), 'previousTarget': array([  5.13258707, -10.8469604 ,  28.        ])}
episode index:19262
target thresh 86.15842296102821
target distance 42.49328728626582
model initialize at round 19262
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.878969  , -12.09236581,  65.07709217]), 'distance': 27.5, 'localFrame': array([[-0.00221881,  0.59086864, -0.80676473],
       [-0.99999295, -0.00375513,  0.        ],
       [-0.00302951,  0.80675904,  0.59087281]]), 'currentState': array([-1.37750748e+00, -3.59716448e+01,  7.87071535e+01, -2.21880529e-03,
        5.90868641e-01, -8.06764728e-01]), 'targetState': array([-2.24971377,  5.56226464, 55.        ]), 'previousTarget': array([ -2.25348079, -13.22879467,  66.05531045])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278739883524961
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.24971377,  5.56226464, 55.        ]), 'distance': 1.8460538373448852, 'localFrame': array([[-0.77083325,  0.53187788, -0.35060236],
       [-0.5679274 , -0.82307865,  0.        ],
       [-0.28857332,  0.19911669,  0.93652442]]), 'currentState': array([-1.97419666,  4.11311254, 56.10993841, -0.77083325,  0.53187788,
       -0.35060236]), 'targetState': array([-2.24971377,  5.56226464, 55.        ]), 'previousTarget': array([-2.24971377,  5.56226464, 55.        ])}
episode index:19263
target thresh 86.15980704952652
target distance 62.0
model initialize at round 19263
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.02451869,  3.28159697, 67.43182631]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.11301198, -0.33698858, -0.93470155],
       [ 0.94810571,  0.3179553 ,  0.        ],
       [ 0.29719331, -0.88619587,  0.35543355]]), 'currentState': array([-7.65368423,  5.88763501, 94.22307602,  0.11301198, -0.33698858,
       -0.93470155]), 'targetState': array([4.99991228e+00, 2.96168557e-02, 3.40000000e+01]), 'previousTarget': array([-2.51753513,  3.23348619, 69.21191037])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6278757744847964
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([4.99991228e+00, 2.96168557e-02, 3.40000000e+01]), 'distance': 2.8644466091703786, 'localFrame': array([[-0.0096784 , -0.77689277, -0.62955854],
       [ 0.99992241, -0.01245687,  0.        ],
       [-0.00784233, -0.6295097 ,  0.77695305]]), 'currentState': array([ 5.68136414e+00, -1.68223549e+00,  3.61932258e+01, -9.67840324e-03,
       -7.76892765e-01, -6.29558544e-01]), 'targetState': array([4.99991228e+00, 2.96168557e-02, 3.40000000e+01]), 'previousTarget': array([4.99991228e+00, 2.96168557e-02, 3.40000000e+01])}
episode index:19264
target thresh 86.16119099962292
target distance 70.51653173640072
model initialize at round 19264
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.8320582 , 19.81989844, 49.31240557]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.44080197, -0.76599723,  0.46791224],
       [ 0.86673338,  0.49877175,  0.        ],
       [-0.2333814 ,  0.40555516,  0.88377493]]), 'currentState': array([-3.28458033, 39.06029113, 33.11124398,  0.44080197, -0.76599723,
        0.46791224]), 'targetState': array([ 36.43654533, -29.68801382,  91.        ]), 'previousTarget': array([ 7.33130656, 21.38899335, 48.98905103])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6278625685935931
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([ 36.43654533, -29.68801382,  91.        ]), 'distance': 2.509249804409691, 'localFrame': array([[ 0.84792061, -0.20311944,  0.48966635],
       [ 0.23295921,  0.97248651,  0.        ],
       [-0.47619392,  0.11407228,  0.87190989]]), 'currentState': array([ 35.11235809, -29.23423859,  88.9174653 ,   0.84792061,
        -0.20311944,   0.48966635]), 'targetState': array([ 36.43654533, -29.68801382,  91.        ]), 'previousTarget': array([ 36.43654533, -29.68801382,  91.        ])}
episode index:19265
target thresh 86.16257481133121
target distance 70.91651748204751
model initialize at round 19265
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.19581103, -5.11356408, 14.75276977]), 'distance': 27.5, 'localFrame': array([[-0.94756589,  0.28433793,  0.1458452 ],
       [-0.28741109, -0.95780732,  0.        ],
       [ 0.1396916 , -0.04191753,  0.98930742]]), 'currentState': array([ 43.52820087, -12.6075393 ,  12.16518777,  -0.94756589,
         0.28433793,   0.1458452 ]), 'targetState': array([-26.02590011,   7.186969  ,  19.        ]), 'previousTarget': array([18.65067875, -6.11167254, 14.59008165])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278299794433495
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 38, 'trapConfig': [], 'currentTarget': array([-26.02590011,   7.186969  ,  19.        ]), 'distance': 18.730800752938766, 'localFrame': array([[-0.61283085,  0.08020787,  0.78613297],
       [-0.12977415, -0.99154358,  0.        ],
       [ 0.7794851 , -0.10201974,  0.61805741]]), 'currentState': array([-11.92389798,  -1.70275177,  27.54103608,  -0.61283085,
         0.08020787,   0.78613297]), 'targetState': array([-26.02590011,   7.186969  ,  19.        ]), 'previousTarget': array([-26.02590011,   7.186969  ,  19.        ])}
episode index:19266
target thresh 86.16395848466526
target distance 8.300010499112842
model initialize at round 19266
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.23932952,   8.39682942, 100.        ]), 'distance': 10.085488354317087, 'localFrame': array([[-0.92325379, -0.33959229, -0.17966499],
       [ 0.34520959, -0.93852562,  0.        ],
       [-0.16862019, -0.06202208,  0.98372785]]), 'currentState': array([ 9.8068006 , 16.0351392 , 99.50844871, -0.92325379, -0.33959229,
       -0.17966499]), 'targetState': array([  3.23932952,   8.39682942, 100.        ]), 'previousTarget': array([  3.23932952,   8.39682942, 100.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6278457699332266
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([  3.23932952,   8.39682942, 100.        ]), 'distance': 2.844679152689085, 'localFrame': array([[-0.58900632, -0.54084451,  0.60046545],
       [ 0.67635096, -0.73657952,  0.        ],
       [ 0.44229055,  0.40612538,  0.7996507 ]]), 'currentState': array([ 5.0127721 , 10.61852312, 99.89427392, -0.58900632, -0.54084451,
        0.60046545]), 'targetState': array([  3.23932952,   8.39682942, 100.        ]), 'previousTarget': array([  3.23932952,   8.39682942, 100.        ])}
episode index:19267
target thresh 86.16534201963889
target distance 27.87701228641879
model initialize at round 19267
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.14150974,  3.74975138, 63.91168775]), 'distance': 27.499999999999996, 'localFrame': array([[-0.2103832 , -0.67401351, -0.7081276 ],
       [ 0.95457912, -0.29795754,  0.        ],
       [-0.21099196, -0.67596382,  0.70608449]]), 'currentState': array([24.00610435, 17.24941262, 79.87619232, -0.2103832 , -0.67401351,
       -0.7081276 ]), 'targetState': array([-2.71182445, -2.94040952, 56.        ]), 'previousTarget': array([ 7.31729241,  4.63017679, 64.99407437])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6278539613579138
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.71182445, -2.94040952, 56.        ]), 'distance': 1.9486388531732641, 'localFrame': array([[-0.64780067, -0.65602081, -0.3872867 ],
       [ 0.7115507 , -0.70263476,  0.        ],
       [-0.2721211 , -0.27557413,  0.92195933]]), 'currentState': array([-2.43497112, -2.96817789, 57.9286717 , -0.64780067, -0.65602081,
       -0.3872867 ]), 'targetState': array([-2.71182445, -2.94040952, 56.        ]), 'previousTarget': array([-2.71182445, -2.94040952, 56.        ])}
episode index:19268
target thresh 86.16672541626595
target distance 34.71602812391595
model initialize at round 19268
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.22100782,  -3.72857644,  66.90169526]), 'distance': 27.5, 'localFrame': array([[ 0.41167645,  0.88667498, -0.21054688],
       [-0.90700665,  0.42111629,  0.        ],
       [ 0.08866472,  0.19096742,  0.97758376]]), 'currentState': array([-25.03276541, -22.63159313,  84.29868847,   0.41167645,
         0.88667498,  -0.21054688]), 'targetState': array([-7.9445604, 10.2899932, 54.       ]), 'previousTarget': array([-15.57235375,  -5.17522548,  67.80981076])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6278564459231587
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-7.9445604, 10.2899932, 54.       ]), 'distance': 2.9511102118780497, 'localFrame': array([[-0.04804314,  0.99754422,  0.05096449],
       [-0.99884225, -0.04810566,  0.        ],
       [ 0.00245168, -0.05090549,  0.99870047]]), 'currentState': array([-6.53934260e+00,  9.18262663e+00,  5.63469456e+01, -4.80431412e-02,
        9.97544223e-01,  5.09644922e-02]), 'targetState': array([-7.9445604, 10.2899932, 54.       ]), 'previousTarget': array([-7.9445604, 10.2899932, 54.       ])}
episode index:19269
target thresh 86.16810867456024
target distance 29.0
model initialize at round 19269
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.30702432, 23.36448604, 69.74412236]), 'distance': 27.5, 'localFrame': array([[-0.66644482,  0.11065646,  0.73729672],
       [-0.1637974 , -0.986494  ,  0.        ],
       [ 0.72733879, -0.12076729,  0.67556906]]), 'currentState': array([-6.20231299, 12.9767558 , 94.07424595, -0.66644482,  0.11065646,
        0.73729672]), 'targetState': array([ 3.07991101, 25.8169353 , 64.        ]), 'previousTarget': array([ 1.67046521, 23.43900181, 68.9686613 ])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6278650477829262
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 3.07991101, 25.8169353 , 64.        ]), 'distance': 2.6431082953401472, 'localFrame': array([[ 0.73392661, -0.27676796, -0.62028319],
       [ 0.35285026,  0.9356798 ,  0.        ],
       [ 0.58038645, -0.21886708,  0.78437795]]), 'currentState': array([ 1.5085348 , 24.34360416, 65.53169632,  0.73392661, -0.27676796,
       -0.62028319]), 'targetState': array([ 3.07991101, 25.8169353 , 64.        ]), 'previousTarget': array([ 3.07991101, 25.8169353 , 64.        ])}
episode index:19270
target thresh 86.16949179453565
target distance 80.0
model initialize at round 19270
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.13477625, 18.36166798, 63.66240937]), 'distance': 27.5, 'localFrame': array([[-0.92814079, -0.33146316, -0.16937193],
       [ 0.33632227, -0.94174696,  0.        ],
       [-0.1595055 , -0.05696355,  0.98555221]]), 'currentState': array([-4.87830669, 27.45659601, 89.55627709, -0.92814079, -0.33146316,
       -0.16937193]), 'targetState': array([ 0.54584712, -0.83788479,  9.        ]), 'previousTarget': array([-1.97007064, 18.55461689, 63.13568756])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6278617288136288
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 0.54584712, -0.83788479,  9.        ]), 'distance': 3.478554267443076, 'localFrame': array([[-0.28624245, -0.73274129, -0.61737789],
       [ 0.93145072, -0.36386749,  0.        ],
       [-0.22464375, -0.57505708,  0.78666673]]), 'currentState': array([ 1.50197103,  1.75077508, 11.11778349, -0.28624245, -0.73274129,
       -0.61737789]), 'targetState': array([ 0.54584712, -0.83788479,  9.        ]), 'previousTarget': array([ 0.54584712, -0.83788479,  9.        ])}
episode index:19271
target thresh 86.17087477620595
target distance 31.903689703615616
model initialize at round 19271
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.85444927, 25.42785689, 10.04964211]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.90140384,  0.42602428, -0.07729441],
       [-0.42730263,  0.90410865,  0.        ],
       [ 0.06988255,  0.03302811,  0.99700831]]), 'currentState': array([-10.29101747,  20.10507478,  16.70791214,   0.90140384,
         0.42602428,  -0.07729441]), 'targetState': array([19.97614797, 26.26696618,  9.        ]), 'previousTarget': array([14.40114277, 25.11340246, 10.22321389])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278724517293945
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.97614797, 26.26696618,  9.        ]), 'distance': 1.9191003113155736, 'localFrame': array([[ 0.7054439 ,  0.31762084,  0.63361338],
       [-0.41054857,  0.91183873,  0.        ],
       [-0.57775322, -0.26012906,  0.77364985]]), 'currentState': array([18.18078345, 26.01860869,  8.36918242,  0.7054439 ,  0.31762084,
        0.63361338]), 'targetState': array([19.97614797, 26.26696618,  9.        ]), 'previousTarget': array([19.97614797, 26.26696618,  9.        ])}
episode index:19272
target thresh 86.17225761958501
target distance 14.0
model initialize at round 19272
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.46253563, -2.00270992, 92.        ]), 'distance': 17.705186675544574, 'localFrame': array([[ 0.8417127 ,  0.42262204,  0.33602134],
       [-0.44871272,  0.89367606,  0.        ],
       [-0.30029422, -0.15077705,  0.94185437]]), 'currentState': array([-6.58559552,  1.42209198, 77.83033425,  0.8417127 ,  0.42262204,
        0.33602134]), 'targetState': array([ 3.46253563, -2.00270992, 92.        ]), 'previousTarget': array([ 3.46253563, -2.00270992, 92.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6278877514877456
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.46253563, -2.00270992, 92.        ]), 'distance': 4.390159253326691, 'localFrame': array([[ 0.99616616, -0.06565287, -0.05781598],
       [ 0.06576287,  0.99783528,  0.        ],
       [ 0.05769083, -0.00380215,  0.99832726]]), 'currentState': array([ 1.50826067e+00,  6.86768102e-01,  8.91327686e+01,  9.96166157e-01,
       -6.56528671e-02, -5.78159831e-02]), 'targetState': array([ 3.46253563, -2.00270992, 92.        ]), 'previousTarget': array([ 3.46253563, -2.00270992, 92.        ])}
episode index:19273
target thresh 86.17364032468664
target distance 52.46409308546325
model initialize at round 19273
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.1375534 , -18.71162866,  10.88840304]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.36538753, -0.83861432,  0.40400244],
       [ 0.91676094,  0.39943632,  0.        ],
       [-0.16137325,  0.37037366,  0.91475791]]), 'currentState': array([  7.98682094, -25.48313878,   1.99107395,   0.36538753,
        -0.83861432,   0.40400244]), 'targetState': array([-45.69079053, -11.01597298,  21.        ]), 'previousTarget': array([-18.4040085 , -17.80280391,  11.11802496])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278551745576071
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([-45.69079053, -11.01597298,  21.        ]), 'distance': 20.309724348800074, 'localFrame': array([[ 0.64126782, -0.01927352,  0.76707504],
       [ 0.03004177,  0.99954864,  0.        ],
       [-0.76672882,  0.02304429,  0.64155739]]), 'currentState': array([-3.14194530e+01, -1.65546659e+01,  7.65321350e+00,  6.41267816e-01,
       -1.92735180e-02,  7.67075042e-01]), 'targetState': array([-45.69079053, -11.01597298,  21.        ]), 'previousTarget': array([-45.69079053, -11.01597298,  21.        ])}
episode index:19274
target thresh 86.17502289152468
target distance 22.141430842807075
model initialize at round 19274
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.99150084,  0.39104041, 68.        ]), 'distance': 25.438924853541742, 'localFrame': array([[-0.81900381, -0.18960932, -0.5415543 ],
       [ 0.22554662, -0.97423238,  0.        ],
       [-0.52759973, -0.12214574,  0.84066577]]), 'currentState': array([11.8764106 , -4.28737294, 81.77612494, -0.81900381, -0.18960932,
       -0.5415543 ]), 'targetState': array([-8.99150084,  0.39104041, 68.        ]), 'previousTarget': array([-8.99150084,  0.39104041, 68.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6278658961444757
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.99150084,  0.39104041, 68.        ]), 'distance': 2.6735821020104362, 'localFrame': array([[-0.66058835, -0.30001539, -0.68819605],
       [ 0.41351501, -0.9104973 ,  0.        ],
       [-0.62660064, -0.2845794 ,  0.72552477]]), 'currentState': array([-7.91366806,  0.19801983, 70.43906966, -0.66058835, -0.30001539,
       -0.68819605]), 'targetState': array([-8.99150084,  0.39104041, 68.        ]), 'previousTarget': array([-8.99150084,  0.39104041, 68.        ])}
episode index:19275
target thresh 86.17640532011295
target distance 51.0
model initialize at round 19275
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.1191644 , 18.53074237, 46.81055258]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.68012053,  0.24653822, -0.69040204],
       [-0.34079264,  0.94013849,  0.        ],
       [ 0.64907353,  0.23528394,  0.72342589]]), 'currentState': array([ 4.03408067,  5.10320504, 69.02344287,  0.68012053,  0.24653822,
       -0.69040204]), 'targetState': array([24.49369492, 35.34202752, 19.        ]), 'previousTarget': array([12.10617947, 18.47915574, 47.76197771])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278333237281993
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 53, 'trapConfig': [], 'currentTarget': array([24.49369492, 35.34202752, 19.        ]), 'distance': 18.046052135203215, 'localFrame': array([[ 0.13342735,  0.71980761, -0.68122988],
       [-0.98325028,  0.18226048,  0.        ],
       [ 0.12416128,  0.66981947,  0.73206957]]), 'currentState': array([14.80450132, 33.56777723, 34.12056748,  0.13342735,  0.71980761,
       -0.68122988]), 'targetState': array([24.49369492, 35.34202752, 19.        ]), 'previousTarget': array([24.49369492, 35.34202752, 19.        ])}
episode index:19276
target thresh 86.17778761046526
target distance 33.42247018973405
model initialize at round 19276
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.19796922, 19.08469408, 32.38995804]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.13331243, -0.8127128 ,  0.5672087 ],
       [ 0.98681199,  0.1618706 ,  0.        ],
       [-0.09181441,  0.55972834,  0.8235741 ]]), 'currentState': array([-27.2707044 ,   9.21353964,  23.48413237,   0.13331243,
        -0.8127128 ,   0.5672087 ]), 'targetState': array([ 6.56009112, 23.08603917, 36.        ]), 'previousTarget': array([-2.58993666, 19.75744084, 32.44100629])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6278300074376315
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([ 6.56009112, 23.08603917, 36.        ]), 'distance': 1.7241503036416677, 'localFrame': array([[ 9.90185314e-01,  1.39750499e-01, -1.68576473e-03],
       [-1.39750698e-01,  9.90186721e-01,  0.00000000e+00],
       [ 1.66922185e-03,  2.35586798e-04,  9.99998579e-01]]), 'currentState': array([ 4.84042405e+00,  2.29967035e+01,  3.60863631e+01,  9.90185314e-01,
        1.39750499e-01, -1.68576473e-03]), 'targetState': array([ 6.56009112, 23.08603917, 36.        ]), 'previousTarget': array([ 6.56009112, 23.08603917, 36.        ])}
episode index:19277
target thresh 86.17916976259545
target distance 43.14550698004153
model initialize at round 19277
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.4722505 , -27.89251233,  20.75378513]), 'distance': 27.5, 'localFrame': array([[-0.80668462,  0.33814867, -0.48468073],
       [-0.38659208, -0.92225081,  0.        ],
       [-0.4469972 ,  0.18737373,  0.87469114]]), 'currentState': array([-16.80451486,  -3.67011085,  16.57759996,  -0.80668462,
         0.33814867,  -0.48468073]), 'targetState': array([  5.11381389, -46.72096861,  24.        ]), 'previousTarget': array([ -3.78801324, -28.17491964,  21.42090657])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6278342985061985
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.11381389, -46.72096861,  24.        ]), 'distance': 2.8922360332773898, 'localFrame': array([[ 0.74411889, -0.570823  , -0.34705645],
       [ 0.60865437,  0.79343548,  0.        ],
       [ 0.2753669 , -0.21123743,  0.93784424]]), 'currentState': array([  4.40880529, -43.94879505,  23.57216132,   0.74411889,
        -0.570823  ,  -0.34705645]), 'targetState': array([  5.11381389, -46.72096861,  24.        ]), 'previousTarget': array([  5.11381389, -46.72096861,  24.        ])}
episode index:19278
target thresh 86.18055177651736
target distance 32.84763169129823
model initialize at round 19278
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.71498339,   0.81791982,  11.73052933]), 'distance': 27.5, 'localFrame': array([[ 0.5047234 , -0.37228212, -0.77888402],
       [ 0.59359235,  0.80476588,  0.        ],
       [ 0.62681928, -0.46233959,  0.62716799]]), 'currentState': array([ 7.01129362,  0.1725875 , 28.57696614,  0.5047234 , -0.37228212,
       -0.77888402]), 'targetState': array([-25.97445035,   1.15235806,   3.        ]), 'previousTarget': array([-14.37089625,   1.21384471,  12.53785538])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6278017327974736
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 72, 'trapConfig': [], 'currentTarget': array([-25.97445035,   1.15235806,   3.        ]), 'distance': 20.618092562838626, 'localFrame': array([[ 0.39491249, -0.36130907, -0.84468922],
       [ 0.67501972,  0.73779969,  0.        ],
       [ 0.62321144, -0.57018188,  0.53525706]]), 'currentState': array([-5.45867606, -0.74959376,  3.76897706,  0.39491249, -0.36130907,
       -0.84468922]), 'targetState': array([-25.97445035,   1.15235806,   3.        ]), 'previousTarget': array([-25.97445035,   1.15235806,   3.        ])}
episode index:19279
target thresh 86.18193365224475
target distance 33.0
model initialize at round 19279
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.58627751, 11.2817107 , 82.86150764]), 'distance': 27.500000000000004, 'localFrame': array([[-0.37020964,  0.59847338,  0.7104748 ],
       [-0.85043927, -0.52607323,  0.        ],
       [ 0.37376177, -0.60421567,  0.70372265]]), 'currentState': array([ 0.74024676,  3.89587144, 58.26973957, -0.37020964,  0.59847338,
        0.7104748 ]), 'targetState': array([13.44438088, 13.42567029, 90.        ]), 'previousTarget': array([10.66024806, 11.00821145, 81.98378457])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6278120215366353
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.44438088, 13.42567029, 90.        ]), 'distance': 1.879748334994925, 'localFrame': array([[ 0.39617417,  0.83322312,  0.38572693],
       [-0.90311227,  0.4294045 ,  0.        ],
       [-0.16563288, -0.34835472,  0.92261299]]), 'currentState': array([13.64484842, 13.63428354, 88.14265052,  0.39617417,  0.83322312,
        0.38572693]), 'targetState': array([13.44438088, 13.42567029, 90.        ]), 'previousTarget': array([13.44438088, 13.42567029, 90.        ])}
episode index:19280
target thresh 86.18331538979152
target distance 12.0
model initialize at round 19280
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.19052124,  9.07962179, 73.        ]), 'distance': 11.750125617252241, 'localFrame': array([[ 0.42261103,  0.6966703 ,  0.57969855],
       [-0.85498746,  0.51864868,  0.        ],
       [-0.30065989, -0.49563499,  0.81483102]]), 'currentState': array([ 3.01533624,  3.89765496, 62.51994217,  0.42261103,  0.6966703 ,
        0.57969855]), 'targetState': array([ 4.19052124,  9.07962179, 73.        ]), 'previousTarget': array([ 4.19052124,  9.07962179, 73.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6278287830131336
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.19052124,  9.07962179, 73.        ]), 'distance': 3.0045871746961614, 'localFrame': array([[-0.42915769,  0.15520824,  0.8897944 ],
       [-0.34009926, -0.94038955,  0.        ],
       [ 0.83675335, -0.30261841,  0.45636161]]), 'currentState': array([ 3.60859056,  7.13224196, 70.78717091, -0.42915769,  0.15520824,
        0.8897944 ]), 'targetState': array([ 4.19052124,  9.07962179, 73.        ]), 'previousTarget': array([ 4.19052124,  9.07962179, 73.        ])}
episode index:19281
target thresh 86.18469698917141
target distance 44.102238960465485
model initialize at round 19281
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.90981663, -18.51297817,  77.49872622]), 'distance': 27.5, 'localFrame': array([[ 0.77447697, -0.61042614, -0.16602817],
       [ 0.61901748,  0.78537721,  0.        ],
       [ 0.13039474, -0.10277434,  0.98612101]]), 'currentState': array([-3.86541882,  3.94597903, 83.28989701,  0.77447697, -0.61042614,
       -0.16602817]), 'targetState': array([ 24.93892706, -39.83779509,  72.        ]), 'previousTarget': array([  9.70995304, -17.80986336,  77.99369073])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.627829549677332
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 24.93892706, -39.83779509,  72.        ]), 'distance': 2.042173789524195, 'localFrame': array([[ 0.40898413, -0.90843751,  0.08644813],
       [ 0.91185116,  0.41052097,  0.        ],
       [-0.03548877,  0.07882783,  0.99625635]]), 'currentState': array([ 22.90264344, -39.98299143,  71.94577025,   0.40898413,
        -0.90843751,   0.08644813]), 'targetState': array([ 24.93892706, -39.83779509,  72.        ]), 'previousTarget': array([ 24.93892706, -39.83779509,  72.        ])}
episode index:19282
target thresh 86.18607845039827
target distance 31.0
model initialize at round 19282
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.54901165e-02, -6.79875998e-02,  8.38354845e+01]), 'distance': 27.5, 'localFrame': array([[-0.34382336,  0.16016303,  0.92527472],
       [-0.42226214, -0.90647376,  0.        ],
       [ 0.83873726, -0.39070848,  0.37929764]]), 'currentState': array([-1.17025504, -0.93066701, 56.37043272, -0.34382336,  0.16016303,
        0.92527472]), 'targetState': array([ 0.,  0., 86.]), 'previousTarget': array([-3.7866759e-02, -2.2634168e-01,  8.2442946e+01])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.627829983009095
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 86.]), 'distance': 2.830533906077016, 'localFrame': array([[ 0.3182567 ,  0.43859319,  0.84044553],
       [-0.80936778,  0.58730214,  0.        ],
       [-0.49359546, -0.68022953,  0.54189603]]), 'currentState': array([-0.64325564,  0.47977272, 83.28560089,  0.3182567 ,  0.43859319,
        0.84044553]), 'targetState': array([ 0.,  0., 86.]), 'previousTarget': array([ 0.,  0., 86.])}
episode index:19283
target thresh 86.18745977348593
target distance 74.70837385725522
model initialize at round 19283
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.94695128,  6.61684657, 63.10672506]), 'distance': 27.499999999999996, 'localFrame': array([[-0.20574466, -0.54262651,  0.81438664],
       [ 0.93504277, -0.35453494,  0.        ],
       [ 0.28872852,  0.76148634,  0.58032267]]), 'currentState': array([-0.35758329, 32.69695041, 54.53047339, -0.20574466, -0.54262651,
        0.81438664]), 'targetState': array([ -4.89232455, -41.71408827,  79.        ]), 'previousTarget': array([-1.29560528,  7.0848823 , 62.01698766])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6278300863926355
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.89232455, -41.71408827,  79.        ]), 'distance': 3.3578391313070814, 'localFrame': array([[ 0.45265668, -0.78778514,  0.41772778],
       [ 0.86705835,  0.4982066 ,  0.        ],
       [-0.20811474,  0.36219436,  0.90857223]]), 'currentState': array([ -3.70317249, -40.00062188,  76.36845408,   0.45265668,
        -0.78778514,   0.41772778]), 'targetState': array([ -4.89232455, -41.71408827,  79.        ]), 'previousTarget': array([ -4.89232455, -41.71408827,  79.        ])}
episode index:19284
target thresh 86.18884095844818
target distance 41.85848133239877
model initialize at round 19284
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.96243309,   4.59393703,  69.30016036]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.00917659,  0.91865151, -0.39496226],
       [-0.99995011,  0.0099887 ,  0.        ],
       [ 0.00394516,  0.39494256,  0.91869735]]), 'currentState': array([-1.60092939e+00, -1.77460719e+01,  8.06178560e+01,  9.17659404e-03,
        9.18651514e-01, -3.94962259e-01]), 'targetState': array([-22.29859209,  22.95153134,  60.        ]), 'previousTarget': array([-12.78043323,   3.44096142,  70.25437437])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6278305196516281
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([-22.29859209,  22.95153134,  60.        ]), 'distance': 1.8422117650163603, 'localFrame': array([[ 0.53181323,  0.03236612, -0.84624294],
       [-0.06074753,  0.99815316,  0.        ],
       [ 0.84468007,  0.05140717,  0.53279722]]), 'currentState': array([-2.34078684e+01,  2.36084389e+01,  6.13159493e+01,  5.31813235e-01,
        3.23661168e-02, -8.46242943e-01]), 'targetState': array([-22.29859209,  22.95153134,  60.        ]), 'previousTarget': array([-22.29859209,  22.95153134,  60.        ])}
episode index:19285
target thresh 86.19022200529885
target distance 76.63105341171814
model initialize at round 19285
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.78643729, -10.28042714,  63.70664851]), 'distance': 27.500000000000004, 'localFrame': array([[-0.68879434,  0.41591146, -0.59378449],
       [-0.51690155, -0.85604485,  0.        ],
       [-0.50830615,  0.30692812,  0.80462413]]), 'currentState': array([47.50368945,  0.18147724, 54.52566236, -0.68879434,  0.41591146,
       -0.59378449]), 'targetState': array([-28.63740107, -33.40507836,  84.        ]), 'previousTarget': array([ 24.01029321, -10.99119997,  64.76321055])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6278277987261031
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-28.63740107, -33.40507836,  84.        ]), 'distance': 3.178109447042378, 'localFrame': array([[-0.45703002, -0.2862234 ,  0.84213997],
       [ 0.53077167, -0.84751486,  0.        ],
       [ 0.71372614,  0.44698404,  0.539259  ]]), 'currentState': array([-25.72410179, -33.20981722,  82.74503385,  -0.45703002,
        -0.2862234 ,   0.84213997]), 'targetState': array([-28.63740107, -33.40507836,  84.        ]), 'previousTarget': array([-28.63740107, -33.40507836,  84.        ])}
episode index:19286
target thresh 86.19160291405173
target distance 47.16663272351191
model initialize at round 19286
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.44336521,  8.17798333, 71.85373031]), 'distance': 27.5, 'localFrame': array([[ 0.74902575,  0.11166631,  0.65306283],
       [-0.14745247,  0.98906914,  0.        ],
       [-0.6459243 , -0.09629573,  0.75730373]]), 'currentState': array([-7.42698744, 21.1912024 , 55.49657208,  0.74902575,  0.11166631,
        0.65306283]), 'targetState': array([ 39.00841625, -12.62313202,  98.        ]), 'previousTarget': array([ 9.73760531,  7.88997047, 70.69434753])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.627829931974397
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 39.00841625, -12.62313202,  98.        ]), 'distance': 3.766265762530515, 'localFrame': array([[ 0.73699348, -0.66748762, -0.10630565],
       [ 0.67129149,  0.74119345,  0.        ],
       [ 0.07879305, -0.07136208,  0.9943335 ]]), 'currentState': array([39.5234026 , -9.78226257, 95.58152783,  0.73699348, -0.66748762,
       -0.10630565]), 'targetState': array([ 39.00841625, -12.62313202,  98.        ]), 'previousTarget': array([ 39.00841625, -12.62313202,  98.        ])}
episode index:19287
target thresh 86.19298368472064
target distance 55.0
model initialize at round 19287
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.60574992, -17.49286881,  45.14275285]), 'distance': 27.5, 'localFrame': array([[-0.22074881,  0.57250543, -0.78962491],
       [-0.93304253, -0.35976607,  0.        ],
       [-0.28408025,  0.73675362,  0.61358985]]), 'currentState': array([-10.44998507, -28.86793101,  69.70677829,  -0.22074881,
         0.57250543,  -0.78962491]), 'targetState': array([ 0.1414493 , -3.99749823, 16.        ]), 'previousTarget': array([ -6.00618266, -18.48486573,  46.55270772])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6278331266630062
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.1414493 , -3.99749823, 16.        ]), 'distance': 2.929249179833401, 'localFrame': array([[ 0.5659638 ,  0.56867296, -0.59690538],
       [-0.70879311,  0.70541642,  0.        ],
       [ 0.42106686,  0.42308242,  0.80231164]]), 'currentState': array([-1.88376025, -5.49468515, 17.49581361,  0.5659638 ,  0.56867296,
       -0.59690538]), 'targetState': array([ 0.1414493 , -3.99749823, 16.        ]), 'previousTarget': array([ 0.1414493 , -3.99749823, 16.        ])}
episode index:19288
target thresh 86.1943643173194
target distance 61.864172316152505
model initialize at round 19288
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.12350055,  -1.89972308,  25.07152566]), 'distance': 27.5, 'localFrame': array([[ 0.78887132, -0.13150396,  0.60032387],
       [ 0.1644299 ,  0.98638877,  0.        ],
       [-0.59215272,  0.09871119,  0.799757  ]]), 'currentState': array([-39.27547644,   6.60446437,  25.12586878,   0.78887132,
        -0.13150396,   0.60032387]), 'targetState': array([ 21.29737315, -13.09281852,  25.        ]), 'previousTarget': array([-14.28624993,  -2.14286861,  24.42481049])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6278301077442345
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([ 21.29737315, -13.09281852,  25.        ]), 'distance': 3.106068854143279, 'localFrame': array([[ 0.81812009,  0.56551137, -0.10429004],
       [-0.56861205,  0.82260582,  0.        ],
       [ 0.0857896 ,  0.05930058,  0.99454693]]), 'currentState': array([ 19.22858326, -11.02528721,  23.95449228,   0.81812009,
         0.56551137,  -0.10429004]), 'targetState': array([ 21.29737315, -13.09281852,  25.        ]), 'previousTarget': array([ 21.29737315, -13.09281852,  25.        ])}
episode index:19289
target thresh 86.19574481186179
target distance 30.0
model initialize at round 19289
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.54849907,  1.18054683, 16.66595725]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.41477272, -0.28164975, -0.86523813],
       [ 0.5617702 ,  0.82729332,  0.        ],
       [ 0.71580572, -0.486065  ,  0.50136113]]), 'currentState': array([-0.6377087 ,  2.78769432, 44.03176753,  0.41477272, -0.28164975,
       -0.86523813]), 'targetState': array([ 1.68158954,  1.082708  , 15.        ]), 'previousTarget': array([ 1.32985378,  1.2994492 , 17.79550125])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6278351440442974
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([ 1.68158954,  1.082708  , 15.        ]), 'distance': 2.9131034898918196, 'localFrame': array([[-0.76735307, -0.14283568, -0.62511377],
       [ 0.18299747, -0.98311338,  0.        ],
       [-0.61455772, -0.11439424,  0.78053365]]), 'currentState': array([ 0.9942043 ,  0.75433206, 17.81173305, -0.76735307, -0.14283568,
       -0.62511377]), 'targetState': array([ 1.68158954,  1.082708  , 15.        ]), 'previousTarget': array([ 1.68158954,  1.082708  , 15.        ])}
episode index:19290
target thresh 86.19712516836162
target distance 42.0
model initialize at round 19290
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.63339089, -5.98511618, 64.80325179]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.88959391,  0.09422577, -0.4469275 ],
       [-0.10533078,  0.99443724,  0.        ],
       [ 0.44444135,  0.04707522,  0.89457018]]), 'currentState': array([-10.65206417, -19.6597776 ,  86.89209892,   0.88959391,
         0.09422577,  -0.4469275 ]), 'targetState': array([ 6.45209481,  6.27458943, 45.        ]), 'previousTarget': array([-2.58507872, -5.97432801, 65.0836533 ])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6278417214194029
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.45209481,  6.27458943, 45.        ]), 'distance': 3.1716621273622025, 'localFrame': array([[-0.09250167,  0.8090503 , -0.58041455],
       [-0.9935273 , -0.11359359,  0.        ],
       [-0.06593137,  0.57665771,  0.81432116]]), 'currentState': array([ 5.22289429,  3.8784616 , 46.67543371, -0.09250167,  0.8090503 ,
       -0.58041455]), 'targetState': array([ 6.45209481,  6.27458943, 45.        ]), 'previousTarget': array([ 6.45209481,  6.27458943, 45.        ])}
episode index:19291
target thresh 86.19850538683271
target distance 63.0
model initialize at round 19291
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.78356608, -7.20260659, 76.56091017]), 'distance': 27.5, 'localFrame': array([[ 0.69858822,  0.67872546,  0.22650883],
       [-0.69683687,  0.71722966,  0.        ],
       [-0.16245885, -0.1578397 ,  0.97400911]]), 'currentState': array([ 30.56157577, -22.04618381,  98.69632975,   0.69858822,
         0.67872546,   0.22650883]), 'targetState': array([11.36355112, 19.99674238, 36.        ]), 'previousTarget': array([23.31164353, -8.44872762, 77.0222009 ])}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.6278321426437552
{'scaleFactor': 20, 'timeStep': 82, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([11.36355112, 19.99674238, 36.        ]), 'distance': 2.814808969257832, 'localFrame': array([[-0.62437656,  0.1185062 , -0.77208172],
       [-0.18647027, -0.9824606 ,  0.        ],
       [-0.75853988,  0.14397029,  0.63552326]]), 'currentState': array([11.9049388 , 17.78348592, 34.34726144, -0.62437656,  0.1185062 ,
       -0.77208172]), 'targetState': array([11.36355112, 19.99674238, 36.        ]), 'previousTarget': array([11.36355112, 19.99674238, 36.        ])}
episode index:19292
target thresh 86.19988546728885
target distance 55.79658184863628
model initialize at round 19292
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.95714754, -13.61965099,  17.97556915]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.44952159,  0.85215892, -0.2678722 ],
       [-0.88448283,  0.46657275,  0.        ],
       [ 0.12498187,  0.23692836,  0.96345445]]), 'currentState': array([-19.95823941, -26.86262013,  20.96355097,   0.44952159,
         0.85215892,  -0.2678722 ]), 'targetState': array([35.77704488,  4.00038247, 14.        ]), 'previousTarget': array([  3.56668943, -14.88226661,  18.04097313])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.6278223352192576
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 30, 'trapConfig': [], 'currentTarget': array([35.77704488,  4.00038247, 14.        ]), 'distance': 2.2626457946289147, 'localFrame': array([[ 0.76091263, -0.60167995, -0.24288519],
       [ 0.62025341,  0.78440149,  0.        ],
       [ 0.1905195 , -0.15065036,  0.97005504]]), 'currentState': array([33.94011374,  5.02860619, 14.82946125,  0.76091263, -0.60167995,
       -0.24288519]), 'targetState': array([35.77704488,  4.00038247, 14.        ]), 'previousTarget': array([35.77704488,  4.00038247, 14.        ])}
episode index:19293
target thresh 86.20126540974385
target distance 63.0
model initialize at round 19293
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.71931982, -15.08711762,  50.71905094]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.5811695 , -0.64051702, -0.50197606],
       [ 0.74058354,  0.6719643 ,  0.        ],
       [ 0.33730999, -0.37175521,  0.86488152]]), 'currentState': array([ 4.65246521, -3.60543742, 26.47829404,  0.5811695 , -0.64051702,
       -0.50197606]), 'targetState': array([ 20.80063296, -34.16626506,  91.        ]), 'previousTarget': array([ 10.52380944, -14.6445512 ,  51.94404622])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277897954485921
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 45, 'trapConfig': [], 'currentTarget': array([ 20.80063296, -34.16626506,  91.        ]), 'distance': 14.940037705449964, 'localFrame': array([[ 0.91032659, -0.33009825,  0.24968111],
       [ 0.34089503,  0.94010137,  0.        ],
       [-0.23472555,  0.08511505,  0.96832812]]), 'currentState': array([ 13.99305489, -28.16140991,  79.13394244,   0.91032659,
        -0.33009825,   0.24968111]), 'targetState': array([ 20.80063296, -34.16626506,  91.        ]), 'previousTarget': array([ 20.80063296, -34.16626506,  91.        ])}
episode index:19294
target thresh 86.2026452142115
target distance 56.0
model initialize at round 19294
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.45906753, -7.64776497, 74.68354997]), 'distance': 27.499999999999996, 'localFrame': array([[-0.88479355,  0.30944408, -0.34840312],
       [-0.33012834, -0.94393606,  0.        ],
       [-0.32887027,  0.11501774,  0.9373448 ]]), 'currentState': array([ 34.5709156 , -18.60694507,  96.22945008,  -0.88479355,
         0.30944408,  -0.34840312]), 'targetState': array([ 0.35224742,  9.99379416, 40.        ]), 'previousTarget': array([22.73759866, -7.94470287, 74.78403068])}
done in step count: 90
reward sum = 0.4047319726783238
running average episode reward sum: 0.6277782350535276
{'scaleFactor': 20, 'timeStep': 91, 'trapCount': 29, 'trapConfig': [], 'currentTarget': array([ 0.35224742,  9.99379416, 40.        ]), 'distance': 3.2212176786816498, 'localFrame': array([[-0.30329658, -0.15988137, -0.93938764],
       [ 0.46632098, -0.88461559,  0.        ],
       [-0.83099695, -0.43805616,  0.34285692]]), 'currentState': array([-1.17290901, 10.35469158, 42.81423066, -0.30329658, -0.15988137,
       -0.93938764]), 'targetState': array([ 0.35224742,  9.99379416, 40.        ]), 'previousTarget': array([ 0.35224742,  9.99379416, 40.        ])}
episode index:19295
target thresh 86.2040248807056
target distance 23.0
model initialize at round 19295
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.62021812, -18.00595165,  30.73740509]), 'distance': 27.499999999999996, 'localFrame': array([[-0.09951827, -0.9202552 , -0.37845274],
       [ 0.99420344, -0.10751519,  0.        ],
       [-0.04068942, -0.37625902,  0.92562061]]), 'currentState': array([-14.75248393,  -5.43351512,   6.63121877,  -0.09951827,
        -0.9202552 ,  -0.37845274]), 'targetState': array([-10.57520428, -18.14290645,  31.        ]), 'previousTarget': array([-10.57520428, -18.14290645,  31.        ])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6277898270692844
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.57520428, -18.14290645,  31.        ]), 'distance': 2.759257628731861, 'localFrame': array([[ 0.77379487,  0.06706657,  0.62987584],
       [-0.08634856,  0.99626499,  0.        ],
       [-0.62752324, -0.05438887,  0.77669584]]), 'currentState': array([-10.66876505, -19.04869727,  28.39533264,   0.77379487,
         0.06706657,   0.62987584]), 'targetState': array([-10.57520428, -18.14290645,  31.        ]), 'previousTarget': array([-10.57520428, -18.14290645,  31.        ])}
episode index:19296
target thresh 86.20540440923996
target distance 79.0
model initialize at round 19296
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.96977728,   4.58397194,  29.38522138]), 'distance': 27.5, 'localFrame': array([[-0.76098537,  0.19952869,  0.61732453],
       [-0.25362464, -0.96730271,  0.        ],
       [ 0.59713969, -0.15656871,  0.7867086 ]]), 'currentState': array([-12.86669192,  -6.42090251,   5.20487984,  -0.76098537,
         0.19952869,   0.61732453]), 'targetState': array([-35.71936247,  28.98494686,  83.        ]), 'previousTarget': array([-18.99125492,   4.80876128,  28.2299748 ])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6277824270352483
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-35.71936247,  28.98494686,  83.        ]), 'distance': 2.3274940031120646, 'localFrame': array([[ 0.44386815, -0.05214559,  0.89457359],
       [ 0.11667749,  0.99316986,  0.        ],
       [-0.88846352,  0.1043766 ,  0.44692069]]), 'currentState': array([-3.66995956e+01,  2.72638264e+01,  8.17776598e+01,  4.43868154e-01,
       -5.21455857e-02,  8.94573586e-01]), 'targetState': array([-35.71936247,  28.98494686,  83.        ]), 'previousTarget': array([-35.71936247,  28.98494686,  83.        ])}
episode index:19297
target thresh 86.20678379982836
target distance 70.0
model initialize at round 19297
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.72954047,  9.09348769, 56.25221638]), 'distance': 27.5, 'localFrame': array([[ 0.67665858,  0.51826494, -0.52300537],
       [-0.60805711,  0.79389329,  0.        ],
       [ 0.41521045,  0.31801713,  0.85232939]]), 'currentState': array([-14.17845383,  17.40327459,  79.83440934,   0.67665858,
         0.51826494,  -0.52300537]), 'targetState': array([19.72543728, -7.2046599 , 10.        ]), 'previousTarget': array([-3.93523052,  8.66257214, 56.54473798])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6277779655702743
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([19.72543728, -7.2046599 , 10.        ]), 'distance': 2.5328685216937252, 'localFrame': array([[ 0.98909847,  0.1362281 , -0.05591174],
       [-0.13644153,  0.99064813,  0.        ],
       [ 0.05538886,  0.00762868,  0.99843572]]), 'currentState': array([18.23250377, -6.47602674, 11.91197966,  0.98909847,  0.1362281 ,
       -0.05591174]), 'targetState': array([19.72543728, -7.2046599 , 10.        ]), 'previousTarget': array([19.72543728, -7.2046599 , 10.        ])}
episode index:19298
target thresh 86.2081630524846
target distance 50.891483613563906
model initialize at round 19298
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.8883881 , -18.73210651,   8.02523281]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.13642954,  0.6670832 , -0.73238445],
       [-0.9797205 ,  0.20036904,  0.        ],
       [ 0.14674717,  0.71753206,  0.68089134]]), 'currentState': array([-11.06457761, -45.07258485,  11.3315155 ,   0.13642954,
         0.6670832 ,  -0.73238445]), 'targetState': array([2.67779092, 5.36930496, 5.        ]), 'previousTarget': array([ -4.30394614, -19.35486108,   8.88657029])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277818864847999
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([2.67779092, 5.36930496, 5.        ]), 'distance': 3.84392087024819, 'localFrame': array([[-3.29053692e-01,  9.44307921e-01, -2.49337119e-03],
       [-9.44310857e-01, -3.29054715e-01,  0.00000000e+00],
       [-8.20455546e-04,  2.35451748e-03,  9.99996892e-01]]), 'currentState': array([ 2.33194685e-01,  3.28177995e+00,  7.10758540e+00, -3.29053692e-01,
        9.44307921e-01, -2.49337119e-03]), 'targetState': array([2.67779092, 5.36930496, 5.        ]), 'previousTarget': array([2.67779092, 5.36930496, 5.        ])}
episode index:19299
target thresh 86.20954216722245
target distance 66.35436487458306
model initialize at round 19299
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.19925007,  -1.74973909,  11.57394293]), 'distance': 27.500000000000004, 'localFrame': array([[-0.60776233, -0.57732706,  0.54526912],
       [ 0.68871989, -0.72502752,  0.        ],
       [ 0.39533512,  0.37553769,  0.83826105]]), 'currentState': array([-9.48204533, 24.58097659,  4.56496294, -0.60776233, -0.57732706,
        0.54526912]), 'targetState': array([-18.72869785, -40.91742754,  22.        ]), 'previousTarget': array([-12.87093533,  -0.7445981 ,  10.49685682])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.627780392915887
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-18.72869785, -40.91742754,  22.        ]), 'distance': 3.885538920901166, 'localFrame': array([[ 0.62877075, -0.32806332,  0.70499773],
       [ 0.462576  ,  0.88657963,  0.        ],
       [-0.62503663,  0.32611503,  0.70920956]]), 'currentState': array([-17.61343715, -38.3406719 ,  19.31412274,   0.62877075,
        -0.32806332,   0.70499773]), 'targetState': array([-18.72869785, -40.91742754,  22.        ]), 'previousTarget': array([-18.72869785, -40.91742754,  22.        ])}
episode index:19300
target thresh 86.21092114405575
target distance 54.0
model initialize at round 19300
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.46128156, -15.97689691,  35.27649309]), 'distance': 27.499999999999996, 'localFrame': array([[-0.61682266,  0.66574736, -0.41989314],
       [-0.73354646, -0.67963931,  0.        ],
       [-0.28537588,  0.30801112,  0.90757355]]), 'currentState': array([-18.5175468 , -32.43859291,  15.14512023,  -0.61682266,
         0.66574736,  -0.41989314]), 'targetState': array([-42.44357327,  11.59927103,  69.        ]), 'previousTarget': array([-26.5277178 , -17.1573834 ,  34.82184051])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6277745567791329
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-42.44357327,  11.59927103,  69.        ]), 'distance': 2.899129263157567, 'localFrame': array([[-0.51887302,  0.85039513, -0.08717175],
       [-0.8536447 , -0.52085577,  0.        ],
       [-0.04540391,  0.0744137 ,  0.9961933 ]]), 'currentState': array([-42.17854022,   9.88135744,  66.67976707,  -0.51887302,
         0.85039513,  -0.08717175]), 'targetState': array([-42.44357327,  11.59927103,  69.        ]), 'previousTarget': array([-42.44357327,  11.59927103,  69.        ])}
episode index:19301
target thresh 86.21229998299825
target distance 58.12017433059904
model initialize at round 19301
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.21426861,  1.76031664, 12.78529688]), 'distance': 27.5, 'localFrame': array([[-0.48726884, -0.59399612, -0.64010756],
       [ 0.77314523, -0.63422902,  0.        ],
       [-0.40597479, -0.4948961 ,  0.76828531]]), 'currentState': array([-5.86326632, 29.20375466, 11.65282048, -0.48726884, -0.59399612,
       -0.64010756]), 'targetState': array([ -8.66336432, -27.67573158,  14.        ]), 'previousTarget': array([-7.17549694,  2.98082893, 13.47253151])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.627774336638117
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ -8.66336432, -27.67573158,  14.        ]), 'distance': 3.2733866351107372, 'localFrame': array([[ 0.91636471, -0.15080888,  0.37085362],
       [ 0.16238859,  0.98672689,  0.        ],
       [-0.36593123,  0.0602224 ,  0.92869134]]), 'currentState': array([ -7.09807422, -25.60343217,  12.00738812,   0.91636471,
        -0.15080888,   0.37085362]), 'targetState': array([ -8.66336432, -27.67573158,  14.        ]), 'previousTarget': array([ -8.66336432, -27.67573158,  14.        ])}
episode index:19302
target thresh 86.21367868406374
target distance 60.0
model initialize at round 19302
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.59455212,  1.95365499, 41.79606559]), 'distance': 27.5, 'localFrame': array([[ 0.26761786, -0.44619992,  0.85398262],
       [ 0.85757948,  0.51435147,  0.        ],
       [-0.43924721,  0.73235797,  0.52030153]]), 'currentState': array([ 0.64021181,  3.69149283, 14.37882142,  0.26761786, -0.44619992,
        0.85398262]), 'targetState': array([-1.99985356e+00, -2.42016240e-02,  7.30000000e+01]), 'previousTarget': array([-1.23039084,  2.13700607, 40.43202751])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6277775317243331
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-1.99985356e+00, -2.42016240e-02,  7.30000000e+01]), 'distance': 2.2237129918429885, 'localFrame': array([[ 0.60803201,  0.56851794,  0.55415198],
       [-0.68297375,  0.73044292,  0.        ],
       [-0.40477639, -0.37847126,  0.83241551]]), 'currentState': array([-1.51346037, -0.58230031, 70.90313401,  0.60803201,  0.56851794,
        0.55415198]), 'targetState': array([-1.99985356e+00, -2.42016240e-02,  7.30000000e+01]), 'previousTarget': array([-1.99985356e+00, -2.42016240e-02,  7.30000000e+01])}
episode index:19303
target thresh 86.21505724726603
target distance 53.0
model initialize at round 19303
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.15627959,  -2.09690271,  40.15496906]), 'distance': 27.5, 'localFrame': array([[-0.69378235,  0.12003537, -0.71011095],
       [-0.17048304, -0.98536061,  0.        ],
       [-0.69971536,  0.12106187,  0.70408979]]), 'currentState': array([ -6.35138441, -10.78698691,  64.71522674,  -0.69378235,
         0.12003537,  -0.71011095]), 'targetState': array([-24.89138332,   7.51126064,  13.        ]), 'previousTarget': array([-14.44697412,  -2.88154045,  41.56057364])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6277730718996462
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-24.89138332,   7.51126064,  13.        ]), 'distance': 2.67515438928115, 'localFrame': array([[ 0.69009483, -0.36022672, -0.62769884],
       [ 0.46274496,  0.88649145,  0.        ],
       [ 0.55644965, -0.29046448,  0.77845627]]), 'currentState': array([-24.24316512,   9.26330494,  11.08516191,   0.69009483,
        -0.36022672,  -0.62769884]), 'targetState': array([-24.89138332,   7.51126064,  13.        ]), 'previousTarget': array([-24.89138332,   7.51126064,  13.        ])}
episode index:19304
target thresh 86.21643567261889
target distance 15.0
model initialize at round 19304
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.41259132, 20.72086438, 73.        ]), 'distance': 20.07187415216061, 'localFrame': array([[ 0.97038556,  0.23582113, -0.05234742],
       [-0.2361449 ,  0.97171785,  0.        ],
       [ 0.05086692,  0.01236158,  0.99862893]]), 'currentState': array([-1.66398659e+01,  1.62293178e+01,  8.74133739e+01,  9.70385563e-01,
        2.35821133e-01, -5.23474168e-02]), 'targetState': array([-3.41259132, 20.72086438, 73.        ]), 'previousTarget': array([-3.41259132, 20.72086438, 73.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6277874002603355
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.41259132, 20.72086438, 73.        ]), 'distance': 3.6314231776178305, 'localFrame': array([[ 0.88868151,  0.26513497,  0.37409707],
       [-0.28589385,  0.95826129,  0.        ],
       [-0.35848274, -0.10695205,  0.92738955]]), 'currentState': array([-5.73116223, 21.52533616, 75.67661883,  0.88868151,  0.26513497,
        0.37409707]), 'targetState': array([-3.41259132, 20.72086438, 73.        ]), 'previousTarget': array([-3.41259132, 20.72086438, 73.        ])}
episode index:19305
target thresh 86.2178139601361
target distance 51.0
model initialize at round 19305
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 16.12693425, -15.24220582,  42.91644513]), 'distance': 27.499999999999996, 'localFrame': array([[-0.37877353, -0.63656091,  0.67180416],
       [ 0.85937106, -0.51135249,  0.        ],
       [ 0.34352873,  0.57732905,  0.74072881]]), 'currentState': array([29.76799202, -3.7237916 , 22.        , -0.37877353, -0.63656091,
        0.67180416]), 'targetState': array([ -3.49262859, -31.80882811,  73.        ]), 'previousTarget': array([ 16.12693425, -15.24220582,  42.91644513])}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.6277778312445563
{'scaleFactor': 20, 'timeStep': 82, 'trapCount': 44, 'trapConfig': [], 'currentTarget': array([ -3.49262859, -31.80882811,  73.        ]), 'distance': 3.2566345550867233, 'localFrame': array([[ 0.15607467, -0.68935919,  0.70740696],
       [ 0.97531533,  0.22081669,  0.        ],
       [-0.15620726,  0.68994485,  0.70680647]]), 'currentState': array([ -1.47295504, -30.59941158,  70.74968912,   0.15607467,
        -0.68935919,   0.70740696]), 'targetState': array([ -3.49262859, -31.80882811,  73.        ]), 'previousTarget': array([ -3.49262859, -31.80882811,  73.        ])}
episode index:19306
target thresh 86.21919210983145
target distance 67.0
model initialize at round 19306
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.76569106, -25.07389883,  48.31765383]), 'distance': 27.5, 'localFrame': array([[ 0.39587014,  0.64187744,  0.65671926],
       [-0.85114378,  0.52493263,  0.        ],
       [-0.34473337, -0.55896251,  0.75413514]]), 'currentState': array([  3.97169635, -42.10452779,  26.80046861,   0.39587014,
         0.64187744,   0.65671926]), 'targetState': array([ 9.49108039, 10.29171478, 93.        ]), 'previousTarget': array([  4.77337226, -25.87658332,  47.46344126])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6277745229811912
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.49108039, 10.29171478, 93.        ]), 'distance': 1.9744788698407667, 'localFrame': array([[ 0.15197831, -0.37627443,  0.9139585 ],
       [ 0.92722359,  0.37450824,  0.        ],
       [-0.34228499,  0.84744388,  0.40580765]]), 'currentState': array([ 8.62173484, 11.60059648, 91.80432704,  0.15197831, -0.37627443,
        0.9139585 ]), 'targetState': array([ 9.49108039, 10.29171478, 93.        ]), 'previousTarget': array([ 9.49108039, 10.29171478, 93.        ])}
episode index:19307
target thresh 86.22057012171874
target distance 40.76952776441299
model initialize at round 19307
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.93188883, -5.68674822, 56.66689576]), 'distance': 27.5, 'localFrame': array([[ 0.8022738 , -0.58723077, -0.10731624],
       [ 0.59064176,  0.8069339 ,  0.        ],
       [ 0.08659711, -0.06338545,  0.99422494]]), 'currentState': array([ 7.27942131, 12.151817  , 71.6114863 ,  0.8022738 , -0.58723077,
       -0.10731624]), 'targetState': array([ 40.23390141, -27.9684318 ,  38.        ]), 'previousTarget': array([20.80063403, -5.04709399, 56.55317413])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277784422463671
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 40.23390141, -27.9684318 ,  38.        ]), 'distance': 3.645762164935241, 'localFrame': array([[ 0.00733346, -0.66389722, -0.74778787],
       [ 0.999939  ,  0.01104541,  0.        ],
       [ 0.00825962, -0.74774225,  0.66393773]]), 'currentState': array([ 3.85163945e+01, -2.57030013e+01,  4.02824497e+01,  7.33346371e-03,
       -6.63897224e-01, -7.47787868e-01]), 'targetState': array([ 40.23390141, -27.9684318 ,  38.        ]), 'previousTarget': array([ 40.23390141, -27.9684318 ,  38.        ])}
episode index:19308
target thresh 86.2219479958117
target distance 82.0
model initialize at round 19308
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.5281658 , -23.49025671,  66.17515526]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.0188017 ,  0.90994402, -0.4143047 ],
       [-0.9997866 ,  0.02065807,  0.        ],
       [ 0.00855874,  0.41421628,  0.91013824]]), 'currentState': array([ 2.92914058e+01, -3.79008163e+01,  8.69775214e+01,  1.88017006e-02,
        9.09944018e-01, -4.14304696e-01]), 'targetState': array([-13.12414125,  18.88800986,   5.        ]), 'previousTarget': array([ 18.22234823, -24.90127695,  66.3349781 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277459300270785
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 33, 'trapConfig': [], 'currentTarget': array([-13.12414125,  18.88800986,   5.        ]), 'distance': 18.70381723539114, 'localFrame': array([[-0.87533729,  0.11521387, -0.46958534],
       [-0.13049672, -0.99144874,  0.        ],
       [-0.4655698 ,  0.06127935,  0.88288709]]), 'currentState': array([ 3.3941343 , 10.12588475,  5.45223381, -0.87533729,  0.11521387,
       -0.46958534]), 'targetState': array([-13.12414125,  18.88800986,   5.        ]), 'previousTarget': array([-13.12414125,  18.88800986,   5.        ])}
episode index:19309
target thresh 86.22332573212415
target distance 66.0
model initialize at round 19309
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.94281587, 34.9488384 , 27.41379278]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.81651016, -0.33305007,  0.47158118],
       [ 0.37768373,  0.92593466,  0.        ],
       [-0.43665336,  0.17810854,  0.88182265]]), 'currentState': array([11.79799543, 46.82133721,  3.38183586,  0.81651016, -0.33305007,
        0.47158118]), 'targetState': array([28.32045415, 14.89804943, 68.        ]), 'previousTarget': array([17.29209363, 35.81939229, 25.96676591])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6277477185361907
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.32045415, 14.89804943, 68.        ]), 'distance': 3.0325829289250392, 'localFrame': array([[-0.91055015, -0.32293794, -0.25808817],
       [ 0.33426229, -0.94248009,  0.        ],
       [-0.24324296, -0.08626914,  0.96612137]]), 'currentState': array([28.71488837, 13.59512581, 65.29013452, -0.91055015, -0.32293794,
       -0.25808817]), 'targetState': array([28.32045415, 14.89804943, 68.        ]), 'previousTarget': array([28.32045415, 14.89804943, 68.        ])}
episode index:19310
target thresh 86.22470333066987
target distance 36.0
model initialize at round 19310
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.19074103, 30.3008973 , 29.34516238]), 'distance': 27.5, 'localFrame': array([[-0.94847556, -0.16355141, -0.27137622],
       [ 0.16992825, -0.98545644,  0.        ],
       [-0.26742944, -0.04611448,  0.96247335]]), 'currentState': array([10.61555035, 33.60888057, 52.42916254, -0.94847556, -0.16355141,
       -0.27137622]), 'targetState': array([33.61684937, 28.38850891, 16.        ]), 'previousTarget': array([26.25195012, 30.26560216, 28.50170459])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.62771521127512
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 77, 'trapConfig': [], 'currentTarget': array([33.61684937, 28.38850891, 16.        ]), 'distance': 8.237018311401307, 'localFrame': array([[ 0.04232851, -0.69693239, -0.71588655],
       [ 0.99816069,  0.06062375,  0.        ],
       [ 0.04339972, -0.71456981,  0.69821662]]), 'currentState': array([27.59759042, 32.50290294, 19.83259105,  0.04232851, -0.69693239,
       -0.71588655]), 'targetState': array([33.61684937, 28.38850891, 16.        ]), 'previousTarget': array([33.61684937, 28.38850891, 16.        ])}
episode index:19311
target thresh 86.2260807914626
target distance 35.0
model initialize at round 19311
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.7305873 , -30.72653006,  27.80495832]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.39547396, -0.83155849,  0.39001388],
       [ 0.90307383,  0.42948535,  0.        ],
       [-0.16750525,  0.35221133,  0.92080898]]), 'currentState': array([  2.3659242 , -18.23748544,  52.07330822,   0.39547396,
        -0.83155849,   0.39001388]), 'targetState': array([  7.22863083, -36.28700727,  17.        ]), 'previousTarget': array([  5.24759483, -30.10088736,  28.20733721])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6277242168353036
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.22863083, -36.28700727,  17.        ]), 'distance': 3.248481565603614, 'localFrame': array([[ 0.40073435, -0.89714224, -0.18587034],
       [ 0.91305284,  0.40784128,  0.        ],
       [ 0.0758056 , -0.16970944,  0.98257428]]), 'currentState': array([  4.90712393, -35.9916625 ,  19.25300015,   0.40073435,
        -0.89714224,  -0.18587034]), 'targetState': array([  7.22863083, -36.28700727,  17.        ]), 'previousTarget': array([  7.22863083, -36.28700727,  17.        ])}
episode index:19312
target thresh 86.22745811451617
target distance 45.06788698316138
model initialize at round 19312
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.05925905,  2.25905616, 36.63705612]), 'distance': 27.5, 'localFrame': array([[ 0.68469939,  0.47325124,  0.55427431],
       [-0.56858329,  0.8226257 ,  0.        ],
       [-0.45596029, -0.31515111,  0.83233406]]), 'currentState': array([-9.81277867, -2.18558189, 20.57128012,  0.68469939,  0.47325124,
        0.55427431]), 'targetState': array([34.33586279,  6.78590646, 53.        ]), 'previousTarget': array([10.93556548,  2.00950365, 35.34640757])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6277267024580564
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.33586279,  6.78590646, 53.        ]), 'distance': 2.270362800375073, 'localFrame': array([[ 0.40938907,  0.32547042,  0.85233186],
       [-0.62231278,  0.78276868,  0.        ],
       [-0.66717868, -0.53041701,  0.52300134]]), 'currentState': array([32.34067696,  5.7563878 , 53.33744936,  0.40938907,  0.32547042,
        0.85233186]), 'targetState': array([34.33586279,  6.78590646, 53.        ]), 'previousTarget': array([34.33586279,  6.78590646, 53.        ])}
episode index:19313
target thresh 86.22883529984429
target distance 49.0137806486447
model initialize at round 19313
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.47392188, -19.32768128,  42.04095742]), 'distance': 27.5, 'localFrame': array([[-0.51155539,  0.83279566, -0.21157141],
       [-0.8520847 , -0.52340393,  0.        ],
       [-0.11073731,  0.18027676,  0.97736254]]), 'currentState': array([ -2.80846555, -42.686252  ,  56.45808018,  -0.51155539,
         0.83279566,  -0.21157141]), 'targetState': array([-6.21144317,  5.04162412, 27.        ]), 'previousTarget': array([ -3.80124523, -20.38295131,  42.04296706])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6277261620955465
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-6.21144317,  5.04162412, 27.        ]), 'distance': 2.8140710986945066, 'localFrame': array([[-0.62106511, -0.74135137, -0.25431529],
       [ 0.76655465, -0.64217908,  0.        ],
       [-0.16331596, -0.19494657,  0.96712136]]), 'currentState': array([-4.72120015,  6.32936195, 29.00995104, -0.62106511, -0.74135137,
       -0.25431529]), 'targetState': array([-6.21144317,  5.04162412, 27.        ]), 'previousTarget': array([-6.21144317,  5.04162412, 27.        ])}
episode index:19314
target thresh 86.23021234746079
target distance 10.23006593059885
model initialize at round 19314
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.68152629,  29.37990624, 100.        ]), 'distance': 13.683198209983766, 'localFrame': array([[ 0.15022278, -0.00551111,  0.98863681],
       [ 0.03666157,  0.99932774,  0.        ],
       [-0.98797219,  0.03624497,  0.15032384]]), 'currentState': array([ 3.17906754e+00,  2.06080746e+01,  9.55291879e+01,  1.50222783e-01,
       -5.51110731e-03,  9.88636811e-01]), 'targetState': array([ 12.68152629,  29.37990624, 100.        ]), 'previousTarget': array([ 12.68152629,  29.37990624, 100.        ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6277355849123275
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([ 12.68152629,  29.37990624, 100.        ]), 'distance': 3.671632624663251, 'localFrame': array([[ 0.46981164,  0.76458013, -0.44124171],
       [-0.85200591,  0.52353217,  0.        ],
       [ 0.23100423,  0.37594055,  0.89738829]]), 'currentState': array([10.18216823, 26.73721131, 99.49974118,  0.46981164,  0.76458013,
       -0.44124171]), 'targetState': array([ 12.68152629,  29.37990624, 100.        ]), 'previousTarget': array([ 12.68152629,  29.37990624, 100.        ])}
episode index:19315
target thresh 86.2315892573794
target distance 36.0
model initialize at round 19315
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.34884911, -2.62564731, 30.99190068]), 'distance': 27.499999999999996, 'localFrame': array([[-0.21408112,  0.8144782 ,  0.53925369],
       [-0.96714909, -0.25420982,  0.        ],
       [ 0.13708358, -0.52153871,  0.84214337]]), 'currentState': array([ 5.6079026 ,  3.40388403,  5.36798891, -0.21408112,  0.8144782 ,
        0.53925369]), 'targetState': array([-5.14604925, -4.74533214, 40.        ]), 'previousTarget': array([-1.80685879, -2.64882703, 29.6868315 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6277030866940155
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.14604925, -4.74533214, 40.        ]), 'distance': 15.839466561758698, 'localFrame': array([[-0.91455043, -0.06970354,  0.39842055],
       [ 0.07599577, -0.99710814,  0.        ],
       [ 0.39726837,  0.03027828,  0.91720285]]), 'currentState': array([-5.7463853 , -9.19096418, 24.80906016, -0.91455043, -0.06970354,
        0.39842055]), 'targetState': array([-5.14604925, -4.74533214, 40.        ]), 'previousTarget': array([-5.14604925, -4.74533214, 40.        ])}
episode index:19316
target thresh 86.2329660296139
target distance 27.59729122043911
model initialize at round 19316
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.76996309,  4.75040308, 11.28881729]), 'distance': 27.500000000000004, 'localFrame': array([[-0.33111131,  0.7530825 ,  0.56853501],
       [-0.91542478, -0.4024891 ,  0.        ],
       [ 0.22882914, -0.52045103,  0.82265907]]), 'currentState': array([ -0.49228553, -13.4677867 ,  15.00215024,  -0.33111131,
         0.7530825 ,   0.56853501]), 'targetState': array([26.80254989, 11.07354142, 10.        ]), 'previousTarget': array([19.09217696,  3.79477337, 11.11755503])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.627711675563765
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.80254989, 11.07354142, 10.        ]), 'distance': 2.1559416765817705, 'localFrame': array([[ 0.86363239,  0.49981328, -0.06577067],
       [-0.50089784,  0.86550642,  0.        ],
       [ 0.05692494,  0.03294439,  0.99783477]]), 'currentState': array([25.08620433,  9.7817715 , 10.18322915,  0.86363239,  0.49981328,
       -0.06577067]), 'targetState': array([26.80254989, 11.07354142, 10.        ]), 'previousTarget': array([26.80254989, 11.07354142, 10.        ])}
episode index:19317
target thresh 86.23434266417806
target distance 75.0
model initialize at round 19317
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.5656536 , -15.04963912,  41.83911545]), 'distance': 27.5, 'localFrame': array([[-0.80498196,  0.42657467,  0.41235677],
       [-0.46823728, -0.88360277,  0.        ],
       [ 0.36435958, -0.19308081,  0.91102244]]), 'currentState': array([  8.18407657, -21.86920509,  19.02051689,  -0.80498196,
         0.42657467,   0.41235677]), 'targetState': array([-36.99607147,   0.53916196,  94.        ]), 'previousTarget': array([ -4.48225492, -15.99732764,  41.57351203])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6277089652974026
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-36.99607147,   0.53916196,  94.        ]), 'distance': 2.661219358933364, 'localFrame': array([[-0.87197104, -0.4895386 ,  0.00429759],
       [ 0.48954312, -0.87197909,  0.        ],
       [ 0.00374741,  0.00210385,  0.99999077]]), 'currentState': array([-3.51415586e+01, -1.35721134e+00,  9.42159600e+01, -8.71971035e-01,
       -4.89538604e-01,  4.29758655e-03]), 'targetState': array([-36.99607147,   0.53916196,  94.        ]), 'previousTarget': array([-36.99607147,   0.53916196,  94.        ])}
episode index:19318
target thresh 86.23571916108565
target distance 22.744485965822527
model initialize at round 19318
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.93126618, -33.50327884,  65.        ]), 'distance': 27.48493888550959, 'localFrame': array([[-0.05880042, -0.75075398,  0.6579597 ],
       [ 0.99694689, -0.0780827 ,  0.        ],
       [ 0.05137527,  0.65595088,  0.75305314]]), 'currentState': array([-11.47264674, -12.49441675,  48.49739978,  -0.05880042,
        -0.75075398,   0.6579597 ]), 'targetState': array([-17.93126618, -33.50327884,  65.        ]), 'previousTarget': array([-17.60817564, -32.29634946,  64.09790006])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.627710072687178
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([-17.93126618, -33.50327884,  65.        ]), 'distance': 1.8524026759905252, 'localFrame': array([[-0.54408574, -0.21328738,  0.81146731],
       [ 0.36496951, -0.93101947,  0.        ],
       [ 0.75549187,  0.29616083,  0.58439781]]), 'currentState': array([-17.40790868, -33.77223642,  63.24353923,  -0.54408574,
        -0.21328738,   0.81146731]), 'targetState': array([-17.93126618, -33.50327884,  65.        ]), 'previousTarget': array([-17.93126618, -33.50327884,  65.        ])}
episode index:19319
target thresh 86.23709552035044
target distance 47.36478442341233
model initialize at round 19319
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.9423624 ,  5.57457032, 70.70374696]), 'distance': 27.499999999999996, 'localFrame': array([[-0.21008441, -0.71072025,  0.67137267],
       [ 0.95898144, -0.28346885,  0.        ],
       [ 0.19031324,  0.64383394,  0.74111992]]), 'currentState': array([-16.07371674,  20.35407909,  86.46030658,  -0.21008441,
        -0.71072025,   0.67137267]), 'targetState': array([ 30.86064166, -20.4112909 ,  43.        ]), 'previousTarget': array([ 0.66869346,  6.22059683, 69.77224947])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6277056200475962
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 30.86064166, -20.4112909 ,  43.        ]), 'distance': 1.769672696069278, 'localFrame': array([[ 0.96070694, -0.01097268,  0.27734774],
       [ 0.01142072,  0.99993478,  0.        ],
       [-0.27732966,  0.00316751,  0.9607696 ]]), 'currentState': array([ 2.94560301e+01, -2.02485924e+01,  4.19358867e+01,  9.60706943e-01,
       -1.09726784e-02,  2.77347743e-01]), 'targetState': array([ 30.86064166, -20.4112909 ,  43.        ]), 'previousTarget': array([ 30.86064166, -20.4112909 ,  43.        ])}
episode index:19320
target thresh 86.23847174198616
target distance 76.0
model initialize at round 19320
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.03369095,  1.96187488, 60.39337843]), 'distance': 27.500000000000004, 'localFrame': array([[-0.01926238, -0.47052716, -0.88217524],
       [ 0.9991631 , -0.0409036 ,  0.        ],
       [-0.03608415, -0.88143695,  0.47092127]]), 'currentState': array([-1.50293668e+01,  5.68077854e+00,  8.61129735e+01, -1.92623777e-02,
       -4.70527157e-01, -8.82175241e-01]), 'targetState': array([10.89235878, -5.03552581, 12.        ]), 'previousTarget': array([-6.18106953,  2.03930701, 62.19369651])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6277038220467864
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([10.89235878, -5.03552581, 12.        ]), 'distance': 2.3537851522046096, 'localFrame': array([[-0.58378559,  0.52347651, -0.62061802],
       [-0.66760336, -0.74451712,  0.        ],
       [-0.46206074,  0.41432668,  0.78411305]]), 'currentState': array([11.25484599, -4.63022715, 14.29011802, -0.58378559,  0.52347651,
       -0.62061802]), 'targetState': array([10.89235878, -5.03552581, 12.        ]), 'previousTarget': array([10.89235878, -5.03552581, 12.        ])}
episode index:19321
target thresh 86.23984782600662
target distance 32.8240406395698
model initialize at round 19321
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.47023095, -5.59548018, 11.74192912]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.02036849,  0.40558823, -0.91382893],
       [-0.99874138,  0.05015641,  0.        ],
       [ 0.04583438,  0.91267876,  0.40609936]]), 'currentState': array([ 1.15974280e+01, -2.63399062e+01,  2.68563776e+01,  2.03684856e-02,
        4.05588233e-01, -9.13828928e-01]), 'targetState': array([26.52728222,  5.03023838,  4.        ]), 'previousTarget': array([20.94055239, -7.00606992, 12.80060448])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6277042610108697
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([26.52728222,  5.03023838,  4.        ]), 'distance': 2.7204629234773274, 'localFrame': array([[ 0.01584654,  0.89693019,  0.44188814],
       [-0.99984397,  0.01766478,  0.        ],
       [-0.00780586, -0.44181919,  0.89707016]]), 'currentState': array([2.41079428e+01, 3.78696171e+00, 3.95552001e+00, 1.58465447e-02,
       8.96930185e-01, 4.41888142e-01]), 'targetState': array([26.52728222,  5.03023838,  4.        ]), 'previousTarget': array([26.52728222,  5.03023838,  4.        ])}
episode index:19322
target thresh 86.24122377242556
target distance 53.0
model initialize at round 19322
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.96587861,  19.03204806,  57.49131507]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.79249028,  0.15214948, -0.59060112],
       [-0.18854564,  0.98206443,  0.        ],
       [ 0.58000836,  0.11135527,  0.80696364]]), 'currentState': array([-26.97237926,  27.21731657,  79.6964793 ,   0.79249028,
         0.15214948,  -0.59060112]), 'targetState': array([ 6.26733307,  7.7923383 , 27.        ]), 'previousTarget': array([-14.1259031 ,  18.71005828,  57.96790182])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6277063966780829
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.26733307,  7.7923383 , 27.        ]), 'distance': 2.828797487209081, 'localFrame': array([[ 0.56420581, -0.82510891, -0.02944655],
       [ 0.82546687,  0.56445058,  0.        ],
       [ 0.01662112, -0.02430715,  0.99956636]]), 'currentState': array([ 6.02370363e+00,  8.36861723e+00,  2.97587393e+01,  5.64205806e-01,
       -8.25108907e-01, -2.94465527e-02]), 'targetState': array([ 6.26733307,  7.7923383 , 27.        ]), 'previousTarget': array([ 6.26733307,  7.7923383 , 27.        ])}
episode index:19323
target thresh 86.24259958125673
target distance 37.0
model initialize at round 19323
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.52252294, -14.11001613,  49.04312044]), 'distance': 27.5, 'localFrame': array([[ 0.5709161 ,  0.75603849, -0.32009469],
       [-0.79802619,  0.60262276,  0.        ],
       [ 0.19289635,  0.25544395,  0.94738555]]), 'currentState': array([ -7.75585789, -32.76435483,  69.05839491,   0.5709161 ,
         0.75603849,  -0.32009469]), 'targetState': array([-12.87835402,   1.77425974,  32.        ]), 'previousTarget': array([-11.22174044, -14.74082756,  49.20728583])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.627710683928683
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.87835402,   1.77425974,  32.        ]), 'distance': 3.7705215324241665, 'localFrame': array([[ 0.71515391,  0.63346605,  0.29542451],
       [-0.66306106,  0.74856531,  0.        ],
       [-0.22114454, -0.19588449,  0.95536609]]), 'currentState': array([-14.78665924,  -0.51435091,  34.31029546,   0.71515391,
         0.63346605,   0.29542451]), 'targetState': array([-12.87835402,   1.77425974,  32.        ]), 'previousTarget': array([-12.87835402,   1.77425974,  32.        ])}
episode index:19324
target thresh 86.24397525251389
target distance 63.0
model initialize at round 19324
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.74659058,  -8.87192495,  31.70942961]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.75377783, -0.00881503, -0.65707022],
       [ 0.01169366,  0.99993163,  0.        ],
       [ 0.65702529, -0.00768356,  0.75382937]]), 'currentState': array([-1.00206310e+01, -1.23255331e+01,  4.56367976e+00,  7.53777833e-01,
       -8.81502794e-03, -6.57070220e-01]), 'targetState': array([-16.49128682,  -4.1276457 ,  69.        ]), 'previousTarget': array([-13.56781554,  -8.97801078,  33.1629217 ])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.627703298711935
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-16.49128682,  -4.1276457 ,  69.        ]), 'distance': 2.3825375536921873, 'localFrame': array([[-0.35646944, -0.32598518,  0.87559306],
       [ 0.67484843, -0.73795637,  0.        ],
       [ 0.64614947,  0.5908926 ,  0.48304947]]), 'currentState': array([-16.6133562 ,  -3.12398323,  66.84262986,  -0.35646944,
        -0.32598518,   0.87559306]), 'targetState': array([-16.49128682,  -4.1276457 ,  69.        ]), 'previousTarget': array([-16.49128682,  -4.1276457 ,  69.        ])}
episode index:19325
target thresh 86.2453507862108
target distance 50.0
model initialize at round 19325
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.00919769, 18.94877166, 63.77845147]), 'distance': 27.5, 'localFrame': array([[-0.56272088,  0.56406506, -0.60429779],
       [-0.70794981, -0.70626275,  0.        ],
       [-0.42679302,  0.42781251,  0.79675854]]), 'currentState': array([32.7632981 ,  9.22248386, 85.51491962, -0.56272088,  0.56406506,
       -0.60429779]), 'targetState': array([ 2.06470111, 30.93116566, 37.        ]), 'previousTarget': array([19.50535168, 18.10806522, 65.18915889])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6277072180121671
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.06470111, 30.93116566, 37.        ]), 'distance': 3.1932616897354946, 'localFrame': array([[-0.90603817, -0.41441491, -0.08576197],
       [ 0.41594741, -0.90938867,  0.        ],
       [-0.07799097, -0.03567247,  0.99631565]]), 'currentState': array([ 4.05477353, 28.65746102, 38.0328597 , -0.90603817, -0.41441491,
       -0.08576197]), 'targetState': array([ 2.06470111, 30.93116566, 37.        ]), 'previousTarget': array([ 2.06470111, 30.93116566, 37.        ])}
episode index:19326
target thresh 86.24672618236123
target distance 81.0
model initialize at round 19326
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.36175093,  6.24920908, 29.68554515]), 'distance': 27.500000000000004, 'localFrame': array([[-0.63887279, -0.49010512, -0.59299117],
       [ 0.60866824, -0.79342484,  0.        ],
       [-0.47049393, -0.36093489,  0.80520896]]), 'currentState': array([-1.79500777,  2.82919736,  3.46917778, -0.63887279, -0.49010512,
       -0.59299117]), 'targetState': array([-25.61560526,  13.59561573,  86.        ]), 'previousTarget': array([-9.22262322,  6.92951773, 31.17064392])}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: 0.6276958925584708
{'scaleFactor': 20, 'timeStep': 90, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([-25.61560526,  13.59561573,  86.        ]), 'distance': 1.9138358028161626, 'localFrame': array([[-0.17862583, -0.62969437,  0.75602765],
       [ 0.96204156, -0.27290298,  0.        ],
       [ 0.2063222 ,  0.72733002,  0.65453968]]), 'currentState': array([-26.94623563,  13.73208262,  84.63121693,  -0.17862583,
        -0.62969437,   0.75602765]), 'targetState': array([-25.61560526,  13.59561573,  86.        ]), 'previousTarget': array([-25.61560526,  13.59561573,  86.        ])}
episode index:19327
target thresh 86.24810144097891
target distance 31.107044765398317
model initialize at round 19327
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.27527785, 11.68403112, 33.34475254]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.69383617,  0.34302751, -0.6331852 ],
       [-0.44318781,  0.89642878,  0.        ],
       [ 0.56760544,  0.28061996,  0.77400032]]), 'currentState': array([ 0.77651221,  4.93868523, 51.52574391,  0.69383617,  0.34302751,
       -0.6331852 ]), 'targetState': array([31.36982716, 15.52204703, 23.        ]), 'previousTarget': array([19.3881932 , 11.08072233, 34.55522877])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6277020740672546
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([31.36982716, 15.52204703, 23.        ]), 'distance': 2.204127422172505, 'localFrame': array([[ 0.53195001, -0.84631431,  0.02795132],
       [ 0.8466451 ,  0.53215793,  0.        ],
       [-0.01487451,  0.02366484,  0.99960929]]), 'currentState': array([ 2.95662000e+01,  1.47598340e+01,  2.40119970e+01,  5.31950012e-01,
       -8.46314308e-01,  2.79513159e-02]), 'targetState': array([31.36982716, 15.52204703, 23.        ]), 'previousTarget': array([31.36982716, 15.52204703, 23.        ])}
episode index:19328
target thresh 86.24947656207762
target distance 41.97127151489258
model initialize at round 19328
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.63140322, 12.98233839, 92.95989142]), 'distance': 27.500000000000004, 'localFrame': array([[-0.27443188, -0.95812088, -0.08180175],
       [ 0.9613427 , -0.2753547 ,  0.        ],
       [-0.0225245 , -0.07863952,  0.99664862]]), 'currentState': array([ 1.95236897e+00,  4.01428336e+01,  9.70602022e+01, -2.74431884e-01,
       -9.58120877e-01, -8.18017498e-02]), 'targetState': array([ 0., -0., 91.]), 'previousTarget': array([ 0.54643192, 14.76628131, 93.11091265])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6277098405986495
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.11022302e-16, 0.00000000e+00, 9.10000000e+01]), 'distance': 3.5670545966477523, 'localFrame': array([[-0.7702601 , -0.45541426, -0.44642718],
       [ 0.50894513, -0.86079896,  0.        ],
       [-0.38428405, -0.22720694,  0.89481997]]), 'currentState': array([ 0.98115027, -2.5844983 , 93.25423849, -0.7702601 , -0.45541426,
       -0.44642718]), 'targetState': array([ 0., -0., 91.]), 'previousTarget': array([2.22044605e-16, 0.00000000e+00, 9.10000000e+01])}
episode index:19329
target thresh 86.2508515456711
target distance 36.665785206636485
model initialize at round 19329
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.5200793 , -2.82529559, 41.39569973]), 'distance': 27.5, 'localFrame': array([[ 0.94002888,  0.20984955, -0.2689031 ],
       [-0.21787447,  0.9759768 ,  0.        ],
       [ 0.26244319,  0.05858712,  0.96316723]]), 'currentState': array([-23.83521428, -19.78222663,  57.63729448,   0.94002888,
         0.20984955,  -0.2689031 ]), 'targetState': array([ 7.57504369, 17.42465819, 22.        ]), 'previousTarget': array([-10.66290869,  -2.75111661,  41.80941875])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6277130345585704
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.57504369, 17.42465819, 22.        ]), 'distance': 3.1435079423730796, 'localFrame': array([[-0.94624772,  0.0757066 , -0.31445789],
       [-0.07975232, -0.99681471,  0.        ],
       [-0.31345625,  0.02507875,  0.94927142]]), 'currentState': array([ 5.90748875, 17.05961236, 24.63962957, -0.94624772,  0.0757066 ,
       -0.31445789]), 'targetState': array([ 7.57504369, 17.42465819, 22.        ]), 'previousTarget': array([ 7.57504369, 17.42465819, 22.        ])}
episode index:19330
target thresh 86.25222639177306
target distance 74.5413649190087
model initialize at round 19330
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.79327213, 15.09208481, 27.67496117]), 'distance': 27.5, 'localFrame': array([[ 0.30211964, -0.93341867, -0.19352857],
       [ 0.95140533,  0.30794138,  0.        ],
       [ 0.05959545, -0.18412411,  0.98109464]]), 'currentState': array([-31.58995518,  27.82578503,  22.40042283,   0.30211964,
        -0.93341867,  -0.19352857]), 'targetState': array([ 43.30093599, -12.24863021,  39.        ]), 'previousTarget': array([-7.66339796, 16.32738857, 28.06071355])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276805627239752
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 47, 'trapConfig': [], 'currentTarget': array([ 43.30093599, -12.24863021,  39.        ]), 'distance': 16.223148309986655, 'localFrame': array([[ 0.90194488, -0.13894621,  0.40888799],
       [ 0.15225571,  0.98834114,  0.        ],
       [-0.40412082,  0.06225553,  0.91258458]]), 'currentState': array([29.84434315, -3.85175626, 42.40634086,  0.90194488, -0.13894621,
        0.40888799]), 'targetState': array([ 43.30093599, -12.24863021,  39.        ]), 'previousTarget': array([ 43.30093599, -12.24863021,  39.        ])}
episode index:19331
target thresh 86.25360110039732
target distance 60.0
model initialize at round 19331
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.16621061,  0.15060369, 59.68727652]), 'distance': 27.5, 'localFrame': array([[ 0.08010392,  0.34090119, -0.93668017],
       [-0.97348596,  0.22874676,  0.        ],
       [ 0.21426255,  0.911845  ,  0.35018604]]), 'currentState': array([ 3.09506939e+00,  7.25058742e+00,  8.15665117e+01,  8.01039215e-02,
        3.40901191e-01, -9.36680169e-01]), 'targetState': array([ 43.4376207 , -11.75470579,  23.        ]), 'previousTarget': array([ 1.71940743e+01, -5.63473738e-02,  6.10534673e+01])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.62768133507238
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 43.4376207 , -11.75470579,  23.        ]), 'distance': 4.260618611868793, 'localFrame': array([[ 0.94838266, -0.31562081, -0.03088411],
       [ 0.31577144,  0.94883528,  0.        ],
       [ 0.02930393, -0.00975232,  0.99952297]]), 'currentState': array([ 4.13941874e+01, -8.82752854e+00,  2.53257009e+01,  9.48382664e-01,
       -3.15620808e-01, -3.08841102e-02]), 'targetState': array([ 43.4376207 , -11.75470579,  23.        ]), 'previousTarget': array([ 43.4376207 , -11.75470579,  23.        ])}
episode index:19332
target thresh 86.25497567155757
target distance 28.47367266609269
model initialize at round 19332
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.88048644, -22.60316038,  76.81175925]), 'distance': 27.499999999999996, 'localFrame': array([[-0.70673855, -0.25850813,  0.65855461],
       [ 0.34351736, -0.93914633,  0.        ],
       [ 0.61847914,  0.22622494,  0.75253294]]), 'currentState': array([ 22.97446442, -10.79664775,  92.69496624,  -0.70673855,
        -0.25850813,   0.65855461]), 'targetState': array([ -4.30826167, -27.66656613,  70.        ]), 'previousTarget': array([  4.71931545, -21.93646236,  76.97509939])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.62768991795908
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.30826167, -27.66656613,  70.        ]), 'distance': 3.1024606966838655, 'localFrame': array([[-0.16689507,  0.44654378, -0.87905898],
       [-0.93671403, -0.35009546,  0.        ],
       [-0.30775456,  0.82342687,  0.47671303]]), 'currentState': array([ -2.78863522, -26.15912331,  72.24579918,  -0.16689507,
         0.44654378,  -0.87905898]), 'targetState': array([ -4.30826167, -27.66656613,  70.        ]), 'previousTarget': array([ -4.30826167, -27.66656613,  70.        ])}
episode index:19333
target thresh 86.25635010526759
target distance 16.130295796093478
model initialize at round 19333
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 24.64398446, -26.24259953,  35.        ]), 'distance': 16.015610476171844, 'localFrame': array([[-0.02051129, -0.81086438, -0.58487455],
       [ 0.99968022, -0.0252875 ,  0.        ],
       [-0.01479002, -0.58468752,  0.81112376]]), 'currentState': array([ 1.99756939e+01, -1.09999382e+01,  3.34611316e+01, -2.05112930e-02,
       -8.10864382e-01, -5.84874552e-01]), 'targetState': array([ 24.64398446, -26.24259953,  35.        ]), 'previousTarget': array([ 24.64398446, -26.24259953,  35.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6277047016215154
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 24.64398446, -26.24259953,  35.        ]), 'distance': 1.8985395378234455, 'localFrame': array([[ 0.20581569, -0.95002528, -0.23471657],
       [ 0.97732806,  0.21173063,  0.        ],
       [ 0.04969669, -0.22939509,  0.97206385]]), 'currentState': array([ 25.13248682, -24.68074947,  34.03748132,   0.20581569,
        -0.95002528,  -0.23471657]), 'targetState': array([ 24.64398446, -26.24259953,  35.        ]), 'previousTarget': array([ 24.64398446, -26.24259953,  35.        ])}
episode index:19334
target thresh 86.2577244015411
target distance 55.28724686320131
model initialize at round 19334
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.67663758,  7.39032973, 57.11786044]), 'distance': 27.5, 'localFrame': array([[ 0.62785372,  0.01867432,  0.77810731],
       [-0.02972996,  0.99955797,  0.        ],
       [-0.77776336, -0.0231331 ,  0.62813137]]), 'currentState': array([-8.78965762e+00,  1.24382382e+01,  5.16129682e+01,  6.27853717e-01,
        1.86743184e-02,  7.78107306e-01]), 'targetState': array([45.95665375,  1.99649085, 63.        ]), 'previousTarget': array([17.0535049 ,  7.09271967, 56.20384073])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276722369356286
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([45.95665375,  1.99649085, 63.        ]), 'distance': 15.978478087153693, 'localFrame': array([[ 0.47052333, -0.51168779, -0.71887649],
       [ 0.73609544,  0.67687776,  0.        ],
       [ 0.4865915 , -0.52916171,  0.69513783]]), 'currentState': array([31.95786426,  9.37960884, 65.19891417,  0.47052333, -0.51168779,
       -0.71887649]), 'targetState': array([45.95665375,  1.99649085, 63.        ]), 'previousTarget': array([45.95665375,  1.99649085, 63.        ])}
episode index:19335
target thresh 86.25909856039186
target distance 57.0
model initialize at round 19335
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.98143605,  -8.74358017,  48.42328437]), 'distance': 27.5, 'localFrame': array([[-0.0455565 ,  0.99655815, -0.06925646],
       [-0.99895676, -0.04566615,  0.        ],
       [-0.00316268,  0.06918421,  0.99759889]]), 'currentState': array([-3.21061609e+01, -2.07762907e+01,  7.17782137e+01, -4.55565010e-02,
        9.96558151e-01, -6.92564579e-02]), 'targetState': array([-12.00624608,   8.99166587,  14.        ]), 'previousTarget': array([-23.86243675,  -9.73757755,  47.96956731])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.627671064709222
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-12.00624608,   8.99166587,  14.        ]), 'distance': 3.311458178930568, 'localFrame': array([[ 0.30485294,  0.6659863 , -0.68082812],
       [-0.9092668 ,  0.41621375,  0.        ],
       [ 0.28337002,  0.61905441,  0.73244322]]), 'currentState': array([-11.63880415,  10.59065081,  16.87645421,   0.30485294,
         0.6659863 ,  -0.68082812]), 'targetState': array([-12.00624608,   8.99166587,  14.        ]), 'previousTarget': array([-12.00624608,   8.99166587,  14.        ])}
episode index:19336
target thresh 86.2604725818336
target distance 49.069747837810255
model initialize at round 19336
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.95379662,  -9.73129568,  77.0760253 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.64429171,  0.695672  , -0.31769271],
       [-0.73368129,  0.67949375,  0.        ],
       [ 0.21587021,  0.2330852 ,  0.94819373]]), 'currentState': array([-40.94998671, -10.60448664,  92.13134924,   0.64429171,
         0.695672  ,  -0.31769271]), 'targetState': array([ 6.60145942, -8.79890525, 61.        ]), 'previousTarget': array([-19.45816466, -10.23787089,  77.99433984])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6276753509047313
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.60145942, -8.79890525, 61.        ]), 'distance': 3.200272401351992, 'localFrame': array([[ 0.63963277, -0.61185601,  0.46529791],
       [ 0.69124233,  0.72262303,  0.        ],
       [-0.33623499,  0.32163361,  0.88515414]]), 'currentState': array([ 4.604514  , -8.06458338, 63.39054886,  0.63963277, -0.61185601,
        0.46529791]), 'targetState': array([ 6.60145942, -8.79890525, 61.        ]), 'previousTarget': array([ 6.60145942, -8.79890525, 61.        ])}
episode index:19337
target thresh 86.26184646588008
target distance 46.0
model initialize at round 19337
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.69019722, -19.39137999,  69.25298508]), 'distance': 27.5, 'localFrame': array([[-0.73021859,  0.45826811, -0.50672592],
       [-0.53156742, -0.84701598,  0.        ],
       [-0.42920495,  0.26935899,  0.86210721]]), 'currentState': array([ 25.81741633, -36.80375897,  90.34341391,  -0.73021859,
         0.45826811,  -0.50672592]), 'targetState': array([31.99375793,  0.63202357, 45.        ]), 'previousTarget': array([ 29.59607149, -19.52628247,  69.74064393])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6276781888013133
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([31.99375793,  0.63202357, 45.        ]), 'distance': 4.250112560558896, 'localFrame': array([[ 0.03089543,  0.95817576,  0.2845078 ],
       [-0.99948057,  0.03222726,  0.        ],
       [-0.00916891, -0.28436002,  0.95867372]]), 'currentState': array([ 3.05282866e+01, -2.23534190e+00,  4.77738179e+01,  3.08954278e-02,
        9.58175758e-01,  2.84507802e-01]), 'targetState': array([31.99375793,  0.63202357, 45.        ]), 'previousTarget': array([31.99375793,  0.63202357, 45.        ])}
episode index:19338
target thresh 86.263220212545
target distance 40.417160790645184
model initialize at round 19338
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.81687764, -22.87023824,   1.87500244]), 'distance': 27.500000000000004, 'localFrame': array([[-0.18317672,  0.5180205 , -0.83552442],
       [-0.94279256, -0.33337995,  0.        ],
       [-0.27854709,  0.7877262 ,  0.54945332]]), 'currentState': array([ 29.86805419, -31.67583376,   1.65662675,  -0.18317672,
         0.5180205 ,  -0.83552442]), 'targetState': array([-11.09473495, -17.82994269,   2.        ]), 'previousTarget': array([  3.54255137, -23.25338877,   2.36215523])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6276847579671648
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-11.09473495, -17.82994269,   2.        ]), 'distance': 1.7406347394531458, 'localFrame': array([[-0.91941237, -0.35250368,  0.1744192 ],
       [ 0.35799115, -0.93372498,  0.        ],
       [ 0.16285956,  0.06244053,  0.98467149]]), 'currentState': array([ -9.55200293, -18.06505101,   1.22895442,  -0.91941237,
        -0.35250368,   0.1744192 ]), 'targetState': array([-11.09473495, -17.82994269,   2.        ]), 'previousTarget': array([-11.09473495, -17.82994269,   2.        ])}
episode index:19339
target thresh 86.26459382184214
target distance 35.781021682807676
model initialize at round 19339
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.04596966,  10.53052446,  49.64355106]), 'distance': 27.5, 'localFrame': array([[-0.98985969,  0.12589433,  0.06579065],
       [-0.12616768, -0.99200893,  0.        ],
       [ 0.06526491, -0.00830065,  0.99783345]]), 'currentState': array([-32.97558188,  18.01374162,  32.23487456,  -0.98985969,
         0.12589433,   0.06579065]), 'targetState': array([ 4.53423366,  3.92946881, 65.        ]), 'previousTarget': array([-11.56271799,  10.20474363,  50.60403173])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.627689788762304
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.53423366,  3.92946881, 65.        ]), 'distance': 2.1507128662634694, 'localFrame': array([[ 0.24094765, -0.85887598,  0.45196934],
       [ 0.96282932,  0.27011054,  0.        ],
       [-0.12208168,  0.43516933,  0.89203347]]), 'currentState': array([ 4.14664982,  4.42485298, 62.94331842,  0.24094765, -0.85887598,
        0.45196934]), 'targetState': array([ 4.53423366,  3.92946881, 65.        ]), 'previousTarget': array([ 4.53423366,  3.92946881, 65.        ])}
episode index:19340
target thresh 86.26596729378522
target distance 23.16334409858381
model initialize at round 19340
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.36893512, 21.46230979, 24.        ]), 'distance': 25.975111624289447, 'localFrame': array([[ 0.7266305 ,  0.00656521, -0.6869971 ],
       [-0.00903477,  0.99995919,  0.        ],
       [ 0.68696906,  0.00620686,  0.72666016]]), 'currentState': array([ 9.94757144e+00, -8.31930167e-01,  2.88355985e+01,  7.26630499e-01,
        6.56520933e-03, -6.86997100e-01]), 'targetState': array([22.36893512, 21.46230979, 24.        ]), 'previousTarget': array([22.36893512, 21.46230979, 24.        ])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6277013583803348
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.36893512, 21.46230979, 24.        ]), 'distance': 3.9336885370080914, 'localFrame': array([[ 0.41491383,  0.8208023 ,  0.39259405],
       [-0.89245593,  0.45113459,  0.        ],
       [-0.17711276, -0.35037289,  0.91971186]]), 'currentState': array([22.86395674, 18.65967776, 26.71553178,  0.41491383,  0.8208023 ,
        0.39259405]), 'targetState': array([22.36893512, 21.46230979, 24.        ]), 'previousTarget': array([22.36893512, 21.46230979, 24.        ])}
episode index:19341
target thresh 86.26734062838797
target distance 28.08172402154512
model initialize at round 19341
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.73655035,  18.64302679,  15.78710744]), 'distance': 27.499999999999996, 'localFrame': array([[-0.10886361,  0.98689491,  0.1191098 ],
       [-0.9939709 , -0.10964415,  0.        ],
       [ 0.01305969, -0.11839167,  0.99288109]]), 'currentState': array([-11.59396136,  -1.74193189,   5.23222904,  -0.10886361,
         0.98689491,   0.1191098 ]), 'targetState': array([-31.34593661,  24.8481842 ,  19.        ]), 'previousTarget': array([-25.72436368,  17.27022976,  15.22204954])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6277087175876259
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-31.34593661,  24.8481842 ,  19.        ]), 'distance': 2.37547498925607, 'localFrame': array([[-0.88841776,  0.20701297, -0.40970662],
       [-0.22693383, -0.97391018,  0.        ],
       [-0.39901745,  0.09297629,  0.91221735]]), 'currentState': array([-31.60916559,  22.49293075,  18.83760197,  -0.88841776,
         0.20701297,  -0.40970662]), 'targetState': array([-31.34593661,  24.8481842 ,  19.        ]), 'previousTarget': array([-31.34593661,  24.8481842 ,  19.        ])}
episode index:19342
target thresh 86.26871382566412
target distance 36.0
model initialize at round 19342
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.90412422,  8.76375129, 28.15583102]), 'distance': 27.5, 'localFrame': array([[ 0.46555056, -0.3942959 ,  0.79233416],
       [ 0.64629417,  0.76308836,  0.        ],
       [-0.60462097,  0.51208095,  0.61008736]]), 'currentState': array([ 3.23130799,  2.00828661,  6.59251661,  0.46555056, -0.3942959 ,
        0.79233416]), 'targetState': array([28.23962322, 12.78763779, 41.        ]), 'previousTarget': array([17.81600489,  8.54061199, 26.64233892])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6277156779348708
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.23962322, 12.78763779, 41.        ]), 'distance': 3.096010694814426, 'localFrame': array([[ 0.95648932, -0.26388925, -0.12446143],
       [ 0.26595721,  0.96398484,  0.        ],
       [ 0.11997893, -0.03310142,  0.99222445]]), 'currentState': array([26.49419501, 13.59192646, 38.57268001,  0.95648932, -0.26388925,
       -0.12446143]), 'targetState': array([28.23962322, 12.78763779, 41.        ]), 'previousTarget': array([28.23962322, 12.78763779, 41.        ])}
episode index:19343
target thresh 86.2700868856274
target distance 77.0
model initialize at round 19343
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.74339034, -2.55696349, 65.36164264]), 'distance': 27.5, 'localFrame': array([[-0.60889856,  0.74013284, -0.28538731],
       [-0.77224892, -0.63532008,  0.        ],
       [-0.18131229,  0.22039004,  0.95841227]]), 'currentState': array([ 33.14461895, -20.23079745,  84.68274382,  -0.60889856,
         0.74013284,  -0.28538731]), 'targetState': array([ 0.23614097, 48.99943099,  9.        ]), 'previousTarget': array([25.39835736, -3.68898363, 66.67453318])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276832277860942
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 38, 'trapConfig': [], 'currentTarget': array([ 0.92327792, 47.73568331,  8.8744536 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13776036, -0.60676871, -0.78284981],
       [ 0.97518202, -0.22140467,  0.        ],
       [-0.17332661, -0.76342106,  0.62221072]]), 'currentState': array([14.00983295, 23.66755045,  6.48341656, -0.13776036, -0.60676871,
       -0.78284981]), 'targetState': array([ 0.23614097, 48.99943099,  9.        ]), 'previousTarget': array([ 0.86387477, 47.88378372,  8.96825683])}
episode index:19344
target thresh 86.27145980829157
target distance 73.0
model initialize at round 19344
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.80536895, 15.50306182, 43.22580588]), 'distance': 27.5, 'localFrame': array([[-0.96568943, -0.22368133,  0.13194918],
       [ 0.22565434, -0.97420743,  0.        ],
       [ 0.12854587,  0.0297749 ,  0.99125648]]), 'currentState': array([19.3141579 , 19.23093011, 18.52963664, -0.96568943, -0.22368133,
        0.13194918]), 'targetState': array([-14.92414432,   8.14063366,  92.        ]), 'previousTarget': array([ 8.86112954, 16.15194738, 43.43085924])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276507809922051
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 54, 'trapConfig': [], 'currentTarget': array([-14.92414432,   8.14063366,  92.        ]), 'distance': 16.3697558828676, 'localFrame': array([[-0.6386574 ,  0.11667437,  0.76059438],
       [-0.17971265, -0.98371915,  0.        ],
       [ 0.74821126, -0.13668843,  0.64922737]]), 'currentState': array([-12.04025772,  11.93884936,  76.34031727,  -0.6386574 ,
         0.11667437,   0.76059438]), 'targetState': array([-14.92414432,   8.14063366,  92.        ]), 'previousTarget': array([-14.92414432,   8.14063366,  92.        ])}
episode index:19345
target thresh 86.27283259367033
target distance 61.0
model initialize at round 19345
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([32.77504497, -4.31536921, 59.83489361]), 'distance': 27.499999999999996, 'localFrame': array([[-0.04335932,  0.42393733,  0.90465303],
       [-0.99481032, -0.10174687,  0.        ],
       [ 0.09204561, -0.89995817,  0.42614891]]), 'currentState': array([ 4.51438022e+01, -6.75536878e+00,  3.53949622e+01, -4.33593166e-02,
        4.23937329e-01,  9.04653033e-01]), 'targetState': array([14.97840483, -0.80460466, 95.        ]), 'previousTarget': array([32.47319674, -4.90546115, 58.65209895])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6276477803937084
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([14.97840483, -0.80460466, 95.        ]), 'distance': 3.41629219102334, 'localFrame': array([[-0.63269272, -0.08959258,  0.76920289],
       [ 0.14020647, -0.99012229,  0.        ],
       [ 0.76160493,  0.10784722,  0.63900463]]), 'currentState': array([ 1.76858692e+01, -2.88754991e+00,  9.49549633e+01, -6.32692722e-01,
       -8.95925800e-02,  7.69202892e-01]), 'targetState': array([14.97840483, -0.80460466, 95.        ]), 'previousTarget': array([14.97840483, -0.80460466, 95.        ])}
episode index:19346
target thresh 86.27420524177741
target distance 39.51271579931488
model initialize at round 19346
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.85311765, 10.06088628, 29.04265503]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.90024128, -0.11149417, -0.42087372],
       [ 0.12291016,  0.9924178 ,  0.        ],
       [ 0.41768257, -0.05172966,  0.90711924]]), 'currentState': array([ 7.67670572, 34.09117035, 15.67237732,  0.90024128, -0.11149417,
       -0.42087372]), 'targetState': array([ 7.97130387, -6.03807209, 38.        ]), 'previousTarget': array([ 7.1793958 ,  9.47171864, 29.3644155 ])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6276430572037985
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([ 7.97130387, -6.03807209, 38.        ]), 'distance': 2.684599691410673, 'localFrame': array([[-0.54769859,  0.16304379,  0.82063571],
       [-0.28531509, -0.95843377,  0.        ],
       [ 0.78652498, -0.23413976,  0.57145168]]), 'currentState': array([ 8.54538079, -5.81778385, 35.38676746, -0.54769859,  0.16304379,
        0.82063571]), 'targetState': array([ 7.97130387, -6.03807209, 38.        ]), 'previousTarget': array([ 7.97130387, -6.03807209, 38.        ])}
episode index:19347
target thresh 86.27557775262656
target distance 28.0
model initialize at round 19347
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.90947026,  18.289492  ,  34.25428669]), 'distance': 27.499999999999996, 'localFrame': array([[-0.68340128,  0.66367673, -0.30413136],
       [-0.69667831, -0.71738367,  0.        ],
       [-0.21817887,  0.21188172,  0.9526301 ]]), 'currentState': array([-7.6185427 , 10.78605192, 57.68250657, -0.68340128,  0.66367673,
       -0.30413136]), 'targetState': array([-21.61673621,  19.33175408,  31.        ]), 'previousTarget': array([-19.06874701,  17.75544525,  35.74606608])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6276516354147992
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-21.61673621,  19.33175408,  31.        ]), 'distance': 3.5028926152788724, 'localFrame': array([[-0.76387435, -0.16433938, -0.62409017],
       [ 0.21032686, -0.97763112,  0.        ],
       [-0.61012997, -0.13126293,  0.78135233]]), 'currentState': array([-19.56645128,  19.58249304,  33.82908437,  -0.76387435,
        -0.16433938,  -0.62409017]), 'targetState': array([-21.61673621,  19.33175408,  31.        ]), 'previousTarget': array([-21.61673621,  19.33175408,  31.        ])}
episode index:19348
target thresh 86.27695012623147
target distance 40.0
model initialize at round 19348
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.09666306, -2.22503613, 28.35383101]), 'distance': 27.5, 'localFrame': array([[ 0.23879849, -0.0781355 ,  0.96792052],
       [ 0.31097889,  0.95041682,  0.        ],
       [-0.91992794,  0.30100285,  0.25125659]]), 'currentState': array([ -2.56423047, -19.12761787,   6.71580011,   0.23879849,
        -0.0781355 ,   0.96792052]), 'targetState': array([-5.2755657 , 10.77814486, 45.        ]), 'previousTarget': array([-4.06105423, -3.1414604 , 26.8090572 ])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6276527440505403
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.2755657 , 10.77814486, 45.        ]), 'distance': 3.2488395222267927, 'localFrame': array([[ 0.0426818 ,  0.63244127,  0.77343151],
       [-0.99773048,  0.06733421,  0.        ],
       [-0.0520784 , -0.77167619,  0.63387988]]), 'currentState': array([-6.18130171e+00,  8.20071558e+00,  4.32417455e+01,  4.26817987e-02,
        6.32441273e-01,  7.73431510e-01]), 'targetState': array([-5.2755657 , 10.77814486, 45.        ]), 'previousTarget': array([-5.2755657 , 10.77814486, 45.        ])}
episode index:19349
target thresh 86.27832236260588
target distance 33.11795796459383
model initialize at round 19349
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.6780417 ,  -1.65836169,  42.45617427]), 'distance': 27.500000000000004, 'localFrame': array([[-0.66789343, -0.50868801,  0.54328158],
       [ 0.60590516, -0.79553688,  0.        ],
       [ 0.43220053,  0.32917711,  0.83955055]]), 'currentState': array([-13.86111088,  17.24747055,  23.68521368,  -0.66789343,
        -0.50868801,   0.54328158]), 'targetState': array([-25.2335034 , -14.29231633,  55.        ]), 'previousTarget': array([-20.15854234,  -0.30965578,  41.48935004])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6276605047024962
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.2335034 , -14.29231633,  55.        ]), 'distance': 4.473002866575857, 'localFrame': array([[ 0.39756822, -0.04248434,  0.91658856],
       [ 0.10625554,  0.99433886,  0.        ],
       [-0.91139962,  0.09739261,  0.39983172]]), 'currentState': array([-2.77719883e+01, -1.16077453e+01,  5.24787052e+01,  3.97568220e-01,
       -4.24843358e-02,  9.16588562e-01]), 'targetState': array([-25.2335034 , -14.29231633,  55.        ]), 'previousTarget': array([-25.2335034 , -14.29231633,  55.        ])}
episode index:19350
target thresh 86.27969446176351
target distance 61.60927106212375
model initialize at round 19350
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.95026588,  13.58808255,   6.78996386]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.79107724,  0.49854861,  0.35446591],
       [-0.53316776,  0.84600954,  0.        ],
       [-0.29988154, -0.1889898 ,  0.93506894]]), 'currentState': array([-41.8226643 ,  -1.66195914,   6.05537588,   0.79107724,
         0.49854861,   0.35446591]), 'targetState': array([18.72585965, 38.70842518,  8.        ]), 'previousTarget': array([-20.14743195,  12.28816634,   6.73807007])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6276612773290986
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.72585965, 38.70842518,  8.        ]), 'distance': 4.1320520926592055, 'localFrame': array([[ 0.66442676,  0.67770515, -0.31504414],
       [-0.7140676 ,  0.70007676,  0.        ],
       [ 0.22055508,  0.22496281,  0.94907702]]), 'currentState': array([16.67985934, 35.85245937,  5.82486862,  0.66442676,  0.67770515,
       -0.31504414]), 'targetState': array([18.72585965, 38.70842518,  8.        ]), 'previousTarget': array([18.72585965, 38.70842518,  8.        ])}
episode index:19351
target thresh 86.2810664237181
target distance 25.66259915970684
model initialize at round 19351
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.34061551, 22.66845246, 42.69717489]), 'distance': 27.500000000000004, 'localFrame': array([[-0.37488315,  0.66726975, -0.64359436],
       [-0.87183004, -0.48980851,  0.        ],
       [-0.31523799,  0.5611049 ,  0.76536677]]), 'currentState': array([ 2.87258644,  4.98065683, 32.58165276, -0.37488315,  0.66726975,
       -0.64359436]), 'targetState': array([29.19633448, 30.19228466, 47.        ]), 'previousTarget': array([22.08019887, 22.96319521, 43.33793279])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6276678429559008
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.19633448, 30.19228466, 47.        ]), 'distance': 2.83805961717886, 'localFrame': array([[-0.40027863,  0.90148955,  0.16460134],
       [-0.91395575, -0.40581386,  0.        ],
       [ 0.0667975 , -0.15043834,  0.98636018]]), 'currentState': array([28.96518756, 31.13514287, 44.33313445, -0.40027863,  0.90148955,
        0.16460134]), 'targetState': array([29.19633448, 30.19228466, 47.        ]), 'previousTarget': array([29.19633448, 30.19228466, 47.        ])}
episode index:19352
target thresh 86.28243824848334
target distance 73.25482754904763
model initialize at round 19352
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.28068308,  -1.84985638,  20.83457081]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13885777,  0.70045213, -0.70006095],
       [-0.98091123, -0.19445604,  0.        ],
       [-0.13613108,  0.68669765,  0.71408309]]), 'currentState': array([-31.8415028 , -25.61420057,  26.64247519,  -0.13885777,
         0.70045213,  -0.70006095]), 'targetState': array([ 6.3140755 , 46.57394605,  9.        ]), 'previousTarget': array([-19.90295715,  -2.98715411,  21.85459177])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6276609794627289
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([ 6.3140755 , 46.57394605,  9.        ]), 'distance': 3.596458353361109, 'localFrame': array([[ 0.61158799,  0.71236072, -0.34424168],
       [-0.75873371,  0.65140092,  0.        ],
       [ 0.22423934,  0.26118776,  0.93888107]]), 'currentState': array([ 4.14669189, 43.78524219,  9.67830066,  0.61158799,  0.71236072,
       -0.34424168]), 'targetState': array([ 6.3140755 , 46.57394605,  9.        ]), 'previousTarget': array([ 6.3140755 , 46.57394605,  9.        ])}
episode index:19353
target thresh 86.28380993607297
target distance 35.0
model initialize at round 19353
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.75762766, -2.60908196, 85.84508259]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.46850047,  0.27999138,  0.83792132],
       [-0.51300105,  0.85838798,  0.        ],
       [-0.71926159, -0.42985452,  0.54579104]]), 'currentState': array([ 1.71752411, -2.2542224 , 58.4231561 ,  0.46850047,  0.27999138,
        0.83792132]), 'targetState': array([ 4.21553382, -2.68873105, 92.        ]), 'previousTarget': array([ 3.40692073, -2.52297753, 84.34164738])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276285489067993
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.21553382, -2.68873105, 92.        ]), 'distance': 26.101756075072593, 'localFrame': array([[ 0.32044074, -0.58474559, -0.74524514],
       [ 0.87695539,  0.48057179,  0.        ],
       [ 0.3581438 , -0.65354674,  0.66679058]]), 'currentState': array([-1.3759521 , -7.19666708, 66.90586785,  0.32044074, -0.58474559,
       -0.74524514]), 'targetState': array([ 4.21553382, -2.68873105, 92.        ]), 'previousTarget': array([ 4.21553382, -2.68873105, 92.        ])}
episode index:19354
target thresh 86.2851814865007
target distance 57.0
model initialize at round 19354
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.98594645,  19.11073932,  34.03260225]), 'distance': 27.500000000000004, 'localFrame': array([[-0.04400558,  0.97750328, -0.20627855],
       [-0.99898821, -0.0449728 ,  0.        ],
       [-0.00927692,  0.20606984,  0.97849331]]), 'currentState': array([-25.86822164,  27.40182289,   9.36251671,  -0.04400558,
         0.97750328,  -0.20627855]), 'targetState': array([-5.47632995,  8.36718652, 66.        ]), 'previousTarget': array([-16.57464646,  18.09373681,  33.97573695])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6276306849548315
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.47632995,  8.36718652, 66.        ]), 'distance': 2.499443696971478, 'localFrame': array([[ 0.36007612, -0.81881972,  0.44707881],
       [ 0.91539931,  0.40254701,  0.        ],
       [-0.17997024,  0.40925564,  0.89449457]]), 'currentState': array([-4.33330833,  9.10219107, 63.90226582,  0.36007612, -0.81881972,
        0.44707881]), 'targetState': array([-5.47632995,  8.36718652, 66.        ]), 'previousTarget': array([-5.47632995,  8.36718652, 66.        ])}
episode index:19355
target thresh 86.28655289978025
target distance 50.0
model initialize at round 19355
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.21978041, 15.30167146, 72.29355463]), 'distance': 27.5, 'localFrame': array([[ 0.12140271, -0.82267514, -0.55539806],
       [ 0.98928613,  0.1459896 ,  0.        ],
       [ 0.08108234, -0.54944759,  0.83158463]]), 'currentState': array([16.84590739, 25.20529983, 96.45464066,  0.12140271, -0.82267514,
       -0.55539806]), 'targetState': array([-0.81066689,  4.93384426, 47.        ]), 'previousTarget': array([ 8.68549191, 16.3887485 , 73.10776541])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6276292035186625
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([-0.81066689,  4.93384426, 47.        ]), 'distance': 1.9161229584876998, 'localFrame': array([[-0.60193195, -0.23930843, -0.76184605],
       [ 0.36944103, -0.92925418,  0.        ],
       [-0.70794862, -0.28145719,  0.64775813]]), 'currentState': array([ 0.05057655,  5.34365637, 48.66187875, -0.60193195, -0.23930843,
       -0.76184605]), 'targetState': array([-0.81066689,  4.93384426, 47.        ]), 'previousTarget': array([-0.81066689,  4.93384426, 47.        ])}
episode index:19356
target thresh 86.28792417592531
target distance 56.0
model initialize at round 19356
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.90264716,  5.06976942, 58.76017423]), 'distance': 27.5, 'localFrame': array([[ 0.80318613, -0.59373927,  0.04863866],
       [ 0.59444283,  0.80413788,  0.        ],
       [-0.03911219,  0.0289129 ,  0.99881644]]), 'currentState': array([15.31079123,  3.86567002, 32.9476391 ,  0.80318613, -0.59373927,
        0.04863866]), 'targetState': array([-4.75467464,  6.43374456, 88.        ]), 'previousTarget': array([ 5.35160391,  5.6682919 , 58.0489789 ])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6276331203700073
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.75467464,  6.43374456, 88.        ]), 'distance': 2.789819985149496, 'localFrame': array([[ 0.5198996 , -0.33010407,  0.78786783],
       [ 0.53601866,  0.84420614,  0.        ],
       [-0.66512285,  0.42231185,  0.61584437]]), 'currentState': array([-6.15081944,  5.43560178, 85.80054865,  0.5198996 , -0.33010407,
        0.78786783]), 'targetState': array([-4.75467464,  6.43374456, 88.        ]), 'previousTarget': array([-4.75467464,  6.43374456, 88.        ])}
episode index:19357
target thresh 86.28929531494963
target distance 63.0
model initialize at round 19357
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.10493668, 14.37390218, 28.59734994]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.75945562, -0.38903343,  0.52142128],
       [ 0.45591669,  0.89002245,  0.        ],
       [-0.46407665,  0.23772467,  0.85329939]]), 'currentState': array([12.53798369, 21.96571778,  2.27825767,  0.75945562, -0.38903343,
        0.52142128]), 'targetState': array([ 6.83216756,  4.16190898, 64.        ]), 'previousTarget': array([ 9.7823883 , 15.22007021, 27.25583847])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6276356049205131
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.83216756,  4.16190898, 64.        ]), 'distance': 2.19701877284122, 'localFrame': array([[-0.56796099,  0.42516433,  0.70473797],
       [-0.5992723 , -0.80054526,  0.        ],
       [ 0.56417464, -0.42232995,  0.70946768]]), 'currentState': array([ 6.90562288,  4.15469348, 61.80422138, -0.56796099,  0.42516433,
        0.70473797]), 'targetState': array([ 6.83216756,  4.16190898, 64.        ]), 'previousTarget': array([ 6.83216756,  4.16190898, 64.        ])}
episode index:19358
target thresh 86.29066631686689
target distance 34.8069646127216
model initialize at round 19358
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.99123313,  25.03166644,  88.375558  ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.57564138,  0.58342858, -0.5729294 ],
       [-0.7118414 ,  0.70234024,  0.        ],
       [ 0.40239137,  0.40783486,  0.81960473]]), 'currentState': array([-14.47485865,   2.4229039 ,  77.7705132 ,   0.57564138,
         0.58342858,  -0.5729294 ]), 'targetState': array([-32.09900369,  37.02234409,  94.        ]), 'previousTarget': array([-26.68408095,  25.42598335,  89.00256879])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.627643362849873
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-32.09900369,  37.02234409,  94.        ]), 'distance': 3.62605268792069, 'localFrame': array([[ 0.16542033,  0.94617663,  0.27818321],
       [-0.98505884,  0.17221811,  0.        ],
       [-0.04790819, -0.27402683,  0.96052803]]), 'currentState': array([-30.15709993,  34.94495463,  91.75017314,   0.16542033,
         0.94617663,   0.27818321]), 'targetState': array([-32.09900369,  37.02234409,  94.        ]), 'previousTarget': array([-32.09900369,  37.02234409,  94.        ])}
episode index:19359
target thresh 86.29203718169083
target distance 45.93718278307596
model initialize at round 19359
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.36455743,   2.76362558,  88.61551211]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.08028061, -0.69643372, -0.71311647],
       [ 0.99342146,  0.11451553,  0.        ],
       [ 0.08166291, -0.7084252 ,  0.70104558]]), 'currentState': array([ 7.05600398e+00,  2.09966960e+01,  8.60067167e+01,  8.02806090e-02,
       -6.96433717e-01, -7.13116471e-01]), 'targetState': array([-39.85691535, -20.89081852,  92.        ]), 'previousTarget': array([-13.91765794,   3.4308868 ,  89.17665997])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6276409621810335
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-39.85691535, -20.89081852,  92.        ]), 'distance': 1.887856472442377, 'localFrame': array([[ 0.48823077,  0.4951009 ,  0.71868339],
       [-0.71202967,  0.70214938,  0.        ],
       [-0.5046231 , -0.5117239 ,  0.69533746]]), 'currentState': array([-38.95122126, -21.58407934,  90.49563645,   0.48823077,
         0.4951009 ,   0.71868339]), 'targetState': array([-39.85691535, -20.89081852,  92.        ]), 'previousTarget': array([-39.85691535, -20.89081852,  92.        ])}
episode index:19360
target thresh 86.29340790943513
target distance 57.0
model initialize at round 19360
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.42110019, 36.30060432, 53.98672355]), 'distance': 27.5, 'localFrame': array([[ 0.4132134 ,  0.47060976,  0.77960319],
       [-0.75144412,  0.65979674,  0.        ],
       [-0.51437964, -0.58582824,  0.62627379]]), 'currentState': array([-4.47538205, 34.55978085, 26.61093654,  0.4132134 ,  0.47060976,
        0.77960319]), 'targetState': array([-8.41212753, 38.08196568, 82.        ]), 'previousTarget': array([-6.82668962, 36.26783093, 52.40929755])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6276382615864255
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-8.41212753, 38.08196568, 82.        ]), 'distance': 2.2987430222153304, 'localFrame': array([[-0.6529946 ,  0.04788125,  0.7558475 ],
       [-0.07312932, -0.99732247,  0.        ],
       [ 0.75382369, -0.05527461,  0.6547477 ]]), 'currentState': array([-8.71353798e+00,  3.85570461e+01,  7.97711730e+01, -6.52994596e-01,
        4.78812543e-02,  7.55847500e-01]), 'targetState': array([-8.41212753, 38.08196568, 82.        ]), 'previousTarget': array([-8.41212753, 38.08196568, 82.        ])}
episode index:19361
target thresh 86.2947785001135
target distance 38.65847294843676
model initialize at round 19361
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.84861622,  25.62664199,  27.70190868]), 'distance': 27.5, 'localFrame': array([[-0.27684679, -0.84847866,  0.45104302],
       [ 0.95067402, -0.31019172,  0.        ],
       [ 0.13990981,  0.42879489,  0.89250221]]), 'currentState': array([-9.79791342, 41.53155013, 42.28125815, -0.27684679, -0.84847866,
        0.45104302]), 'targetState': array([-48.72069543,   5.22435038,   9.        ]), 'previousTarget': array([-27.0140859 ,  26.34204874,  26.96789816])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6276410978808902
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-48.72069543,   5.22435038,   9.        ]), 'distance': 3.443510308172918, 'localFrame': array([[-0.71277494, -0.45907302,  0.53028658],
       [ 0.54147561, -0.84071646,  0.        ],
       [ 0.44582065,  0.28713725,  0.84781846]]), 'currentState': array([-46.05010438,   7.07692249,   7.86259802,  -0.71277494,
        -0.45907302,   0.53028658]), 'targetState': array([-48.72069543,   5.22435038,   9.        ]), 'previousTarget': array([-48.72069543,   5.22435038,   9.        ])}
episode index:19362
target thresh 86.29614895373967
target distance 34.0
model initialize at round 19362
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.82108227, -26.11539411,  62.36236879]), 'distance': 27.500000000000004, 'localFrame': array([[-0.79841114, -0.04349582, -0.60053956],
       [ 0.05439731, -0.99851937,  0.        ],
       [-0.59965038, -0.03266774,  0.79959505]]), 'currentState': array([ 11.21448378, -15.07164413,  38.28700378,  -0.79841114,
        -0.04349582,  -0.60053956]), 'targetState': array([  0.55432895, -30.99504347,  73.        ]), 'previousTarget': array([  4.29356645, -25.7952065 ,  62.48576446])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6276476608199658
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.55432895, -30.99504347,  73.        ]), 'distance': 1.7374414406494894, 'localFrame': array([[ 0.0322215 , -0.59776431,  0.8010241 ],
       [ 0.99855037,  0.05382521,  0.        ],
       [-0.04311529,  0.79986291,  0.5986321 ]]), 'currentState': array([ 6.88515328e-01, -3.14905500e+01,  7.13401295e+01,  3.22215010e-02,
       -5.97764309e-01,  8.01024098e-01]), 'targetState': array([  0.55432895, -30.99504347,  73.        ]), 'previousTarget': array([  0.55432895, -30.99504347,  73.        ])}
episode index:19363
target thresh 86.29751927032733
target distance 35.25693396718722
model initialize at round 19363
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.65249394, 11.59926074, 39.58567046]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69185465,  0.11991448, -0.7120096 ],
       [-0.17077706,  0.9853097 ,  0.        ],
       [ 0.70154996,  0.1215949 ,  0.70216973]]), 'currentState': array([-21.5230932 ,  35.33198636,  38.80195449,   0.69185465,
         0.11991448,  -0.7120096 ]), 'targetState': array([-0.31948143, -0.94759254, 40.        ]), 'previousTarget': array([-7.82839494, 11.01673576, 40.        ])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6276508523829202
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.31948143, -0.94759254, 40.        ]), 'distance': 3.380792189366289, 'localFrame': array([[-0.43167591,  0.86987823, -0.23867922],
       [-0.89576728, -0.44452332,  0.        ],
       [-0.10609848,  0.21380103,  0.97109847]]), 'currentState': array([ 2.26837468, -2.92751824, 40.90147145, -0.43167591,  0.86987823,
       -0.23867922]), 'targetState': array([-0.31948143, -0.94759254, 40.        ]), 'previousTarget': array([-0.31948143, -0.94759254, 40.        ])}
episode index:19364
target thresh 86.29888944989017
target distance 62.74393889344638
model initialize at round 19364
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.00543643,   8.16589783,  72.11917958]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.25446336, -0.79752667, -0.54699141],
       [ 0.95268219,  0.30396816,  0.        ],
       [ 0.16626797, -0.52110898,  0.83713822]]), 'currentState': array([-36.69913844,  17.32085206,  86.32551992,   0.25446336,
        -0.79752667,  -0.54699141]), 'targetState': array([26.40673491, -9.31044313, 45.        ]), 'previousTarget': array([-14.9383226 ,   9.33788751,  72.67585915])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6276496830162666
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.40673491, -9.31044313, 45.        ]), 'distance': 2.8454010819175397, 'localFrame': array([[ 0.75920368, -0.28542287, -0.58493039],
       [ 0.35190319,  0.9360364 ,  0.        ],
       [ 0.54751614, -0.20583887,  0.81108349]]), 'currentState': array([26.69388258, -7.22844348, 43.0818939 ,  0.75920368, -0.28542287,
       -0.58493039]), 'targetState': array([26.40673491, -9.31044313, 45.        ]), 'previousTarget': array([26.40673491, -9.31044313, 45.        ])}
episode index:19365
target thresh 86.30025949244191
target distance 52.61322039788065
model initialize at round 19365
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.48219096,  7.35579565, 93.92156126]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.80544227, -0.58066869, -0.11868709],
       [ 0.58480224,  0.8111759 ,  0.        ],
       [ 0.0962761 , -0.06940847,  0.99293171]]), 'currentState': array([ 3.43136733, 34.76125001, 91.64373596,  0.80544227, -0.58066869,
       -0.11868709]), 'targetState': array([  3.52856581, -17.65075702,  96.        ]), 'previousTarget': array([ 2.61432256,  7.55950576, 94.08335148])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6276446874498067
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 32, 'trapConfig': [], 'currentTarget': array([  3.52856581, -17.65075702,  96.        ]), 'distance': 2.3925441251452035, 'localFrame': array([[ 0.31649314, -0.82465528,  0.46880247],
       [ 0.93360405,  0.35830642,  0.        ],
       [-0.16797493,  0.43767588,  0.88330303]]), 'currentState': array([  3.30636313, -15.64299343,  94.71790066,   0.31649314,
        -0.82465528,   0.46880247]), 'targetState': array([  3.52856581, -17.65075702,  96.        ]), 'previousTarget': array([  3.52856581, -17.65075702,  96.        ])}
episode index:19366
target thresh 86.30162939799627
target distance 24.481283818121714
model initialize at round 19366
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.05703952, 35.72460381, 23.15014366]), 'distance': 27.5, 'localFrame': array([[ 0.40809682, -0.29921967, -0.86251062],
       [ 0.5912977 ,  0.80645337,  0.        ],
       [ 0.69557459, -0.51000055,  0.50603896]]), 'currentState': array([-19.94467948,  40.19074833,  41.61051554,   0.40809682,
        -0.29921967,  -0.86251062]), 'targetState': array([ 4.41397348, 34.72055354, 19.        ]), 'previousTarget': array([-0.79139414, 36.13909352, 24.10303397])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6276545114932903
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.41397348, 34.72055354, 19.        ]), 'distance': 2.5870142812925283, 'localFrame': array([[ 0.28452126, -0.93620561,  0.20631699],
       [ 0.95679077,  0.29077727,  0.        ],
       [-0.05999229,  0.19740219,  0.97848521]]), 'currentState': array([ 3.89083027, 36.75447988, 20.51066466,  0.28452126, -0.93620561,
        0.20631699]), 'targetState': array([ 4.41397348, 34.72055354, 19.        ]), 'previousTarget': array([ 4.41397348, 34.72055354, 19.        ])}
episode index:19367
target thresh 86.30299916656689
target distance 22.0
model initialize at round 19367
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.182668  ,  1.24415375, 83.196211  ]), 'distance': 27.5, 'localFrame': array([[ 0.83955363, -0.44142467, -0.31669221],
       [ 0.46537846,  0.88511179,  0.        ],
       [ 0.28030801, -0.14738174,  0.94852836]]), 'currentState': array([-17.80743042,  -6.73063314,  60.67913697,   0.83955363,
        -0.44142467,  -0.31669221]), 'targetState': array([-3.69630666,  1.52882865, 84.        ]), 'previousTarget': array([-3.91945351,  1.41492914, 83.67899049])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6276651919584885
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.69630666,  1.52882865, 84.        ]), 'distance': 2.2069634770999063, 'localFrame': array([[-0.16453473,  0.47316617,  0.86547218],
       [-0.94452455, -0.32844084,  0.        ],
       [ 0.28425641, -0.81745972,  0.50095698]]), 'currentState': array([-4.93284169,  2.14800729, 82.28003292, -0.16453473,  0.47316617,
        0.86547218]), 'targetState': array([-3.69630666,  1.52882865, 84.        ]), 'previousTarget': array([-3.69630666,  1.52882865, 84.        ])}
episode index:19368
target thresh 86.3043687981675
target distance 25.0
model initialize at round 19368
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.61858917, -42.82805767,  61.00497416]), 'distance': 27.500000000000004, 'localFrame': array([[-0.52357865,  0.58715978, -0.61734009],
       [-0.74636135, -0.66554093,  0.        ],
       [-0.4108651 ,  0.46075878,  0.78669639]]), 'currentState': array([ -0.19595336, -42.3600897 ,  82.27661648,  -0.52357865,
         0.58715978,  -0.61734009]), 'targetState': array([-19.26077242, -42.87216633,  59.        ]), 'previousTarget': array([-16.68207046, -42.88901065,  62.30899841])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6276763065230724
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.26077242, -42.87216633,  59.        ]), 'distance': 3.0485451134819916, 'localFrame': array([[-0.47740413,  0.86415587,  0.15912239],
       [-0.87530829, -0.4835653 ,  0.        ],
       [ 0.07694607, -0.13928115,  0.98725886]]), 'currentState': array([-19.76969011, -43.17742516,  61.99022526,  -0.47740413,
         0.86415587,   0.15912239]), 'targetState': array([-19.26077242, -42.87216633,  59.        ]), 'previousTarget': array([-19.26077242, -42.87216633,  59.        ])}
episode index:19369
target thresh 86.30573829281182
target distance 65.51529522700518
model initialize at round 19369
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.84881423,  3.68967527, 44.5452523 ]), 'distance': 27.5, 'localFrame': array([[-0.96655634,  0.02981409, -0.25471544],
       [-0.03083102, -0.99952461,  0.        ],
       [-0.25459435,  0.00785314,  0.96701605]]), 'currentState': array([ 2.59551741e+01,  2.64082765e+01,  5.53509881e+01, -9.66556344e-01,
        2.98140938e-02, -2.54715436e-01]), 'targetState': array([ -6.26802316, -39.50584622,  24.        ]), 'previousTarget': array([16.0327796 ,  3.50148449, 44.34986254])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6276702306035952
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([ -6.26802316, -39.50584622,  24.        ]), 'distance': 2.146042829090966, 'localFrame': array([[ 0.0330721 , -0.55851556, -0.82883449],
       [ 0.99825143,  0.05911075,  0.        ],
       [ 0.04899303, -0.82738521,  0.55949387]]), 'currentState': array([-6.47945180e+00, -3.90636426e+01,  2.60893190e+01,  3.30721024e-02,
       -5.58515555e-01, -8.28834489e-01]), 'targetState': array([ -6.26802316, -39.50584622,  24.        ]), 'previousTarget': array([ -6.26802316, -39.50584622,  24.        ])}
episode index:19370
target thresh 86.3071076505135
target distance 63.0
model initialize at round 19370
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.11628823,  3.26013634, 55.83564277]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.79506378,  0.11378031,  0.59575802],
       [-0.14166511,  0.98991464,  0.        ],
       [-0.58974959, -0.08439812,  0.80316398]]), 'currentState': array([15.81200572,  2.32068644, 30.35645134,  0.79506378,  0.11378031,
        0.59575802]), 'targetState': array([40.74186042,  4.59356178, 92.        ]), 'previousTarget': array([25.34418862,  2.72360864, 54.41991098])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6276655121065947
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([40.74186042,  4.59356178, 92.        ]), 'distance': 2.2184431283049753, 'localFrame': array([[ 0.10807342, -0.69500233,  0.71083887],
       [ 0.98812468,  0.15365419,  0.        ],
       [-0.10922337,  0.70239743,  0.70335489]]), 'currentState': array([40.37071154,  4.54130893, 89.81344836,  0.10807342, -0.69500233,
        0.71083887]), 'targetState': array([40.74186042,  4.59356178, 92.        ]), 'previousTarget': array([40.74186042,  4.59356178, 92.        ])}
episode index:19371
target thresh 86.30847687128627
target distance 20.383276646831135
model initialize at round 19371
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.75696879, 14.89096104, 25.3596763 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.29521068,  0.91902278, -0.26124277],
       [-0.95208574, -0.30583124,  0.        ],
       [-0.0798962 ,  0.24872551,  0.96527313]]), 'currentState': array([-14.08528766,   6.85025671,   9.32230058,  -0.29521068,
         0.91902278,  -0.26124277]), 'targetState': array([ 7.58913678, 15.21200193, 26.        ]), 'previousTarget': array([ 7.50865963, 15.17438427, 25.93682888])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6276666187097468
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([ 7.58913678, 15.21200193, 26.        ]), 'distance': 2.738727475472143, 'localFrame': array([[-0.67494939, -0.72841835, -0.11768616],
       [ 0.73351567, -0.67967254,  0.        ],
       [-0.07998805, -0.08632464,  0.99305084]]), 'currentState': array([ 6.72102887, 17.25587625, 27.60299551, -0.67494939, -0.72841835,
       -0.11768616]), 'targetState': array([ 7.58913678, 15.21200193, 26.        ]), 'previousTarget': array([ 7.58913678, 15.21200193, 26.        ])}
episode index:19372
target thresh 86.30984595514381
target distance 67.0
model initialize at round 19372
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.13285755,  4.63709861, 34.17462417]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.27916376,  0.42793868,  0.85961391],
       [-0.83754493,  0.54636846,  0.        ],
       [-0.46966593, -0.71996528,  0.51094414]]), 'currentState': array([1.98698787, 0.51133965, 7.47227791, 0.27916376, 0.42793868,
       0.85961391]), 'targetState': array([-10.57714432,  10.63597753,  73.        ]), 'previousTarget': array([-3.6516332 ,  4.06600346, 32.75747899])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6276639184635933
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([-10.57714432,  10.63597753,  73.        ]), 'distance': 1.9949555787038111, 'localFrame': array([[-0.13220679,  0.69563717,  0.70612343],
       [-0.9824152 , -0.18670935,  0.        ],
       [ 0.13183985, -0.69370639,  0.70808877]]), 'currentState': array([-9.43006763, 10.14700362, 71.44276934, -0.13220679,  0.69563717,
        0.70612343]), 'targetState': array([-10.57714432,  10.63597753,  73.        ]), 'previousTarget': array([-10.57714432,  10.63597753,  73.        ])}
episode index:19373
target thresh 86.3112149020998
target distance 35.0
model initialize at round 19373
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.80211359, -1.82120193, 28.74370333]), 'distance': 27.5, 'localFrame': array([[-0.34819656, -0.68520714, -0.63972677],
       [ 0.89149752, -0.45302559,  0.        ],
       [-0.28981259, -0.57031482,  0.76860241]]), 'currentState': array([-1.95925622,  4.28004325,  1.94231533, -0.34819656, -0.68520714,
       -0.63972677]), 'targetState': array([-3.09320814, -3.92836651, 38.        ]), 'previousTarget': array([-2.5000086 , -1.7928273 , 29.59168223])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276315212343963
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-3.09320814, -3.92836651, 38.        ]), 'distance': 12.050263438349445, 'localFrame': array([[ 0.68322836, -0.72603229,  0.07794951],
       [ 0.72824812,  0.68531356,  0.        ],
       [-0.05341986,  0.05676659,  0.99695731]]), 'currentState': array([-4.94018197, -9.16096771, 27.30339207,  0.68322836, -0.72603229,
        0.07794951]), 'targetState': array([-3.09320814, -3.92836651, 38.        ]), 'previousTarget': array([-3.09320814, -3.92836651, 38.        ])}
episode index:19374
target thresh 86.31258371216795
target distance 47.544390770554585
model initialize at round 19374
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.24092722, 25.66254577, 62.33152004]), 'distance': 27.500000000000004, 'localFrame': array([[-0.73192644, -0.19710875,  0.65225135],
       [ 0.26003699, -0.96559866,  0.        ],
       [ 0.62981303,  0.16960948,  0.75800275]]), 'currentState': array([15.27532805, 31.40432259, 49.28285204, -0.73192644, -0.19710875,
        0.65225135]), 'targetState': array([-31.07200918,  20.08806227,  75.        ]), 'previousTarget': array([-6.98792534, 25.55553341, 61.32288156])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6276365456893497
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.07200918,  20.08806227,  75.        ]), 'distance': 3.0541072220928323, 'localFrame': array([[-0.52895674, -0.62429399,  0.57485805],
       [ 0.76295947, -0.64644632,  0.        ],
       [ 0.37161487,  0.43859339,  0.81825315]]), 'currentState': array([-28.10506771,  19.60042376,  75.5357592 ,  -0.52895674,
        -0.62429399,   0.57485805]), 'targetState': array([-31.07200918,  20.08806227,  75.        ]), 'previousTarget': array([-31.07200918,  20.08806227,  75.        ])}
episode index:19375
target thresh 86.31395238536193
target distance 63.2343359254632
model initialize at round 19375
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.3153787 ,  13.72278236,  36.51985309]), 'distance': 27.5, 'localFrame': array([[ 0.72303921, -0.58902502, -0.3609208 ],
       [ 0.63159688,  0.77529696,  0.        ],
       [ 0.2798208 , -0.22795645,  0.93259647]]), 'currentState': array([-24.16778855,  34.8344575 ,  54.14158371,   0.72303921,
        -0.58902502,  -0.3609208 ]), 'targetState': array([-24.60449843, -27.63365081,   2.        ]), 'previousTarget': array([-25.43641747,  14.36267692,  36.53517792])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6276347563572229
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.60449843, -27.63365081,   2.        ]), 'distance': 2.4131089420765552, 'localFrame': array([[-0.15009689, -0.98158477,  0.11816199],
       [ 0.98850995, -0.15115584,  0.        ],
       [ 0.01786088,  0.11680431,  0.99299433]]), 'currentState': array([-22.98546433, -26.22467795,   3.10300446,  -0.15009689,
        -0.98158477,   0.11816199]), 'targetState': array([-24.60449843, -27.63365081,   2.        ]), 'previousTarget': array([-24.60449843, -27.63365081,   2.        ])}
episode index:19376
target thresh 86.31532092169545
target distance 78.75075523784179
model initialize at round 19376
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.11358974, 16.07915248, 70.63407818]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.30044392,  0.27718505, -0.91263459],
       [-0.67808486,  0.73498362,  0.        ],
       [ 0.67077148,  0.6188437 ,  0.40877634]]), 'currentState': array([-4.64742818, 43.26098993, 68.31385462,  0.30044392,  0.27718505,
       -0.91263459]), 'targetState': array([-14.63580191, -35.06840889,  75.        ]), 'previousTarget': array([-8.51622342, 16.42859715, 71.73038018])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.627633903928218
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.63580191, -35.06840889,  75.        ]), 'distance': 3.63616254544731, 'localFrame': array([[-0.47689235, -0.87296628, -0.10248686],
       [ 0.87758734, -0.47941679,  0.        ],
       [-0.04913392, -0.08994117,  0.99473436]]), 'currentState': array([-11.87127726, -32.93436361,  76.01238935,  -0.47689235,
        -0.87296628,  -0.10248686]), 'targetState': array([-14.63580191, -35.06840889,  75.        ]), 'previousTarget': array([-14.63580191, -35.06840889,  75.        ])}
episode index:19377
target thresh 86.31668932118217
target distance 47.0
model initialize at round 19377
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([7.32851909e-01, 5.50331746e-02, 7.85839166e+01]), 'distance': 27.499999999999996, 'localFrame': array([[-0.17877991, -0.6517902 ,  0.73702597],
       [ 0.9643801 , -0.26452037,  0.        ],
       [ 0.19495838,  0.71077317,  0.67586443]]), 'currentState': array([-1.76394896,  4.78133833, 51.60840877, -0.17877991, -0.6517902 ,
        0.73702597]), 'targetState': array([ 2.43740899, -3.17159856, 97.        ]), 'previousTarget': array([ 0.43938136,  0.56425331, 76.90829312])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276015149353432
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([ 1.44557711, -3.98907721, 92.57509007]), 'distance': 27.5, 'localFrame': array([[-0.80736028,  0.5579144 , -0.19209606],
       [-0.56850211, -0.82268181,  0.        ],
       [-0.15803394,  0.10920702,  0.98137613]]), 'currentState': array([-4.47381413, -8.86790407, 66.16660931, -0.80736028,  0.5579144 ,
       -0.19209606]), 'targetState': array([ 2.43740899, -3.17159856, 97.        ]), 'previousTarget': array([ 1.8197034 , -3.87308203, 93.37457171])}
episode index:19378
target thresh 86.31805758383577
target distance 37.0
model initialize at round 19378
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.44961863, -19.97107545,  25.57940416]), 'distance': 27.5, 'localFrame': array([[-0.62040741, -0.31441018,  0.71849905],
       [ 0.45204536, -0.89199495,  0.        ],
       [ 0.64089753,  0.32479416,  0.69552794]]), 'currentState': array([-36.47749863,  -1.26993037,   7.08426114,  -0.62040741,
        -0.31441018,   0.71849905]), 'targetState': array([-20.88814951, -37.58570486,  43.        ]), 'previousTarget': array([-27.73851434, -20.03285741,  24.99738197])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6276047064091516
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-20.88814951, -37.58570486,  43.        ]), 'distance': 2.275661151228951, 'localFrame': array([[ 0.09757564, -0.89352574, -0.43828158],
       [ 0.99409016,  0.10855757,  0.        ],
       [ 0.04757878, -0.43569141,  0.89883772]]), 'currentState': array([-21.08008246, -35.33654435,  42.71177714,   0.09757564,
        -0.89352574,  -0.43828158]), 'targetState': array([-20.88814951, -37.58570486,  43.        ]), 'previousTarget': array([-20.88814951, -37.58570486,  43.        ])}
episode index:19379
target thresh 86.31942570966996
target distance 42.19025089256
model initialize at round 19379
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([24.18340289, 13.62622425, 18.6516565 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.44995216,  0.33617955,  0.82736108],
       [-0.59853527,  0.80109645,  0.        ],
       [-0.66279603, -0.49520479,  0.5616704 ]]), 'currentState': array([-0.58589745,  1.91225636, 21.        ,  0.44995216,  0.33617955,
        0.82736108]), 'targetState': array([41.60435345, 21.86498969, 17.        ]), 'previousTarget': array([24.18340289, 13.62622425, 18.6516565 ])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6276038556626667
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([41.60435345, 21.86498969, 17.        ]), 'distance': 2.071772621324661, 'localFrame': array([[ 0.81127934,  0.05615605,  0.58195561],
       [-0.0690539 ,  0.99761293,  0.        ],
       [-0.58056645, -0.04018631,  0.81322055]]), 'currentState': array([39.90944392, 20.7056679 , 17.27476623,  0.81127934,  0.05615605,
        0.58195561]), 'targetState': array([41.60435345, 21.86498969, 17.        ]), 'previousTarget': array([41.60435345, 21.86498969, 17.        ])}
episode index:19380
target thresh 86.3207936986984
target distance 82.0
model initialize at round 19380
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.92993738, -2.56898684, 27.88323059]), 'distance': 27.5, 'localFrame': array([[ 0.58972883,  0.45362529,  0.66816465],
       [-0.6097004 ,  0.79263196,  0.        ],
       [-0.52960866, -0.40738026,  0.74401344]]), 'currentState': array([ 19.17143848, -15.79421607,   4.46992236,   0.58972883,
         0.45362529,   0.66816465]), 'targetState': array([38.97779642, 29.69396212, 85.        ]), 'previousTarget': array([24.06713647, -2.7590731 , 26.47437183])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275714732337073
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([35.07328894, 23.47063953, 77.66545488]), 'distance': 27.5, 'localFrame': array([[-0.05007433,  0.29525965,  0.95410393],
       [-0.98592187, -0.16720664,  0.        ],
       [ 0.15953252, -0.94067193,  0.2994757 ]]), 'currentState': array([ 2.47302287e+01,  6.98502621e+00,  5.82362074e+01, -5.00743271e-02,
        2.95259645e-01,  9.54103927e-01]), 'targetState': array([38.97779642, 29.69396212, 85.        ]), 'previousTarget': array([34.63037432, 22.33650509, 76.39120213])}
episode index:19381
target thresh 86.32216155093477
target distance 78.0
model initialize at round 19381
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.31278561, -13.02866581,  39.70893314]), 'distance': 27.500000000000004, 'localFrame': array([[-0.70632512, -0.70581709,  0.05410226],
       [ 0.70685235, -0.70736112,  0.        ],
       [ 0.03826983,  0.03824231,  0.9985354 ]]), 'currentState': array([ 20.37700823, -13.05390129,  12.27716089,  -0.70632512,
        -0.70581709,   0.05410226]), 'targetState': array([ 25.93224139, -12.9814813 ,  91.        ]), 'previousTarget': array([ 23.28790946, -12.44026319,  40.4608643 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627539094146243
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 43, 'trapConfig': [], 'currentTarget': array([ 25.93224139, -12.9814813 ,  91.        ]), 'distance': 14.626550798296401, 'localFrame': array([[ 0.63746879,  0.61789828,  0.46025564],
       [-0.69599879,  0.71804296,  0.        ],
       [-0.33048332, -0.32033737,  0.88778643]]), 'currentState': array([ 22.10572076, -18.03851592,  77.8197068 ,   0.63746879,
         0.61789828,   0.46025564]), 'targetState': array([ 25.93224139, -12.9814813 ,  91.        ]), 'previousTarget': array([ 25.93224139, -12.9814813 ,  91.        ])}
episode index:19382
target thresh 86.32352926639277
target distance 41.0
model initialize at round 19382
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.65484405, 25.15165651, 63.11387445]), 'distance': 27.499999999999996, 'localFrame': array([[-0.81933896, -0.36255042, -0.44411806],
       [ 0.40464649, -0.91447319,  0.        ],
       [-0.40613406, -0.17971082,  0.89596827]]), 'currentState': array([11.88356434, 14.69323707, 88.53584507, -0.81933896, -0.36255042,
       -0.44411806]), 'targetState': array([13.11338541, 31.36939788, 48.        ]), 'previousTarget': array([13.33680159, 24.99507859, 63.5557811 ])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.627547252792823
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.11338541, 31.36939788, 48.        ]), 'distance': 2.6619930950232726, 'localFrame': array([[-0.65828536,  0.39972259, -0.63787322],
       [-0.51902494, -0.8547591 ,  0.        ],
       [-0.54522793,  0.33107211,  0.77014139]]), 'currentState': array([12.73876328, 31.0903507 , 50.62068658, -0.65828536,  0.39972259,
       -0.63787322]), 'targetState': array([13.11338541, 31.36939788, 48.        ]), 'previousTarget': array([13.11338541, 31.36939788, 48.        ])}
episode index:19383
target thresh 86.32489684508606
target distance 30.65435626750579
model initialize at round 19383
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.46372628, -24.59289584,  80.44158146]), 'distance': 27.500000000000004, 'localFrame': array([[-0.82730681, -0.18158195, -0.5315933 ],
       [ 0.21438254, -0.97674978,  0.        ],
       [-0.51923364, -0.11396432,  0.84699974]]), 'currentState': array([-10.60564721,  -4.32162254,  92.58152414,  -0.82730681,
        -0.18158195,  -0.5315933 ]), 'targetState': array([ 10.92908405, -35.34904697,  74.        ]), 'previousTarget': array([  4.38361385, -25.19410423,  80.29417595])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6275558200148025
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.92908405, -35.34904697,  74.        ]), 'distance': 4.086943981927587, 'localFrame': array([[ 0.71880177, -0.4534495 ,  0.52697967],
       [ 0.53354665,  0.84577064,  0.        ],
       [-0.44570393,  0.28116824,  0.84987789]]), 'currentState': array([  8.02686211, -32.68482242,  75.08725639,   0.71880177,
        -0.4534495 ,   0.52697967]), 'targetState': array([ 10.92908405, -35.34904697,  74.        ]), 'previousTarget': array([ 10.92908405, -35.34904697,  74.        ])}
episode index:19384
target thresh 86.32626428702831
target distance 41.47660300743015
model initialize at round 19384
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.23329777,   3.78435644,  68.20462994]), 'distance': 27.5, 'localFrame': array([[-0.06486745, -0.99007145,  0.12470261],
       [ 0.99786058, -0.06537777,  0.        ],
       [ 0.00815278,  0.12443582,  0.99219416]]), 'currentState': array([-25.66982111,  25.6471275 ,  59.84663764,  -0.06486745,
        -0.99007145,   0.12470261]), 'targetState': array([  0.50415245, -13.99091957,  75.        ]), 'previousTarget': array([-11.98145239,   5.37258908,  67.99719339])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6275569315344494
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([  0.50415245, -13.99091957,  75.        ]), 'distance': 3.442692630791466, 'localFrame': array([[-0.71309769, -0.69760256,  0.069587  ],
       [ 0.69929774, -0.71483052,  0.        ],
       [ 0.04974291,  0.04866203,  0.99757589]]), 'currentState': array([-1.70340954e+00, -1.40724967e+01,  7.23595175e+01, -7.13097692e-01,
       -6.97602559e-01,  6.95870004e-02]), 'targetState': array([  0.50415245, -13.99091957,  75.        ]), 'previousTarget': array([  0.50415245, -13.99091957,  75.        ])}
episode index:19385
target thresh 86.3276315922332
target distance 47.97749332828265
model initialize at round 19385
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.61336953,   7.60371228,  52.90788373]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.06286163,  0.78492482,  0.6163939 ],
       [-0.99680845,  0.07983058,  0.        ],
       [-0.04920708, -0.61442665,  0.78743797]]), 'currentState': array([  2.26802032, -15.06966882,  57.45893036,   0.06286163,
         0.78492482,   0.6163939 ]), 'targetState': array([-28.66157496,  32.05486111,  48.        ]), 'previousTarget': array([-12.94577549,   7.11517561,  52.15856416])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6275623347346
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.66157496,  32.05486111,  48.        ]), 'distance': 4.001416373792543, 'localFrame': array([[-0.71986104,  0.52738505,  0.4512927 ],
       [-0.59098973, -0.80667908,  0.        ],
       [ 0.36404838, -0.26670935,  0.89237599]]), 'currentState': array([-26.03043564,  29.10749603,  48.6336229 ,  -0.71986104,
         0.52738505,   0.4512927 ]), 'targetState': array([-28.66157496,  32.05486111,  48.        ]), 'previousTarget': array([-28.66157496,  32.05486111,  48.        ])}
episode index:19386
target thresh 86.32899876071443
target distance 40.0
model initialize at round 19386
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.3897297 ,  5.46982437, 53.19137725]), 'distance': 27.5, 'localFrame': array([[-0.64760639, -0.08314335,  0.75742534],
       [ 0.12734045, -0.99185907,  0.        ],
       [ 0.75125919,  0.09645088,  0.65292178]]), 'currentState': array([10.134496  , -8.92343312, 29.79247712, -0.64760639, -0.08314335,
        0.75742534]), 'targetState': array([12.18413808, 14.57898417, 68.        ]), 'previousTarget': array([11.52719197,  4.86093877, 51.63216289])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.627570490499085
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.18413808, 14.57898417, 68.        ]), 'distance': 3.1979169009766992, 'localFrame': array([[ 0.73969818,  0.47011702,  0.48149412],
       [-0.53638814,  0.84397143,  0.        ],
       [-0.40636727, -0.25826773,  0.87644932]]), 'currentState': array([10.78439414, 13.76943111, 65.24101227,  0.73969818,  0.47011702,
        0.48149412]), 'targetState': array([12.18413808, 14.57898417, 68.        ]), 'previousTarget': array([12.18413808, 14.57898417, 68.        ])}
episode index:19387
target thresh 86.33036579248564
target distance 24.0
model initialize at round 19387
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.21086567, -6.90805409, 23.37636213]), 'distance': 27.5, 'localFrame': array([[-0.14440109, -0.88005522, -0.45238384],
       [ 0.98680443, -0.1619167 ,  0.        ],
       [-0.0732485 , -0.44641438,  0.89182333]]), 'currentState': array([-19.576158  ,   8.04554532,  43.99675445,  -0.14440109,
        -0.88005522,  -0.45238384]), 'targetState': array([-7.51366321, -9.3565413 , 20.        ]), 'previousTarget': array([-9.55846807, -6.11612535, 24.04205394])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.627580733852362
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.51366321, -9.3565413 , 20.        ]), 'distance': 2.114004892109482, 'localFrame': array([[ 0.92595263,  0.21456612,  0.31076214],
       [-0.22574318,  0.97418685,  0.        ],
       [-0.30274039, -0.07015243,  0.95048771]]), 'currentState': array([-9.01899797, -8.60584057, 18.71959679,  0.92595263,  0.21456612,
        0.31076214]), 'targetState': array([-7.51366321, -9.3565413 , 20.        ]), 'previousTarget': array([-7.51366321, -9.3565413 , 20.        ])}
episode index:19388
target thresh 86.33173268756047
target distance 22.847794204300687
model initialize at round 19388
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.91482303,   1.36624971,  63.        ]), 'distance': 26.45237807326296, 'localFrame': array([[ 0.11296259,  0.86190716, -0.49432328],
       [-0.99152056,  0.12994987,  0.        ],
       [ 0.06423725,  0.49013169,  0.86927815]]), 'currentState': array([-22.85084934, -20.31589882,  53.66490402,   0.11296259,
         0.86190716,  -0.49432328]), 'targetState': array([-10.91482303,   1.36624971,  63.        ]), 'previousTarget': array([-10.91482303,   1.36624971,  63.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6275936246816542
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.91482303,   1.36624971,  63.        ]), 'distance': 3.981155911076055, 'localFrame': array([[ 0.85284932,  0.48784522, -0.18615877],
       [-0.49652463,  0.86802263,  0.        ],
       [ 0.16159002,  0.09243241,  0.98251968]]), 'currentState': array([-11.99068431,  -1.4207409 ,  60.36849697,   0.85284932,
         0.48784522,  -0.18615877]), 'targetState': array([-10.91482303,   1.36624971,  63.        ]), 'previousTarget': array([-10.91482303,   1.36624971,  63.        ])}
episode index:19389
target thresh 86.33309944595267
target distance 17.928078919014176
model initialize at round 19389
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.75723961, -12.32733764,  75.        ]), 'distance': 16.8755240565094, 'localFrame': array([[-0.69467149, -0.19605894, -0.69209278],
       [ 0.27162181, -0.96240407,  0.        ],
       [-0.66607291, -0.18798749,  0.72180855]]), 'currentState': array([-16.51195675,  -8.08584027,  73.30182538,  -0.69467149,
        -0.19605894,  -0.69209278]), 'targetState': array([-32.75723961, -12.32733764,  75.        ]), 'previousTarget': array([-32.75723961, -12.32733764,  75.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.62760884650062
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.75723961, -12.32733764,  75.        ]), 'distance': 3.017151014158671, 'localFrame': array([[-0.81670968, -0.1184292 ,  0.56476528],
       [ 0.14350678, -0.98964933,  0.        ],
       [ 0.55891958,  0.08104765,  0.82525159]]), 'currentState': array([-30.74806546, -10.56899108,  73.59478227,  -0.81670968,
        -0.1184292 ,   0.56476528]), 'targetState': array([-32.75723961, -12.32733764,  75.        ]), 'previousTarget': array([-32.75723961, -12.32733764,  75.        ])}
episode index:19390
target thresh 86.33446606767585
target distance 85.0
model initialize at round 19390
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.70933547, -3.23800677, 62.78073332]), 'distance': 27.499999999999996, 'localFrame': array([[-0.26368418,  0.82429445, -0.5010083 ],
       [-0.95245434, -0.30468134,  0.        ],
       [-0.15264788,  0.47718753,  0.86544248]]), 'currentState': array([ 31.92621589, -14.38684661,  86.53860036,  -0.26368418,
         0.82429445,  -0.5010083 ]), 'targetState': array([ 3.03361   , 24.81526164,  3.        ]), 'previousTarget': array([24.3582114 , -4.05601159, 64.27157054])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6276035856910188
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.03361   , 24.81526164,  3.        ]), 'distance': 1.9874261954183001, 'localFrame': array([[-0.73398464,  0.67902737, -0.01372541],
       [-0.67909134, -0.73405378,  0.        ],
       [-0.01007519,  0.0093208 ,  0.9999058 ]]), 'currentState': array([ 4.97115834e+00,  2.45195988e+01,  2.67083014e+00, -7.33984636e-01,
        6.79027369e-01, -1.37254067e-02]), 'targetState': array([ 3.03361   , 24.81526164,  3.        ]), 'previousTarget': array([ 3.03361   , 24.81526164,  3.        ])}
episode index:19391
target thresh 86.33583255274368
target distance 23.0
model initialize at round 19391
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([30.17178455,  2.22369541, 46.24583438]), 'distance': 27.500000000000004, 'localFrame': array([[-0.74184705,  0.64514067, -0.1829111 ],
       [-0.65621131, -0.75457718,  0.        ],
       [-0.13802054,  0.12002833,  0.98312946]]), 'currentState': array([ 42.57624486, -12.94695261,  65.53911826,  -0.74184705,
         0.64514067,  -0.1829111 ]), 'targetState': array([27.44195979,  5.56226957, 42.        ]), 'previousTarget': array([30.7320029 ,  1.55841916, 46.64498294])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627571221644727
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([27.44195979,  5.56226957, 42.        ]), 'distance': 16.316517803422308, 'localFrame': array([[-0.68686365,  0.01355316, -0.72665992],
       [-0.01972811, -0.99980538,  0.        ],
       [-0.7265185 ,  0.01433562,  0.68699735]]), 'currentState': array([ 4.05618792e+01, -4.12728622e+00,  4.24571409e+01, -6.86863649e-01,
        1.35531574e-02, -7.26659920e-01]), 'targetState': array([27.44195979,  5.56226957, 42.        ]), 'previousTarget': array([27.44195979,  5.56226957, 42.        ])}
episode index:19392
target thresh 86.33719890116986
target distance 44.0
model initialize at round 19392
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.33472455, 19.39386679, 49.61655108]), 'distance': 27.499999999999996, 'localFrame': array([[-0.71970063, -0.44218309, -0.53526174],
       [ 0.52348796, -0.85203307,  0.        ],
       [-0.4560607 , -0.28020307,  0.84468626]]), 'currentState': array([11.77557479, 31.8295626 , 73.7387881 , -0.71970063, -0.44218309,
       -0.53526174]), 'targetState': array([ 3.72336109,  9.28097959, 30.        ]), 'previousTarget': array([ 8.21546338, 19.77586224, 50.09038237])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275388609361391
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 68, 'trapConfig': [], 'currentTarget': array([ 3.72336109,  9.28097959, 30.        ]), 'distance': 21.73065335920407, 'localFrame': array([[-0.53759832,  0.06136542, -0.84096512],
       [-0.11341089, -0.99354817,  0.        ],
       [-0.83553936,  0.09537461,  0.54108934]]), 'currentState': array([16.42304391, 21.62384078, 42.59337641, -0.53759832,  0.06136542,
       -0.84096512]), 'targetState': array([ 3.72336109,  9.28097959, 30.        ]), 'previousTarget': array([ 3.72336109,  9.28097959, 30.        ])}
episode index:19393
target thresh 86.338565112968
target distance 25.844724908605862
model initialize at round 19393
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.1670344 , -8.34467602, 89.        ]), 'distance': 26.616053083663587, 'localFrame': array([[-0.39067452, -0.5833679 ,  0.71207817],
       [ 0.83088981, -0.55643699,  0.        ],
       [ 0.39622663,  0.59165849,  0.7021002 ]]), 'currentState': array([ 18.79901253, -10.9317506 ,  94.24268383,  -0.39067452,
        -0.5833679 ,   0.71207817]), 'targetState': array([-7.1670344 , -8.34467602, 89.        ]), 'previousTarget': array([-7.1670344 , -8.34467602, 89.        ])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6275504067188635
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.1670344 , -8.34467602, 89.        ]), 'distance': 1.7154971666623537, 'localFrame': array([[ 0.16768677,  0.10503206, -0.98022927],
       [-0.53082669,  0.8474804 ,  0.        ],
       [ 0.83072509,  0.52033185,  0.19786507]]), 'currentState': array([-7.63819361, -8.85079729, 90.56996204,  0.16768677,  0.10503206,
       -0.98022927]), 'targetState': array([-7.1670344 , -8.34467602, 89.        ]), 'previousTarget': array([-7.1670344 , -8.34467602, 89.        ])}
episode index:19394
target thresh 86.33993118815181
target distance 37.0
model initialize at round 19394
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.05933353,  0.66137229, 21.59444498]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.47786843,  0.25786608, -0.83973023],
       [-0.47488815,  0.88004616,  0.        ],
       [ 0.73900136,  0.39877793,  0.54300382]]), 'currentState': array([-9.46003251,  0.32518174, 48.07777238,  0.47786843,  0.25786608,
       -0.83973023]), 'targetState': array([ 0.62180989,  0.78316822, 12.        ]), 'previousTarget': array([-2.76101255,  0.47849793, 22.77270885])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6275532427172286
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 0.62180989,  0.78316822, 12.        ]), 'distance': 3.149340239479309, 'localFrame': array([[-0.66337435,  0.41857055, -0.62026862],
       [-0.53362592, -0.84572063,  0.        ],
       [-0.52457397,  0.33099141,  0.78438947]]), 'currentState': array([ 1.32428539,  2.60704927, 14.46947971, -0.66337435,  0.41857055,
       -0.62026862]), 'targetState': array([ 0.62180989,  0.78316822, 12.        ]), 'previousTarget': array([ 0.62180989,  0.78316822, 12.        ])}
episode index:19395
target thresh 86.34129712673493
target distance 70.0
model initialize at round 19395
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-35.88251713,   5.4731369 ,  32.80640034]), 'distance': 27.499999999999996, 'localFrame': array([[-0.27060831, -0.65585178,  0.70471951],
       [ 0.92440409, -0.38141458,  0.        ],
       [ 0.26879029,  0.65144559,  0.70948602]]), 'currentState': array([-44.68842356,  19.52658155,  10.86997616,  -0.27060831,
        -0.65585178,   0.70471951]), 'targetState': array([-16.93765998, -24.76117272,  80.        ]), 'previousTarget': array([-35.0775749 ,   6.27660741,  32.0094498 ])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6275401426469943
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 29, 'trapConfig': [], 'currentTarget': array([-16.93765998, -24.76117272,  80.        ]), 'distance': 1.8229526914690537, 'localFrame': array([[ 0.12485228,  0.66363815,  0.73756105],
       [-0.98275931,  0.18488952,  0.        ],
       [-0.13636731, -0.72484499,  0.67528045]]), 'currentState': array([-17.61336365, -24.42533366,  78.34054431,   0.12485228,
         0.66363815,   0.73756105]), 'targetState': array([-16.93765998, -24.76117272,  80.        ]), 'previousTarget': array([-16.93765998, -24.76117272,  80.        ])}
episode index:19396
target thresh 86.34266292873102
target distance 25.044198855156093
model initialize at round 19396
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.15770517, -15.84558825,  15.        ]), 'distance': 24.951806299877465, 'localFrame': array([[ 0.86060174,  0.4954974 ,  0.11767316],
       [-0.49896401,  0.86662271,  0.        ],
       [-0.10197824, -0.05871467,  0.99305238]]), 'currentState': array([-17.5852441 ,  -8.42868385,  13.03670516,   0.86060174,
         0.4954974 ,   0.11767316]), 'targetState': array([  6.15770517, -15.84558825,  15.        ]), 'previousTarget': array([  6.15770517, -15.84558825,  15.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.62755212997555
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.15770517, -15.84558825,  15.        ]), 'distance': 4.054009661122591, 'localFrame': array([[ 0.91529851,  0.4024948 ,  0.01505224],
       [-0.40254041,  0.91540222,  0.        ],
       [-0.01377886, -0.00605914,  0.99988671]]), 'currentState': array([ 3.69715949e+00, -1.79094746e+01,  1.25259179e+01,  9.15298511e-01,
        4.02494803e-01,  1.50522430e-02]), 'targetState': array([  6.15770517, -15.84558825,  15.        ]), 'previousTarget': array([  6.15770517, -15.84558825,  15.        ])}
episode index:19397
target thresh 86.34402859415373
target distance 54.781927711282925
model initialize at round 19397
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.1185875 , -10.21358574,  52.18569441]), 'distance': 27.5, 'localFrame': array([[-0.64900694,  0.27957354,  0.70755115],
       [-0.39562548, -0.91841194,  0.        ],
       [ 0.64982342, -0.27992526,  0.70666214]]), 'currentState': array([ 31.28210891, -27.45982128,  36.11671744,  -0.64900694,
         0.27957354,   0.70755115]), 'targetState': array([-12.68602557,  26.07805122,  86.        ]), 'previousTarget': array([ 17.91596068, -11.42357804,  51.08733497])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6275500413402247
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-12.68602557,  26.07805122,  86.        ]), 'distance': 1.879367756621884, 'localFrame': array([[-0.741192  ,  0.35546085,  0.56945764],
       [-0.43242325, -0.90167075,  0.        ],
       [ 0.5134633 , -0.24624672,  0.82202068]]), 'currentState': array([-13.27826915,  24.31507147,  85.72949467,  -0.741192  ,
         0.35546085,   0.56945764]), 'targetState': array([-12.68602557,  26.07805122,  86.        ]), 'previousTarget': array([-12.68602557,  26.07805122,  86.        ])}
episode index:19398
target thresh 86.34539412301675
target distance 21.271800230618524
model initialize at round 19398
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([35.77930594,  0.12658579, 16.90082284]), 'distance': 27.499999999999996, 'localFrame': array([[-0.32350113,  0.02123082, -0.94598957],
       [-0.0654874 , -0.9978534 ,  0.        ],
       [-0.94395891,  0.0619504 ,  0.32419705]]), 'currentState': array([16.29018037, 19.46184015, 15.29710107, -0.32350113,  0.02123082,
       -0.94598957]), 'targetState': array([36.98454994, -1.06914253, 17.        ]), 'previousTarget': array([35.5059776 ,  0.45628994, 17.        ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6275594324339347
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([36.98454994, -1.06914253, 17.        ]), 'distance': 2.2709603803391754, 'localFrame': array([[-0.24307051,  0.12502545,  0.96191754],
       [-0.4573994 , -0.88926137,  0.        ],
       [ 0.85539612, -0.43998051,  0.27333979]]), 'currentState': array([35.74608084, -1.29779839, 15.11024556, -0.24307051,  0.12502545,
        0.96191754]), 'targetState': array([36.98454994, -1.06914253, 17.        ]), 'previousTarget': array([36.98454994, -1.06914253, 17.        ])}
episode index:19399
target thresh 86.34675951533369
target distance 35.447694648906925
model initialize at round 19399
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.2344484 ,   4.25006894,  90.15790459]), 'distance': 27.5, 'localFrame': array([[-0.9048738 ,  0.39724445,  0.15297143],
       [-0.40197545, -0.91565044,  0.        ],
       [ 0.14006836, -0.06149076,  0.98823061]]), 'currentState': array([-3.62962623,  3.85851102, 73.14825023, -0.9048738 ,  0.39724445,
        0.15297143]), 'targetState': array([-37.73539149,   4.47663147, 100.        ]), 'previousTarget': array([-23.85998528,   4.00870999,  89.03986909])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6275667769552425
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-37.73539149,   4.47663147, 100.        ]), 'distance': 2.3135922953512913, 'localFrame': array([[-0.36086038, -0.35983104,  0.8604077 ],
       [ 0.70609612, -0.708116  ,  0.        ],
       [ 0.60926846,  0.60753054,  0.50960631]]), 'currentState': array([-37.28418001,   4.71728791,  97.7436308 ,  -0.36086038,
        -0.35983104,   0.8604077 ]), 'targetState': array([-37.73539149,   4.47663147, 100.        ]), 'previousTarget': array([-37.73539149,   4.47663147, 100.        ])}
episode index:19400
target thresh 86.34812477111822
target distance 56.66718785925665
model initialize at round 19400
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.56702188,  9.38534939, 89.17509249]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.6178563 , -0.60636034,  0.50058039],
       [ 0.70043584,  0.71371537,  0.        ],
       [-0.35727192,  0.35062445,  0.86569005]]), 'currentState': array([23.02458208, 36.82410316, 90.28878438,  0.6178563 , -0.60636034,
        0.50058039]), 'targetState': array([ 20.02910277, -19.56617086,  88.        ]), 'previousTarget': array([20.9088854 ,  9.61777942, 88.51500615])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6275692594186262
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 20.02910277, -19.56617086,  88.        ]), 'distance': 2.4081010643224143, 'localFrame': array([[ 0.59605063, -0.55247153, -0.58266531],
       [ 0.67978741,  0.73340922,  0.        ],
       [ 0.42733211, -0.39608854,  0.81271221]]), 'currentState': array([ 19.97399589, -17.60857127,  89.40132716,   0.59605063,
        -0.55247153,  -0.58266531]), 'targetState': array([ 20.02910277, -19.56617086,  88.        ]), 'previousTarget': array([ 20.02910277, -19.56617086,  88.        ])}
episode index:19401
target thresh 86.349489890384
target distance 50.26500700545275
model initialize at round 19401
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.74535756, -9.25920453, 72.50066053]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.3665807 , -0.92346114, -0.11330539],
       [ 0.92944658,  0.36895671,  0.        ],
       [ 0.04180478, -0.10531131,  0.99356021]]), 'currentState': array([-7.84205789, 14.27743654, 76.70288236,  0.3665807 , -0.92346114,
       -0.11330539]), 'targetState': array([ 20.29774126, -34.46740054,  68.        ]), 'previousTarget': array([ 4.95986243, -7.81401178, 72.2420587 ])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6275735365018059
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 20.29774126, -34.46740054,  68.        ]), 'distance': 1.9532979240564383, 'localFrame': array([[-0.36876342, -0.84170071,  0.39440266],
       [ 0.91594967, -0.40129315,  0.        ],
       [ 0.15827109,  0.36125299,  0.91893772]]), 'currentState': array([ 20.45729781, -33.04128813,  69.32518598,  -0.36876342,
        -0.84170071,   0.39440266]), 'targetState': array([ 20.29774126, -34.46740054,  68.        ]), 'previousTarget': array([ 20.29774126, -34.46740054,  68.        ])}
episode index:19402
target thresh 86.35085487314468
target distance 57.8222396898135
model initialize at round 19402
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.18425493,  29.43603505,  65.73805566]), 'distance': 27.500000000000004, 'localFrame': array([[-0.05059631,  0.58549886,  0.80909276],
       [-0.99628694, -0.08609487,  0.        ],
       [ 0.06965873, -0.80608855,  0.58768095]]), 'currentState': array([-3.87473329e+01,  2.26385264e+01,  5.82165584e+01, -5.05963129e-02,
        5.85498862e-01,  8.09092761e-01]), 'targetState': array([18.29402619, 37.80646249, 75.        ]), 'previousTarget': array([-14.17344157,  28.65383261,  64.89291278])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6275778131441175
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.29402619, 37.80646249, 75.        ]), 'distance': 2.918066713544828, 'localFrame': array([[ 0.91712396,  0.38722346, -0.09456019],
       [-0.38896636,  0.92125196,  0.        ],
       [ 0.08711376,  0.03678073,  0.99551915]]), 'currentState': array([15.85930464, 36.80883946, 73.7382581 ,  0.91712396,  0.38722346,
       -0.09456019]), 'targetState': array([18.29402619, 37.80646249, 75.        ]), 'previousTarget': array([18.29402619, 37.80646249, 75.        ])}
episode index:19403
target thresh 86.35221971941392
target distance 34.58465757578826
model initialize at round 19403
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.33951712, 13.0218873 , 40.99569749]), 'distance': 27.500000000000004, 'localFrame': array([[-0.81321046, -0.10957144,  0.57156176],
       [ 0.13353267, -0.99104441,  0.        ],
       [ 0.56644309,  0.07632217,  0.82055905]]), 'currentState': array([34.61574364, 30.70294786, 25.5095923 , -0.81321046, -0.10957144,
        0.57156176]), 'targetState': array([ 7.4293249 , -2.96734419, 55.        ]), 'previousTarget': array([21.51443791, 14.01383325, 40.26990409])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6275832102558733
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.4293249 , -2.96734419, 55.        ]), 'distance': 1.4360474876050533, 'localFrame': array([[-0.76369965,  0.20148859, -0.61332307],
       [-0.25510306, -0.96691387,  0.        ],
       [-0.59303058,  0.15646059,  0.78983214]]), 'currentState': array([ 8.84364084, -3.061436  , 54.76958833, -0.76369965,  0.20148859,
       -0.61332307]), 'targetState': array([ 7.4293249 , -2.96734419, 55.        ]), 'previousTarget': array([ 7.4293249 , -2.96734419, 55.        ])}
episode index:19404
target thresh 86.35358442920536
target distance 40.0
model initialize at round 19404
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.36147934, 11.55729256, 34.14486268]), 'distance': 27.5, 'localFrame': array([[-0.47083174, -0.55334699, -0.68711322],
       [ 0.76160859, -0.64803731,  0.        ],
       [-0.445275  , -0.52331133,  0.72655036]]), 'currentState': array([-13.96360455,  25.36854313,  11.61250706,  -0.47083174,
        -0.55334699,  -0.68711322]), 'targetState': array([-0.,  0., 53.]), 'previousTarget': array([-6.28005416, 11.95035166, 35.00000023])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6275878558277539
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 53.]), 'distance': 3.025410058274972, 'localFrame': array([[ 0.53646402,  0.62170913,  0.5706874 ],
       [-0.75710401,  0.65329435,  0.        ],
       [-0.37282686, -0.43206972,  0.82116739]]), 'currentState': array([ 1.29338823,  0.50481116, 50.31198612,  0.53646402,  0.62170913,
        0.5706874 ]), 'targetState': array([-0.,  0., 53.]), 'previousTarget': array([ 0.,  0., 53.])}
episode index:19405
target thresh 86.35494900253265
target distance 54.10382612916458
model initialize at round 19405
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-9.6931922 , 16.02952006, 20.49610335]), 'distance': 27.5, 'localFrame': array([[ 0.8739337 , -0.36792576, -0.31760121],
       [ 0.38801551,  0.92165285,  0.        ],
       [ 0.29271805, -0.12323419,  0.94822438]]), 'currentState': array([-36.9559288 ,  12.46030712,  21.        ,   0.8739337 ,
        -0.36792576,  -0.31760121]), 'targetState': array([17.14789733, 19.54353134, 20.        ]), 'previousTarget': array([-9.6931922 , 16.02952006, 20.49610335])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6275800104287111
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([17.14789733, 19.54353134, 20.        ]), 'distance': 3.4781749962854946, 'localFrame': array([[ 0.71185423, -0.63816847, -0.29326532],
       [ 0.66751853,  0.74459318,  0.        ],
       [ 0.21836336, -0.19576004,  0.95603109]]), 'currentState': array([14.42101414, 21.45592115, 21.00228469,  0.71185423, -0.63816847,
       -0.29326532]), 'targetState': array([17.14789733, 19.54353134, 20.        ]), 'previousTarget': array([17.14789733, 19.54353134, 20.        ])}
episode index:19406
target thresh 86.3563134394094
target distance 49.140786358047485
model initialize at round 19406
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.92299311, 14.65117972, 17.72179046]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.22854895, -0.16483166, -0.95947689],
       [ 0.58495056,  0.81106895,  0.        ],
       [ 0.77820191, -0.56124655,  0.28178733]]), 'currentState': array([  3.61845454, -11.9545762 ,  22.22130409,   0.22854895,
        -0.16483166,  -0.95947689]), 'targetState': array([13.31066312, 36.65823574, 14.        ]), 'previousTarget': array([ 8.84302141, 13.97677094, 18.61560884])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6275804538499322
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([13.31066312, 36.65823574, 14.        ]), 'distance': 2.631448337923847, 'localFrame': array([[ 0.80037088,  0.38937266,  0.4558458 ],
       [-0.43746848,  0.89923375,  0.        ],
       [-0.40991193, -0.19941817,  0.89005877]]), 'currentState': array([11.53720118, 35.90102807, 12.20947224,  0.80037088,  0.38937266,
        0.4558458 ]), 'targetState': array([13.31066312, 36.65823574, 14.        ]), 'previousTarget': array([13.31066312, 36.65823574, 14.        ])}
episode index:19407
target thresh 86.35767773984931
target distance 21.30316668346095
model initialize at round 19407
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.92409581,  0.95136155, 80.        ]), 'distance': 24.48165873538519, 'localFrame': array([[ 0.87228365,  0.08942198, -0.48075455],
       [-0.10198032,  0.99478642,  0.        ],
       [ 0.47824809,  0.0490275 ,  0.87685521]]), 'currentState': array([-2.62020438e+01,  1.07013916e+01,  8.96484897e+01,  8.72283655e-01,
        8.94219790e-02, -4.80754548e-01]), 'targetState': array([-5.92409581,  0.95136155, 80.        ]), 'previousTarget': array([-5.92409581,  0.95136155, 80.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6275911161184607
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-5.92409581,  0.95136155, 80.        ]), 'distance': 2.948436735925218, 'localFrame': array([[ 0.85280725, -0.5042299 ,  0.13591175],
       [ 0.50895249,  0.86079461,  0.        ],
       [-0.1169921 ,  0.06917262,  0.99072095]]), 'currentState': array([-7.76470446,  2.56959784, 81.63913095,  0.85280725, -0.5042299 ,
        0.13591175]), 'targetState': array([-5.92409581,  0.95136155, 80.        ]), 'previousTarget': array([-5.92409581,  0.95136155, 80.        ])}
episode index:19408
target thresh 86.35904190386599
target distance 59.0
model initialize at round 19408
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.43935821, -9.40498276, 56.94891807]), 'distance': 27.5, 'localFrame': array([[ 0.80039436, -0.32390153,  0.50443698],
       [ 0.37512547,  0.92697405,  0.        ],
       [-0.46759999,  0.18922716,  0.86344851]]), 'currentState': array([25.19707106,  6.21684708, 34.3510165 ,  0.80039436, -0.32390153,
        0.50443698]), 'targetState': array([ 28.36624103, -33.63564136,  92.        ]), 'previousTarget': array([25.67097008, -8.5621364 , 55.60421691])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6275932481521798
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.36624103, -33.63564136,  92.        ]), 'distance': 2.3312976785101105, 'localFrame': array([[-0.06858284, -0.45040288,  0.89018742],
       [ 0.98860471, -0.15053484,  0.        ],
       [ 0.13400422,  0.88004347,  0.45559451]]), 'currentState': array([ 2.85219714e+01, -3.38684778e+01,  8.96855921e+01, -6.85828449e-02,
       -4.50402877e-01,  8.90187419e-01]), 'targetState': array([ 28.36624103, -33.63564136,  92.        ]), 'previousTarget': array([ 28.36624103, -33.63564136,  92.        ])}
episode index:19409
target thresh 86.36040593147308
target distance 86.0
model initialize at round 19409
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.71688258,   5.14758365,  66.37421556]), 'distance': 27.500000000000004, 'localFrame': array([[-0.67913278,  0.43623248, -0.59032186],
       [-0.54044821, -0.84137728,  0.        ],
       [-0.4966834 ,  0.31903839,  0.80716795]]), 'currentState': array([-20.44914835,  -2.90100875,  92.46620808,  -0.67913278,
         0.43623248,  -0.59032186]), 'targetState': array([-31.15284808,  23.46273761,   7.        ]), 'previousTarget': array([-22.48753923,   4.86641107,  66.98004218])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275609146515021
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([-31.15284808,  23.46273761,   7.        ]), 'distance': 19.216413934296316, 'localFrame': array([[-0.4823161 ,  0.62798425, -0.61074296],
       [-0.79308072, -0.60911655,  0.        ],
       [-0.37201364,  0.48436847,  0.79182892]]), 'currentState': array([-39.06862546,  18.32742549,  23.74035849,  -0.4823161 ,
         0.62798425,  -0.61074296]), 'targetState': array([-31.15284808,  23.46273761,   7.        ]), 'previousTarget': array([-31.15284808,  23.46273761,   7.        ])}
episode index:19410
target thresh 86.36176982268424
target distance 44.0
model initialize at round 19410
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.13181403, -2.32192993, 22.7410378 ]), 'distance': 27.5, 'localFrame': array([[-0.0558406 , -0.47864863, -0.87622903],
       [ 0.99326355, -0.11587714,  0.        ],
       [-0.10153491, -0.87032636,  0.48189488]]), 'currentState': array([-7.78301647, -1.40645897, 49.40887271, -0.0558406 , -0.47864863,
       -0.87622903]), 'targetState': array([ 2.79414507, -2.86229861,  7.        ]), 'previousTarget': array([-0.99592243, -2.15872137, 24.16805875])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275285844822861
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 86, 'trapConfig': [], 'currentTarget': array([ 2.55132964, -2.59312295,  9.18187136]), 'distance': 27.500000000000004, 'localFrame': array([[-0.54675982, -0.45890361, -0.70032934],
       [ 0.64288432, -0.76596328,  0.        ],
       [-0.53642656, -0.45023075,  0.71381988]]), 'currentState': array([-0.46769564,  0.75364999, 36.30998588, -0.54675982, -0.45890361,
       -0.70032934]), 'targetState': array([ 2.79414507, -2.86229861,  7.        ]), 'previousTarget': array([ 2.55132964, -2.59312295,  9.18187136])}
episode index:19411
target thresh 86.36313357751308
target distance 35.610902678971485
model initialize at round 19411
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.49915397, -36.85505355,  72.62415129]), 'distance': 27.5, 'localFrame': array([[ 0.55796722,  0.5037525 ,  0.65947404],
       [-0.67012661,  0.74224681,  0.        ],
       [-0.4894925 , -0.4419311 ,  0.75172734]]), 'currentState': array([-17.7854787 , -40.05045532,  71.36390225,   0.55796722,
         0.5037525 ,   0.65947404]), 'targetState': array([ 17.63635052, -35.9020771 ,  73.        ]), 'previousTarget': array([  9.12613232, -37.18113525,  72.28306635])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6275388173299757
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.63635052, -35.9020771 ,  73.        ]), 'distance': 3.0023126493926586, 'localFrame': array([[ 0.62988356,  0.45816452,  0.62716184],
       [-0.58822829,  0.80869493,  0.        ],
       [-0.5071826 , -0.36891434,  0.77888897]]), 'currentState': array([ 15.66169227, -38.01301224,  72.18848361,   0.62988356,
         0.45816452,   0.62716184]), 'targetState': array([ 17.63635052, -35.9020771 ,  73.        ]), 'previousTarget': array([ 17.63635052, -35.9020771 ,  73.        ])}
episode index:19412
target thresh 86.36449719597327
target distance 29.779139489440922
model initialize at round 19412
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.94340741, -3.5196709 , 79.7389754 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.77523121, -0.33107419, -0.53796511],
       [ 0.39274864,  0.91964586,  0.        ],
       [ 0.49473739, -0.21128507,  0.8429671 ]]), 'currentState': array([-13.15512296, -15.99152893,  84.20703888,   0.77523121,
        -0.33107419,  -0.53796511]), 'targetState': array([14.92907617, -1.45694367, 79.        ]), 'previousTarget': array([ 9.39277562, -4.20379182, 80.11547224])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6275490491234391
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([14.92907617, -1.45694367, 79.        ]), 'distance': 3.537438366140853, 'localFrame': array([[ 0.32767317,  0.53903525, -0.77593253],
       [-0.85450485,  0.51944342,  0.        ],
       [ 0.40305305,  0.66303811,  0.63081591]]), 'currentState': array([12.20625761, -2.60812322, 80.94281109,  0.32767317,  0.53903525,
       -0.77593253]), 'targetState': array([14.92907617, -1.45694367, 79.        ]), 'previousTarget': array([14.92907617, -1.45694367, 79.        ])}
episode index:19413
target thresh 86.36586067807843
target distance 42.0
model initialize at round 19413
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.02090698, -12.8910531 ,  54.22929534]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.21527954,  0.85002597, -0.4807396 ],
       [-0.96939382,  0.24551092,  0.        ],
       [ 0.11802682,  0.466026  ,  0.87686341]]), 'currentState': array([-11.90487143, -16.37980451,  80.01987093,   0.21527954,
         0.85002597,  -0.4807396 ]), 'targetState': array([  2.56952594, -10.6956784 ,  38.        ]), 'previousTarget': array([ -3.2984552 , -13.62240865,  54.45394518])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6275567895329517
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.56952594, -10.6956784 ,  38.        ]), 'distance': 3.8949567845113107, 'localFrame': array([[ 0.96097992,  0.02825779,  0.27517101],
       [-0.02939247,  0.99956795,  0.        ],
       [-0.27505213, -0.00808796,  0.96139529]]), 'currentState': array([ 9.88101103e-01, -8.52482977e+00,  4.08208510e+01,  9.60979922e-01,
        2.82577858e-02,  2.75171015e-01]), 'targetState': array([  2.56952594, -10.6956784 ,  38.        ]), 'previousTarget': array([  2.56952594, -10.6956784 ,  38.        ])}
episode index:19414
target thresh 86.3672240238422
target distance 31.16719373184779
model initialize at round 19414
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.42215291, 11.11129286, 21.63160428]), 'distance': 27.5, 'localFrame': array([[ 0.60012013, -0.32675662, -0.73012735],
       [ 0.47819598,  0.87825316,  0.        ],
       [ 0.64123665, -0.34914396,  0.6833111 ]]), 'currentState': array([ 9.42888107, 33.27064299,  6.49448373,  0.60012013, -0.32675662,
       -0.73012735]), 'targetState': array([ 0.89503939,  1.78854815, 28.        ]), 'previousTarget': array([ 2.92639769, 10.26774163, 22.55889834])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6275614340728983
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 0.89503939,  1.78854815, 28.        ]), 'distance': 3.9769036990392093, 'localFrame': array([[ 0.69280569, -0.60716358,  0.38906641],
       [ 0.65909377,  0.75206077,  0.        ],
       [-0.29260158,  0.25643125,  0.92120971]]), 'currentState': array([ 3.5655724 ,  4.16826786, 29.73808819,  0.69280569, -0.60716358,
        0.38906641]), 'targetState': array([ 0.89503939,  1.78854815, 28.        ]), 'previousTarget': array([ 0.89503939,  1.78854815, 28.        ])}
episode index:19415
target thresh 86.36858723327822
target distance 58.4625148767222
model initialize at round 19415
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.4140218 ,  24.20326373,  80.93515565]), 'distance': 27.500000000000004, 'localFrame': array([[-0.76646301, -0.32645242,  0.55313947],
       [ 0.39185795, -0.92002573,  0.        ],
       [ 0.50890255,  0.2167521 ,  0.83308867]]), 'currentState': array([ 8.9586322 , 37.58117104, 86.50297761, -0.76646301, -0.32645242,
        0.55313947]), 'targetState': array([-47.72429085,   5.13732062,  73.        ]), 'previousTarget': array([-12.87987957,  24.42144686,  80.74816731])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6275622092154617
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-47.72429085,   5.13732062,  73.        ]), 'distance': 3.3189337993638346, 'localFrame': array([[-0.73798269,  0.24309443,  0.62951303],
       [-0.3128669 , -0.94979698,  0.        ],
       [ 0.59790957, -0.19695379,  0.77698993]]), 'currentState': array([-45.73361765,   2.48763885,  72.82187562,  -0.73798269,
         0.24309443,   0.62951303]), 'targetState': array([-47.72429085,   5.13732062,  73.        ]), 'previousTarget': array([-47.72429085,   5.13732062,  73.        ])}
episode index:19416
target thresh 86.36995030640009
target distance 50.57410629449498
model initialize at round 19416
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.72916573,  2.39349608, 37.0596991 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.79670714, -0.60194825, -0.05400027],
       [ 0.60282782, -0.7978713 ,  0.        ],
       [-0.04308527, -0.03255287,  0.99854092]]), 'currentState': array([-10.14808452,  28.48449803,  28.48670157,  -0.79670714,
        -0.60194825,  -0.05400027]), 'targetState': array([ -7.41496471, -21.77196129,  45.        ]), 'previousTarget': array([-7.88586972,  2.58742902, 37.29348231])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6275650413927185
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.41496471, -21.77196129,  45.        ]), 'distance': 3.048260031576311, 'localFrame': array([[ 0.94559991, -0.20530782,  0.25236781],
       [ 0.21217566,  0.97723154,  0.        ],
       [-0.24662179,  0.05354631,  0.96763138]]), 'currentState': array([ -9.50911702, -21.36325951,  42.82298871,   0.94559991,
        -0.20530782,   0.25236781]), 'targetState': array([ -7.41496471, -21.77196129,  45.        ]), 'previousTarget': array([ -7.41496471, -21.77196129,  45.        ])}
episode index:19417
target thresh 86.37131324322148
target distance 11.0
model initialize at round 19417
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.51917324,  3.55181922, 37.        ]), 'distance': 11.35164228001995, 'localFrame': array([[-0.61287138, -0.40924489,  0.67594918],
       [ 0.55532379, -0.83163423,  0.        ],
       [ 0.56214247,  0.37537066,  0.73694824]]), 'currentState': array([-0.41818076, -1.30000083, 27.21718495, -0.61287138, -0.40924489,
        0.67594918]), 'targetState': array([-3.51917324,  3.55181922, 37.        ]), 'previousTarget': array([-3.51917324,  3.55181922, 37.        ])}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.6275562374328025
{'scaleFactor': 20, 'timeStep': 79, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([-3.51917324,  3.55181922, 37.        ]), 'distance': 4.122860553364435, 'localFrame': array([[ 0.40614374, -0.34132823,  0.84766874],
       [ 0.64337716,  0.76554936,  0.        ],
       [-0.64893227,  0.54537071,  0.53052587]]), 'currentState': array([-6.14646479,  5.73311255, 39.31025488,  0.40614374, -0.34132823,
        0.84766874]), 'targetState': array([-3.51917324,  3.55181922, 37.        ]), 'previousTarget': array([-3.51917324,  3.55181922, 37.        ])}
episode index:19418
target thresh 86.37267604375599
target distance 9.0
model initialize at round 19418
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.95630389,  32.09177518,  72.        ]), 'distance': 8.88284564720554, 'localFrame': array([[-0.53223597,  0.22404713, -0.81641151],
       [-0.38798018, -0.92166772,  0.        ],
       [-0.75246013,  0.31675148,  0.57747056]]), 'currentState': array([-32.80229109,  28.30140297,  80.03207977,  -0.53223597,
         0.22404713,  -0.81641151]), 'targetState': array([-32.95630389,  32.09177518,  72.        ]), 'previousTarget': array([-32.95630389,  32.09177518,  72.        ])}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6275733876347989
{'scaleFactor': 20, 'timeStep': 5, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.95630389,  32.09177518,  72.        ]), 'distance': 1.8841533217339959, 'localFrame': array([[ 0.07913085,  0.93271541, -0.3518242 ],
       [-0.99642046,  0.08453554,  0.        ],
       [ 0.02974165,  0.35056484,  0.93606609]]), 'currentState': array([-32.64174346,  31.29973255,  73.68040291,   0.07913085,
         0.93271541,  -0.3518242 ]), 'targetState': array([-32.95630389,  32.09177518,  72.        ]), 'previousTarget': array([-32.95630389,  32.09177518,  72.        ])}
episode index:19419
target thresh 86.37403870801727
target distance 42.0
model initialize at round 19419
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.24464188, -19.82890658,  35.33414063]), 'distance': 27.5, 'localFrame': array([[-0.12580178, -0.42432122,  0.8967304 ],
       [ 0.95875072, -0.28424822,  0.        ],
       [ 0.25489402,  0.85974092,  0.44257721]]), 'currentState': array([ 3.59967776, -2.66359002, 56.80938053, -0.12580178, -0.42432122,
        0.8967304 ]), 'targetState': array([  4.91540129, -37.6807488 ,  13.        ]), 'previousTarget': array([  4.03010568, -20.13438474,  33.93488314])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6275731791902703
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([  4.91540129, -37.6807488 ,  13.        ]), 'distance': 3.6549830604426026, 'localFrame': array([[-0.28489598, -0.72961882, -0.62168372],
       [ 0.93150553, -0.36372716,  0.        ],
       [-0.22612326, -0.57910183,  0.78326838]]), 'currentState': array([  2.87631099, -38.65172027,  15.87371299,  -0.28489598,
        -0.72961882,  -0.62168372]), 'targetState': array([  4.91540129, -37.6807488 ,  13.        ]), 'previousTarget': array([  4.91540129, -37.6807488 ,  13.        ])}
episode index:19420
target thresh 86.37540123601893
target distance 64.0
model initialize at round 19420
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.99785783, -0.80566937, 30.40244098]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.64676466, -0.30540148,  0.69887439],
       [ 0.42698897,  0.90425683,  0.        ],
       [-0.63196194,  0.29841166,  0.71524443]]), 'currentState': array([-12.87139035,  -5.06859977,   4.40080815,   0.64676466,
        -0.30540148,   0.69887439]), 'targetState': array([ 6.08421817,  5.19444793, 67.        ]), 'previousTarget': array([-5.86723657, -1.0988781 , 28.91800897])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275408650365609
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 5.44289367,  2.75912669, 63.94489501]), 'distance': 27.500000000000004, 'localFrame': array([[-0.65901468, -0.27935292,  0.69832772],
       [ 0.39027862, -0.9206968 ,  0.        ],
       [ 0.64294809,  0.27254238,  0.71577818]]), 'currentState': array([  0.98842085, -14.15598004,  42.72493186,  -0.65901468,
        -0.27935292,   0.69832772]), 'targetState': array([ 6.08421817,  5.19444793, 67.        ]), 'previousTarget': array([ 5.47474154,  2.09775812, 62.9753843 ])}
episode index:19421
target thresh 86.3767636277746
target distance 31.167569964075255
model initialize at round 19421
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.51014066, -10.45882843,  76.1060961 ]), 'distance': 27.5, 'localFrame': array([[ 0.83728116, -0.01722755, -0.54650112],
       [ 0.02057123,  0.99978839,  0.        ],
       [ 0.54638547, -0.0112422 ,  0.83745837]]), 'currentState': array([ 9.48913604e+00, -2.43987407e+01,  9.26050126e+01,  8.37281160e-01,
       -1.72275464e-02, -5.46501118e-01]), 'targetState': array([38.9993194 , -0.23040416, 64.        ]), 'previousTarget': array([ 25.45565948, -10.44703855,  76.60175683])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6275361655905806
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([38.9993194 , -0.23040416, 64.        ]), 'distance': 2.1987431987458295, 'localFrame': array([[ 0.1251933 , -0.16516304,  0.9782882 ],
       [ 0.79693004,  0.60407161,  0.        ],
       [-0.59095613,  0.77962726,  0.20724911]]), 'currentState': array([37.07186445,  0.26035139, 63.06269107,  0.1251933 , -0.16516304,
        0.9782882 ]), 'targetState': array([38.9993194 , -0.23040416, 64.        ]), 'previousTarget': array([38.9993194 , -0.23040416, 64.        ])}
episode index:19422
target thresh 86.37812588329791
target distance 71.0
model initialize at round 19422
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.76492543,  17.19011948,  38.88566933]), 'distance': 27.5, 'localFrame': array([[ 0.24768767, -0.84924987, -0.46628905],
       [ 0.9600031 ,  0.27998936,  0.        ],
       [ 0.13055598, -0.44763894,  0.88463242]]), 'currentState': array([-3.26133863, 22.77977788, 14.54078877,  0.24768767, -0.84924987,
       -0.46628905]), 'targetState': array([-37.50019325,   6.14292325,  87.        ]), 'previousTarget': array([-15.42377418,  17.67997053,  40.26784197])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275038566699406
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 47, 'trapConfig': [], 'currentTarget': array([-37.50019325,   6.14292325,  87.        ]), 'distance': 13.98090331617021, 'localFrame': array([[-0.7189785 , -0.18768466,  0.66921177],
       [ 0.25257946, -0.96757615,  0.        ],
       [ 0.64751335,  0.16902915,  0.74307174]]), 'currentState': array([-35.21466824,   7.82898774,  73.31061654,  -0.7189785 ,
        -0.18768466,   0.66921177]), 'targetState': array([-37.50019325,   6.14292325,  87.        ]), 'previousTarget': array([-37.50019325,   6.14292325,  87.        ])}
episode index:19423
target thresh 86.37948800260249
target distance 5.791104529719444
model initialize at round 19423
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.4381449 , 30.96662299, 12.        ]), 'distance': 7.008829812366767, 'localFrame': array([[-0.67178928, -0.5174174 , -0.53007395],
       [ 0.61019699, -0.79224973,  0.        ],
       [-0.41995095, -0.32344953,  0.84795142]]), 'currentState': array([ 3.56248411, 26.08031094, 11.50871655, -0.67178928, -0.5174174 ,
       -0.53007395]), 'targetState': array([-1.4381449 , 30.96662299, 12.        ]), 'previousTarget': array([-1.4381449 , 30.96662299, 12.        ])}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6275220092720477
{'scaleFactor': 20, 'timeStep': 3, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.4381449 , 30.96662299, 12.        ]), 'distance': 4.273018073461773, 'localFrame': array([[ 0.26039441,  0.88769307, -0.37973117],
       [-0.95956768,  0.28147799,  0.        ],
       [ 0.10688597,  0.36437776,  0.92509688]]), 'currentState': array([ 1.3608462 , 28.09470019, 10.5247401 ,  0.26039441,  0.88769307,
       -0.37973117]), 'targetState': array([-1.4381449 , 30.96662299, 12.        ]), 'previousTarget': array([-1.4381449 , 30.96662299, 12.        ])}
episode index:19424
target thresh 86.38084998570193
target distance 30.50545676393409
model initialize at round 19424
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.4001064 , -23.08602057,  51.18925594]), 'distance': 27.499999999999996, 'localFrame': array([[-0.37595483, -0.68002923,  0.62945866],
       [ 0.87516024, -0.48383319,  0.        ],
       [ 0.304553  ,  0.55087719,  0.77703397]]), 'currentState': array([ -9.56129461, -18.1628556 ,  36.68404575,  -0.37595483,
        -0.68002923,   0.62945866]), 'targetState': array([-39.97474178, -24.71881915,  56.        ]), 'previousTarget': array([-31.90113118, -22.53625313,  50.70677613])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6275318102979589
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-39.97474178, -24.71881915,  56.        ]), 'distance': 3.3016897314472424, 'localFrame': array([[-0.24638704, -0.42332018,  0.87183339],
       [ 0.86426698, -0.50303339,  0.        ],
       [ 0.4385613 ,  0.7534968 ,  0.48980256]]), 'currentState': array([-37.33005854, -25.24585927,  54.09499754,  -0.24638704,
        -0.42332018,   0.87183339]), 'targetState': array([-39.97474178, -24.71881915,  56.        ]), 'previousTarget': array([-39.97474178, -24.71881915,  56.        ])}
episode index:19425
target thresh 86.38221183260988
target distance 55.0
model initialize at round 19425
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.9078981 ,  2.03520269, 62.72155995]), 'distance': 27.5, 'localFrame': array([[ 0.64717737, -0.53311077, -0.54493519],
       [ 0.63580757,  0.77184761,  0.        ],
       [ 0.42060692, -0.34647392,  0.83847817]]), 'currentState': array([-8.55318407, -4.90933693, 87.59144724,  0.64717737, -0.53311077,
       -0.54493519]), 'targetState': array([12.21466912, 10.33449846, 33.        ]), 'previousTarget': array([ 0.38647086,  2.85096027, 63.06875474])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.627533943518811
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([12.21466912, 10.33449846, 33.        ]), 'distance': 3.07372826040404, 'localFrame': array([[ 0.10258885,  0.48242703, -0.86990786],
       [-0.9781287 ,  0.20800058,  0.        ],
       [ 0.18094134,  0.85088185,  0.49321426]]), 'currentState': array([13.54187661,  8.10778439, 34.65168706,  0.10258885,  0.48242703,
       -0.86990786]), 'targetState': array([12.21466912, 10.33449846, 33.        ]), 'previousTarget': array([12.21466912, 10.33449846, 33.        ])}
episode index:19426
target thresh 86.38357354333995
target distance 19.091616189796504
model initialize at round 19426
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.41701148, 17.29102846, 24.        ]), 'distance': 26.92720805043231, 'localFrame': array([[ 0.83117618, -0.45513682,  0.3193691 ],
       [ 0.48028939,  0.87711009,  0.        ],
       [-0.28012186,  0.15338959,  0.9476304 ]]), 'currentState': array([18.23408781, 36.41937969, 42.91511047,  0.83117618, -0.45513682,
        0.3193691 ]), 'targetState': array([19.41701148, 17.29102846, 24.        ]), 'previousTarget': array([19.41701148, 17.29102846, 24.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6275459126551225
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.41701148, 17.29102846, 24.        ]), 'distance': 2.1798934150855582, 'localFrame': array([[-0.14065497, -0.72558911, -0.67359975],
       [ 0.98172469, -0.19030668,  0.        ],
       [-0.12819053, -0.66128951,  0.73909632]]), 'currentState': array([18.50722617, 18.91951692, 25.1279412 , -0.14065497, -0.72558911,
       -0.67359975]), 'targetState': array([19.41701148, 17.29102846, 24.        ]), 'previousTarget': array([19.41701148, 17.29102846, 24.        ])}
episode index:19427
target thresh 86.38493511790575
target distance 30.0
model initialize at round 19427
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.07462303,  18.71102659,  50.52413787]), 'distance': 27.500000000000004, 'localFrame': array([[-0.23307137, -0.46737041, -0.85278522],
       [ 0.89489683, -0.44627307,  0.        ],
       [-0.38057508, -0.76315479,  0.52226178]]), 'currentState': array([-28.21357994,  33.72784071,  69.44801446,  -0.23307137,
        -0.46737041,  -0.85278522]), 'targetState': array([-8.46195906, 11.15326181, 41.        ]), 'previousTarget': array([-15.80763343,  19.67588227,  51.88187643])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.62754839274244
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 17, 'trapConfig': [], 'currentTarget': array([-8.46195906, 11.15326181, 41.        ]), 'distance': 2.8043125289520447, 'localFrame': array([[ 0.68561763, -0.25414777, -0.68215641],
       [ 0.34757329,  0.93765282,  0.        ],
       [ 0.63962588, -0.23709935,  0.73120628]]), 'currentState': array([-10.33815135,  11.69233092,  43.01332455,   0.68561763,
        -0.25414777,  -0.68215641]), 'targetState': array([-8.46195906, 11.15326181, 41.        ]), 'previousTarget': array([-8.46195906, 11.15326181, 41.        ])}
episode index:19428
target thresh 86.3862965563209
target distance 25.511952767326022
model initialize at round 19428
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.72683147, -16.65622628,  71.13275677]), 'distance': 27.500000000000004, 'localFrame': array([[-0.99879776, -0.03040403,  0.03845286],
       [ 0.03042654, -0.99953701,  0.        ],
       [ 0.03843506,  0.00116999,  0.99926042]]), 'currentState': array([ 2.99768273e+01, -2.75553354e+01,  5.91997309e+01, -9.98797763e-01,
       -3.04040345e-02,  3.84528629e-02]), 'targetState': array([  6.1097933 , -15.86412386,  72.        ]), 'previousTarget': array([  8.6601247 , -17.04148561,  70.80040634])}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6275599172356385
{'scaleFactor': 20, 'timeStep': 17, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.1097933 , -15.86412386,  72.        ]), 'distance': 1.7344801083740982, 'localFrame': array([[ 0.10318612,  0.38688684,  0.91633574],
       [-0.96622481,  0.25770064,  0.        ],
       [-0.23614031, -0.88538633,  0.40041079]]), 'currentState': array([  6.71909508, -16.70775955,  70.61239364,   0.10318612,
         0.38688684,   0.91633574]), 'targetState': array([  6.1097933 , -15.86412386,  72.        ]), 'previousTarget': array([  6.1097933 , -15.86412386,  72.        ])}
episode index:19429
target thresh 86.38765785859903
target distance 38.0
model initialize at round 19429
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.82054014,  3.04741602, 67.34906022]), 'distance': 27.5, 'localFrame': array([[-0.82470537,  0.14197047, -0.54745359],
       [-0.16965147, -0.98550412,  0.        ],
       [-0.53951777,  0.09287631,  0.83683604]]), 'currentState': array([19.94973474,  8.85420561, 89.5670409 , -0.82470537,  0.14197047,
       -0.54745359]), 'targetState': array([-4.95036243, -0.70278859, 53.        ]), 'previousTarget': array([ 5.86553768,  3.46332106, 68.81774091])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6275680550752459
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.95036243, -0.70278859, 53.        ]), 'distance': 2.318923147988625, 'localFrame': array([[-0.70119052, -0.02176451, -0.71264168],
       [ 0.03102442, -0.99951863,  0.        ],
       [-0.71229863, -0.0221093 ,  0.70152822]]), 'currentState': array([-3.67041503e+00, -6.58063887e-01,  5.49331681e+01, -7.01190520e-01,
       -2.17645086e-02, -7.12641677e-01]), 'targetState': array([-4.95036243, -0.70278859, 53.        ]), 'previousTarget': array([-4.95036243, -0.70278859, 53.        ])}
episode index:19430
target thresh 86.38901902475374
target distance 33.91702367536071
model initialize at round 19430
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.96490148, 13.02138517, 51.27660159]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.63034556, -0.35739692, -0.68915304],
       [ 0.49322263,  0.86990312,  0.        ],
       [ 0.59949638, -0.33990588,  0.72461582]]), 'currentState': array([-22.79277755,  30.14846847,  56.67221056,   0.63034556,
        -0.35739692,  -0.68915304]), 'targetState': array([10.68328192,  2.62058916, 48.        ]), 'previousTarget': array([-2.75421912, 13.9925232 , 51.96187506])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6275757877346213
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.68328192,  2.62058916, 48.        ]), 'distance': 2.5700872435793016, 'localFrame': array([[ 0.98591805,  0.08228051, -0.14558677],
       [-0.08316661,  0.99653566,  0.        ],
       [ 0.1450824 ,  0.01210796,  0.98934549]]), 'currentState': array([ 9.17972635,  4.5408191 , 48.81079348,  0.98591805,  0.08228051,
       -0.14558677]), 'targetState': array([10.68328192,  2.62058916, 48.        ]), 'previousTarget': array([10.68328192,  2.62058916, 48.        ])}
episode index:19431
target thresh 86.39038005479863
target distance 26.5756835800617
model initialize at round 19431
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.21202178, -15.72030029,  39.90357851]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13804731, -0.95603726, -0.25871934],
       [ 0.98973523, -0.14291314,  0.        ],
       [-0.03697439, -0.25606365,  0.96595254]]), 'currentState': array([-0.85570927, -1.61620365, 56.21360655, -0.13804731, -0.95603726,
       -0.25871934]), 'targetState': array([ 26.57568358, -24.28441974,  30.        ]), 'previousTarget': array([ 16.45749314, -15.03858481,  39.89900977])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6275815578347476
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.57568358, -24.28441974,  30.        ]), 'distance': 3.4315701146219926, 'localFrame': array([[-0.05342392, -0.84591603, -0.53063355],
       [ 0.99801166, -0.06302953,  0.        ],
       [-0.03344559, -0.52957847,  0.84760135]]), 'currentState': array([ 25.13898413, -21.39616083,  31.17026859,  -0.05342392,
        -0.84591603,  -0.53063355]), 'targetState': array([ 26.57568358, -24.28441974,  30.        ]), 'previousTarget': array([ 26.57568358, -24.28441974,  30.        ])}
episode index:19432
target thresh 86.39174094874731
target distance 69.0
model initialize at round 19432
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.05458959, -4.70329296, 62.22336932]), 'distance': 27.500000000000004, 'localFrame': array([[-0.36283695,  0.91269019, -0.18800522],
       [-0.92926074, -0.36942451,  0.        ],
       [-0.06945374,  0.17470587,  0.98216803]]), 'currentState': array([19.38665743, -8.08075151, 89.48264974, -0.36283695,  0.91269019,
       -0.18800522]), 'targetState': array([15.99127788,  0.5282347 , 20.        ]), 'previousTarget': array([18.23101687, -5.74952159, 61.84240678])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6275810282501805
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([15.99127788,  0.5282347 , 20.        ]), 'distance': 2.7156573277753964, 'localFrame': array([[ 0.08976646,  0.84143473, -0.53285043],
       [-0.99435753,  0.10608067,  0.        ],
       [ 0.05652513,  0.52984384,  0.84620944]]), 'currentState': array([15.08281743,  0.90603411, 22.53115822,  0.08976646,  0.84143473,
       -0.53285043]), 'targetState': array([15.99127788,  0.5282347 , 20.        ]), 'previousTarget': array([15.99127788,  0.5282347 , 20.        ])}
episode index:19433
target thresh 86.39310170661341
target distance 78.0
model initialize at round 19433
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.16438608,  1.04344088, 74.05262733]), 'distance': 27.500000000000004, 'localFrame': array([[-0.30204886,  0.85697692, -0.41756083],
       [-0.9431331 , -0.33241534,  0.        ],
       [-0.13880363,  0.39381544,  0.90864897]]), 'currentState': array([-14.73004403,  -8.53375448,  97.99057872,  -0.30204886,
         0.85697692,  -0.41756083]), 'targetState': array([16.43516252, 22.66904129, 20.        ]), 'previousTarget': array([-4.47835754e+00, -3.05976363e-02,  7.41125837e+01])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6275763296393416
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.43516252, 22.66904129, 20.        ]), 'distance': 3.2887526297621004, 'localFrame': array([[ 0.77249544, -0.11771253,  0.62401487],
       [ 0.15064071,  0.98858858,  0.        ],
       [-0.61689398,  0.09400204,  0.78141246]]), 'currentState': array([13.64484903, 22.18978955, 21.6734283 ,  0.77249544, -0.11771253,
        0.62401487]), 'targetState': array([16.43516252, 22.66904129, 20.        ]), 'previousTarget': array([16.43516252, 22.66904129, 20.        ])}
episode index:19434
target thresh 86.39446232841051
target distance 48.00178204053353
model initialize at round 19434
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.16385902,   9.8735137 ,  74.59250843]), 'distance': 27.499999999999996, 'localFrame': array([[-0.5590268 ,  0.68343522, -0.46947347],
       [-0.77403915, -0.63313774,  0.        ],
       [-0.29724137,  0.36339084,  0.88294658]]), 'currentState': array([-26.92610268,  -7.73837715,  60.53466451,  -0.5590268 ,
         0.68343522,  -0.46947347]), 'targetState': array([15.08160135, 39.19879208, 98.        ]), 'previousTarget': array([-10.89580685,   9.26821613,  75.5529014 ])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6275697569972145
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([15.08160135, 39.19879208, 98.        ]), 'distance': 3.58565280424234, 'localFrame': array([[ 0.89973281,  0.26522859,  0.34660447],
       [-0.28275626,  0.95919179,  0.        ],
       [-0.33246016, -0.09800458,  0.93801138]]), 'currentState': array([12.46698878, 40.42795348, 95.87635469,  0.89973281,  0.26522859,
        0.34660447]), 'targetState': array([15.08160135, 39.19879208, 98.        ]), 'previousTarget': array([15.08160135, 39.19879208, 98.        ])}
episode index:19435
target thresh 86.39582281415225
target distance 35.3605679903165
model initialize at round 19435
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-21.19705196,  -2.2582924 ,  40.90341837]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.71119894, -0.61268746, -0.34468849],
       [ 0.65268592,  0.75762859,  0.        ],
       [ 0.26114586, -0.22497333,  0.93871713]]), 'currentState': array([-40.26218414, -15.09822845,  56.        ,   0.71119894,
        -0.61268746,  -0.34468849]), 'targetState': array([-4.90161615,  8.71631568, 28.        ]), 'previousTarget': array([-21.19705196,  -2.2582924 ,  40.90341837])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6275743958516908
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-4.90161615,  8.71631568, 28.        ]), 'distance': 2.604797636869632, 'localFrame': array([[ 0.8658219 , -0.47368155, -0.1611776 ],
       [ 0.47995679,  0.87729213,  0.        ],
       [ 0.14139984, -0.07735828,  0.98692542]]), 'currentState': array([-7.02870315,  7.83103065, 29.21521276,  0.8658219 , -0.47368155,
       -0.1611776 ]), 'targetState': array([-4.90161615,  8.71631568, 28.        ]), 'previousTarget': array([-4.90161615,  8.71631568, 28.        ])}
episode index:19436
target thresh 86.39718316385222
target distance 72.0
model initialize at round 19436
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.80230131,   4.15082037,  30.50714276]), 'distance': 27.5, 'localFrame': array([[-0.82261912, -0.16256767,  0.54485736],
       [ 0.1938725 , -0.98102673,  0.        ],
       [ 0.53451964,  0.10563286,  0.83852874]]), 'currentState': array([-0.85507003, -0.1106556 ,  5.64255048, -0.82261912, -0.16256767,
        0.54485736]), 'targetState': array([-31.83161948,  11.94771951,  76.        ]), 'previousTarget': array([-10.99377386,   4.12641671,  28.86683779])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275421082355026
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([-31.83161948,  11.94771951,  76.        ]), 'distance': 19.64963813109633, 'localFrame': array([[-0.74353028,  0.19131867,  0.64074947],
       [-0.24919396, -0.9684536 ,  0.        ],
       [ 0.62053613, -0.1596709 ,  0.76775004]]), 'currentState': array([-31.19457336,   6.36304812,  57.17146059,  -0.74353028,
         0.19131867,   0.64074947]), 'targetState': array([-31.83161948,  11.94771951,  76.        ]), 'previousTarget': array([-31.83161948,  11.94771951,  76.        ])}
episode index:19437
target thresh 86.39854337752402
target distance 32.0
model initialize at round 19437
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.13583436, -19.60649367,  72.57380046]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.82555842,  0.39660007, -0.40144947],
       [-0.43302568,  0.90138158,  0.        ],
       [ 0.36185916,  0.17383793,  0.91588117]]), 'currentState': array([ 3.2488053 , -5.64454293, 94.10425768,  0.82555842,  0.39660007,
       -0.40144947]), 'targetState': array([ 17.99144292, -26.46333278,  62.        ]), 'previousTarget': array([ 12.66758941, -20.00393555,  72.49821026])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6275510643257024
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.99144292, -26.46333278,  62.        ]), 'distance': 4.258706226963878, 'localFrame': array([[ 0.38262756, -0.47666229, -0.79144754],
       [ 0.77983206,  0.62598878,  0.        ],
       [ 0.49543728, -0.61719617,  0.6112371 ]]), 'currentState': array([ 15.7295696 , -23.92183504,  64.5615029 ,   0.38262756,
        -0.47666229,  -0.79144754]), 'targetState': array([ 17.99144292, -26.46333278,  62.        ]), 'previousTarget': array([ 17.99144292, -26.46333278,  62.        ])}
episode index:19438
target thresh 86.39990345518125
target distance 33.0
model initialize at round 19438
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 34.71511093, -10.0509099 ,  31.64249907]), 'distance': 27.500000000000004, 'localFrame': array([[-0.30302295, -0.60579439, -0.73565634],
       [ 0.89435292, -0.44736211,  0.        ],
       [-0.32910477, -0.65793639,  0.67735496]]), 'currentState': array([ 29.4631189 , -13.82111607,  58.37173634,  -0.30302295,
        -0.60579439,  -0.73565634]), 'targetState': array([35.82379802, -9.25502543, 26.        ]), 'previousTarget': array([34.87181763, -9.83377108, 31.96713326])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.62756085679822
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([35.82379802, -9.25502543, 26.        ]), 'distance': 2.3478333084077176, 'localFrame': array([[-0.21268296, -0.24876114, -0.94492532],
       [ 0.76007259, -0.64983817,  0.        ],
       [-0.61404854, -0.71821184,  0.32728603]]), 'currentState': array([35.78672759, -9.20655021, 28.34704009, -0.21268296, -0.24876114,
       -0.94492532]), 'targetState': array([35.82379802, -9.25502543, 26.        ]), 'previousTarget': array([35.82379802, -9.25502543, 26.        ])}
episode index:19439
target thresh 86.4012633968375
target distance 72.0
model initialize at round 19439
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.95858989,   9.80336059,  47.0620483 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.21217951, -0.89203619, -0.39906302],
       [ 0.97285781, -0.23140372,  0.        ],
       [-0.09234467, -0.38823158,  0.9169235 ]]), 'currentState': array([-13.24980682,   5.25748617,  73.92894157,  -0.21217951,
        -0.89203619,  -0.39906302]), 'targetState': array([-23.17908441,  17.4278526 ,   2.        ]), 'previousTarget': array([-16.32963547,  10.76433586,  47.09734055])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6275616310135126
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.17908441,  17.4278526 ,   2.        ]), 'distance': 2.9816546519414775, 'localFrame': array([[ 0.0561558 ,  0.86662334, -0.49579281],
       [-0.99790717,  0.06466279,  0.        ],
       [ 0.03205934,  0.4947552 ,  0.86844084]]), 'currentState': array([-21.38783763,  16.83813676,   4.30953127,   0.0561558 ,
         0.86662334,  -0.49579281]), 'targetState': array([-23.17908441,  17.4278526 ,   2.        ]), 'previousTarget': array([-23.17908441,  17.4278526 ,   2.        ])}
episode index:19440
target thresh 86.40262320250642
target distance 44.0
model initialize at round 19440
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.41585746, -1.70152364, 26.01080291]), 'distance': 27.5, 'localFrame': array([[-0.67862533, -0.3351311 ,  0.65357081],
       [ 0.44278826, -0.89662621,  0.        ],
       [ 0.58600872,  0.28939348,  0.75686538]]), 'currentState': array([19.30029838, -3.8822394 ,  4.41427232, -0.67862533, -0.3351311 ,
        0.65357081]), 'targetState': array([-13.99376245,   0.41786645,  47.        ]), 'previousTarget': array([ 3.07653428, -1.22608521, 24.77279348])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6275620746046371
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.99376245,   0.41786645,  47.        ]), 'distance': 3.6943507290352477, 'localFrame': array([[ 0.8551632 , -0.19590629,  0.47991315],
       [ 0.22330186,  0.97474934,  0.        ],
       [-0.46779503,  0.1071655 ,  0.877316  ]]), 'currentState': array([-16.39921834,  -0.83744565,  44.49277029,   0.8551632 ,
        -0.19590629,   0.47991315]), 'targetState': array([-13.99376245,   0.41786645,  47.        ]), 'previousTarget': array([-13.99376245,   0.41786645,  47.        ])}
episode index:19441
target thresh 86.40398287220154
target distance 21.0
model initialize at round 19441
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.87074134,  -1.68136335,  26.        ]), 'distance': 23.66860913328859, 'localFrame': array([[-0.89302828, -0.44421831,  0.07190672],
       [ 0.44537122, -0.89534601,  0.        ],
       [ 0.06438139,  0.03202518,  0.99741136]]), 'currentState': array([-2.3439    , -6.06246295,  4.35971346, -0.89302828, -0.44421831,
        0.07190672]), 'targetState': array([-10.87074134,  -1.68136335,  26.        ]), 'previousTarget': array([-10.87074134,  -1.68136335,  26.        ])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6275731527405685
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.87074134,  -1.68136335,  26.        ]), 'distance': 3.678278484957083, 'localFrame': array([[-0.83975559,  0.43908899, -0.31939225],
       [-0.46335851, -0.88617092,  0.        ],
       [-0.28303613,  0.14799312,  0.9476226 ]]), 'currentState': array([-11.8150828 ,  -4.12744282,  23.42033975,  -0.83975559,
         0.43908899,  -0.31939225]), 'targetState': array([-10.87074134,  -1.68136335,  26.        ]), 'previousTarget': array([-10.87074134,  -1.68136335,  26.        ])}
episode index:19442
target thresh 86.40534240593651
target distance 42.0
model initialize at round 19442
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.66886168, -0.99598567, 48.6215423 ]), 'distance': 27.5, 'localFrame': array([[ 0.06935572, -0.86320798, -0.50006176],
       [ 0.99678777,  0.08008838,  0.        ],
       [ 0.04004914, -0.49845545,  0.86598974]]), 'currentState': array([-10.59216855,  13.09761053,  65.74460984,   0.06935572,
        -0.86320798,  -0.50006176]), 'targetState': array([ 29.05086623, -21.26140097,  24.        ]), 'previousTarget': array([ 5.72452275,  0.22090399, 48.9820973 ])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6275687353113254
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 29.05086623, -21.26140097,  24.        ]), 'distance': 2.8655999438119153, 'localFrame': array([[ 0.55260586, -0.70562825,  0.44352626],
       [ 0.78730186,  0.61656775,  0.        ],
       [-0.27346399,  0.34918905,  0.89626138]]), 'currentState': array([ 26.92596298, -19.70162736,  22.87592017,   0.55260586,
        -0.70562825,   0.44352626]), 'targetState': array([ 29.05086623, -21.26140097,  24.        ]), 'previousTarget': array([ 29.05086623, -21.26140097,  24.        ])}
episode index:19443
target thresh 86.4067018037249
target distance 18.0
model initialize at round 19443
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.86551092, -4.58888062, 29.        ]), 'distance': 18.916998443519542, 'localFrame': array([[-0.18438444, -0.83179836, -0.52355884],
       [ 0.97630117, -0.21641632,  0.        ],
       [-0.11330668, -0.51115111,  0.85198952]]), 'currentState': array([ 4.58942587,  0.44187375, 10.77857613, -0.18438444, -0.83179836,
       -0.52355884]), 'targetState': array([ 3.86551092, -4.58888062, 29.        ]), 'previousTarget': array([ 3.86551092, -4.58888062, 29.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6275820461597312
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.86551092, -4.58888062, 29.        ]), 'distance': 3.2371280663152415, 'localFrame': array([[-0.6223903 ,  0.17612307,  0.76263424],
       [-0.2722865 , -0.96221622,  0.        ],
       [ 0.73381904, -0.20765501,  0.64682998]]), 'currentState': array([ 3.69421698, -2.82348045, 26.29204528, -0.6223903 ,  0.17612307,
        0.76263424]), 'targetState': array([ 3.86551092, -4.58888062, 29.        ]), 'previousTarget': array([ 3.86551092, -4.58888062, 29.        ])}
episode index:19444
target thresh 86.4080610655803
target distance 37.788275796655775
model initialize at round 19444
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.1629354 , 11.74526523, 76.00745111]), 'distance': 27.500000000000004, 'localFrame': array([[-0.40365888,  0.72592277, -0.55686231],
       [-0.87396889, -0.48598187,  0.        ],
       [-0.27062499,  0.48668034,  0.83060482]]), 'currentState': array([11.57322226, -8.05657814, 88.13109151, -0.40365888,  0.72592277,
       -0.55686231]), 'targetState': array([-15.32688721,  28.09068402,  66.        ]), 'previousTarget': array([-2.79407637, 10.39983138, 76.76761514])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6275747131344864
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([-15.32688721,  28.09068402,  66.        ]), 'distance': 3.3380915319298134, 'localFrame': array([[ 0.24231848,  0.69077812,  0.68125424],
       [-0.94362551,  0.33101496,  0.        ],
       [-0.22550535, -0.64284888,  0.7320469 ]]), 'currentState': array([-15.56787455,  25.97727783,  63.42739539,   0.24231848,
         0.69077812,   0.68125424]), 'targetState': array([-15.32688721,  28.09068402,  66.        ]), 'previousTarget': array([-15.32688721,  28.09068402,  66.        ])}
episode index:19445
target thresh 86.4094201915163
target distance 19.0
model initialize at round 19445
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.06621096, 37.80574204, 55.        ]), 'distance': 24.18396977692211, 'localFrame': array([[-0.03461564,  0.52140955, -0.85260415],
       [-0.99780354, -0.06624276,  0.        ],
       [-0.05647885,  0.85073143,  0.52255733]]), 'currentState': array([ 2.74295645e+00,  2.52458657e+01,  7.29037515e+01, -3.46156389e-02,
        5.21409554e-01, -8.52604149e-01]), 'targetState': array([13.06621096, 37.80574204, 55.        ]), 'previousTarget': array([13.06621096, 37.80574204, 55.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275424404453404
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 90, 'trapConfig': [], 'currentTarget': array([13.06621096, 37.80574204, 55.        ]), 'distance': 9.15909275164265, 'localFrame': array([[-0.20201357,  0.87882491, -0.43226994],
       [-0.97458332, -0.22402535,  0.        ],
       [-0.09683942,  0.42128307,  0.90174426]]), 'currentState': array([ 9.13813566, 30.86527263, 59.50434111, -0.20201357,  0.87882491,
       -0.43226994]), 'targetState': array([13.06621096, 37.80574204, 55.        ]), 'previousTarget': array([13.06621096, 37.80574204, 55.        ])}
episode index:19446
target thresh 86.41077918154653
target distance 25.655779111257207
model initialize at round 19446
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.43583349, -9.85082611, 14.82047188]), 'distance': 27.5, 'localFrame': array([[-0.80334703,  0.13877922, -0.57911474],
       [-0.17022986, -0.98540438,  0.        ],
       [-0.5706622 ,  0.09858262,  0.81524605]]), 'currentState': array([14.43470754, -2.42224492, 31.11423445, -0.80334703,  0.13877922,
       -0.57911474]), 'targetState': array([-10.04855179, -11.13672335,  12.        ]), 'previousTarget': array([-5.51066126, -9.7901119 , 15.53751917])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.627550980160628
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.04855179, -11.13672335,  12.        ]), 'distance': 3.1857356211514207, 'localFrame': array([[-0.12388863, -0.74422662, -0.65633707],
       [ 0.98642596, -0.16420665,  0.        ],
       [-0.10777491, -0.64742792,  0.7544678 ]]), 'currentState': array([-11.2356442 ,  -8.95871676,  10.00099767,  -0.12388863,
        -0.74422662,  -0.65633707]), 'targetState': array([-10.04855179, -11.13672335,  12.        ]), 'previousTarget': array([-10.04855179, -11.13672335,  12.        ])}
episode index:19447
target thresh 86.41213803568452
target distance 21.84879549726438
model initialize at round 19447
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-23.8017142 ,  30.89463385,  79.        ]), 'distance': 26.428511461183273, 'localFrame': array([[ 0.05040892,  0.5977107 , -0.80012553],
       [-0.99646252,  0.08403831,  0.        ],
       [ 0.06724119,  0.7972951 ,  0.5998326 ]]), 'currentState': array([-1.19654837e+01,  9.04583836e+00,  8.80000000e+01,  5.04089152e-02,
        5.97710703e-01, -8.00125526e-01]), 'targetState': array([-23.8017142 ,  30.89463385,  79.        ]), 'previousTarget': array([-23.8017142 ,  30.89463385,  79.        ])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.627551754565293
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 28, 'trapConfig': [], 'currentTarget': array([-23.8017142 ,  30.89463385,  79.        ]), 'distance': 3.455839781499772, 'localFrame': array([[ 0.49170269,  0.54029192, -0.68287122],
       [-0.73958007,  0.67306858,  0.        ],
       [ 0.45961917,  0.50503795,  0.73053877]]), 'currentState': array([-23.68907361,  27.96353863,  80.82724423,   0.49170269,
         0.54029192,  -0.68287122]), 'targetState': array([-23.8017142 ,  30.89463385,  79.        ]), 'previousTarget': array([-23.8017142 ,  30.89463385,  79.        ])}
episode index:19448
target thresh 86.41349675394392
target distance 42.0
model initialize at round 19448
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.36276587, 12.71056589, 62.52132298]), 'distance': 27.5, 'localFrame': array([[-0.29252771, -0.68871955,  0.663395  ],
       [ 0.92041658, -0.39093903,  0.        ],
       [ 0.25934699,  0.61059975,  0.74826939]]), 'currentState': array([-1.94480928, 25.35652729, 38.14266306, -0.29252771, -0.68871955,
        0.663395  ]), 'targetState': array([-4.32122908,  4.16256883, 79.        ]), 'previousTarget': array([-3.17537306, 13.85684421, 61.13297495])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6275556568708324
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-4.32122908,  4.16256883, 79.        ]), 'distance': 3.363680769558698, 'localFrame': array([[-0.27876712, -0.59151997,  0.7565666 ],
       [ 0.90458008, -0.42630375,  0.        ],
       [ 0.32252718,  0.68437507,  0.65391664]]), 'currentState': array([-2.30034788,  5.27430119, 76.55164572, -0.27876712, -0.59151997,
        0.7565666 ]), 'targetState': array([-4.32122908,  4.16256883, 79.        ]), 'previousTarget': array([-4.32122908,  4.16256883, 79.        ])}
episode index:19449
target thresh 86.41485533633826
target distance 55.0
model initialize at round 19449
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.23358226, -16.72869994,  72.29082417]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.18382024, -0.62970266, -0.75477459],
       [ 0.95993565,  0.28022052,  0.        ],
       [ 0.21150333, -0.72453504,  0.65598424]]), 'currentState': array([-7.94671863, -2.97517178, 93.31679229,  0.18382024, -0.62970266,
       -0.75477459]), 'targetState': array([ 20.40383232, -37.85080747,  40.        ]), 'previousTarget': array([  2.55706889, -16.18471199,  73.83521079])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6275571017841334
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 20.40383232, -37.85080747,  40.        ]), 'distance': 3.6474641913936643, 'localFrame': array([[ 0.39325555, -0.91779993, -0.05471162],
       [ 0.91917667,  0.39384545,  0.        ],
       [ 0.02154792, -0.05028965,  0.9985022 ]]), 'currentState': array([ 19.45822661, -35.5737428 ,  42.68789905,   0.39325555,
        -0.91779993,  -0.05471162]), 'targetState': array([ 20.40383232, -37.85080747,  40.        ]), 'previousTarget': array([ 20.40383232, -37.85080747,  40.        ])}
episode index:19450
target thresh 86.41621378288119
target distance 42.39847270746931
model initialize at round 19450
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.7356777 ,  9.79996158, 32.22992927]), 'distance': 27.5, 'localFrame': array([[ 0.85484566, -0.03242087, -0.5178685 ],
       [ 0.03789874,  0.99928158,  0.        ],
       [ 0.51749645, -0.01962656,  0.85546024]]), 'currentState': array([-7.79958142e+00,  4.75288878e+00,  4.71610763e+01,  8.54845661e-01,
       -3.24208669e-02, -5.17868500e-01]), 'targetState': array([33.1940469 , 13.93396034, 20.        ]), 'previousTarget': array([13.30905294,  9.23212328, 33.13207281])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6275621104332606
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.1940469 , 13.93396034, 20.        ]), 'distance': 3.3231241963131137, 'localFrame': array([[ 0.54295842, -0.21474406, -0.81183812],
       [ 0.36778638,  0.92991031,  0.        ],
       [ 0.75493664, -0.29858301,  0.58388258]]), 'currentState': array([35.15121633, 14.26565032, 22.66507485,  0.54295842, -0.21474406,
       -0.81183812]), 'targetState': array([33.1940469 , 13.93396034, 20.        ]), 'previousTarget': array([33.1940469 , 13.93396034, 20.        ])}
episode index:19451
target thresh 86.41757209358623
target distance 62.0
model initialize at round 19451
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 19.18400937, -18.9821727 ,  47.63256604]), 'distance': 27.5, 'localFrame': array([[ 0.82518471,  0.23904286,  0.51178971],
       [-0.27824451,  0.96051028,  0.        ],
       [-0.49157927, -0.14240268,  0.85911076]]), 'currentState': array([ 10.5072819 , -14.35758076,  73.31480796,   0.82518471,
         0.23904286,   0.51178971]), 'targetState': array([ 31.89814549, -25.75865513,  10.        ]), 'previousTarget': array([ 18.70595787, -19.61477986,  46.45939504])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6275649375196567
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 31.89814549, -25.75865513,  10.        ]), 'distance': 2.80310312491734, 'localFrame': array([[-0.37435548, -0.43189847, -0.82056181],
       [ 0.75565144, -0.65497397,  0.        ],
       [-0.53744663, -0.62005871,  0.5715578 ]]), 'currentState': array([ 31.89813117, -24.98579425,  12.6944523 ,  -0.37435548,
        -0.43189847,  -0.82056181]), 'targetState': array([ 31.89814549, -25.75865513,  10.        ]), 'previousTarget': array([ 31.89814549, -25.75865513,  10.        ])}
episode index:19452
target thresh 86.418930268467
target distance 29.16795933319259
model initialize at round 19452
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.81962915,  -4.99148173,  67.87256988]), 'distance': 27.5, 'localFrame': array([[ 0.73452194,  0.22496937,  0.64020801],
       [-0.29285199,  0.95615779,  0.        ],
       [-0.61213988, -0.18748619,  0.7682016 ]]), 'currentState': array([-29.35394516, -24.83147901,  63.50179901,   0.73452194,
         0.22496937,   0.64020801]), 'targetState': array([-1.79823151,  4.66544354, 70.        ]), 'previousTarget': array([-11.46712145,  -5.21494266,  67.29007132])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6275734734445075
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.79823151,  4.66544354, 70.        ]), 'distance': 3.9272810288597646, 'localFrame': array([[ 0.34078421,  0.88490134, -0.31751494],
       [-0.93319092,  0.359381  ,  0.        ],
       [ 0.11410884,  0.29630206,  0.94825327]]), 'currentState': array([-3.51350188,  1.8539997 , 67.86056845,  0.34078421,  0.88490134,
       -0.31751494]), 'targetState': array([-1.79823151,  4.66544354, 70.        ]), 'previousTarget': array([-1.79823151,  4.66544354, 70.        ])}
episode index:19453
target thresh 86.42028830753705
target distance 44.0
model initialize at round 19453
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.65454951, -9.27477991, 50.59372336]), 'distance': 27.5, 'localFrame': array([[-0.14465436,  0.71750461, -0.68136792],
       [-0.98027645, -0.19763115,  0.        ],
       [-0.13465953,  0.66792893,  0.73194109]]), 'currentState': array([13.53944789,  8.70992213, 70.22529658, -0.14465436,  0.71750461,
       -0.68136792]), 'targetState': array([ -1.26919087, -29.97314055,  28.        ]), 'previousTarget': array([ 6.88810118, -9.27776316, 51.87850247])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6275729448471752
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ -1.26919087, -29.97314055,  28.        ]), 'distance': 3.333756790929969, 'localFrame': array([[-0.51514082, -0.82952014, -0.2156995 ],
       [ 0.84951797, -0.52755969,  0.        ],
       [-0.11379436, -0.1832406 ,  0.97645979]]), 'currentState': array([ -0.21218674, -28.18066236,  30.60455343,  -0.51514082,
        -0.82952014,  -0.2156995 ]), 'targetState': array([ -1.26919087, -29.97314055,  28.        ]), 'previousTarget': array([ -1.26919087, -29.97314055,  28.        ])}
episode index:19454
target thresh 86.42164621081001
target distance 15.951017882481255
model initialize at round 19454
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-38.4713378, -10.9524503,  48.       ]), 'distance': 18.444194338783443, 'localFrame': array([[ 0.05448788, -0.89498   , -0.44276617],
       [ 0.99815185,  0.06076915,  0.        ],
       [ 0.02690652, -0.44194787,  0.89663712]]), 'currentState': array([-3.94031755e+01,  3.35393418e+00,  5.96037643e+01,  5.44878770e-02,
       -8.94979997e-01, -4.42766165e-01]), 'targetState': array([-38.4713378, -10.9524503,  48.       ]), 'previousTarget': array([-38.4713378, -10.9524503,  48.       ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6275876425754012
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-38.4713378, -10.9524503,  48.       ]), 'distance': 2.941568001142284, 'localFrame': array([[ 0.32704082, -0.92596285,  0.18877792],
       [ 0.94291668,  0.33302874,  0.        ],
       [-0.06286847,  0.17800185,  0.9820198 ]]), 'currentState': array([-37.74864963,  -8.43000012,  49.32958234,   0.32704082,
        -0.92596285,   0.18877792]), 'targetState': array([-38.4713378, -10.9524503,  48.       ]), 'previousTarget': array([-38.4713378, -10.9524503,  48.       ])}
episode index:19455
target thresh 86.42300397829942
target distance 54.206042352143626
model initialize at round 19455
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.53481307, 14.61903457, 53.00791169]), 'distance': 27.5, 'localFrame': array([[ 0.18498827, -0.61740552, -0.7645847 ],
       [ 0.95792589,  0.28701566,  0.        ],
       [ 0.21944778, -0.73241548,  0.64452326]]), 'currentState': array([10.99781951, 34.36192811, 71.33302717,  0.18498827, -0.61740552,
       -0.7645847 ]), 'targetState': array([ 25.90395499, -18.78789813,  22.        ]), 'previousTarget': array([16.20605594, 15.79453755, 54.53704095])}
done in step count: 94
reward sum = 0.3887839180742268
running average episode reward sum: 0.627575368535285
{'scaleFactor': 20, 'timeStep': 95, 'trapCount': 46, 'trapConfig': [], 'currentTarget': array([ 25.90395499, -18.78789813,  22.        ]), 'distance': 1.6994469257012386, 'localFrame': array([[-0.18889896,  0.59177277, -0.78365947],
       [-0.95264275, -0.30409176,  0.        ],
       [-0.23830439,  0.74654751,  0.62119066]]), 'currentState': array([ 25.49636373, -18.57989734,  23.63668106,  -0.18889896,
         0.59177277,  -0.78365947]), 'targetState': array([ 25.90395499, -18.78789813,  22.        ]), 'previousTarget': array([ 25.90395499, -18.78789813,  22.        ])}
episode index:19456
target thresh 86.42436161001888
target distance 20.002106833107064
model initialize at round 19456
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.22476925,  8.52653425, 26.        ]), 'distance': 25.61164034167003, 'localFrame': array([[ 0.1914367 ,  0.65361108,  0.73221892],
       [-0.9596836 ,  0.28108253,  0.        ],
       [-0.20581395, -0.70269848,  0.68106935]]), 'currentState': array([ 21.09200818, -10.7346566 ,  20.23863065,   0.1914367 ,
         0.65361108,   0.73221892]), 'targetState': array([ 5.22476925,  8.52653425, 26.        ]), 'previousTarget': array([ 5.22476925,  8.52653425, 26.        ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6275847303330794
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.22476925,  8.52653425, 26.        ]), 'distance': 3.1227173135563, 'localFrame': array([[-0.58354145,  0.47723242,  0.65706057],
       [-0.63307044, -0.77409419,  0.        ],
       [ 0.50862677, -0.41596562,  0.75383779]]), 'currentState': array([ 6.99762399,  6.15224538, 25.014555  , -0.58354145,  0.47723242,
        0.65706057]), 'targetState': array([ 5.22476925,  8.52653425, 26.        ]), 'previousTarget': array([ 5.22476925,  8.52653425, 26.        ])}
episode index:19457
target thresh 86.42571910598194
target distance 11.0
model initialize at round 19457
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 60.]), 'distance': 10.904201665460043, 'localFrame': array([[ 0.67069269,  0.2524218 , -0.69746294],
       [-0.35223898,  0.93591009,  0.        ],
       [ 0.6527626 ,  0.24567363,  0.71662085]]), 'currentState': array([-2.13489114, -3.75883169, 70.01074613,  0.67069269,  0.2524218 ,
       -0.69746294]), 'targetState': array([ 0.,  0., 60.]), 'previousTarget': array([ 0.,  0., 60.])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6276013510196642
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 60.]), 'distance': 2.407060158042962, 'localFrame': array([[-0.44198104,  0.23351561, -0.86609654],
       [-0.46714635, -0.88418001,  0.        ],
       [-0.76578525,  0.40459384,  0.49987677]]), 'currentState': array([-1.16328411,  0.26398751, 62.09069828, -0.44198104,  0.23351561,
       -0.86609654]), 'targetState': array([ 0.,  0., 60.]), 'previousTarget': array([ 0.,  0., 60.])}
episode index:19458
target thresh 86.42707646620221
target distance 54.0
model initialize at round 19458
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.591327  ,  3.35469552, 75.74433298]), 'distance': 27.5, 'localFrame': array([[ 0.24290504,  0.6819134 , -0.6899212 ],
       [-0.94201964,  0.33555774,  0.        ],
       [ 0.2315084 ,  0.64991932,  0.72388448]]), 'currentState': array([14.9581372 , -5.07137558, 96.02650645,  0.24290504,  0.6819134 ,
       -0.6899212 ]), 'targetState': array([-28.30942834,  16.9580738 ,  43.        ]), 'previousTarget': array([-2.27795913,  2.73250968, 76.43060039])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6275998789324782
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-28.30942834,  16.9580738 ,  43.        ]), 'distance': 2.501106642129929, 'localFrame': array([[-0.64847872,  0.69235626, -0.31641455],
       [-0.72985549, -0.68360146,  0.        ],
       [-0.21630145,  0.2309369 ,  0.94862102]]), 'currentState': array([-28.24943411,  16.2486466 ,  45.39763387,  -0.64847872,
         0.69235626,  -0.31641455]), 'targetState': array([-28.30942834,  16.9580738 ,  43.        ]), 'previousTarget': array([-28.30942834,  16.9580738 ,  43.        ])}
episode index:19459
target thresh 86.42843369069323
target distance 20.0
model initialize at round 19459
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.51573815, -1.30481334, 23.        ]), 'distance': 18.699261603408516, 'localFrame': array([[ 0.3434407 , -0.07259802,  0.93636425],
       [ 0.20681436,  0.9783802 ,  0.        ],
       [-0.91612025,  0.19365358,  0.3510299 ]]), 'currentState': array([ 2.10408903,  0.25864909,  4.72119203,  0.3434407 , -0.07259802,
        0.93636425]), 'targetState': array([-1.51573815, -1.30481334, 23.        ]), 'previousTarget': array([-1.51573815, -1.30481334, 23.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6276141020669116
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.51573815, -1.30481334, 23.        ]), 'distance': 3.3980407894382445, 'localFrame': array([[ 0.82858009,  0.2158137 ,  0.5166038 ],
       [-0.25205268,  0.96771351,  0.        ],
       [-0.49992448, -0.13021137,  0.85622457]]), 'currentState': array([ 0.49521089, -1.10096948, 20.26847793,  0.82858009,  0.2158137 ,
        0.5166038 ]), 'targetState': array([-1.51573815, -1.30481334, 23.        ]), 'previousTarget': array([-1.51573815, -1.30481334, 23.        ])}
episode index:19460
target thresh 86.42979077946859
target distance 15.0
model initialize at round 19460
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.99798156, -0.12705695, 56.        ]), 'distance': 17.991644607335402, 'localFrame': array([[ 0.55229625, -0.28926451, -0.7818535 ],
       [ 0.4639648 ,  0.88585364,  0.        ],
       [ 0.69260777, -0.3627525 ,  0.62346219]]), 'currentState': array([-2.545671  , 11.70220043, 69.47808367,  0.55229625, -0.28926451,
       -0.7818535 ]), 'targetState': array([-3.99798156, -0.12705695, 56.        ]), 'previousTarget': array([-3.99798156, -0.12705695, 56.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6276287931488405
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.99798156, -0.12705695, 56.        ]), 'distance': 2.184029870168221, 'localFrame': array([[-0.29418433, -0.95573837,  0.00444276],
       [ 0.95574781, -0.29418724,  0.        ],
       [ 0.001307  ,  0.00424615,  0.99999013]]), 'currentState': array([-4.44833050e+00,  1.46134666e+00,  5.74297364e+01, -2.94184333e-01,
       -9.55738374e-01,  4.44275556e-03]), 'targetState': array([-3.99798156, -0.12705695, 56.        ]), 'previousTarget': array([-3.99798156, -0.12705695, 56.        ])}
episode index:19461
target thresh 86.43114773254186
target distance 79.0
model initialize at round 19461
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([20.73404691, 25.13281261, 39.01452044]), 'distance': 27.5, 'localFrame': array([[ 0.87955572, -0.46768599, -0.0874732 ],
       [ 0.46948559,  0.88294014,  0.        ],
       [ 0.0772336 , -0.04106741,  0.99616687]]), 'currentState': array([27.65968895, 34.21902466, 14.        ,  0.87955572, -0.46768599,
       -0.0874732 ]), 'targetState': array([ 5.78736395,  5.52326159, 93.        ]), 'previousTarget': array([20.73404691, 25.13281261, 39.01452044])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.627626707442787
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 5.78736395,  5.52326159, 93.        ]), 'distance': 1.960283185416292, 'localFrame': array([[ 0.22592405, -0.35863113,  0.90572735],
       [ 0.84610595,  0.53301475,  0.        ],
       [-0.48276604,  0.7663413 ,  0.42386079]]), 'currentState': array([ 6.70779171,  5.71272587, 91.27964359,  0.22592405, -0.35863113,
        0.90572735]), 'targetState': array([ 5.78736395,  5.52326159, 93.        ]), 'previousTarget': array([ 5.78736395,  5.52326159, 93.        ])}
episode index:19462
target thresh 86.43250454992659
target distance 47.959071852694194
model initialize at round 19462
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.07801459,  9.11633463, 12.32558375]), 'distance': 27.5, 'localFrame': array([[ 0.47065924, -0.32563957,  0.82002363],
       [ 0.56897202,  0.82235688,  0.        ],
       [-0.67435207,  0.4665705 ,  0.57232967]]), 'currentState': array([-13.7453964 ,  -0.13091655,  10.35332861,   0.47065924,
        -0.32563957,   0.82002363]), 'targetState': array([34.00171921, 16.96711792, 14.        ]), 'previousTarget': array([12.02259709,  9.69168931, 11.70855427])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.627626496719232
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([34.00171921, 16.96711792, 14.        ]), 'distance': 4.199585412007244, 'localFrame': array([[ 0.26044997, -0.66156285,  0.70320723],
       [ 0.93048785,  0.36632276,  0.        ],
       [-0.25760081,  0.65432578,  0.71098495]]), 'currentState': array([31.14147887, 18.64030815, 11.42008193,  0.26044997, -0.66156285,
        0.70320723]), 'targetState': array([34.00171921, 16.96711792, 14.        ]), 'previousTarget': array([34.00171921, 16.96711792, 14.        ])}
episode index:19463
target thresh 86.43386123163639
target distance 9.36267656897586
model initialize at round 19463
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.52860401, -2.58135038,  9.        ]), 'distance': 12.780726084145527, 'localFrame': array([[-0.34289367, -0.37866548,  0.85967225],
       [ 0.74125176, -0.67122711,  0.        ],
       [ 0.57703532,  0.63723357,  0.51084598]]), 'currentState': array([ -1.4026716 , -13.36869048,   2.14694274,  -0.34289367,
        -0.37866548,   0.85967225]), 'targetState': array([-1.52860401, -2.58135038,  9.        ]), 'previousTarget': array([-1.52860401, -2.58135038,  9.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6276421378439334
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.52860401, -2.58135038,  9.        ]), 'distance': 1.7824696992546574, 'localFrame': array([[ 0.35315334,  0.79271855,  0.49687022],
       [-0.91345453,  0.4069408 ,  0.        ],
       [-0.20219677, -0.45386835,  0.86782486]]), 'currentState': array([-2.02359635, -4.28866351,  8.86861251,  0.35315334,  0.79271855,
        0.49687022]), 'targetState': array([-1.52860401, -2.58135038,  9.        ]), 'previousTarget': array([-1.52860401, -2.58135038,  9.        ])}
episode index:19464
target thresh 86.4352177776848
target distance 73.10804173962484
model initialize at round 19464
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.67084138, -13.6952784 ,  50.64905735]), 'distance': 27.5, 'localFrame': array([[-0.09442683, -0.1308494 ,  0.98689514],
       [ 0.81090127, -0.58518299,  0.        ],
       [ 0.57751424,  0.80027452,  0.16136292]]), 'currentState': array([  1.91020691, -34.34408771,  33.79170004,  -0.09442683,
        -0.1308494 ,   0.98689514]), 'targetState': array([25.65566512, 38.18097494, 93.        ]), 'previousTarget': array([  8.82077164, -14.42189566,  49.10913615])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6276338274733876
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([25.65566512, 38.18097494, 93.        ]), 'distance': 2.921003691654998, 'localFrame': array([[ 0.42496953,  0.88874243, -0.17186564],
       [-0.9021663 ,  0.43138841,  0.        ],
       [ 0.07414084,  0.15505139,  0.9851204 ]]), 'currentState': array([24.42617757, 38.16325343, 90.35041345,  0.42496953,  0.88874243,
       -0.17186564]), 'targetState': array([25.65566512, 38.18097494, 93.        ]), 'previousTarget': array([25.65566512, 38.18097494, 93.        ])}
episode index:19465
target thresh 86.43657418808537
target distance 25.957136253145666
model initialize at round 19465
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.34965971,  3.910833  , 95.54687876]), 'distance': 27.499999999999996, 'localFrame': array([[-0.32847504, -0.85564284,  0.39997434],
       [ 0.93357147, -0.35839128,  0.        ],
       [ 0.14334731,  0.37340463,  0.91652634]]), 'currentState': array([17.71048249, 24.47128624, 98.25778751, -0.32847504, -0.85564284,
        0.39997434]), 'targetState': array([-3.99297974, -0.23688136, 95.        ]), 'previousTarget': array([ 0.85476849,  5.22664467, 95.63144786])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6276323542471979
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 33, 'trapConfig': [], 'currentTarget': array([-3.99297974, -0.23688136, 95.        ]), 'distance': 3.232091600603357, 'localFrame': array([[-0.04984641, -0.99703829, -0.0585661 ],
       [ 0.99875261, -0.04993212,  0.        ],
       [-0.00292433, -0.05849304,  0.99828353]]), 'currentState': array([-2.69181797e+00,  2.16175961e+00,  9.67320265e+01, -4.98464130e-02,
       -9.97038288e-01, -5.85660976e-02]), 'targetState': array([-3.99297974, -0.23688136, 95.        ]), 'previousTarget': array([-3.99297974, -0.23688136, 95.        ])}
episode index:19466
target thresh 86.4379304628517
target distance 23.79092711627495
model initialize at round 19466
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.66856824,  -4.13818855,  33.        ]), 'distance': 24.171733332405424, 'localFrame': array([[-0.27149193, -0.9584517 , -0.08753551],
       [ 0.96214499, -0.27253809,  0.        ],
       [-0.02385676, -0.08422186,  0.9961614 ]]), 'currentState': array([-16.46482943,  18.20823908,  32.55160672,  -0.27149193,
        -0.9584517 ,  -0.08753551]), 'targetState': array([-25.66856824,  -4.13818855,  33.        ]), 'previousTarget': array([-25.66856824,  -4.13818855,  33.        ])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6276434145461209
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.66856824,  -4.13818855,  33.        ]), 'distance': 2.418741158874607, 'localFrame': array([[-0.82955236,  0.51082798, -0.22560508],
       [-0.52434623, -0.85150515,  0.        ],
       [-0.19210389,  0.11829517,  0.97421884]]), 'currentState': array([-23.68055791,  -4.46134789,  34.33928778,  -0.82955236,
         0.51082798,  -0.22560508]), 'targetState': array([-25.66856824,  -4.13818855,  33.        ]), 'previousTarget': array([-25.66856824,  -4.13818855,  33.        ])}
episode index:19467
target thresh 86.43928660199732
target distance 26.210821503501208
model initialize at round 19467
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.48051871, -7.35430474, 13.43261726]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.30381236, -0.75183379, -0.58518715],
       [ 0.92716167,  0.3746615 ,  0.        ],
       [ 0.21924709, -0.54256309,  0.81089827]]), 'currentState': array([-10.48519322,  15.47575087,  25.8404534 ,   0.30381236,
        -0.75183379,  -0.58518715]), 'targetState': array([-0.44083282, -9.9902786 , 12.        ]), 'previousTarget': array([-2.09325899, -6.14356469, 14.20140787])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276111747980962
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 88, 'trapConfig': [], 'currentTarget': array([-0.44083282, -9.9902786 , 12.        ]), 'distance': 21.188983896727677, 'localFrame': array([[ 0.8123356 , -0.18572105,  0.55282779],
       [ 0.22287538,  0.97484695,  0.        ],
       [-0.53892249,  0.1232117 ,  0.83329552]]), 'currentState': array([-7.39513941,  9.48485375, 16.61842816,  0.8123356 , -0.18572105,
        0.55282779]), 'targetState': array([-0.44083282, -9.9902786 , 12.        ]), 'previousTarget': array([-0.44083282, -9.9902786 , 12.        ])}
episode index:19468
target thresh 86.44064260553583
target distance 53.0
model initialize at round 19468
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.21742945, 12.40120277, 68.35868467]), 'distance': 27.5, 'localFrame': array([[-0.09715815,  0.71156575,  0.69586959],
       [-0.99080658, -0.13528607,  0.        ],
       [ 0.09414146, -0.68947217,  0.71816817]]), 'currentState': array([-0.39579183,  1.41248738, 47.81721691, -0.09715815,  0.71156575,
        0.69586959]), 'targetState': array([ 36.7270937 ,  29.32781253, 100.        ]), 'previousTarget': array([13.53385293, 11.21278735, 67.40997388])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6276116152065028
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 36.7270937 ,  29.32781253, 100.        ]), 'distance': 2.8339922441470415, 'localFrame': array([[ 0.67261099, -0.20311729,  0.71157419],
       [ 0.28908927,  0.95730214,  0.        ],
       [-0.68119149,  0.20570846,  0.70261097]]), 'currentState': array([34.87171984, 28.343821  , 98.09714411,  0.67261099, -0.20311729,
        0.71157419]), 'targetState': array([ 36.7270937 ,  29.32781253, 100.        ]), 'previousTarget': array([ 36.7270937 ,  29.32781253, 100.        ])}
episode index:19469
target thresh 86.44199847348074
target distance 51.13709599766797
model initialize at round 19469
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.73561547, -13.81818446,  43.28215509]), 'distance': 27.5, 'localFrame': array([[ 0.18934194, -0.96462177, -0.18345153],
       [ 0.98127523,  0.19261078,  0.        ],
       [ 0.03533474, -0.18001644,  0.98302876]]), 'currentState': array([-16.27974915,  10.80666306,  49.50213336,   0.18934194,
        -0.96462177,  -0.18345153]), 'targetState': array([  4.91392059, -38.6891895 ,  37.        ]), 'previousTarget': array([ -6.3603878 , -12.27523273,  43.19838642])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6276166161680206
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.91392059, -38.6891895 ,  37.        ]), 'distance': 2.9810369911058276, 'localFrame': array([[ 0.91143387, -0.40433028, -0.07619265],
       [ 0.40550905,  0.91409103,  0.        ],
       [ 0.06964701, -0.03089681,  0.99709312]]), 'currentState': array([  2.68569662, -37.69735156,  38.71401779,   0.91143387,
        -0.40433028,  -0.07619265]), 'targetState': array([  4.91392059, -38.6891895 ,  37.        ]), 'previousTarget': array([  4.91392059, -38.6891895 ,  37.        ])}
episode index:19470
target thresh 86.44335420584565
target distance 55.24401605558362
model initialize at round 19470
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.82654853,  17.56913027,  93.27188325]), 'distance': 27.499999999999996, 'localFrame': array([[-0.5975981 , -0.65280479,  0.46553456],
       [ 0.73760777, -0.67522942,  0.        ],
       [ 0.31434263,  0.34338191,  0.8850297 ]]), 'currentState': array([-2.49057843, 43.60809946, 96.22716428, -0.5975981 , -0.65280479,
        0.46553456]), 'targetState': array([-20.05555947, -11.2594198 ,  90.        ]), 'previousTarget': array([-10.0298231 ,  18.05884245,  92.6535238 ])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6276212442773335
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.05555947, -11.2594198 ,  90.        ]), 'distance': 2.2807484221265244, 'localFrame': array([[ 0.13195369, -0.96915556,  0.20814832],
       [ 0.99085805,  0.13490856,  0.        ],
       [-0.02808099,  0.20624544,  0.97809727]]), 'currentState': array([-21.03905673,  -9.2477286 ,  89.56681991,   0.13195369,
        -0.96915556,   0.20814832]), 'targetState': array([-20.05555947, -11.2594198 ,  90.        ]), 'previousTarget': array([-20.05555947, -11.2594198 ,  90.        ])}
episode index:19471
target thresh 86.4447098026441
target distance 53.8258651876114
model initialize at round 19471
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.38916793, -1.10066033, 78.38547768]), 'distance': 27.5, 'localFrame': array([[ 0.51490498, -0.16089837, -0.84201222],
       [ 0.29825905,  0.95448496,  0.        ],
       [ 0.803688  , -0.25113777,  0.53945846]]), 'currentState': array([-11.03243845,  -3.13414366,  78.79833217,   0.51490498,
        -0.16089837,  -0.84201222]), 'targetState': array([41.99241868,  0.79798057, 78.        ]), 'previousTarget': array([15.6107879 , -0.56965127, 78.98025849])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275890122906718
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 74, 'trapConfig': [], 'currentTarget': array([41.99241868,  0.79798057, 78.        ]), 'distance': 17.319932232030446, 'localFrame': array([[ 0.32839736,  0.71207817, -0.62056415],
       [-0.90808264,  0.41879101,  0.        ],
       [ 0.25988669,  0.56352353,  0.78415569]]), 'currentState': array([25.28877905,  4.54707143, 80.62921909,  0.32839736,  0.71207817,
       -0.62056415]), 'targetState': array([41.99241868,  0.79798057, 78.        ]), 'previousTarget': array([41.99241868,  0.79798057, 78.        ])}
episode index:19472
target thresh 86.44606526388962
target distance 45.82947625350161
model initialize at round 19472
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.63622616, -7.88357436, 31.73461889]), 'distance': 27.5, 'localFrame': array([[-0.62286289, -0.4656976 , -0.62862355],
       [ 0.59880644, -0.80089378,  0.        ],
       [-0.50346069, -0.37642383,  0.77770973]]), 'currentState': array([26.06132428, 14.95158758, 21.52322763, -0.62286289, -0.4656976 ,
       -0.62862355]), 'targetState': array([  3.15072106, -30.83947076,  42.        ]), 'previousTarget': array([15.16566826, -7.88100183, 32.48187007])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6275884834111284
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  3.15072106, -30.83947076,  42.        ]), 'distance': 2.9060927387722413, 'localFrame': array([[ 0.75691162,  0.43757325,  0.48540133],
       [-0.50048899,  0.8657429 ,  0.        ],
       [-0.42023275, -0.24293802,  0.87429146]]), 'currentState': array([  3.92394317, -30.58066752,  39.21064103,   0.75691162,
         0.43757325,   0.48540133]), 'targetState': array([  3.15072106, -30.83947076,  42.        ]), 'previousTarget': array([  3.15072106, -30.83947076,  42.        ])}
episode index:19473
target thresh 86.44742058959582
target distance 29.0
model initialize at round 19473
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.70636142e-03, -3.53215543e+00,  6.52840715e+01]), 'distance': 27.499999999999996, 'localFrame': array([[-0.91344506, -0.34383843,  0.21769991],
       [ 0.35228778, -0.93589172,  0.        ],
       [ 0.20374354,  0.07669302,  0.97601575]]), 'currentState': array([13.98613277, -2.85874336, 41.61189049, -0.91344506, -0.34383843,
        0.21769991]), 'targetState': array([-3.36879174, -3.69475875, 71.        ]), 'previousTarget': array([ 0.6303095 , -3.48375626, 64.94136717])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6275966013970273
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-3.36879174, -3.69475875, 71.        ]), 'distance': 2.9090813966668727, 'localFrame': array([[-0.01123244, -0.2315682 ,  0.97275382],
       [ 0.99882566, -0.04844901,  0.        ],
       [ 0.04712896,  0.97161148,  0.23184046]]), 'currentState': array([-3.56608007e+00, -4.27785983e+00,  6.81567932e+01, -1.12324401e-02,
       -2.31568197e-01,  9.72753824e-01]), 'targetState': array([-3.36879174, -3.69475875, 71.        ]), 'previousTarget': array([-3.36879174, -3.69475875, 71.        ])}
episode index:19474
target thresh 86.44877577977623
target distance 40.23774792346938
model initialize at round 19474
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.1386069 , 29.31314469, 75.6735201 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.60082061, -0.24137139, -0.76207247],
       [ 0.3727791 , -0.92792012,  0.        ],
       [-0.70714237, -0.28408469,  0.64749174]]), 'currentState': array([27.35193576, 18.40562944, 76.9241835 , -0.60082061, -0.24137139,
       -0.76207247]), 'targetState': array([-11.43953396,  35.18717185,  75.        ]), 'previousTarget': array([ 3.55125383, 28.81150059, 76.11766602])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6276039157253667
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.43953396,  35.18717185,  75.        ]), 'distance': 2.745109258972898, 'localFrame': array([[-0.67667145,  0.47899949, -0.55917371],
       [-0.57776876, -0.8162005 ,  0.        ],
       [-0.45639787,  0.3230731 ,  0.82905052]]), 'currentState': array([-10.10932468,  33.12090741,  73.77659517,  -0.67667145,
         0.47899949,  -0.55917371]), 'targetState': array([-11.43953396,  35.18717185,  75.        ]), 'previousTarget': array([-11.43953396,  35.18717185,  75.        ])}
episode index:19475
target thresh 86.4501308344444
target distance 54.977743497657144
model initialize at round 19475
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.67134488,  9.6603442 , 63.5045255 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.47187102, -0.43997494,  0.76404175],
       [ 0.68195533,  0.73139383,  0.        ],
       [-0.55881542,  0.52104234,  0.64516681]]), 'currentState': array([-22.07840292,  24.70299942,  48.4392391 ,   0.47187102,
        -0.43997494,   0.76404175]), 'targetState': array([ 32.87527691, -22.78631536,  96.        ]), 'previousTarget': array([-4.97013605, 10.68392853, 62.26952804])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6275963442292097
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 32.87527691, -22.78631536,  96.        ]), 'distance': 2.5048898986724346, 'localFrame': array([[ 0.11891943, -0.31672105,  0.94103451],
       [ 0.93618427,  0.35150963,  0.        ],
       [-0.33078269,  0.8809817 ,  0.33831059]]), 'currentState': array([ 30.91111569, -22.37102245,  94.50197601,   0.11891943,
        -0.31672105,   0.94103451]), 'targetState': array([ 32.87527691, -22.78631536,  96.        ]), 'previousTarget': array([ 32.87527691, -22.78631536,  96.        ])}
episode index:19476
target thresh 86.45148575361385
target distance 22.0
model initialize at round 19476
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.30302547, -9.23920737, 65.        ]), 'distance': 25.30778535553062, 'localFrame': array([[ 0.53826253, -0.45876761, -0.70696939],
       [ 0.64866937,  0.76107033,  0.        ],
       [ 0.53805343, -0.45858939,  0.70724414]]), 'currentState': array([11.0596303 , -3.88182767, 85.89006861,  0.53826253, -0.45876761,
       -0.70696939]), 'targetState': array([24.30302547, -9.23920737, 65.        ]), 'previousTarget': array([24.30302547, -9.23920737, 65.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6276091760143291
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.30302547, -9.23920737, 65.        ]), 'distance': 4.099286744530114, 'localFrame': array([[-0.15198852, -0.91828426, -0.36558653],
       [ 0.98657778, -0.16329202,  0.        ],
       [-0.05969736, -0.36067954,  0.93077736]]), 'currentState': array([21.87619499, -7.13136892, 67.54394629, -0.15198852, -0.91828426,
       -0.36558653]), 'targetState': array([24.30302547, -9.23920737, 65.        ]), 'previousTarget': array([24.30302547, -9.23920737, 65.        ])}
episode index:19477
target thresh 86.45284053729819
target distance 11.0
model initialize at round 19477
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.37543242, 21.87138132, 12.        ]), 'distance': 12.629218127654884, 'localFrame': array([[ 0.75512607, -0.07861931,  0.65084839],
       [ 0.10355442,  0.99462379,  0.        ],
       [-0.64734929,  0.06739822,  0.75920773]]), 'currentState': array([-1.94439588, 27.89174496,  1.77300359,  0.75512607, -0.07861931,
        0.65084839]), 'targetState': array([ 2.37543242, 21.87138132, 12.        ]), 'previousTarget': array([ 2.37543242, 21.87138132, 12.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6276257783797612
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.37543242, 21.87138132, 12.        ]), 'distance': 3.956396809780513, 'localFrame': array([[-0.14647924, -0.76740477,  0.6242065 ],
       [ 0.98226629, -0.18749117,  0.        ],
       [ 0.11703321,  0.61313701,  0.78125939]]), 'currentState': array([-0.16576468, 23.43338522,  9.40085844, -0.14647924, -0.76740477,
        0.6242065 ]), 'targetState': array([ 2.37543242, 21.87138132, 12.        ]), 'previousTarget': array([ 2.37543242, 21.87138132, 12.        ])}
episode index:19478
target thresh 86.45419518551091
target distance 60.505181415076294
model initialize at round 19478
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.19734892,  6.82364495, 29.31420264]), 'distance': 27.499999999999996, 'localFrame': array([[-0.63649452,  0.38539963,  0.6680882 ],
       [-0.51795311, -0.85540901,  0.        ],
       [ 0.57148867, -0.34603836,  0.74408209]]), 'currentState': array([ 32.78767297, -10.54018303,  34.86271896,  -0.63649452,
         0.38539963,   0.6680882 ]), 'targetState': array([-26.07824813,  39.10147024,  19.        ]), 'previousTarget': array([13.54443819,  6.60253268, 28.82296525])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6276246171440076
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.07824813,  39.10147024,  19.        ]), 'distance': 3.2822298427137016, 'localFrame': array([[ 0.24731253,  0.51196398, -0.82263564],
       [-0.90044315,  0.43497371,  0.        ],
       [ 0.35782488,  0.74073663,  0.5685689 ]]), 'currentState': array([-25.88773233,  37.33215892,  21.75794741,   0.24731253,
         0.51196398,  -0.82263564]), 'targetState': array([-26.07824813,  39.10147024,  19.        ]), 'previousTarget': array([-26.07824813,  39.10147024,  19.        ])}
episode index:19479
target thresh 86.4555496982656
target distance 31.0
model initialize at round 19479
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.96254348,  7.4465415 , 39.05567151]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.36547158,  0.34200914,  0.86571374],
       [-0.6832811 ,  0.73015542,  0.        ],
       [-0.63210557, -0.59152584,  0.50053944]]), 'currentState': array([ 5.9183489 ,  4.24501099, 12.0437354 ,  0.36547158,  0.34200914,
        0.86571374]), 'targetState': array([10.4033648 ,  7.79551158, 42.        ]), 'previousTarget': array([ 9.70900899,  7.12700983, 37.80246209])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6276352377366312
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.4033648 ,  7.79551158, 42.        ]), 'distance': 1.8616007225359903, 'localFrame': array([[-0.25333893,  0.21232518,  0.94378886],
       [-0.64234116, -0.76641884,  0.        ],
       [ 0.72333756, -0.60623443,  0.33054893]]), 'currentState': array([ 9.79773132,  8.01570827, 40.25349527, -0.25333893,  0.21232518,
        0.94378886]), 'targetState': array([10.4033648 ,  7.79551158, 42.        ]), 'previousTarget': array([10.4033648 ,  7.79551158, 42.        ])}
episode index:19480
target thresh 86.45690407557578
target distance 40.528106829629245
model initialize at round 19480
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.89090369,   2.47327324,  46.88670504]), 'distance': 27.5, 'localFrame': array([[-0.04657316,  0.74023659, -0.67073148],
       [-0.99802661, -0.06279243,  0.        ],
       [-0.04211686,  0.66940787,  0.74170026]]), 'currentState': array([-4.18214287e+00, -7.17599841e+00,  4.83703404e+01, -4.65731616e-02,
        7.40236594e-01, -6.70731485e-01]), 'targetState': array([-45.25592389,   8.24022773,  46.        ]), 'previousTarget': array([-30.0167772 ,   1.82842333,  47.12804282])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276030199224668
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 79, 'trapConfig': [], 'currentTarget': array([-45.25592389,   8.24022773,  46.        ]), 'distance': 9.641969194661597, 'localFrame': array([[-0.94856516,  0.20477442,  0.24143647],
       [-0.21101701, -0.97748239,  0.        ],
       [ 0.2359999 , -0.0509472 ,  0.97041663]]), 'currentState': array([-36.26849989,   9.14728592,  49.37209508,  -0.94856516,
         0.20477442,   0.24143647]), 'targetState': array([-45.25592389,   8.24022773,  46.        ]), 'previousTarget': array([-45.25592389,   8.24022773,  46.        ])}
episode index:19481
target thresh 86.45825831745499
target distance 28.0
model initialize at round 19481
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.38761208, 19.49364798, 26.20937552]), 'distance': 27.5, 'localFrame': array([[ 0.24689343,  0.86015653,  0.44628956],
       [-0.96118838,  0.27589292,  0.        ],
       [-0.12312813, -0.42896833,  0.89488861]]), 'currentState': array([-14.24377257,   5.66577749,   3.17146368,   0.24689343,
         0.86015653,   0.44628956]), 'targetState': array([-7.42404795, 21.76886566, 30.        ]), 'previousTarget': array([-8.88801928, 18.58938805, 24.80548566])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6276132121821892
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.42404795, 21.76886566, 30.        ]), 'distance': 2.181760431207984, 'localFrame': array([[-0.34300292,  0.62375329,  0.70233954],
       [-0.87625232, -0.48185254,  0.        ],
       [ 0.33842409, -0.61542665,  0.71184209]]), 'currentState': array([-6.26939262, 22.87605626, 28.51643046, -0.34300292,  0.62375329,
        0.70233954]), 'targetState': array([-7.42404795, 21.76886566, 30.        ]), 'previousTarget': array([-7.42404795, 21.76886566, 30.        ])}
episode index:19482
target thresh 86.4596124239168
target distance 39.852671272907784
model initialize at round 19482
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.39936543, 19.40628926, 70.46702598]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.55096314, -0.39911334, -0.73290392],
       [ 0.58664483,  0.80984433,  0.        ],
       [ 0.59353808, -0.4299543 ,  0.68033216]]), 'currentState': array([-30.91056255,   9.29784945,  77.76570125,   0.55096314,
        -0.39911334,  -0.73290392]), 'targetState': array([ 8.60228217, 25.59298227, 66.        ]), 'previousTarget': array([-6.63289618, 19.86231217, 70.96973759])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6276209218853774
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.60228217, 25.59298227, 66.        ]), 'distance': 3.7873331296769495, 'localFrame': array([[ 0.93513639,  0.08443176, -0.34408023],
       [-0.0899224 ,  0.99594877,  0.        ],
       [ 0.34268629,  0.03094052,  0.93894025]]), 'currentState': array([ 5.67832953, 23.45907417, 67.11392522,  0.93513639,  0.08443176,
       -0.34408023]), 'targetState': array([ 8.60228217, 25.59298227, 66.        ]), 'previousTarget': array([ 8.60228217, 25.59298227, 66.        ])}
episode index:19483
target thresh 86.46096639497472
target distance 44.0
model initialize at round 19483
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.06946602e+01, 6.69127836e-02, 6.83181790e+01]), 'distance': 27.5, 'localFrame': array([[ 0.79942627, -0.27581036, -0.53370993],
       [ 0.32614511,  0.94531972,  0.        ],
       [ 0.50452652, -0.17406688,  0.84566761]]), 'currentState': array([-3.25508935, -9.22599986, 90.11946938,  0.79942627, -0.27581036,
       -0.53370993]), 'targetState': array([24.33528663,  9.1538967 , 47.        ]), 'previousTarget': array([10.00418244,  0.57889586, 69.07062591])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6276230441824769
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([24.33528663,  9.1538967 , 47.        ]), 'distance': 3.636596489755502, 'localFrame': array([[ 0.23853295,  0.80301349, -0.54614227],
       [-0.95860181,  0.28475003,  0.        ],
       [ 0.15551403,  0.52353296,  0.83769244]]), 'currentState': array([24.47380227, 11.29125145, 49.93893895,  0.23853295,  0.80301349,
       -0.54614227]), 'targetState': array([24.33528663,  9.1538967 , 47.        ]), 'previousTarget': array([24.33528663,  9.1538967 , 47.        ])}
episode index:19484
target thresh 86.4623202306423
target distance 19.339394636458536
model initialize at round 19484
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.99682922,  0.17803866, 27.        ]), 'distance': 24.008837858974495, 'localFrame': array([[ 0.05773825,  0.70490292, -0.70694991],
       [-0.9966622 ,  0.08163611,  0.        ],
       [ 0.05771264,  0.70459025,  0.70726362]]), 'currentState': array([ -9.18309244, -18.3841149 ,  21.4501737 ,   0.05773825,
         0.70490292,  -0.70694991]), 'targetState': array([ 4.99682922,  0.17803866, 27.        ]), 'previousTarget': array([ 4.99682922,  0.17803866, 27.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.627633662130502
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.99682922,  0.17803866, 27.        ]), 'distance': 4.179684714444652, 'localFrame': array([[-0.7778674 , -0.19919427,  0.59602345],
       [ 0.24807279, -0.96874139,  0.        ],
       [ 0.57739259,  0.1478572 ,  0.80296703]]), 'currentState': array([ 2.59383383, -1.92541509, 24.30354681, -0.7778674 , -0.19919427,
        0.59602345]), 'targetState': array([ 4.99682922,  0.17803866, 27.        ]), 'previousTarget': array([ 4.99682922,  0.17803866, 27.        ])}
episode index:19485
target thresh 86.46367393093311
target distance 21.444755512977743
model initialize at round 19485
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.9114622 , 21.44475551,  8.        ]), 'distance': 20.373997058628692, 'localFrame': array([[-0.47982814,  0.85267081, -0.20668199],
       [-0.87148782, -0.49041714,  0.        ],
       [-0.10136039,  0.18012084,  0.97840817]]), 'currentState': array([-0.17447845,  1.83234458,  5.16908521, -0.47982814,  0.85267081,
       -0.20668199]), 'targetState': array([-4.9114622 , 21.44475551,  8.        ]), 'previousTarget': array([-4.9114622 , 21.44475551,  8.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6276014526641092
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 93, 'trapConfig': [], 'currentTarget': array([-4.9114622 , 21.44475551,  8.        ]), 'distance': 15.141632884100208, 'localFrame': array([[-0.79324104,  0.54690464, -0.26770128],
       [-0.56762172, -0.82328949,  0.        ],
       [-0.22039565,  0.15195306,  0.96350196]]), 'currentState': array([ 0.62106011,  7.64429784,  5.13510709, -0.79324104,  0.54690464,
       -0.26770128]), 'targetState': array([-4.9114622 , 21.44475551,  8.        ]), 'previousTarget': array([-4.9114622 , 21.44475551,  8.        ])}
episode index:19486
target thresh 86.46502749586065
target distance 22.0
model initialize at round 19486
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.04299708, -27.51528276,  16.09274482]), 'distance': 27.5, 'localFrame': array([[-0.26216374,  0.19450249, -0.945219  ],
       [-0.5958349 , -0.80310695,  0.        ],
       [-0.75911195,  0.56319447,  0.32643689]]), 'currentState': array([-12.69956759, -44.22633679,  36.66128187,  -0.26216374,
         0.19450249,  -0.945219  ]), 'targetState': array([-20.07610906, -27.43993158,  16.        ]), 'previousTarget': array([-19.53666575, -28.65829854,  17.49154897])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275692465034551
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 92, 'trapConfig': [], 'currentTarget': array([-20.07610906, -27.43993158,  16.        ]), 'distance': 17.097018992836457, 'localFrame': array([[-0.47227926,  0.87913873,  0.06377611],
       [-0.88093211, -0.47324267,  0.        ],
       [ 0.03018158, -0.05618243,  0.99796423]]), 'currentState': array([-13.61067787, -38.83841961,  26.98092567,  -0.47227926,
         0.87913873,   0.06377611]), 'targetState': array([-20.07610906, -27.43993158,  16.        ]), 'previousTarget': array([-20.07610906, -27.43993158,  16.        ])}
episode index:19487
target thresh 86.46638092543844
target distance 76.0
model initialize at round 19487
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.8554428 , 28.84193294, 29.47102468]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.68247167, -0.72282462,  0.10842967],
       [ 0.72711158,  0.6865193 ,  0.        ],
       [-0.07443906,  0.07884047,  0.99410412]]), 'currentState': array([-12.85639717,  41.61997122,   9.34747672,   0.68247167,
        -0.72282462,   0.10842967]), 'targetState': array([39.37331572, -7.05280153, 86.        ]), 'previousTarget': array([-0.20512639, 29.73316007, 29.82027976])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275370436480311
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 63, 'trapConfig': [], 'currentTarget': array([-4.69562509, 21.577358  , 25.73900323]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.69410078,  0.18978923,  0.69440921],
       [-0.26374988,  0.96459111,  0.        ],
       [-0.66982095, -0.18315035,  0.71958032]]), 'currentState': array([-19.85244248,  31.42424998,   5.01318347,   0.69410078,
         0.18978923,   0.69440921]), 'targetState': array([39.37331572, -7.05280153, 86.        ]), 'previousTarget': array([-4.69562509, 21.577358  , 25.73900323])}
episode index:19488
target thresh 86.46773421968007
target distance 25.476435185845354
model initialize at round 19488
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.12702033, 26.62318176, 74.60538374]), 'distance': 27.499999999999996, 'localFrame': array([[-0.3263999 ,  0.8280991 ,  0.4557576 ],
       [-0.93033978, -0.36669863,  0.        ],
       [ 0.16712569, -0.42400943,  0.89010393]]), 'currentState': array([11.28357176,  7.92780425, 69.13164875, -0.3263999 ,  0.8280991 ,
        0.4557576 ]), 'targetState': array([-13.07251501,  31.38645171,  76.        ]), 'previousTarget': array([-6.8956395 , 25.35061898, 74.30281873])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275048440973282
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([-13.07251501,  31.38645171,  76.        ]), 'distance': 8.632913587306806, 'localFrame': array([[-0.05678031,  0.99408902, -0.09253656],
       [-0.99837275, -0.05702499,  0.        ],
       [-0.0052769 ,  0.09238598,  0.99570929]]), 'currentState': array([-7.65436625e+00,  2.47416315e+01,  7.70085758e+01, -5.67803121e-02,
        9.94089021e-01, -9.25365604e-02]), 'targetState': array([-13.07251501,  31.38645171,  76.        ]), 'previousTarget': array([-13.07251501,  31.38645171,  76.        ])}
episode index:19489
target thresh 86.46908737859901
target distance 58.0
model initialize at round 19489
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.19756694, -4.08148307, 42.6285867 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.7973884 , -0.10010262, -0.59510605],
       [ 0.12456041, -0.99221203,  0.        ],
       [-0.59047138, -0.07412665,  0.80364718]]), 'currentState': array([-8.05901579, -4.79948023, 69.6859479 , -0.7973884 , -0.10010262,
       -0.59510605]), 'targetState': array([ 2.30553096, -3.26871947, 12.        ]), 'previousTarget': array([-2.23836285, -4.18531651, 42.80613097])}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.6274916180733956
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 55, 'trapConfig': [], 'currentTarget': array([ 2.30553096, -3.26871947, 12.        ]), 'distance': 2.5223832005841116, 'localFrame': array([[-0.36144593, -0.83917974, -0.40636709],
       [ 0.91843126, -0.39558062,  0.        ],
       [-0.16075094, -0.37322023,  0.91370991]]), 'currentState': array([ 2.38678456, -5.20243886, 13.61757355, -0.36144593, -0.83917974,
       -0.40636709]), 'targetState': array([ 2.30553096, -3.26871947, 12.        ]), 'previousTarget': array([ 2.30553096, -3.26871947, 12.        ])}
episode index:19490
target thresh 86.47044040220887
target distance 51.0
model initialize at round 19490
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.43347103, -28.97569909,  48.82910764]), 'distance': 27.5, 'localFrame': array([[-0.71131239,  0.32777218, -0.62177173],
       [-0.4185046 , -0.90821468,  0.        ],
       [-0.56470221,  0.26021433,  0.78319852]]), 'currentState': array([  1.72297678, -37.87583507,  74.33300197,  -0.71131239,
         0.32777218,  -0.62177173]), 'targetState': array([ -8.45348862, -20.31104454,  24.        ]), 'previousTarget': array([ -2.49123087, -29.11357842,  49.60269422])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6274977583676986
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.45348862, -20.31104454,  24.        ]), 'distance': 3.8098942419635646, 'localFrame': array([[ 0.25334277,  0.7746212 , -0.57946479],
       [-0.95045863,  0.31085107,  0.        ],
       [ 0.18012725,  0.55075731,  0.81499728]]), 'currentState': array([ -6.94697224, -22.54036579,  26.69737452,   0.25334277,
         0.7746212 ,  -0.57946479]), 'targetState': array([ -8.45348862, -20.31104454,  24.        ]), 'previousTarget': array([ -8.45348862, -20.31104454,  24.        ])}
episode index:19491
target thresh 86.4717932905231
target distance 60.8460361537597
model initialize at round 19491
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.5037815 , -10.45586454,  41.68091369]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.16063791, -0.80950374,  0.56471158],
       [ 0.98087384,  0.19464459,  0.        ],
       [-0.10991805,  0.55391081,  0.82528834]]), 'currentState': array([-31.84609565,  11.3591591 ,  31.5647189 ,   0.16063791,
        -0.80950374,   0.56471158]), 'targetState': array([  4.33847246, -47.80353184,  59.        ]), 'previousTarget': array([-18.45351026,  -8.9982025 ,  41.14264576])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6274978776921822
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.33847246, -47.80353184,  59.        ]), 'distance': 2.975049955273064, 'localFrame': array([[ 0.84524696, -0.41150122, -0.34091689],
       [ 0.43772374,  0.89910952,  0.        ],
       [ 0.30652162, -0.14922742,  0.94009344]]), 'currentState': array([  3.3391354 , -45.724591  ,  60.87889665,   0.84524696,
        -0.41150122,  -0.34091689]), 'targetState': array([  4.33847246, -47.80353184,  59.        ]), 'previousTarget': array([  4.33847246, -47.80353184,  59.        ])}
episode index:19492
target thresh 86.47314604355526
target distance 19.020838433955568
model initialize at round 19492
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.02287596,  9.32945263, 44.        ]), 'distance': 22.41819278515904, 'localFrame': array([[-0.11642198,  0.78873051,  0.6036142 ],
       [-0.98928096, -0.14602458,  0.        ],
       [ 0.08814251, -0.59714404,  0.79727655]]), 'currentState': array([ 2.46342227,  7.3228513 , 31.58647521, -0.11642198,  0.78873051,
        0.6036142 ]), 'targetState': array([21.02287596,  9.32945263, 44.        ]), 'previousTarget': array([21.02287596,  9.32945263, 44.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.627511158715833
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.02287596,  9.32945263, 44.        ]), 'distance': 3.023188656233065, 'localFrame': array([[ 0.55837763, -0.61890549, -0.55242232],
       [ 0.74248073,  0.66986743,  0.        ],
       [ 0.37004972, -0.41016293,  0.83356438]]), 'currentState': array([18.42100673,  8.75825694, 42.57042607,  0.55837763, -0.61890549,
       -0.55242232]), 'targetState': array([21.02287596,  9.32945263, 44.        ]), 'previousTarget': array([21.02287596,  9.32945263, 44.        ])}
episode index:19493
target thresh 86.47449866131889
target distance 77.0
model initialize at round 19493
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.40210765, 12.75326755, 40.57707391]), 'distance': 27.5, 'localFrame': array([[ 0.88073658,  0.10328542,  0.46220688],
       [-0.11647346,  0.9931938 ,  0.        ],
       [-0.45906101, -0.05383483,  0.88677213]]), 'currentState': array([-25.62287085,  19.82444535,  19.52629516,   0.88073658,
         0.10328542,   0.46220688]), 'targetState': array([32.53369945, -5.52796525, 95.        ]), 'previousTarget': array([-10.39172582,  12.64517522,  39.11766681])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.6275014688801447
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([32.53369945, -5.52796525, 95.        ]), 'distance': 3.459305299814368, 'localFrame': array([[ 0.16831623, -0.96115699, -0.2187393 ],
       [ 0.98501067,  0.17249345,  0.        ],
       [ 0.0377311 , -0.21546054,  0.97578334]]), 'currentState': array([31.04886851, -2.97389214, 93.20033871,  0.16831623, -0.96115699,
       -0.2187393 ]), 'targetState': array([32.53369945, -5.52796525, 95.        ]), 'previousTarget': array([32.53369945, -5.52796525, 95.        ])}
episode index:19494
target thresh 86.47585114382751
target distance 35.9036123439509
model initialize at round 19494
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.52449313,  15.55780715,  44.04348505]), 'distance': 27.5, 'localFrame': array([[-0.39645874,  0.31164265,  0.86353884],
       [-0.61799239, -0.78618408,  0.        ],
       [ 0.67890048, -0.53366043,  0.50428234]]), 'currentState': array([-32.19015931,  -2.46391529,  37.35550375,  -0.39645874,
         0.31164265,   0.86353884]), 'targetState': array([ 4.99033376, 31.60848887, 50.        ]), 'previousTarget': array([-11.68251377,  15.854163  ,  43.49870806])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6275050037223693
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 4.99033376, 31.60848887, 50.        ]), 'distance': 2.397099826188315, 'localFrame': array([[ 0.76574108, -0.61263928,  0.19573889],
       [ 0.6247239 ,  0.78084572,  0.        ],
       [-0.15284188,  0.12228277,  0.98065605]]), 'currentState': array([ 3.59964944, 30.59690771, 51.66996654,  0.76574108, -0.61263928,
        0.19573889]), 'targetState': array([ 4.99033376, 31.60848887, 50.        ]), 'previousTarget': array([ 4.99033376, 31.60848887, 50.        ])}
episode index:19495
target thresh 86.47720349109463
target distance 50.0
model initialize at round 19495
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.6911579 ,  -3.01618561,  41.67637317]), 'distance': 27.500000000000004, 'localFrame': array([[-0.79642051,  0.10336774, -0.5958435 ],
       [-0.12871082, -0.99168217,  0.        ],
       [-0.59088738,  0.07669151,  0.80310057]]), 'currentState': array([ -5.93260369, -11.25188844,  66.4091637 ,  -0.79642051,
         0.10336774,  -0.5958435 ]), 'targetState': array([-23.42973319,   5.20073097,  17.        ]), 'previousTarget': array([-13.74689699,  -3.69227345,  42.54775021])}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6274964746376184
{'scaleFactor': 20, 'timeStep': 78, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-23.42973319,   5.20073097,  17.        ]), 'distance': 2.680275893490357, 'localFrame': array([[ 0.3713973 , -0.16964824,  0.91284364],
       [ 0.41548953,  0.90959796,  0.        ],
       [-0.83032072,  0.37927697,  0.4083093 ]]), 'currentState': array([-25.21665491,   5.89133068,  15.12547028,   0.3713973 ,
        -0.16964824,   0.91284364]), 'targetState': array([-23.42973319,   5.20073097,  17.        ]), 'previousTarget': array([-23.42973319,   5.20073097,  17.        ])}
episode index:19496
target thresh 86.4785557031338
target distance 79.0
model initialize at round 19496
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.28258151,  30.21158981,  29.09034921]), 'distance': 27.5, 'localFrame': array([[-0.04554239, -0.66436961,  0.74601535],
       [ 0.99765871, -0.06838928,  0.        ],
       [ 0.05101945,  0.74426872,  0.66592875]]), 'currentState': array([-17.20954736,  35.49317414,   2.76116497,  -0.04554239,
        -0.66436961,   0.74601535]), 'targetState': array([ 0.17769615, 19.99921059, 80.        ]), 'previousTarget': array([-11.61898577,  30.64149478,  27.32833036])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274642903798023
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([ 0.17769615, 19.99921059, 80.        ]), 'distance': 24.4296335314331, 'localFrame': array([[ 0.14639728, -0.36944962,  0.91764634],
       [ 0.92967152,  0.36838956,  0.        ],
       [-0.33805134,  0.85310967,  0.39739802]]), 'currentState': array([-5.36295846, 13.29341475, 57.17149932,  0.14639728, -0.36944962,
        0.91764634]), 'targetState': array([ 0.17769615, 19.99921059, 80.        ]), 'previousTarget': array([ 0.17769615, 19.99921059, 80.        ])}
episode index:19497
target thresh 86.47990777995851
target distance 35.44884234865204
model initialize at round 19497
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.01659089,  16.98955302,  87.91467068]), 'distance': 27.499999999999996, 'localFrame': array([[-0.73829397, -0.19828075,  0.64467571],
       [ 0.25937491, -0.96577671,  0.        ],
       [ 0.62261278,  0.1672127 ,  0.76445617]]), 'currentState': array([-19.17402869,  40.76483815,  74.12061739,  -0.73829397,
        -0.19828075,   0.64467571]), 'targetState': array([-20.44937381,   4.77735394,  95.        ]), 'previousTarget': array([-19.55261328,  16.90625609,  87.47264989])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6274685517879925
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-20.44937381,   4.77735394,  95.        ]), 'distance': 4.009004604574876, 'localFrame': array([[-0.54519039, -0.47538295,  0.69049148],
       [ 0.65720493, -0.75371194,  0.        ],
       [ 0.52043167,  0.4537944 ,  0.72334052]]), 'currentState': array([-21.10659442,   7.73742913,  92.37738034,  -0.54519039,
        -0.47538295,   0.69049148]), 'targetState': array([-20.44937381,   4.77735394,  95.        ]), 'previousTarget': array([-20.44937381,   4.77735394,  95.        ])}
episode index:19498
target thresh 86.4812597215823
target distance 6.50369732143921
model initialize at round 19498
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.52324216, -11.47112744,  84.        ]), 'distance': 7.403420352044394, 'localFrame': array([[ 0.12972735, -0.81126276, -0.57010837],
       [ 0.98745481,  0.15790185,  0.        ],
       [ 0.09002117, -0.56295625,  0.8215695 ]]), 'currentState': array([-1.20788476, -6.21469743, 81.80979122,  0.12972735, -0.81126276,
       -0.57010837]), 'targetState': array([  3.52324216, -11.47112744,  84.        ]), 'previousTarget': array([  3.52324216, -11.47112744,  84.        ])}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6274866363794184
{'scaleFactor': 20, 'timeStep': 3, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.52324216, -11.47112744,  84.        ]), 'distance': 3.8527140909870643, 'localFrame': array([[ 0.25643084, -0.95649167, -0.13916504],
       [ 0.96589056,  0.25895064,  0.        ],
       [ 0.03603688, -0.1344182 ,  0.9902692 ]]), 'currentState': array([ 1.29540301, -8.95078141, 82.12170437,  0.25643084, -0.95649167,
       -0.13916504]), 'targetState': array([  3.52324216, -11.47112744,  84.        ]), 'previousTarget': array([  3.52324216, -11.47112744,  84.        ])}
episode index:19499
target thresh 86.48261152801871
target distance 13.909787862783645
model initialize at round 19499
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.18784305, 5.88124382, 2.        ]), 'distance': 16.96812436741341, 'localFrame': array([[-0.04871115, -0.87812466, -0.47594569],
       [ 0.99846498, -0.05538664,  0.        ],
       [-0.02636103, -0.4752151 ,  0.87947467]]), 'currentState': array([-13.35543248,  12.40963309,   7.81296093,  -0.04871115,
        -0.87812466,  -0.47594569]), 'targetState': array([1.18784305, 5.88124382, 2.        ]), 'previousTarget': array([1.18784305, 5.88124382, 2.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274544575775527
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 94, 'trapConfig': [], 'currentTarget': array([1.18784305, 5.88124382, 2.        ]), 'distance': 10.292219330406846, 'localFrame': array([[-0.14206714, -0.6132185 , -0.77703282],
       [ 0.97419759, -0.22569682,  0.        ],
       [-0.17537384, -0.7569835 ,  0.62946009]]), 'currentState': array([-6.88654553, 12.14470075,  3.22602405, -0.14206714, -0.6132185 ,
       -0.77703282]), 'targetState': array([1.18784305, 5.88124382, 2.        ]), 'previousTarget': array([1.18784305, 5.88124382, 2.        ])}
episode index:19500
target thresh 86.4839631992812
target distance 46.56810356293136
model initialize at round 19500
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.87467077,   1.70036445,  81.97320524]), 'distance': 27.499999999999996, 'localFrame': array([[-0.25669075, -0.44256211, -0.85921396],
       [ 0.86502718, -0.501725  ,  0.        ],
       [-0.43108913, -0.74324343,  0.51161642]]), 'currentState': array([ 0.667782  ,  9.19968228, 95.82412779, -0.25669075, -0.44256211,
       -0.85921396]), 'targetState': array([-44.6161393 ,  -5.86516101,  68.        ]), 'previousTarget': array([-20.49361089,   2.25343534,  83.02215617])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6274590868824612
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-44.6161393 ,  -5.86516101,  68.        ]), 'distance': 1.414368238871673, 'localFrame': array([[-0.92470666, -0.22766836,  0.30509787],
       [ 0.23906683, -0.97100312,  0.        ],
       [ 0.29625099,  0.07293878,  0.952321  ]]), 'currentState': array([-43.22443194,  -6.05804319,  68.16243332,  -0.92470666,
        -0.22766836,   0.30509787]), 'targetState': array([-44.6161393 ,  -5.86516101,  68.        ]), 'previousTarget': array([-44.6161393 ,  -5.86516101,  68.        ])}
episode index:19501
target thresh 86.48531473538334
target distance 54.0
model initialize at round 19501
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.95542202,   7.7640201 ,  30.09098226]), 'distance': 27.5, 'localFrame': array([[-0.06859262, -0.76302878, -0.64271466],
       [ 0.99598375, -0.08953415,  0.        ],
       [-0.05754491, -0.64013335,  0.76610565]]), 'currentState': array([-14.81994909,  15.55649175,  56.30780644,  -0.06859262,
        -0.76302878,  -0.64271466]), 'targetState': array([-8.99538188, -0.28827906,  3.        ]), 'previousTarget': array([-11.55783834,   8.6520117 ,  30.91031988])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6274570141563333
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-8.99538188, -0.28827906,  3.        ]), 'distance': 3.9327677528330005, 'localFrame': array([[ 0.6910097 ,  0.69522867,  0.19789565],
       [-0.70925555,  0.70495146,  0.        ],
       [-0.13950682, -0.14035859,  0.98022309]]), 'currentState': array([-11.35116125,  -2.25218182,   5.46171723,   0.6910097 ,
         0.69522867,   0.19789565]), 'targetState': array([-8.99538188, -0.28827906,  3.        ]), 'previousTarget': array([-8.99538188, -0.28827906,  3.        ])}
episode index:19502
target thresh 86.48666613633864
target distance 36.341785143835146
model initialize at round 19502
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.33650787,  4.0579243 , 81.62292915]), 'distance': 27.5, 'localFrame': array([[ 0.81492468, -0.10502993,  0.5699706 ],
       [ 0.12782571,  0.99179665,  0.        ],
       [-0.56529493,  0.0728569 ,  0.82166509]]), 'currentState': array([-1.00842912,  0.40067145, 63.48572869,  0.81492468, -0.10502993,
        0.5699706 ]), 'targetState': array([34.34189219,  6.75532683, 95.        ]), 'previousTarget': array([18.17011479,  3.75847182, 80.31528784])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6274639303077043
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.34189219,  6.75532683, 95.        ]), 'distance': 2.2877899756535047, 'localFrame': array([[ 0.37699758,  0.9173656 , -0.12772303],
       [-0.92494099,  0.38011073,  0.        ],
       [ 0.0485489 ,  0.11813627,  0.99180987]]), 'currentState': array([33.24296226,  4.7674997 , 94.72635917,  0.37699758,  0.9173656 ,
       -0.12772303]), 'targetState': array([34.34189219,  6.75532683, 95.        ]), 'previousTarget': array([34.34189219,  6.75532683, 95.        ])}
episode index:19503
target thresh 86.4880174021606
target distance 36.0
model initialize at round 19503
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.58570751,  6.76408291, 50.92670068]), 'distance': 27.5, 'localFrame': array([[-0.70814368, -0.34348539, -0.61688761],
       [ 0.43642061, -0.89974277,  0.        ],
       [-0.55504016, -0.26922247,  0.78705126]]), 'currentState': array([ 6.27024452, 18.41219367, 72.26436722, -0.70814368, -0.34348539,
       -0.61688761]), 'targetState': array([-14.97654986,  -0.83842375,  37.        ]), 'previousTarget': array([-5.49828681,  7.06962958, 51.85141088])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6274716393637487
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.97654986,  -0.83842375,  37.        ]), 'distance': 2.6931311838236205, 'localFrame': array([[ 0.53544668, -0.51564606, -0.66888414],
       [ 0.69366315,  0.72029955,  0.        ],
       [ 0.48179694, -0.46398028,  0.74336667]]), 'currentState': array([-14.74966465,   0.88825129,  39.05428138,   0.53544668,
        -0.51564606,  -0.66888414]), 'targetState': array([-14.97654986,  -0.83842375,  37.        ]), 'previousTarget': array([-14.97654986,  -0.83842375,  37.        ])}
episode index:19504
target thresh 86.48936853286271
target distance 42.95437227696905
model initialize at round 19504
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.94712756,   6.34582564,  94.12551989]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.63390431,  0.14622369,  0.75946294],
       [-0.22476914,  0.97441205,  0.        ],
       [-0.74002984, -0.17070383,  0.65055056]]), 'currentState': array([-40.21453278,  16.45765016,  86.0574316 ,   0.63390431,
         0.14622369,   0.75946294]), 'targetState': array([  1.7222861 ,  -1.01672543, 100.        ]), 'previousTarget': array([-16.803761  ,   6.04628628,  93.53056065])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6274773932080976
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.7222861 ,  -1.01672543, 100.        ]), 'distance': 2.9329547521531594, 'localFrame': array([[ 0.53324946,  0.30118678,  0.79052611],
       [-0.49179107,  0.87071324,  0.        ],
       [-0.68832155, -0.38877368,  0.61242833]]), 'currentState': array([-0.27243068, -0.9252264 , 97.85175502,  0.53324946,  0.30118678,
        0.79052611]), 'targetState': array([  1.7222861 ,  -1.01672543, 100.        ]), 'previousTarget': array([  1.7222861 ,  -1.01672543, 100.        ])}
episode index:19505
target thresh 86.49071952845851
target distance 42.0
model initialize at round 19505
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.68508149, -5.94642289, 79.8114208 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.64844396, -0.60095491,  0.46730465],
       [ 0.67973915, -0.73345394,  0.        ],
       [ 0.34274644,  0.31764526,  0.88409636]]), 'currentState': array([ -3.35870338, -15.39729113,  54.12518255,  -0.64844396,
        -0.60095491,   0.46730465]), 'targetState': array([9.99950762e-01, 9.92337472e-03, 9.60000000e+01]), 'previousTarget': array([-0.31075265, -5.22912896, 80.04778953])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6274738450047417
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([9.99950762e-01, 9.92337472e-03, 9.60000000e+01]), 'distance': 3.3288085406962304, 'localFrame': array([[-0.1652295 ,  0.87872235,  0.44782391],
       [-0.98277703, -0.1847953 ,  0.        ],
       [ 0.08275575, -0.44011105,  0.89412178]]), 'currentState': array([ 0.66120436, -1.97112377, 93.3463856 , -0.1652295 ,  0.87872235,
        0.44782391]), 'targetState': array([9.99950762e-01, 9.92337472e-03, 9.60000000e+01]), 'previousTarget': array([9.99950762e-01, 9.92337472e-03, 9.60000000e+01])}
episode index:19506
target thresh 86.49207038896152
target distance 24.0
model initialize at round 19506
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.24598863,  4.8679354 , 39.54182788]), 'distance': 27.500000000000004, 'localFrame': array([[-0.21705634,  0.89412603,  0.39169527],
       [-0.97177577, -0.23590644,  0.        ],
       [ 0.09240344, -0.38063997,  0.920095  ]]), 'currentState': array([-13.80505128,  10.51228298,  17.22910643,  -0.21705634,
         0.89412603,   0.39169527]), 'targetState': array([ 1.5550485 ,  4.75203369, 40.        ]), 'previousTarget': array([ 1.09199105,  4.90309529, 39.23109936])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6274831880110069
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.5550485 ,  4.75203369, 40.        ]), 'distance': 2.7315866963653943, 'localFrame': array([[ 0.05161629,  0.49514025,  0.86727844],
       [-0.9946103 ,  0.10368394,  0.        ],
       [-0.08992284, -0.86260406,  0.49782337]]), 'currentState': array([ 1.1518875 ,  6.46932936, 37.91435318,  0.05161629,  0.49514025,
        0.86727844]), 'targetState': array([ 1.5550485 ,  4.75203369, 40.        ]), 'previousTarget': array([ 1.5550485 ,  4.75203369, 40.        ])}
episode index:19507
target thresh 86.49342111438523
target distance 64.0
model initialize at round 19507
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.89258899,   6.99871095,  53.8487309 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.88420658,  0.00956807,  0.46699805],
       [-0.01082045, -0.99994146,  0.        ],
       [ 0.46697071, -0.00505313,  0.88425835]]), 'currentState': array([-1.44123332e+01,  1.46386729e+01,  7.85065031e+01, -8.84206579e-01,
        9.56807222e-03,  4.66998050e-01]), 'targetState': array([-39.5978366,  -5.6578562,  13.       ]), 'previousTarget': array([-23.37902097,   7.03757269,  52.60240668])}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.6274741948011573
{'scaleFactor': 20, 'timeStep': 80, 'trapCount': 32, 'trapConfig': [], 'currentTarget': array([-39.5978366,  -5.6578562,  13.       ]), 'distance': 2.75492526172066, 'localFrame': array([[ 0.09831653,  0.3317783 , -0.93822013],
       [-0.95878878,  0.28411982,  0.        ],
       [ 0.26656694,  0.89955494,  0.34603899]]), 'currentState': array([-38.259075  ,  -5.25319561,  15.37351646,   0.09831653,
         0.3317783 ,  -0.93822013]), 'targetState': array([-39.5978366,  -5.6578562,  13.       ]), 'previousTarget': array([-39.5978366,  -5.6578562,  13.       ])}
episode index:19508
target thresh 86.49477170474314
target distance 25.0
model initialize at round 19508
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.4316059 ,  -3.49021466,  43.        ]), 'distance': 26.160383941754496, 'localFrame': array([[-0.34333118, -0.58580413, -0.73413706],
       [ 0.86274374, -0.50564141,  0.        ],
       [-0.3712101 , -0.63337215,  0.67900131]]), 'currentState': array([ 0.22847519, -1.41148852, 66.79931211, -0.34333118, -0.58580413,
       -0.73413706]), 'targetState': array([-10.4316059 ,  -3.49021466,  43.        ]), 'previousTarget': array([-10.4316059 ,  -3.49021466,  43.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.627487011799886
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.4316059 ,  -3.49021466,  43.        ]), 'distance': 3.8144178446967567, 'localFrame': array([[-0.72123623, -0.50509638, -0.47402104],
       [ 0.57363842, -0.81910864,  0.        ],
       [-0.38827473, -0.27191668,  0.88051352]]), 'currentState': array([-9.17255663, -5.71033443, 45.83472161, -0.72123623, -0.50509638,
       -0.47402104]), 'targetState': array([-10.4316059 ,  -3.49021466,  43.        ]), 'previousTarget': array([-10.4316059 ,  -3.49021466,  43.        ])}
episode index:19509
target thresh 86.49612216004878
target distance 31.58798977363345
model initialize at round 19509
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.31060887, -21.2739099 ,  21.44298605]), 'distance': 27.500000000000004, 'localFrame': array([[-0.79458587, -0.29023822, -0.53328704],
       [ 0.3430978 , -0.93929968,  0.        ],
       [-0.50091635, -0.18296961,  0.84593436]]), 'currentState': array([ 33.16063542, -10.43377123,  10.64401943,  -0.79458587,
        -0.29023822,  -0.53328704]), 'targetState': array([  2.78416065, -24.84448529,  25.        ]), 'previousTarget': array([ 10.87554258, -21.22210357,  21.67000129])}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.6274738002481612
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 68, 'trapConfig': [], 'currentTarget': array([  2.78416065, -24.84448529,  25.        ]), 'distance': 3.4857315970512452, 'localFrame': array([[ 0.55476566, -0.81927596,  0.14498954],
       [ 0.82802555,  0.56069037,  0.        ],
       [-0.08129424,  0.12005504,  0.98943319]]), 'currentState': array([  5.66277667, -22.88134965,  25.09996511,   0.55476566,
        -0.81927596,   0.14498954]), 'targetState': array([  2.78416065, -24.84448529,  25.        ]), 'previousTarget': array([  2.78416065, -24.84448529,  25.        ])}
episode index:19510
target thresh 86.49747248031564
target distance 40.0
model initialize at round 19510
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.99486396, -8.24109441, 28.69938525]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.71927215, -0.64427172, -0.259926  ],
       [ 0.66720451,  0.74487458,  0.        ],
       [ 0.19361227, -0.1734238 ,  0.96562854]]), 'currentState': array([-14.89310276,  -9.66004279,  50.35655878,   0.71927215,
        -0.64427172,  -0.259926  ]), 'targetState': array([16.57638798, -7.01593627, 10.        ]), 'previousTarget': array([ 1.36163373, -7.591586  , 28.6825944 ])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.627475584279771
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([16.57638798, -7.01593627, 10.        ]), 'distance': 2.192850910551823, 'localFrame': array([[ 0.95110473,  0.26959933,  0.15071823],
       [-0.27271461,  0.96209497,  0.        ],
       [-0.14500526, -0.04110307,  0.98857676]]), 'currentState': array([15.18152794, -5.65349503, 11.00335161,  0.95110473,  0.26959933,
        0.15071823]), 'targetState': array([16.57638798, -7.01593627, 10.        ]), 'previousTarget': array([16.57638798, -7.01593627, 10.        ])}
episode index:19511
target thresh 86.49882266555721
target distance 32.0
model initialize at round 19511
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.49387629,  22.09833701,  57.8944127 ]), 'distance': 27.5, 'localFrame': array([[ 0.52871375,  0.61552403, -0.58445867],
       [-0.75857311,  0.65158793,  0.        ],
       [ 0.38082622,  0.44335463,  0.81142348]]), 'currentState': array([-28.51213708,   8.30039723,  75.480833  ,   0.52871375,
         0.61552403,  -0.58445867]), 'targetState': array([ 0.16158869, 32.99960438, 44.        ]), 'previousTarget': array([-13.7300382 ,  21.2905618 ,  58.70138712])}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6274663616285555
{'scaleFactor': 20, 'timeStep': 81, 'trapCount': 49, 'trapConfig': [], 'currentTarget': array([ 0.16158869, 32.99960438, 44.        ]), 'distance': 1.4486286584911796, 'localFrame': array([[-0.05456631,  0.64336024, -0.76361647],
       [-0.99642253, -0.08451114,  0.        ],
       [-0.0645341 ,  0.76088466,  0.6456701 ]]), 'currentState': array([ 0.34150927, 33.38802176, 45.38393841, -0.05456631,  0.64336024,
       -0.76361647]), 'targetState': array([ 0.16158869, 32.99960438, 44.        ]), 'previousTarget': array([ 0.16158869, 32.99960438, 44.        ])}
episode index:19512
target thresh 86.50017271578703
target distance 17.865490632937753
model initialize at round 19512
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.87631579, -3.53262495, 20.        ]), 'distance': 17.568138516276264, 'localFrame': array([[-0.92798958, -0.04233147, -0.37019372],
       [ 0.04556893, -0.9989612 ,  0.        ],
       [-0.36980916, -0.01686933,  0.92895458]]), 'currentState': array([14.37823456,  0.23622804, 14.50228859, -0.92798958, -0.04233147,
       -0.37019372]), 'targetState': array([-1.87631579, -3.53262495, 20.        ]), 'previousTarget': array([-1.87631579, -3.53262495, 20.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6274791764013413
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.87631579, -3.53262495, 20.        ]), 'distance': 2.060438792799981, 'localFrame': array([[ 0.61596692,  0.38055735,  0.6897542 ],
       [-0.52560005,  0.8507318 ,  0.        ],
       [-0.58679583, -0.36253484,  0.7240436 ]]), 'currentState': array([-1.90168778, -2.29534613, 18.35260648,  0.61596692,  0.38055735,
        0.6897542 ]), 'targetState': array([-1.87631579, -3.53262495, 20.        ]), 'previousTarget': array([-1.87631579, -3.53262495, 20.        ])}
episode index:19513
target thresh 86.50152263101857
target distance 32.09482473409035
model initialize at round 19513
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.84188109, -18.98349135,  12.30458665]), 'distance': 27.499999999999993, 'localFrame': array([[ 0.69010895, -0.59702737, -0.40903295],
       [ 0.65426251,  0.75626753,  0.        ],
       [ 0.30933833, -0.26761492,  0.91251962]]), 'currentState': array([-18.62147456,   8.18835799,   8.24857734,   0.69010895,
        -0.59702737,  -0.40903295]), 'targetState': array([-20.05112296, -23.64217562,  13.        ]), 'previousTarget': array([-20.0901036 , -18.83534755,  12.40092172])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6274893582936972
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.05112296, -23.64217562,  13.        ]), 'distance': 3.547998980183641, 'localFrame': array([[ 0.18625677, -0.50915433,  0.84027988],
       [ 0.93913428,  0.34355029,  0.        ],
       [-0.2886784 ,  0.78913564,  0.54215286]]), 'currentState': array([-18.98553821, -20.69375514,  11.33878279,   0.18625677,
        -0.50915433,   0.84027988]), 'targetState': array([-20.05112296, -23.64217562,  13.        ]), 'previousTarget': array([-20.05112296, -23.64217562,  13.        ])}
episode index:19514
target thresh 86.50287241126533
target distance 46.04091453425268
model initialize at round 19514
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.52981157,  5.4032776 , 72.57771058]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.34719074,  0.87985576,  0.32451877],
       [-0.93019881,  0.36705608,  0.        ],
       [-0.11911659, -0.30186697,  0.94587926]]), 'currentState': array([  7.06541133, -21.71841545,  68.80484693,   0.34719074,
         0.87985576,   0.32451877]), 'targetState': array([ 2.90188196, 22.81620216, 75.        ]), 'previousTarget': array([ 4.19003157,  3.98220348, 72.54557456])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6274962685348477
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.90188196, 22.81620216, 75.        ]), 'distance': 1.941134101637307, 'localFrame': array([[-0.08553808,  0.99454861,  0.05963468],
       [-0.9963218 , -0.08569059,  0.        ],
       [ 0.00511013, -0.05941534,  0.99822027]]), 'currentState': array([ 2.00771178e+00,  2.14289874e+01,  7.39781896e+01, -8.55380809e-02,
        9.94548612e-01,  5.96346837e-02]), 'targetState': array([ 2.90188196, 22.81620216, 75.        ]), 'previousTarget': array([ 2.90188196, 22.81620216, 75.        ])}
episode index:19515
target thresh 86.5042220565408
target distance 15.0
model initialize at round 19515
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.88284162,  1.52625083, 27.        ]), 'distance': 13.497392481536075, 'localFrame': array([[ 0.15337595, -0.32701937,  0.93248815],
       [ 0.90536782,  0.42462821,  0.        ],
       [-0.39596077,  0.84424476,  0.36120057]]), 'currentState': array([ 7.63207515,  0.75692843, 13.71384942,  0.15337595, -0.32701937,
        0.93248815]), 'targetState': array([ 9.88284162,  1.52625083, 27.        ]), 'previousTarget': array([ 9.88284162,  1.52625083, 27.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6275118746569717
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.88284162,  1.52625083, 27.        ]), 'distance': 3.0829828444657634, 'localFrame': array([[-0.67471841,  0.42771279,  0.60151212],
       [-0.53540148, -0.84459769,  0.        ],
       [ 0.50803575, -0.32205048,  0.79886367]]), 'currentState': array([10.55897476,  1.08135553, 24.02515625, -0.67471841,  0.42771279,
        0.60151212]), 'targetState': array([ 9.88284162,  1.52625083, 27.        ]), 'previousTarget': array([ 9.88284162,  1.52625083, 27.        ])}
episode index:19516
target thresh 86.50557156685852
target distance 36.0
model initialize at round 19516
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.05257049,  0.69127545, 59.77699095]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.23499363, -0.92976959,  0.28338401],
       [ 0.96951332,  0.24503862,  0.        ],
       [-0.06944003,  0.27474457,  0.95900652]]), 'currentState': array([-9.36216073,  8.55528835, 37.71746183,  0.23499363, -0.92976959,
        0.28338401]), 'targetState': array([14.34655185, -4.37909237, 74.        ]), 'previousTarget': array([ 4.91993758,  1.46868961, 59.71819132])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6275187830363174
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.34655185, -4.37909237, 74.        ]), 'distance': 2.5993793208236124, 'localFrame': array([[ 0.22875039, -0.59140047,  0.77325205],
       [ 0.93266301,  0.36074882,  0.        ],
       [-0.27894977,  0.72118359,  0.63409878]]), 'currentState': array([13.69501983, -2.76634472, 72.06833649,  0.22875039, -0.59140047,
        0.77325205]), 'targetState': array([14.34655185, -4.37909237, 74.        ]), 'previousTarget': array([14.34655185, -4.37909237, 74.        ])}
episode index:19517
target thresh 86.50692094223193
target distance 32.0
model initialize at round 19517
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.96227181, -37.26268439,  34.71386705]), 'distance': 27.500000000000004, 'localFrame': array([[-0.64314787, -0.42314367, -0.63820863],
       [ 0.5496347 , -0.83540511,  0.        ],
       [-0.53316275, -0.35078161,  0.76986346]]), 'currentState': array([ 23.85865278, -30.21291775,  13.40997392,  -0.64314787,
        -0.42314367,  -0.63820863]), 'targetState': array([ -0.45913117, -40.99742917,  46.        ]), 'previousTarget': array([  8.49407624, -36.66772241,  34.66857042])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6275152348938596
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([ -0.45913117, -40.99742917,  46.        ]), 'distance': 2.91789061466518, 'localFrame': array([[ 0.35379335,  0.10390239,  0.92953459],
       [-0.28178071,  0.95947883,  0.        ],
       [-0.89186877, -0.26192492,  0.36873492]]), 'currentState': array([ -0.21689873, -41.43931694,  43.12595333,   0.35379335,
         0.10390239,   0.92953459]), 'targetState': array([ -0.45913117, -40.99742917,  46.        ]), 'previousTarget': array([ -0.45913117, -40.99742917,  46.        ])}
episode index:19518
target thresh 86.50827018267456
target distance 27.955468031008305
model initialize at round 19518
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.66463305,  5.3607963 , 43.69226028]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.72905483,  0.0897833 , -0.67854109],
       [-0.12222692,  0.99250218,  0.        ],
       [ 0.67345352,  0.08293599,  0.73456244]]), 'currentState': array([-10.2398953 ,  19.91109087,  58.6577804 ,   0.72905483,
         0.0897833 ,  -0.67854109]), 'targetState': array([16.86754021, -2.11803852, 36.        ]), 'previousTarget': array([ 7.01446924,  5.32567621, 44.45894274])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6275202282388601
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.86754021, -2.11803852, 36.        ]), 'distance': 3.6286622200537106, 'localFrame': array([[ 0.16536312, -0.55011232,  0.8185545 ],
       [ 0.9576683 ,  0.28787397,  0.        ],
       [-0.23564053,  0.7839037 ,  0.57442887]]), 'currentState': array([15.48057606, -4.42356309, 33.56523587,  0.16536312, -0.55011232,
        0.8185545 ]), 'targetState': array([16.86754021, -2.11803852, 36.        ]), 'previousTarget': array([16.86754021, -2.11803852, 36.        ])}
episode index:19519
target thresh 86.50961928819989
target distance 26.928374630902674
model initialize at round 19519
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.67767389, -25.69665409,  89.6415861 ]), 'distance': 27.5, 'localFrame': array([[ 0.6805449 ,  0.64967494,  0.33879362],
       [-0.69051128,  0.72332162,  0.        ],
       [-0.24505675, -0.23394081,  0.94086072]]), 'currentState': array([-3.86610359, -6.65750384, 80.36922772,  0.6805449 ,  0.64967494,
        0.33879362]), 'targetState': array([ 21.92401645, -34.64588724,  94.        ]), 'previousTarget': array([ 13.31840045, -25.91905927,  89.13886666])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.627528737155633
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.92401645, -34.64588724,  94.        ]), 'distance': 3.4279271140590297, 'localFrame': array([[ 2.13042016e-01, -9.77042620e-01,  9.04824683e-04],
       [ 9.77043020e-01,  2.13042103e-01,  0.00000000e+00],
       [-1.92765753e-04,  8.84052640e-04,  9.99999591e-01]]), 'currentState': array([ 2.00983917e+01, -3.19066520e+01,  9.30437735e+01,  2.13042016e-01,
       -9.77042620e-01,  9.04824683e-04]), 'targetState': array([ 21.92401645, -34.64588724,  94.        ]), 'previousTarget': array([ 21.92401645, -34.64588724,  94.        ])}
episode index:19520
target thresh 86.51096825882142
target distance 31.574871232864012
model initialize at round 19520
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.27907815, 16.26372846, 86.29848628]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.43363349,  0.55921975, -0.70656583],
       [-0.79025206,  0.61278193,  0.        ],
       [ 0.43297077,  0.5583651 ,  0.70764732]]), 'currentState': array([ 3.19033507, -3.16466762, 71.89458991,  0.43363349,  0.55921975,
       -0.70656583]), 'targetState': array([24.18609681, 28.0005843 , 95.        ]), 'previousTarget': array([15.62528761, 15.92816625, 86.58846302])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274965908138905
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([24.18609681, 28.0005843 , 95.        ]), 'distance': 19.711032156128212, 'localFrame': array([[ 0.86559478, -0.08452856,  0.49355911],
       [ 0.09719141,  0.99526571,  0.        ],
       [-0.49122246,  0.04796971,  0.86971225]]), 'currentState': array([20.79520167, 20.98434252, 76.89478058,  0.86559478, -0.08452856,
        0.49355911]), 'targetState': array([24.18609681, 28.0005843 , 95.        ]), 'previousTarget': array([24.18609681, 28.0005843 , 95.        ])}
episode index:19521
target thresh 86.51231709455263
target distance 85.0
model initialize at round 19521
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.541255  ,  3.03952327, 29.96734914]), 'distance': 27.500000000000004, 'localFrame': array([[-0.87220479, -0.07017875, -0.48408031],
       [ 0.08020212, -0.99677862,  0.        ],
       [-0.4825209 , -0.03882427,  0.87502357]]), 'currentState': array([10.5210383 ,  9.1369688 ,  6.5483899 , -0.87220479, -0.07017875,
       -0.48408031]), 'targetState': array([-37.6987086 , -13.37188731,  93.        ]), 'previousTarget': array([-1.89032031,  3.33371445, 31.17205124])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6274835781968247
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-37.6987086 , -13.37188731,  93.        ]), 'distance': 3.581565584732335, 'localFrame': array([[ 0.79705171,  0.57543563,  0.18325501],
       [-0.58534827,  0.81078197,  0.        ],
       [-0.14857986, -0.107268  ,  0.98306541]]), 'currentState': array([-35.3401382 , -12.34550971,  90.50775066,   0.79705171,
         0.57543563,   0.18325501]), 'targetState': array([-37.6987086 , -13.37188731,  93.        ]), 'previousTarget': array([-37.6987086 , -13.37188731,  93.        ])}
episode index:19522
target thresh 86.51366579540701
target distance 38.41028415841211
model initialize at round 19522
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.83651866, -8.647953  , 12.47901204]), 'distance': 27.5, 'localFrame': array([[ 0.64156806, -0.04453625, -0.76577212],
       [ 0.06925116,  0.99759926,  0.        ],
       [ 0.7639337 , -0.05303061,  0.64311201]]), 'currentState': array([-25.28049052,  -5.91057831,  26.59060173,   0.64156806,
        -0.04453625,  -0.76577212]), 'targetState': array([ 12.24989712, -10.29271687,   4.        ]), 'previousTarget': array([-2.91248426, -8.86695015, 13.47395108])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6274806133668436
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 25, 'trapConfig': [], 'currentTarget': array([ 12.24989712, -10.29271687,   4.        ]), 'distance': 3.283853639906913, 'localFrame': array([[ 0.67073945, -0.65972383,  0.33892928],
       [ 0.70122823,  0.71293687,  0.        ],
       [-0.24163518,  0.23766678,  0.94081185]]), 'currentState': array([11.14872067, -7.47655338,  2.71924696,  0.67073945, -0.65972383,
        0.33892928]), 'targetState': array([ 12.24989712, -10.29271687,   4.        ]), 'previousTarget': array([ 12.24989712, -10.29271687,   4.        ])}
episode index:19523
target thresh 86.51501436139804
target distance 24.794981579205313
model initialize at round 19523
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.50800116, -31.37204154,  22.68222654]), 'distance': 27.5, 'localFrame': array([[-0.50546756,  0.32140781,  0.80074938],
       [-0.5365744 , -0.84385302,  0.        ],
       [ 0.67571478, -0.42966162,  0.59899952]]), 'currentState': array([-2.4294245 , -7.57847837,  9.05153064, -0.50546756,  0.32140781,
        0.80074938]), 'targetState': array([ -4.70895151, -33.67232953,  24.        ]), 'previousTarget': array([ -4.47131564, -31.84709564,  22.82219141])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.627490366815124
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.70895151, -33.67232953,  24.        ]), 'distance': 2.851742637814316, 'localFrame': array([[-0.44121312,  0.89509473,  0.06431495],
       [-0.89695173, -0.44212848,  0.        ],
       [ 0.02843547, -0.05768741,  0.99792965]]), 'currentState': array([ -4.96811237, -33.45962014,  21.16803488,  -0.44121312,
         0.89509473,   0.06431495]), 'targetState': array([ -4.70895151, -33.67232953,  24.        ]), 'previousTarget': array([ -4.70895151, -33.67232953,  24.        ])}
episode index:19524
target thresh 86.51636279253923
target distance 22.496521615118667
model initialize at round 19524
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.82710195, -23.03958123,  95.96624958]), 'distance': 27.5, 'localFrame': array([[-0.64495741, -0.66613942,  0.37455069],
       [ 0.71843701, -0.69559203,  0.        ],
       [ 0.26053448,  0.26909108,  0.92720644]]), 'currentState': array([-7.20739858, -2.09692479, 80.36638243, -0.64495741, -0.66613942,
        0.37455069]), 'targetState': array([-15.84575074, -23.08489081,  96.        ]), 'previousTarget': array([-15.23311617, -21.68918352,  95.00734358])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6275027230479515
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.84575074, -23.08489081,  96.        ]), 'distance': 3.2822815445303517, 'localFrame': array([[-0.57482822, -0.80327697, -0.15594428],
       [ 0.8132261 , -0.58194786,  0.        ],
       [-0.09075144, -0.12681796,  0.98776585]]), 'currentState': array([-15.53329526, -20.15930138,  94.54512192,  -0.57482822,
        -0.80327697,  -0.15594428]), 'targetState': array([-15.84575074, -23.08489081,  96.        ]), 'previousTarget': array([-15.84575074, -23.08489081,  96.        ])}
episode index:19525
target thresh 86.51771108884402
target distance 67.13268893327242
model initialize at round 19525
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.10320582, 21.17200733, 67.12518703]), 'distance': 27.499999999999996, 'localFrame': array([[-0.86213183, -0.2080136 , -0.46201629],
       [ 0.23454763, -0.97210463,  0.        ],
       [-0.44912817, -0.10836483,  0.88687144]]), 'currentState': array([ 5.17055517, 45.35493531, 62.56549565, -0.86213183, -0.2080136 ,
       -0.46201629]), 'targetState': array([-28.30059925, -20.59310763,  75.        ]), 'previousTarget': array([-5.95499286, 22.43570514, 67.3085793 ])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6275031677249471
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.30059925, -20.59310763,  75.        ]), 'distance': 2.2141171112020657, 'localFrame': array([[-0.90052881, -0.09167511,  0.4250218 ],
       [ 0.10127797, -0.99485817,  0.        ],
       [ 0.42283641,  0.04304535,  0.90518312]]), 'currentState': array([-27.57831333, -19.66534833,  73.12386559,  -0.90052881,
        -0.09167511,   0.4250218 ]), 'targetState': array([-28.30059925, -20.59310763,  75.        ]), 'previousTarget': array([-28.30059925, -20.59310763,  75.        ])}
episode index:19526
target thresh 86.51905925032595
target distance 42.50640532550136
model initialize at round 19526
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.83023707, -13.18885236,  46.07263937]), 'distance': 27.5, 'localFrame': array([[ 0.17622111, -0.8344968 ,  0.52207395],
       [ 0.97842254,  0.20661398,  0.        ],
       [-0.10786778,  0.51080892,  0.85290022]]), 'currentState': array([-3.57225697,  7.33050893, 28.5352662 ,  0.17622111, -0.8344968 ,
        0.52207395]), 'targetState': array([-14.20514153, -34.16451308,  64.        ]), 'previousTarget': array([ -8.56674414, -12.02806906,  44.73117634])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6274999107998038
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([-14.20514153, -34.16451308,  64.        ]), 'distance': 3.6567448380002463, 'localFrame': array([[ 0.17778573, -0.7055656 ,  0.68598062],
       [ 0.96968993,  0.24433877,  0.        ],
       [-0.16761166,  0.6651885 ,  0.72761981]]), 'currentState': array([-12.50203536, -32.67235169,  61.12864726,   0.17778573,
        -0.7055656 ,   0.68598062]), 'targetState': array([-14.20514153, -34.16451308,  64.        ]), 'previousTarget': array([-14.20514153, -34.16451308,  64.        ])}
episode index:19527
target thresh 86.52040727699845
target distance 29.0
model initialize at round 19527
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.56605548, 16.84736006, 89.43367745]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.0369528 ,  0.79103536,  0.61065338],
       [-0.99891066,  0.04666359,  0.        ],
       [-0.02849528, -0.60998818,  0.791898  ]]), 'currentState': array([1.24913463e+01, 3.57477230e+01, 7.10974825e+01, 3.69528009e-02,
       7.91035356e-01, 6.10653384e-01]), 'targetState': array([ 0.43128881,  6.98670094, 99.        ]), 'previousTarget': array([ 4.87921737, 16.49929714, 88.98216762])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6275037999735134
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 0.43128881,  6.98670094, 99.        ]), 'distance': 3.4297292179856855, 'localFrame': array([[-0.94994235, -0.13857688,  0.28001069],
       [ 0.14435139, -0.98952649,  0.        ],
       [ 0.27707799,  0.04041993,  0.95999688]]), 'currentState': array([ 2.27556894,  9.13045832, 97.05938732, -0.94994235, -0.13857688,
        0.28001069]), 'targetState': array([ 0.43128881,  6.98670094, 99.        ]), 'previousTarget': array([ 0.43128881,  6.98670094, 99.        ])}
episode index:19528
target thresh 86.52175516887503
target distance 19.05028319970429
model initialize at round 19528
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.33011078, -18.86016784,  79.        ]), 'distance': 20.605817471226594, 'localFrame': array([[ 0.89027364, -0.40818371, -0.20198739],
       [ 0.41677421,  0.90901004,  0.        ],
       [ 0.18360856, -0.08418313,  0.97938812]]), 'currentState': array([-40.67901324,  -7.7528012 ,  78.50836649,   0.89027364,
        -0.40818371,  -0.20198739]), 'targetState': array([-23.33011078, -18.86016784,  79.        ]), 'previousTarget': array([-23.33011078, -18.86016784,  79.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6275175146775066
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.33011078, -18.86016784,  79.        ]), 'distance': 2.6953911015992613, 'localFrame': array([[ 0.93047694,  0.14386571, -0.33692035],
       [-0.15279941,  0.98825722,  0.        ],
       [ 0.33296397,  0.05148123,  0.94153315]]), 'currentState': array([-24.95551732, -17.06150832,  77.82186133,   0.93047694,
         0.14386571,  -0.33692035]), 'targetState': array([-23.33011078, -18.86016784,  79.        ]), 'previousTarget': array([-23.33011078, -18.86016784,  79.        ])}
episode index:19529
target thresh 86.52310292596917
target distance 43.319728504072685
model initialize at round 19529
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.39492786, -5.85241238, 58.29617923]), 'distance': 27.5, 'localFrame': array([[ 0.56887067, -0.78614684,  0.24157671],
       [ 0.81014184,  0.5862339 ,  0.        ],
       [-0.14162046,  0.1957114 ,  0.97038172]]), 'currentState': array([-23.5557435 ,  14.07430331,  50.25364181,   0.56887067,
        -0.78614684,   0.24157671]), 'targetState': array([ 12.17690539, -27.41756691,  67.        ]), 'previousTarget': array([-7.30024024, -4.25565955, 57.91055188])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.627522133879653
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.17690539, -27.41756691,  67.        ]), 'distance': 2.8981845916835662, 'localFrame': array([[-0.41409078, -0.85340783,  0.31658158],
       [ 0.89968276, -0.43654431,  0.        ],
       [ 0.13820189,  0.28482299,  0.94856529]]), 'currentState': array([ 10.2823358 , -25.25628111,  67.3727247 ,  -0.41409078,
        -0.85340783,   0.31658158]), 'targetState': array([ 12.17690539, -27.41756691,  67.        ]), 'previousTarget': array([ 12.17690539, -27.41756691,  67.        ])}
episode index:19530
target thresh 86.52445054829434
target distance 17.63086883011637
model initialize at round 19530
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.6859731 ,  2.48142997, 61.        ]), 'distance': 23.265353466193655, 'localFrame': array([[ 0.42601682,  0.89685749,  0.1189803 ],
       [-0.90327379,  0.42906463,  0.        ],
       [-0.05105024, -0.10747179,  0.99289661]]), 'currentState': array([ -4.59940762, -13.52546433,  44.36966909,   0.42601682,
         0.89685749,   0.1189803 ]), 'targetState': array([-1.6859731 ,  2.48142997, 61.        ]), 'previousTarget': array([-1.6859731 ,  2.48142997, 61.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6275353878214807
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.6859731 ,  2.48142997, 61.        ]), 'distance': 3.138580892778941, 'localFrame': array([[ 0.61504401,  0.77070692, -0.16652838],
       [-0.78162096,  0.6237537 ,  0.        ],
       [ 0.10387269,  0.13016207,  0.98603666]]), 'currentState': array([-0.53917548, -0.34340799, 60.25445607,  0.61504401,  0.77070692,
       -0.16652838]), 'targetState': array([-1.6859731 ,  2.48142997, 61.        ]), 'previousTarget': array([-1.6859731 ,  2.48142997, 61.        ])}
episode index:19531
target thresh 86.52579803586401
target distance 65.0
model initialize at round 19531
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.10837865, -12.17400213,  42.48991065]), 'distance': 27.5, 'localFrame': array([[-0.77303791, -0.32785943, -0.54306592],
       [ 0.39045291, -0.9206229 ,  0.        ],
       [-0.49995892, -0.21204167,  0.83969007]]), 'currentState': array([22.11970023,  1.99407523, 65.52037395, -0.77303791, -0.32785943,
       -0.54306592]), 'targetState': array([  8.29796262, -37.0829316 ,   2.        ]), 'previousTarget': array([ 17.68180537, -11.33147251,  43.97411971])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6275339246133425
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  8.29796262, -37.0829316 ,   2.        ]), 'distance': 2.506548142852436, 'localFrame': array([[ 0.47290434,  0.06191347, -0.87893584],
       [-0.12981395,  0.99153837,  0.        ],
       [ 0.87149861,  0.11409814,  0.47694003]]), 'currentState': array([  8.71879529, -37.13806812,   4.4703529 ,   0.47290434,
         0.06191347,  -0.87893584]), 'targetState': array([  8.29796262, -37.0829316 ,   2.        ]), 'previousTarget': array([  8.29796262, -37.0829316 ,   2.        ])}
episode index:19532
target thresh 86.52714538869166
target distance 33.534375801587245
model initialize at round 19532
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.24207331, 11.82422977, 71.25523582]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.9281994 , -0.12482378, -0.35052089],
       [ 0.13327973,  0.99107846,  0.        ],
       [ 0.34739371, -0.04671733,  0.93655491]]), 'currentState': array([-16.15800716,   7.82369901,  55.81236858,   0.9281994 ,
        -0.12482378,  -0.35052089]), 'targetState': array([16.02544178, 13.57148539, 78.        ]), 'previousTarget': array([ 5.51781887, 11.63087576, 71.41988405])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6275428375639863
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.02544178, 13.57148539, 78.        ]), 'distance': 2.825644203435451, 'localFrame': array([[ 0.95024533, -0.30452913,  0.06554253],
       [ 0.30518535,  0.95229297,  0.        ],
       [-0.06241569,  0.02000262,  0.99784978]]), 'currentState': array([ 1.38242435e+01,  1.19304843e+01,  7.73320877e+01,  9.50245330e-01,
       -3.04529129e-01,  6.55425254e-02]), 'targetState': array([16.02544178, 13.57148539, 78.        ]), 'previousTarget': array([16.02544178, 13.57148539, 78.        ])}
episode index:19533
target thresh 86.52849260679076
target distance 36.803942668603426
model initialize at round 19533
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.51988604,  2.18923747, 21.05389006]), 'distance': 27.5, 'localFrame': array([[ 0.59177532,  0.72272203, -0.35703619],
       [-0.77371731,  0.63353099,  0.        ],
       [ 0.22619349,  0.27624508,  0.93409055]]), 'currentState': array([-21.62428323,  -9.63093469,  30.04727421,   0.59177532,
         0.72272203,  -0.35703619]), 'targetState': array([14.52590083,  8.83165925, 16.        ]), 'previousTarget': array([ 0.59721387,  1.19601009, 21.29838934])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6275467233455689
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([14.52590083,  8.83165925, 16.        ]), 'distance': 3.8608516267922166, 'localFrame': array([[-0.1005185 ,  0.8419291 ,  0.53014283],
       [-0.99294823, -0.11854878,  0.        ],
       [ 0.06284779, -0.52640439,  0.84790835]]), 'currentState': array([13.6940318 ,  6.00162409, 13.50900217, -0.1005185 ,  0.8419291 ,
        0.53014283]), 'targetState': array([14.52590083,  8.83165925, 16.        ]), 'previousTarget': array([14.52590083,  8.83165925, 16.        ])}
episode index:19534
target thresh 86.5298396901748
target distance 55.0
model initialize at round 19534
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.83635918, -1.18695487, 72.34298985]), 'distance': 27.5, 'localFrame': array([[-0.17668737, -0.90947074, -0.37635693],
       [ 0.98164655, -0.19070932,  0.        ],
       [-0.07177478, -0.36944948,  0.92647475]]), 'currentState': array([-0.9927128 , -1.3844512 , 97.62029578, -0.17668737, -0.90947074,
       -0.37635693]), 'targetState': array([21.97880339, -0.96550578, 44.        ]), 'previousTarget': array([ 9.61089532, -0.54500089, 73.62492419])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.627549189807085
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([21.97880339, -0.96550578, 44.        ]), 'distance': 1.8258069593195272, 'localFrame': array([[ 0.1833633 ,  0.46734558, -0.86485028],
       [-0.9309119 ,  0.36524381,  0.        ],
       [ 0.31588121,  0.80509942,  0.50202987]]), 'currentState': array([ 2.20359600e+01, -7.29238200e-03,  4.55531038e+01,  1.83363302e-01,
        4.67345579e-01, -8.64850281e-01]), 'targetState': array([21.97880339, -0.96550578, 44.        ]), 'previousTarget': array([21.97880339, -0.96550578, 44.        ])}
episode index:19535
target thresh 86.53118663885722
target distance 57.91246506168117
model initialize at round 19535
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.63445059, 18.6654329 , 25.10804959]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.11500648, -0.95565788, -0.27109321],
       [ 0.99283653,  0.11948066,  0.        ],
       [ 0.0323904 , -0.26915124,  0.9625531 ]]), 'currentState': array([29.02996604,  6.27317559, 22.36200071,  0.11500648, -0.95565788,
       -0.27109321]), 'targetState': array([-29.94109436,  36.22886789,  29.        ]), 'previousTarget': array([ 3.41556066, 19.77090331, 25.54409582])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6275480358798393
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-29.94109436,  36.22886789,  29.        ]), 'distance': 3.6296222743769198, 'localFrame': array([[-0.41356067,  0.74188731, -0.52779806],
       [-0.8734561 , -0.4869029 ,  0.        ],
       [-0.25698641,  0.46100843,  0.84936989]]), 'currentState': array([-27.27338991,  33.96828494,  28.02671919,  -0.41356067,
         0.74188731,  -0.52779806]), 'targetState': array([-29.94109436,  36.22886789,  29.        ]), 'previousTarget': array([-29.94109436,  36.22886789,  29.        ])}
episode index:19536
target thresh 86.53253345285151
target distance 27.055437088012695
model initialize at round 19536
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.86147541,  0.41887199, 18.14055135]), 'distance': 27.500000000000004, 'localFrame': array([[-0.91599019, -0.12526649, -0.38114338],
       [ 0.13549413, -0.99077815,  0.        ],
       [-0.37762853, -0.05164269,  0.92451594]]), 'currentState': array([25.33097457, 12.31658559, 22.13279661, -0.91599019, -0.12526649,
       -0.38114338]), 'targetState': array([ 0.,  0., 18.]), 'previousTarget': array([ 2.4721754 ,  1.18435992, 18.36549776])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6275569462833639
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([0.00000000e+00, 5.55111512e-17, 1.80000000e+01]), 'distance': 2.515974606397847, 'localFrame': array([[-0.24209843,  0.18201716,  0.95302576],
       [-0.60093634, -0.79929689,  0.        ],
       [ 0.76175053, -0.57270781,  0.30288925]]), 'currentState': array([ 2.07147965, -0.41636347, 16.63407843, -0.24209843,  0.18201716,
        0.95302576]), 'targetState': array([ 0.,  0., 18.]), 'previousTarget': array([ 0.,  0., 18.])}
episode index:19537
target thresh 86.53388013217113
target distance 13.01905530606533
model initialize at round 19537
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.81442362, 13.43625144, 14.        ]), 'distance': 14.209988758432067, 'localFrame': array([[-0.41137652, -0.8220682 , -0.3936664 ],
       [ 0.89427814, -0.44751157,  0.        ],
       [-0.17617027, -0.35204726,  0.91925337]]), 'currentState': array([27.4203794 , 15.9472991 , 20.05873742, -0.41137652, -0.8220682 ,
       -0.3936664 ]), 'targetState': array([14.81442362, 13.43625144, 14.        ]), 'previousTarget': array([14.81442362, 13.43625144, 14.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6275711148333037
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.81442362, 13.43625144, 14.        ]), 'distance': 2.653612375666149, 'localFrame': array([[-0.76275811,  0.60595637,  0.2258693 ],
       [-0.62203114, -0.7829925 ,  0.        ],
       [ 0.17685397, -0.14049774,  0.97415761]]), 'currentState': array([16.09337517, 12.83109883, 11.75506528, -0.76275811,  0.60595637,
        0.2258693 ]), 'targetState': array([14.81442362, 13.43625144, 14.        ]), 'previousTarget': array([14.81442362, 13.43625144, 14.        ])}
episode index:19538
target thresh 86.53522667682957
target distance 31.72118046963538
model initialize at round 19538
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([5.39893629, 3.50466041, 5.76707642]), 'distance': 27.500000000000004, 'localFrame': array([[-0.65621085, -0.31030444, -0.68782154],
       [ 0.42748734, -0.90402133,  0.        ],
       [-0.62180535, -0.29403501,  0.72587983]]), 'currentState': array([30.32806653, 15.07623196,  4.82516477, -0.65621085, -0.31030444,
       -0.68782154]), 'targetState': array([-0.76574186,  0.64314805,  6.        ]), 'previousTarget': array([6.30579194, 4.140811  , 6.        ])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6275817060941983
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.76574186,  0.64314805,  6.        ]), 'distance': 4.179920457268307, 'localFrame': array([[-0.86636966,  0.37204922,  0.3331411 ],
       [-0.39458935, -0.91885757,  0.        ],
       [ 0.30610923, -0.13145393,  0.94287698]]), 'currentState': array([ 1.51829385,  3.16705895,  3.57414146, -0.86636966,  0.37204922,
        0.3331411 ]), 'targetState': array([-0.76574186,  0.64314805,  6.        ]), 'previousTarget': array([-0.76574186,  0.64314805,  6.        ])}
episode index:19539
target thresh 86.53657308684026
target distance 31.106154059517827
model initialize at round 19539
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.86116217,  9.50955435, 15.34734705]), 'distance': 27.500000000000004, 'localFrame': array([[-0.31162196,  0.90853086,  0.27832252],
       [-0.9459058 , -0.32444139,  0.        ],
       [ 0.09029934, -0.26326688,  0.96048768]]), 'currentState': array([22.46190206, -7.43028903,  4.24310543, -0.31162196,  0.90853086,
        0.27832252]), 'targetState': array([-8.95781383, 21.18389887, 23.        ]), 'previousTarget': array([ 4.13231886,  8.37920227, 15.00439499])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6275736715463729
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 36, 'trapConfig': [], 'currentTarget': array([-8.95781383, 21.18389887, 23.        ]), 'distance': 2.387226396942586, 'localFrame': array([[-0.74924014,  0.6559222 ,  0.09168028],
       [-0.65869631, -0.75240892,  0.        ],
       [ 0.06898106, -0.06038946,  0.99578849]]), 'currentState': array([-8.51209107, 18.89753717, 22.47776321, -0.74924014,  0.6559222 ,
        0.09168028]), 'targetState': array([-8.95781383, 21.18389887, 23.        ]), 'previousTarget': array([-8.95781383, 21.18389887, 23.        ])}
episode index:19540
target thresh 86.53791936221668
target distance 69.96240103499828
model initialize at round 19540
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.13874712, -9.67102019, 68.74147073]), 'distance': 27.5, 'localFrame': array([[-0.70066095, -0.37727257, -0.60559033],
       [ 0.47409359, -0.88047446,  0.        ],
       [-0.53320682, -0.28710649,  0.79577657]]), 'currentState': array([-12.00604999, -34.13050046,  56.50372588,  -0.70066095,
        -0.37727257,  -0.60559033]), 'targetState': array([-3.68927514, 36.81561148, 92.        ]), 'previousTarget': array([-8.58170031, -8.52850068, 69.96388073])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6275689990400355
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.68927514, 36.81561148, 92.        ]), 'distance': 2.815324589697054, 'localFrame': array([[-0.67662013,  0.2105995 ,  0.70557285],
       [-0.29718937, -0.95481856,  0.        ],
       [ 0.67369405, -0.20968875,  0.70863739]]), 'currentState': array([-4.12900369, 34.04182338, 91.80304599, -0.67662013,  0.2105995 ,
        0.70557285]), 'targetState': array([-3.68927514, 36.81561148, 92.        ]), 'previousTarget': array([-3.68927514, 36.81561148, 92.        ])}
episode index:19541
target thresh 86.53926550297231
target distance 77.0
model initialize at round 19541
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.97848843,  7.58861932, 58.96128741]), 'distance': 27.5, 'localFrame': array([[ 0.26479356,  0.93803868,  0.22353478],
       [-0.96239107,  0.27166786,  0.        ],
       [-0.06072721, -0.21512787,  0.97469595]]), 'currentState': array([-10.82603359,   8.87676811,  86.        ,   0.26479356,
         0.93803868,   0.22353478]), 'targetState': array([2.97865389, 5.20841828, 9.        ]), 'previousTarget': array([-5.97848843,  7.58861932, 58.96128741])}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6275604867571769
{'scaleFactor': 20, 'timeStep': 78, 'trapCount': 28, 'trapConfig': [], 'currentTarget': array([2.97865389, 5.20841828, 9.        ]), 'distance': 4.589233835231354, 'localFrame': array([[-0.56115328, -0.47336732, -0.67899217],
       [ 0.64478679, -0.76436248,  0.        ],
       [-0.51899614, -0.43780518,  0.73414551]]), 'currentState': array([ 5.41381643,  7.92000485, 11.78896914, -0.56115328, -0.47336732,
       -0.67899217]), 'targetState': array([2.97865389, 5.20841828, 9.        ]), 'previousTarget': array([2.97865389, 5.20841828, 9.        ])}
episode index:19542
target thresh 86.54061150912058
target distance 24.04075154827106
model initialize at round 19542
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.44616841, -39.78458516,  60.98563774]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.46081331, -0.610489  , -0.64416944],
       [ 0.7981471 ,  0.60246262,  0.        ],
       [ 0.38808801, -0.51414197,  0.76488282]]), 'currentState': array([ -7.77383164, -21.54374011,  77.54441702,   0.46081331,
        -0.610489  ,  -0.64416944]), 'targetState': array([  8.12545493, -45.2766715 ,  56.        ]), 'previousTarget': array([  3.65494794, -38.93945627,  62.06286996])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6275702266359489
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.12545493, -45.2766715 ,  56.        ]), 'distance': 2.4745283813528247, 'localFrame': array([[-0.45558204, -0.63515409, -0.62371812],
       [ 0.81258186, -0.58284708,  0.        ],
       [-0.36353228, -0.50682203,  0.78164935]]), 'currentState': array([  7.28486933, -45.32521307,  58.32687565,  -0.45558204,
        -0.63515409,  -0.62371812]), 'targetState': array([  8.12545493, -45.2766715 ,  56.        ]), 'previousTarget': array([  8.12545493, -45.2766715 ,  56.        ])}
episode index:19543
target thresh 86.54195738067496
target distance 31.646302241086865
model initialize at round 19543
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.87300071, -26.39725001,  27.92815009]), 'distance': 27.499999999999996, 'localFrame': array([[-0.28669491,  0.91000655, -0.29948973],
       [-0.95378578, -0.30048743,  0.        ],
       [-0.0899929 ,  0.28564905,  0.95409952]]), 'currentState': array([ 16.46433583, -42.10247153,  44.25317034,  -0.28669491,
         0.91000655,  -0.29948973]), 'targetState': array([-13.38424291, -12.03586481,  13.        ]), 'previousTarget': array([  1.67139766, -27.54053533,  28.18802362])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6275741090279037
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.38424291, -12.03586481,  13.        ]), 'distance': 2.9116756426238446, 'localFrame': array([[-0.80160542,  0.57891883,  0.14927069],
       [-0.58547832, -0.81068807,  0.        ],
       [ 0.12101197, -0.08739475,  0.98879637]]), 'currentState': array([-11.60633037, -13.40321328,  11.1433255 ,  -0.80160542,
         0.57891883,   0.14927069]), 'targetState': array([-13.38424291, -12.03586481,  13.        ]), 'previousTarget': array([-13.38424291, -12.03586481,  13.        ])}
episode index:19544
target thresh 86.54330311764893
target distance 35.0
model initialize at round 19544
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.29195079, 11.42411301, 41.67819623]), 'distance': 27.5, 'localFrame': array([[-0.83129387, -0.05814057, -0.55278402],
       [ 0.06976942, -0.99756314,  0.        ],
       [-0.55143696, -0.03856742,  0.83332457]]), 'currentState': array([ 2.43361036e+01,  2.41992541e+01,  6.15731087e+01, -8.31293874e-01,
       -5.81405710e-02, -5.52784016e-01]), 'targetState': array([-0.06963443,  1.99878739, 27.        ]), 'previousTarget': array([11.42672561, 12.14853069, 42.46998329])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6275665657865398
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([-0.06963443,  1.99878739, 27.        ]), 'distance': 4.237338466451625, 'localFrame': array([[ 0.01615437,  0.11023416,  0.99377435],
       [-0.98943206,  0.14499722,  0.        ],
       [-0.14409452, -0.98327221,  0.11141156]]), 'currentState': array([2.53174155e+00, 4.94083096e+00, 2.85913076e+01, 1.61543656e-02,
       1.10234165e-01, 9.93774353e-01]), 'targetState': array([-0.06963443,  1.99878739, 27.        ]), 'previousTarget': array([-0.06963443,  1.99878739, 27.        ])}
episode index:19545
target thresh 86.54464872005592
target distance 20.630092314011804
model initialize at round 19545
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.24866652, -10.79126961,  84.        ]), 'distance': 24.05648050127648, 'localFrame': array([[ 0.03615117,  0.6940212 ,  0.71904636],
       [-0.9986461 ,  0.0520189 ,  0.        ],
       [-0.037404  , -0.71807284,  0.69496211]]), 'currentState': array([ 6.97056643e+00, -3.03552515e+01,  7.71695380e+01,  3.61511674e-02,
        6.94021203e-01,  7.19046357e-01]), 'targetState': array([ -5.24866652, -10.79126961,  84.        ]), 'previousTarget': array([ -5.24866652, -10.79126961,  84.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6275798072838247
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.24866652, -10.79126961,  84.        ]), 'distance': 3.728944899447651, 'localFrame': array([[-0.80506922,  0.44680657,  0.39016335],
       [-0.48526601, -0.87436657,  0.        ],
       [ 0.34114579, -0.18933301,  0.92074565]]), 'currentState': array([ -3.55054729, -12.4674912 ,  81.1343932 ,  -0.80506922,
         0.44680657,   0.39016335]), 'targetState': array([ -5.24866652, -10.79126961,  84.        ]), 'previousTarget': array([ -5.24866652, -10.79126961,  84.        ])}
episode index:19546
target thresh 86.5459941879094
target distance 63.0
model initialize at round 19546
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.98424773, 18.50462623, 59.14095543]), 'distance': 27.5, 'localFrame': array([[-0.53052205,  0.61869994, -0.5794452 ],
       [-0.7591306 , -0.65093835,  0.        ],
       [-0.3771831 ,  0.43987458,  0.8150112 ]]), 'currentState': array([ 1.39009675, 25.58815872, 85.49787799, -0.53052205,  0.61869994,
       -0.5794452 ]), 'targetState': array([-6.61119284,  8.79159424, 23.        ]), 'previousTarget': array([-1.65724618, 17.55489281, 59.49026759])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.62758192484413
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-6.61119284,  8.79159424, 23.        ]), 'distance': 1.9637457268391942, 'localFrame': array([[-0.49293612,  0.08006071, -0.8663742 ],
       [-0.16031528, -0.98706586,  0.        ],
       [-0.8551684 ,  0.13889302,  0.49939537]]), 'currentState': array([-6.14640811, 10.12801795, 24.36170632, -0.49293612,  0.08006071,
       -0.8663742 ]), 'targetState': array([-6.61119284,  8.79159424, 23.        ]), 'previousTarget': array([-6.61119284,  8.79159424, 23.        ])}
episode index:19547
target thresh 86.54733952122282
target distance 22.066284788532705
model initialize at round 19547
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.97106087, 22.36875606, 89.77423011]), 'distance': 27.5, 'localFrame': array([[ 0.56952065, -0.18042204, -0.80193149],
       [ 0.30200402,  0.95330665,  0.        ],
       [ 0.76448662, -0.24218653,  0.59741601]]), 'currentState': array([-26.02325696,   3.68593612,  96.42374615,   0.56952065,
        -0.18042204,  -0.80193149]), 'targetState': array([-4.75273644, 24.54407253, 89.        ]), 'previousTarget': array([-7.88234782, 21.50567537, 90.27644969])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6275929418928582
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.75273644, 24.54407253, 89.        ]), 'distance': 2.4521196894925437, 'localFrame': array([[ 0.99068905, -0.09117541, -0.10110517],
       [ 0.09164503,  0.99579174,  0.        ],
       [ 0.10067969, -0.00926579,  0.99487574]]), 'currentState': array([-6.12426591, 25.05820235, 90.96658801,  0.99068905, -0.09117541,
       -0.10110517]), 'targetState': array([-4.75273644, 24.54407253, 89.        ]), 'previousTarget': array([-4.75273644, 24.54407253, 89.        ])}
episode index:19548
target thresh 86.54868472000965
target distance 12.0
model initialize at round 19548
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.07163363,  6.29007685, 77.        ]), 'distance': 13.148550668864715, 'localFrame': array([[-0.33883103,  0.76813815,  0.54328383],
       [-0.91494131, -0.40358692,  0.        ],
       [ 0.21926225, -0.49707282,  0.83954909]]), 'currentState': array([-1.58317807e+00, -6.36129813e-02,  6.64715748e+01, -3.38831029e-01,
        7.68138146e-01,  5.43283832e-01]), 'targetState': array([ 3.07163363,  6.29007685, 77.        ]), 'previousTarget': array([ 3.07163363,  6.29007685, 77.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6276089983257963
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.07163363,  6.29007685, 77.        ]), 'distance': 2.9472999005185287, 'localFrame': array([[ 0.69836635,  0.71203794, -0.07270779],
       [-0.7139275 ,  0.70021963,  0.        ],
       [ 0.05091142,  0.05190809,  0.99735329]]), 'currentState': array([ 2.91006514e+00,  4.88925332e+00,  7.44119185e+01,  6.98366348e-01,
        7.12037935e-01, -7.27077873e-02]), 'targetState': array([ 3.07163363,  6.29007685, 77.        ]), 'previousTarget': array([ 3.07163363,  6.29007685, 77.        ])}
episode index:19549
target thresh 86.5500297842833
target distance 32.36931888879316
model initialize at round 19549
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.85629249, -12.80383369,  84.53303218]), 'distance': 27.500000000000004, 'localFrame': array([[-0.86579549, -0.39721065, -0.30433841],
       [ 0.41699104, -0.9089106 ,  0.        ],
       [-0.27661641, -0.12690639,  0.95256398]]), 'currentState': array([ 0.53249913, 11.64861583, 79.18297257, -0.86579549, -0.39721065,
       -0.30433841]), 'targetState': array([-13.97905971, -19.50861065,  86.        ]), 'previousTarget': array([ -9.99033839, -11.37658446,  84.24141547])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6276155001308541
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.97905971, -19.50861065,  86.        ]), 'distance': 3.6657236456342015, 'localFrame': array([[ 0.50263453, -0.66742026,  0.54946221],
       [ 0.79880961,  0.60158392,  0.        ],
       [-0.33054763,  0.4389157 ,  0.83551857]]), 'currentState': array([-12.20557917, -17.30843379,  83.66515132,   0.50263453,
        -0.66742026,   0.54946221]), 'targetState': array([-13.97905971, -19.50861065,  86.        ]), 'previousTarget': array([-13.97905971, -19.50861065,  86.        ])}
episode index:19550
target thresh 86.55137471405727
target distance 47.0
model initialize at round 19550
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.24358542, -1.93834987, 38.89596928]), 'distance': 27.5, 'localFrame': array([[-0.04213685, -0.37504   ,  0.92605048],
       [ 0.99374754, -0.11165047,  0.        ],
       [ 0.10339397,  0.92026038,  0.37739967]]), 'currentState': array([ 2.7529482 , -3.56395301, 11.73653016, -0.04213685, -0.37504   ,
        0.92605048]), 'targetState': array([-3.90760831, -0.85474983, 57.        ]), 'previousTarget': array([-1.39248165, -1.93319007, 37.24001153])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275833986782362
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.90760831, -0.85474983, 57.        ]), 'distance': 3.4677514776151668, 'localFrame': array([[-0.61034252,  0.14131578,  0.77943047],
       [-0.22556795, -0.97422744,  0.        ],
       [ 0.75934255, -0.17581453,  0.62648874]]), 'currentState': array([-3.3377223 , -2.34480892, 53.92099786, -0.61034252,  0.14131578,
        0.77943047]), 'targetState': array([-3.90760831, -0.85474983, 57.        ]), 'previousTarget': array([-3.90760831, -0.85474983, 57.        ])}
episode index:19551
target thresh 86.55271950934498
target distance 39.0
model initialize at round 19551
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.12833847, -1.62859538, 60.38905044]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.23999451, -0.84481655, -0.47821296],
       [ 0.96193841,  0.27326636,  0.        ],
       [ 0.13067951, -0.46001141,  0.87824391]]), 'currentState': array([ 8.74375858,  0.96060125, 87.71918909,  0.23999451, -0.84481655,
       -0.47821296]), 'targetState': array([ 6.45515853, -2.70756872, 49.        ]), 'previousTarget': array([ 6.81023627, -1.17215917, 60.74429079])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6275883801091527
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 6.45515853, -2.70756872, 49.        ]), 'distance': 1.6237020242882982, 'localFrame': array([[ 0.21150126, -0.54970422, -0.80814138],
       [ 0.93330205,  0.35909232,  0.        ],
       [ 0.29019736, -0.75424   ,  0.58898855]]), 'currentState': array([ 5.63608734, -2.21955411, 50.3142954 ,  0.21150126, -0.54970422,
       -0.80814138]), 'targetState': array([ 6.45515853, -2.70756872, 49.        ]), 'previousTarget': array([ 6.45515853, -2.70756872, 49.        ])}
episode index:19552
target thresh 86.55406417015989
target distance 14.273408829477521
model initialize at round 19552
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.80952805,  12.24984991,  26.        ]), 'distance': 18.413767312340635, 'localFrame': array([[-0.84003182,  0.42588187, -0.33611185],
       [-0.4521893 , -0.89192199,  0.        ],
       [-0.29978555,  0.15198618,  0.94182208]]), 'currentState': array([-2.94131805, 12.04611198, 39.16945287, -0.84003182,  0.42588187,
       -0.33611185]), 'targetState': array([-15.80952805,  12.24984991,  26.        ]), 'previousTarget': array([-15.80952805,  12.24984991,  26.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6276025361821287
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.80952805,  12.24984991,  26.        ]), 'distance': 2.4752625944796107, 'localFrame': array([[-0.93666869,  0.06972358,  0.34320603],
       [-0.07423245, -0.99724097,  0.        ],
       [ 0.34225912, -0.02547703,  0.93926014]]), 'currentState': array([-13.35498151,  12.56711913,  26.03829362,  -0.93666869,
         0.06972358,   0.34320603]), 'targetState': array([-15.80952805,  12.24984991,  26.        ]), 'previousTarget': array([-15.80952805,  12.24984991,  26.        ])}
episode index:19553
target thresh 86.55540869651543
target distance 20.35259849220089
model initialize at round 19553
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.24791414,  20.54256417,  13.        ]), 'distance': 25.56814555713202, 'localFrame': array([[-0.70471406,  0.47968187, -0.52276514],
       [-0.56269203, -0.82666661,  0.        ],
       [-0.43215249,  0.29415578,  0.85247675]]), 'currentState': array([ 5.53673005,  4.3537149 , 12.5319394 , -0.70471406,  0.47968187,
       -0.52276514]), 'targetState': array([-14.24791414,  20.54256417,  13.        ]), 'previousTarget': array([-14.24791414,  20.54256417,  13.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275704403175393
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 60, 'trapConfig': [], 'currentTarget': array([-14.24791414,  20.54256417,  13.        ]), 'distance': 20.299776266727143, 'localFrame': array([[ 0.57562409,  0.54843055,  0.60653181],
       [-0.68979837,  0.72400152,  0.        ],
       [-0.43912996, -0.41838466,  0.79505922]]), 'currentState': array([ 2.40389046,  9.19778715, 10.5313254 ,  0.57562409,  0.54843055,
        0.60653181]), 'targetState': array([-14.24791414,  20.54256417,  13.        ]), 'previousTarget': array([-14.24791414,  20.54256417,  13.        ])}
episode index:19554
target thresh 86.55675308842507
target distance 11.230427132208812
model initialize at round 19554
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.85118988, -4.13052021, 69.        ]), 'distance': 12.887422679045594, 'localFrame': array([[ 0.80809396, -0.2528831 , -0.53200966],
       [ 0.29865557,  0.95436096,  0.        ],
       [ 0.50772925, -0.15888765,  0.84673828]]), 'currentState': array([-12.06901306,   7.36527563,  71.58972246,   0.80809396,
        -0.2528831 ,  -0.53200966]), 'targetState': array([-6.85118988, -4.13052021, 69.        ]), 'previousTarget': array([-6.85118988, -4.13052021, 69.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6275860115222229
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.85118988, -4.13052021, 69.        ]), 'distance': 1.497603058623083, 'localFrame': array([[-0.05121937, -0.95009521, -0.30772663],
       [ 0.99855003, -0.05383155,  0.        ],
       [-0.0165654 , -0.30728043,  0.95147481]]), 'currentState': array([-7.60887744e+00, -2.84380818e+00,  6.88855595e+01, -5.12193663e-02,
       -9.50095206e-01, -3.07726627e-01]), 'targetState': array([-6.85118988, -4.13052021, 69.        ]), 'previousTarget': array([-6.85118988, -4.13052021, 69.        ])}
episode index:19555
target thresh 86.55809734590223
target distance 27.0
model initialize at round 19555
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.56312298, -7.64323618, 20.16278865]), 'distance': 27.5, 'localFrame': array([[ 0.01004546,  0.73585598, -0.67706356],
       [-0.99990683,  0.01365013,  0.        ],
       [ 0.009242  ,  0.67700048,  0.73592454]]), 'currentState': array([-1.87243947e+01, -2.03119378e+01,  4.07184430e+01,  1.00454632e-02,
        7.35855977e-01, -6.77063564e-01]), 'targetState': array([-2.25751843, -4.46134627, 15.        ]), 'previousTarget': array([-6.45565736, -8.54946745, 21.5310868 ])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6275957436211221
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.25751843, -4.46134627, 15.        ]), 'distance': 2.8667977359682646, 'localFrame': array([[ 0.7728453 ,  0.34727068, -0.53114331],
       [-0.40986434,  0.91214649,  0.        ],
       [ 0.48448051,  0.2176967 ,  0.847282  ]]), 'currentState': array([-1.63979338, -6.93541846, 16.30992818,  0.7728453 ,  0.34727068,
       -0.53114331]), 'targetState': array([-2.25751843, -4.46134627, 15.        ]), 'previousTarget': array([-2.25751843, -4.46134627, 15.        ])}
episode index:19556
target thresh 86.55944146896036
target distance 76.0
model initialize at round 19556
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.7947684 , 15.26632657, 40.6417625 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.04838932,  0.82187628, -0.56760713],
       [-0.99827126, -0.05877487,  0.        ],
       [-0.03336104,  0.56662589,  0.82329955]]), 'currentState': array([12.06789569, 15.04871139, 13.55192459, -0.04838932,  0.82187628,
       -0.56760713]), 'targetState': array([25.58171572, 15.67085897, 91.        ]), 'previousTarget': array([17.29790847, 14.67795337, 42.10814614])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275636530272877
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([25.58171572, 15.67085897, 91.        ]), 'distance': 12.685902400955102, 'localFrame': array([[ 0.58393439, -0.74443409,  0.32378776],
       [ 0.78682031,  0.61718215,  0.        ],
       [-0.19983603,  0.25476279,  0.94612974]]), 'currentState': array([16.49141002, 16.31747463, 82.17501557,  0.58393439, -0.74443409,
        0.32378776]), 'targetState': array([25.58171572, 15.67085897, 91.        ]), 'previousTarget': array([25.58171572, 15.67085897, 91.        ])}
episode index:19557
target thresh 86.56078545761291
target distance 30.345943334073112
model initialize at round 19557
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-19.72317153,   0.48197694,  82.69859197]), 'distance': 27.5, 'localFrame': array([[ 0.66264119,  0.21077163, -0.7186668 ],
       [-0.30311388,  0.95295434,  0.        ],
       [ 0.68485665,  0.21783788,  0.69535461]]), 'currentState': array([  2.86294985, -13.70414257,  76.        ,   0.66264119,
         0.21077163,  -0.7186668 ]), 'targetState': array([-27.48299349,   5.35584438,  85.        ]), 'previousTarget': array([-19.72317153,   0.48197694,  82.69859197])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6275721431914465
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-27.48299349,   5.35584438,  85.        ]), 'distance': 3.790406770718488, 'localFrame': array([[-0.86645619,  0.34351565, -0.36228533],
       [-0.36855244, -0.92960696,  0.        ],
       [-0.33678297,  0.13352114,  0.93206724]]), 'currentState': array([-25.1247849 ,   3.64754977,  82.57352823,  -0.86645619,
         0.34351565,  -0.36228533]), 'targetState': array([-27.48299349,   5.35584438,  85.        ]), 'previousTarget': array([-27.48299349,   5.35584438,  85.        ])}
episode index:19558
target thresh 86.56212931187332
target distance 51.0
model initialize at round 19558
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.74090227, 26.27958421, 30.84129858]), 'distance': 27.5, 'localFrame': array([[ 0.54871996,  0.45428786, -0.70180406],
       [-0.63771333,  0.77027379,  0.        ],
       [ 0.54058127,  0.4475498 ,  0.71237003]]), 'currentState': array([ 5.71369161, 11.3570404 , 53.94036665,  0.54871996,  0.45428786,
       -0.70180406]), 'targetState': array([ 5.77252127, 43.61969736,  4.        ]), 'previousTarget': array([ 4.93294867, 25.96759931, 31.79369661])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6275719362918964
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 5.77252127, 43.61969736,  4.        ]), 'distance': 3.136720370865002, 'localFrame': array([[ 0.33832465,  0.30776893, -0.88927764],
       [-0.67291345,  0.73972123,  0.        ],
       [ 0.65781754,  0.59840688,  0.45736777]]), 'currentState': array([ 7.53237566, 45.1480646 ,  6.09905236,  0.33832465,  0.30776893,
       -0.88927764]), 'targetState': array([ 5.77252127, 43.61969736,  4.        ]), 'previousTarget': array([ 5.77252127, 43.61969736,  4.        ])}
episode index:19559
target thresh 86.56347303175501
target distance 38.59452840798979
model initialize at round 19559
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.41746234, -7.69014837, 69.31414997]), 'distance': 27.5, 'localFrame': array([[-0.73274233,  0.66550605, -0.14209285],
       [-0.67232794, -0.74025343,  0.        ],
       [-0.10518472,  0.09553299,  0.98985333]]), 'currentState': array([ 40.06124896, -20.84858455,  83.35791032,  -0.73274233,
         0.66550605,  -0.14209285]), 'targetState': array([ 3.1929777 ,  3.84771275, 57.        ]), 'previousTarget': array([21.76096523, -8.35337641, 69.50870773])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6275686813457901
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([ 3.1929777 ,  3.84771275, 57.        ]), 'distance': 2.07395661490546, 'localFrame': array([[-0.94187555, -0.14488317, -0.303116  ],
       [ 0.1520359 , -0.98837497,  0.        ],
       [-0.29959227, -0.04608451,  0.95295367]]), 'currentState': array([ 4.68236762,  5.26129516, 56.70879821, -0.94187555, -0.14488317,
       -0.303116  ]), 'targetState': array([ 3.1929777 ,  3.84771275, 57.        ]), 'previousTarget': array([ 3.1929777 ,  3.84771275, 57.        ])}
episode index:19560
target thresh 86.56481661727145
target distance 18.0
model initialize at round 19560
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.90350771, -0.87328549, 63.        ]), 'distance': 18.25664657457855, 'localFrame': array([[-0.83435507,  0.07796631, -0.5456857 ],
       [-0.09303968, -0.9956624 ,  0.        ],
       [-0.54331873,  0.05077042,  0.83798993]]), 'currentState': array([ 6.21756605e+00,  6.54656426e+00,  7.95195674e+01, -8.34355065e-01,
        7.79663134e-02, -5.45685697e-01]), 'targetState': array([ 3.90350771, -0.87328549, 63.        ]), 'previousTarget': array([ 3.90350771, -0.87328549, 63.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6275828326363
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.90350771, -0.87328549, 63.        ]), 'distance': 2.3771956741271527, 'localFrame': array([[ 0.71342536, -0.68050131,  0.16715927],
       [ 0.69021267,  0.72360657,  0.        ],
       [-0.12095755,  0.11537544,  0.98592991]]), 'currentState': array([ 3.24541606,  0.89778774, 64.44266222,  0.71342536, -0.68050131,
        0.16715927]), 'targetState': array([ 3.90350771, -0.87328549, 63.        ]), 'previousTarget': array([ 3.90350771, -0.87328549, 63.        ])}
episode index:19561
target thresh 86.56616006843603
target distance 47.348822043587155
model initialize at round 19561
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.61842547, -24.9822981 ,  87.09603549]), 'distance': 27.5, 'localFrame': array([[-0.89661247, -0.08587729,  0.43440898],
       [ 0.09534339, -0.99544444,  0.        ],
       [ 0.43243001,  0.04141802,  0.90071574]]), 'currentState': array([ 38.56318045, -12.60457723,  76.07470367,  -0.89661247,
        -0.08587729,   0.43440898]), 'targetState': array([ -7.08375399, -38.35127676,  99.        ]), 'previousTarget': array([ 18.15473185, -24.27677657,  86.74024043])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275507509047471
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 58, 'trapConfig': [], 'currentTarget': array([ -7.08375399, -38.35127676,  99.        ]), 'distance': 15.372522053838928, 'localFrame': array([[-0.45549392, -0.63275819,  0.62621272],
       [ 0.81159028, -0.58422703,  0.        ],
       [ 0.3658504 ,  0.50822815,  0.77965225]]), 'currentState': array([  1.22814321, -28.78946623,  90.29376209,  -0.45549392,
        -0.63275819,   0.62621272]), 'targetState': array([ -7.08375399, -38.35127676,  99.        ]), 'previousTarget': array([ -7.08375399, -38.35127676,  99.        ])}
episode index:19562
target thresh 86.56750338526223
target distance 25.626772994804448
model initialize at round 19562
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-3.78969119, 11.39090689, 24.79364153]), 'distance': 27.500000000000004, 'localFrame': array([[-0.45713027,  0.71428388, -0.52993438],
       [-0.84227751, -0.53904415,  0.        ],
       [-0.28565803,  0.44635181,  0.84803865]]), 'currentState': array([14.53768539, 30.73525238, 18.        , -0.45713027,  0.71428388,
       -0.52993438]), 'targetState': array([-9.74183958,  5.10847939, 27.        ]), 'previousTarget': array([-3.78969119, 11.39090689, 24.79364153])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6275588338874135
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-9.74183958,  5.10847939, 27.        ]), 'distance': 2.898458877148339, 'localFrame': array([[-0.19979737, -0.92247987,  0.33032092],
       [ 0.9773392 , -0.2116792 ,  0.        ],
       [ 0.06992207,  0.32283558,  0.94386868]]), 'currentState': array([-7.34803126,  6.52944837, 26.19278705, -0.19979737, -0.92247987,
        0.33032092]), 'targetState': array([-9.74183958,  5.10847939, 27.        ]), 'previousTarget': array([-9.74183958,  5.10847939, 27.        ])}
episode index:19563
target thresh 86.56884656776347
target distance 21.173631597894055
model initialize at round 19563
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.66199105,  -3.37313716,  82.        ]), 'distance': 23.61047189224332, 'localFrame': array([[-0.76993453,  0.52154411,  0.36768542],
       [-0.56083013, -0.8279309 ,  0.        ],
       [ 0.30441812, -0.20620906,  0.92995023]]), 'currentState': array([ 3.63626462,  3.75585956, 72.27309887, -0.76993453,  0.52154411,
        0.36768542]), 'targetState': array([-16.66199105,  -3.37313716,  82.        ]), 'previousTarget': array([-16.66199105,  -3.37313716,  82.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.627570717935704
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.66199105,  -3.37313716,  82.        ]), 'distance': 1.9582525357442933, 'localFrame': array([[-0.55686506, -0.71006912,  0.43093288],
       [ 0.78688134, -0.61710433,  0.        ],
       [ 0.26593055,  0.33909304,  0.90238398]]), 'currentState': array([-16.72644634,  -4.11133843,  80.18736176,  -0.55686506,
        -0.71006912,   0.43093288]), 'targetState': array([-16.66199105,  -3.37313716,  82.        ]), 'previousTarget': array([-16.66199105,  -3.37313716,  82.        ])}
episode index:19564
target thresh 86.57018961595315
target distance 83.0
model initialize at round 19564
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.16507578, -4.95053984, 65.07783558]), 'distance': 27.499999999999996, 'localFrame': array([[-0.62286084,  0.09510909, -0.77652987],
       [-0.15094753, -0.98854178,  0.        ],
       [-0.76763222,  0.11721526,  0.63008044]]), 'currentState': array([ -8.23664498, -12.60727141,  90.22687597,  -0.62286084,
         0.09510909,  -0.77652987]), 'targetState': array([18.15402077, 12.42704831,  8.        ]), 'previousTarget': array([ 0.97384653, -4.79299084, 65.66223167])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6275652372203815
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([18.15402077, 12.42704831,  8.        ]), 'distance': 3.062699537738094, 'localFrame': array([[-0.39423817, -0.57442169, -0.7173674 ],
       [ 0.82449514, -0.56586904,  0.        ],
       [-0.405936  , -0.59146593,  0.69669506]]), 'currentState': array([20.4550838 , 11.81268538,  9.92556371, -0.39423817, -0.57442169,
       -0.7173674 ]), 'targetState': array([18.15402077, 12.42704831,  8.        ]), 'previousTarget': array([18.15402077, 12.42704831,  8.        ])}
episode index:19565
target thresh 86.57153252984475
target distance 58.14072476232289
model initialize at round 19565
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.55858994, -23.71175478,  61.67550143]), 'distance': 27.500000000000004, 'localFrame': array([[-0.5745823 , -0.79983815,  0.17353415],
       [ 0.81216037, -0.58343426,  0.        ],
       [ 0.10124577,  0.14093756,  0.98482785]]), 'currentState': array([ 26.61174053, -25.54208877,  57.84496368,  -0.5745823 ,
        -0.79983815,   0.17353415]), 'targetState': array([-31.23262654, -21.64539303,  66.        ]), 'previousTarget': array([ -0.24801972, -22.85522215,  61.20368103])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6275680476751393
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-31.23262654, -21.64539303,  66.        ]), 'distance': 3.4750326260774282, 'localFrame': array([[-0.07703252,  0.4063218 ,  0.91047712],
       [-0.98249914, -0.1862671 ,  0.        ],
       [ 0.16959193, -0.89454299,  0.41355945]]), 'currentState': array([-29.52443647, -23.46590932,  63.5826339 ,  -0.07703252,
         0.4063218 ,   0.91047712]), 'targetState': array([-31.23262654, -21.64539303,  66.        ]), 'previousTarget': array([-31.23262654, -21.64539303,  66.        ])}
episode index:19566
target thresh 86.57287530945167
target distance 76.0
model initialize at round 19566
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.37419048, -4.70506721, 53.60296452]), 'distance': 27.5, 'localFrame': array([[-0.57747561,  0.70435562,  0.4128015 ],
       [-0.77331947, -0.63401656,  0.        ],
       [ 0.26172299, -0.31922744,  0.91082101]]), 'currentState': array([ 4.34381819, -9.19976949, 80.05447226, -0.57747561,  0.70435562,
        0.4128015 ]), 'targetState': array([21.68259475,  3.72358495,  4.        ]), 'previousTarget': array([10.80861126, -5.81967688, 53.60015626])}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.6275572924664848
{'scaleFactor': 20, 'timeStep': 88, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([21.68259475,  3.72358495,  4.        ]), 'distance': 3.514494111113536, 'localFrame': array([[-0.56652942, -0.34264218, -0.74942695],
       [ 0.51751833, -0.85567212,  0.        ],
       [-0.64126375, -0.38784218,  0.66208704]]), 'currentState': array([24.09144471,  3.48414902,  6.5478974 , -0.56652942, -0.34264218,
       -0.74942695]), 'targetState': array([21.68259475,  3.72358495,  4.        ]), 'previousTarget': array([21.68259475,  3.72358495,  4.        ])}
episode index:19567
target thresh 86.57421795478733
target distance 42.32183077359348
model initialize at round 19567
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.88928606, -23.2803135 ,  81.58461772]), 'distance': 27.5, 'localFrame': array([[ 0.16121216, -0.92916047,  0.3326732 ],
       [ 0.98527987,  0.17094905,  0.        ],
       [-0.05687017,  0.32777621,  0.94304217]]), 'currentState': array([ 0.42710634, -1.17097505, 97.3578834 ,  0.16121216, -0.92916047,
        0.3326732 ]), 'targetState': array([ -7.60674963, -42.32183077,  68.        ]), 'previousTarget': array([ -4.07667778, -22.68149677,  80.99398779])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6275622711584049
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.60674963, -42.32183077,  68.        ]), 'distance': 3.062957800977948, 'localFrame': array([[ 0.09459517, -0.88909064,  0.44784996],
       [ 0.99438761,  0.10579828,  0.        ],
       [-0.04738176,  0.44533645,  0.89410873]]), 'currentState': array([ -8.46487505, -42.75615614,  65.09196067,   0.09459517,
        -0.88909064,   0.44784996]), 'targetState': array([ -7.60674963, -42.32183077,  68.        ]), 'previousTarget': array([ -7.60674963, -42.32183077,  68.        ])}
episode index:19568
target thresh 86.57556046586518
target distance 32.0
model initialize at round 19568
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.19113781,   8.36249041,  67.19550041]), 'distance': 27.5, 'localFrame': array([[ 0.60814532,  0.70066286, -0.37313646],
       [-0.75520654,  0.6554869 ,  0.        ],
       [ 0.24458606,  0.2817951 ,  0.92777647]]), 'currentState': array([-22.33222391,  11.00980039,  94.0806447 ,   0.60814532,
         0.70066286,  -0.37313646]), 'targetState': array([-16.1976332 ,   7.85090305,  62.        ]), 'previousTarget': array([-17.32190776,   8.09894136,  67.15998486])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6275302019534809
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 86, 'trapConfig': [], 'currentTarget': array([-16.1976332 ,   7.85090305,  62.        ]), 'distance': 11.11743224204017, 'localFrame': array([[ 0.22310201, -0.66615929, -0.71165813],
       [ 0.94823443,  0.31757121,  0.        ],
       [ 0.22600213, -0.67481874,  0.70252595]]), 'currentState': array([-18.28202915,   7.51052659,  72.91497765,   0.22310201,
        -0.66615929,  -0.71165813]), 'targetState': array([-16.1976332 ,   7.85090305,  62.        ]), 'previousTarget': array([-16.1976332 ,   7.85090305,  62.        ])}
episode index:19569
target thresh 86.57690284269864
target distance 45.0
model initialize at round 19569
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([37.7910107 , 15.92770057, 25.71049143]), 'distance': 27.5, 'localFrame': array([[-0.58641083, -0.48765512, -0.64677262],
       [ 0.63939435, -0.76887897,  0.        ],
       [-0.49728986, -0.41354275,  0.76268288]]), 'currentState': array([42.99456787, -0.68343478, 47.        , -0.58641083, -0.48765512,
       -0.64677262]), 'targetState': array([31.99571969, 34.42780739,  2.        ]), 'previousTarget': array([37.7910107 , 15.92770057, 25.71049143])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.627533721780568
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([31.99571969, 34.42780739,  2.        ]), 'distance': 3.1040658663655525, 'localFrame': array([[-0.66852931,  0.56663767, -0.48165372],
       [-0.64657974, -0.76284641,  0.        ],
       [-0.36742781,  0.31142754,  0.87636162]]), 'currentState': array([33.64135212, 32.64489537,  3.93606392, -0.66852931,  0.56663767,
       -0.48165372]), 'targetState': array([31.99571969, 34.42780739,  2.        ]), 'previousTarget': array([31.99571969, 34.42780739,  2.        ])}
episode index:19570
target thresh 86.57824508530112
target distance 19.0
model initialize at round 19570
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.63221041,  -8.32191144,  47.        ]), 'distance': 20.914745021779172, 'localFrame': array([[-0.0507533 , -0.63592314,  0.77008173],
       [ 0.99683028, -0.07955746,  0.        ],
       [ 0.06126575,  0.76764079,  0.63794524]]), 'currentState': array([-32.34110416,  -1.09385574,  28.95132339,  -0.0507533 ,
        -0.63592314,   0.77008173]), 'targetState': array([-24.63221041,  -8.32191144,  47.        ]), 'previousTarget': array([-24.63221041,  -8.32191144,  47.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6275478676266274
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.63221041,  -8.32191144,  47.        ]), 'distance': 4.61979485426286, 'localFrame': array([[ 0.44649197, -0.88331002,  0.14285776],
       [ 0.89246381,  0.45111899,  0.        ],
       [-0.06444585,  0.12749538,  0.98974323]]), 'currentState': array([-27.24820366,  -5.42928345,  44.52375544,   0.44649197,
        -0.88331002,   0.14285776]), 'targetState': array([-24.63221041,  -8.32191144,  47.        ]), 'previousTarget': array([-24.63221041,  -8.32191144,  47.        ])}
episode index:19571
target thresh 86.57958719368605
target distance 66.0
model initialize at round 19571
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.48916143, -11.38872558,  52.06435838]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.76671255,  0.48379607, -0.42201093],
       [-0.53364329,  0.84570967,  0.        ],
       [ 0.35689873,  0.2252033 ,  0.90659074]]), 'currentState': array([-9.48098781, -0.67443464, 77.07474206,  0.76671255,  0.48379607,
       -0.42201093]), 'targetState': array([  1.06498788, -28.98043824,  11.        ]), 'previousTarget': array([ -6.30671071, -12.21375894,  51.90152782])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6275486372840186
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  1.06498788, -28.98043824,  11.        ]), 'distance': 2.3604806506895493, 'localFrame': array([[-0.28071539,  0.42476789, -0.86068061],
       [-0.83427643, -0.55134638,  0.        ],
       [-0.47453314,  0.71804555,  0.50914526]]), 'currentState': array([ -0.31999145, -30.17640821,  12.49109255,  -0.28071539,
         0.42476789,  -0.86068061]), 'targetState': array([  1.06498788, -28.98043824,  11.        ]), 'previousTarget': array([  1.06498788, -28.98043824,  11.        ])}
episode index:19572
target thresh 86.58092916786686
target distance 21.20592963621985
model initialize at round 19572
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.29799783,  -5.63908616,  57.        ]), 'distance': 25.014220868061916, 'localFrame': array([[-0.09018846, -0.7625272 ,  0.64063899],
       [ 0.99307797, -0.11745702,  0.        ],
       [ 0.07524755,  0.63620446,  0.76784223]]), 'currentState': array([-0.59443194, -4.44261329, 44.62119044, -0.09018846, -0.7625272 ,
        0.64063899]), 'targetState': array([-22.29799783,  -5.63908616,  57.        ]), 'previousTarget': array([-22.29799783,  -5.63908616,  57.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6275609602378572
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.29799783,  -5.63908616,  57.        ]), 'distance': 1.8704426084105343, 'localFrame': array([[-0.96780774,  0.14346289, -0.20680083],
       [-0.14663264, -0.98919102,  0.        ],
       [-0.20456552,  0.03032375,  0.97838306]]), 'currentState': array([-20.99950795,  -4.80536368,  55.942935  ,  -0.96780774,
         0.14346289,  -0.20680083]), 'targetState': array([-22.29799783,  -5.63908616,  57.        ]), 'previousTarget': array([-22.29799783,  -5.63908616,  57.        ])}
episode index:19573
target thresh 86.58227100785695
target distance 51.0
model initialize at round 19573
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.69532908,  7.79320284, 29.80355181]), 'distance': 27.5, 'localFrame': array([[ 0.79706523, -0.27407736, -0.5381158 ],
       [ 0.32517121,  0.94565516,  0.        ],
       [ 0.50887199, -0.17497977,  0.84287092]]), 'currentState': array([-12.53825168,  12.51269949,  54.63110466,   0.79706523,
        -0.27407736,  -0.5381158 ]), 'targetState': array([9.57384097, 2.88817747, 4.        ]), 'previousTarget': array([-2.82253356,  7.86322781, 30.45219965])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6275565729953789
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([9.57384097, 2.88817747, 4.        ]), 'distance': 3.481271990133692, 'localFrame': array([[ 0.60081832, -0.62806696,  0.4945192 ],
       [ 0.72260817,  0.69125786,  0.        ],
       [-0.34184029,  0.35734362,  0.8691667 ]]), 'currentState': array([ 7.76308979,  5.41555683,  5.56613806,  0.60081832, -0.62806696,
        0.4945192 ]), 'targetState': array([9.57384097, 2.88817747, 4.        ]), 'previousTarget': array([9.57384097, 2.88817747, 4.        ])}
episode index:19574
target thresh 86.58361271366975
target distance 40.0
model initialize at round 19574
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.35116459,  -3.655395  ,  58.20507275]), 'distance': 27.500000000000004, 'localFrame': array([[-0.54301107, -0.81716768, -0.19332865],
       [ 0.83288076, -0.55345247,  0.        ],
       [-0.10699822, -0.16101971,  0.98113405]]), 'currentState': array([-21.93773552,  -7.59546983,  31.53260388,  -0.54301107,
        -0.81716768,  -0.19332865]), 'targetState': array([-29.94801572,  -1.76531984,  71.        ]), 'previousTarget': array([-27.02974203,  -3.17778568,  57.71584252])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6275650561479025
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.94801572,  -1.76531984,  71.        ]), 'distance': 3.667336208397138, 'localFrame': array([[ 0.38478842, -0.33242929,  0.86106251],
       [ 0.65374605,  0.75671402,  0.        ],
       [-0.65157807,  0.56291621,  0.50849913]]), 'currentState': array([-28.49254759,  -3.87243762,  68.37492437,   0.38478842,
        -0.33242929,   0.86106251]), 'targetState': array([-29.94801572,  -1.76531984,  71.        ]), 'previousTarget': array([-29.94801572,  -1.76531984,  71.        ])}
episode index:19575
target thresh 86.58495428531869
target distance 76.08241931517401
model initialize at round 19575
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.91444094, -29.04917843,  24.33953936]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.60901988, -0.77315415, -0.17699561],
       [ 0.78555681,  0.61878955,  0.        ],
       [ 0.10952304, -0.13904011,  0.98421164]]), 'currentState': array([  1.56231806, -48.96525426,   7.37650659,   0.60901988,
        -0.77315415,  -0.17699561]), 'targetState': array([-31.23102413,  28.08243457,  73.        ]), 'previousTarget': array([ -8.11714925, -28.16086333,  24.21004701])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627532998268042
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 67, 'trapConfig': [], 'currentTarget': array([-6.65247385, -4.38388982, 41.05364872]), 'distance': 27.5, 'localFrame': array([[-0.35098255,  0.76755927, -0.53634319],
       [-0.90943043, -0.4158561 ,  0.        ],
       [-0.22304159,  0.48776681,  0.84399999]]), 'currentState': array([  6.40694774, -21.63435466,  24.07946331,  -0.35098255,
         0.76755927,  -0.53634319]), 'targetState': array([-31.23102413,  28.08243457,  73.        ]), 'previousTarget': array([-6.39804241, -5.76661096, 41.08553817])}
episode index:19576
target thresh 86.58629572281717
target distance 53.17236708896162
model initialize at round 19576
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-36.58249245,  14.58402173,  77.47742892]), 'distance': 27.5, 'localFrame': array([[ 0.14496759, -0.80284169, -0.5782989 ],
       [ 0.98408571,  0.17769447,  0.        ],
       [ 0.10276052, -0.56909568,  0.81582497]]), 'currentState': array([-28.78126165,  36.85369897,  91.60018622,   0.14496759,
        -0.80284169,  -0.5782989 ]), 'targetState': array([-46.78918825, -14.55238342,  59.        ]), 'previousTarget': array([-36.21686634,  16.19192456,  78.08062813])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6275365166937344
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-46.78918825, -14.55238342,  59.        ]), 'distance': 2.7512035949652978, 'localFrame': array([[-0.61483024, -0.13971601, -0.77618504],
       [ 0.22159375, -0.97513907,  0.        ],
       [-0.75688836, -0.17199775,  0.63050518]]), 'currentState': array([-46.26672711, -12.32600744,  57.47048845,  -0.61483024,
        -0.13971601,  -0.77618504]), 'targetState': array([-46.78918825, -14.55238342,  59.        ]), 'previousTarget': array([-46.78918825, -14.55238342,  59.        ])}
episode index:19577
target thresh 86.5876370261786
target distance 39.19795434006514
model initialize at round 19577
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.47962317, -4.92823462, 38.69532522]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.25630642,  0.83679959, -0.48381139],
       [-0.95615412,  0.29286396,  0.        ],
       [ 0.14169092,  0.46259825,  0.87517229]]), 'currentState': array([ -9.68242066, -29.3354613 ,  27.64713128,   0.25630642,
         0.83679959,  -0.48381139]), 'targetState': array([ 0.06001607,  8.99979989, 45.        ]), 'previousTarget': array([-3.87991388, -5.50625261, 39.07885325])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627504463546493
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 46, 'trapConfig': [], 'currentTarget': array([ 0.06001607,  8.99979989, 45.        ]), 'distance': 12.612962622729562, 'localFrame': array([[ 0.3361545 , -0.52233383,  0.78368841],
       [ 0.84090855,  0.54117725,  0.        ],
       [-0.42411434,  0.65901028,  0.62115415]]), 'currentState': array([-9.24421768,  5.20613649, 37.37594738,  0.3361545 , -0.52233383,
        0.78368841]), 'targetState': array([ 0.06001607,  8.99979989, 45.        ]), 'previousTarget': array([ 0.06001607,  8.99979989, 45.        ])}
episode index:19578
target thresh 86.5889781954164
target distance 26.388270322940514
model initialize at round 19578
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.62451834, 18.90531782, 56.47462615]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.8626677 ,  0.19156077, -0.46809071],
       [-0.21677607,  0.97622136,  0.        ],
       [ 0.45696015,  0.10147087,  0.88368042]]), 'currentState': array([-1.82751449, -0.11270579, 67.60545694,  0.8626677 ,  0.19156077,
       -0.46809071]), 'targetState': array([21.2382831 , 26.55061828, 52.        ]), 'previousTarget': array([13.80959219, 18.46151963, 57.21120465])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6275121410017181
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([21.2382831 , 26.55061828, 52.        ]), 'distance': 3.465090217252437, 'localFrame': array([[ 0.61622449,  0.65598952,  0.43582694],
       [-0.72885256,  0.68467069,  0.        ],
       [-0.29839793, -0.31765358,  0.90003049]]), 'currentState': array([18.34337044, 26.7823578 , 53.89013961,  0.61622449,  0.65598952,
        0.43582694]), 'targetState': array([21.2382831 , 26.55061828, 52.        ]), 'previousTarget': array([21.2382831 , 26.55061828, 52.        ])}
episode index:19579
target thresh 86.59031923054398
target distance 14.20839623158835
model initialize at round 19579
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([15.67637006, 12.41979959, 94.        ]), 'distance': 20.681278969731977, 'localFrame': array([[ 0.22638006,  0.2793482 ,  0.93312199],
       [-0.77691692,  0.62960313,  0.        ],
       [-0.58749652, -0.72495826,  0.35955994]]), 'currentState': array([  1.46797383,  -1.35832715, 100.        ,   0.22638006,
         0.2793482 ,   0.93312199]), 'targetState': array([15.67637006, 12.41979959, 94.        ]), 'previousTarget': array([15.67637006, 12.41979959, 94.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6275258195570428
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([15.67637006, 12.41979959, 94.        ]), 'distance': 4.071934444075579, 'localFrame': array([[-0.48488508,  0.87433113, -0.02077353],
       [-0.87451984, -0.48498974,  0.        ],
       [-0.01007495,  0.01816686,  0.99978421]]), 'currentState': array([ 1.38585117e+01,  9.61131717e+00,  9.63213073e+01, -4.84885083e-01,
        8.74331125e-01, -2.07735255e-02]), 'targetState': array([15.67637006, 12.41979959, 94.        ]), 'previousTarget': array([15.67637006, 12.41979959, 94.        ])}
episode index:19580
target thresh 86.59166013157476
target distance 34.77544938110405
model initialize at round 19580
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([22.69362147, 28.58676971, 64.19218857]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.248608  ,  0.81704301,  0.52022571],
       [-0.95669256,  0.29110024,  0.        ],
       [-0.15143783, -0.49769606,  0.85402881]]), 'currentState': array([17.69132233,  6.92943764, 48.        ,  0.248608  ,  0.81704301,
        0.52022571]), 'targetState': array([25.72357671, 41.70488702, 74.        ]), 'previousTarget': array([22.69362147, 28.58676971, 64.19218857])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6275315483019399
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([25.72357671, 41.70488702, 74.        ]), 'distance': 2.453676281081631, 'localFrame': array([[ 0.52440757,  0.61264729,  0.59132055],
       [-0.75969677,  0.65027749,  0.        ],
       [-0.38452244, -0.44922431,  0.80643661]]), 'currentState': array([26.84766645, 40.3373004 , 72.3009837 ,  0.52440757,  0.61264729,
        0.59132055]), 'targetState': array([25.72357671, 41.70488702, 74.        ]), 'previousTarget': array([25.72357671, 41.70488702, 74.        ])}
episode index:19581
target thresh 86.59300089852213
target distance 63.0
model initialize at round 19581
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.38053441, -12.8217977 ,  49.81187098]), 'distance': 27.5, 'localFrame': array([[ 0.55252888, -0.64836747,  0.5237666 ],
       [ 0.76111813,  0.64861328,  0.        ],
       [-0.33972197,  0.39864826,  0.85186181]]), 'currentState': array([-20.56666222, -12.55618031,  22.3252004 ,   0.55252888,
        -0.64836747,   0.5237666 ]), 'targetState': array([-22.4224445 , -13.16183811,  85.        ]), 'previousTarget': array([-22.25143647, -12.31035587,  49.49177806])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6275340096184938
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-22.4224445 , -13.16183811,  85.        ]), 'distance': 1.9207783072242017, 'localFrame': array([[ 0.02812955,  0.49941201,  0.86590783],
       [-0.99841749,  0.05623621,  0.        ],
       [-0.04869537, -0.86453753,  0.50020358]]), 'currentState': array([-2.22672391e+01, -1.34893324e+01,  8.31137211e+01,  2.81295532e-02,
        4.99412007e-01,  8.65907833e-01]), 'targetState': array([-22.4224445 , -13.16183811,  85.        ]), 'previousTarget': array([-22.4224445 , -13.16183811,  85.        ])}
episode index:19582
target thresh 86.59434153139952
target distance 61.250321901844956
model initialize at round 19582
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.48221908, 23.37400365, 42.24613017]), 'distance': 27.500000000000004, 'localFrame': array([[-0.92043183,  0.22364959,  0.32060272],
       [-0.23611312, -0.97172558,  0.        ],
       [ 0.31153786, -0.07569851,  0.94721375]]), 'currentState': array([27.85523627, 36.08054647, 43.11204164, -0.92043183,  0.22364959,
        0.32060272]), 'targetState': array([-31.5929046 ,   5.08806237,  41.        ]), 'previousTarget': array([ 5.19176117, 23.92975387, 42.20112563])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627501964783197
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 81, 'trapConfig': [], 'currentTarget': array([-17.45464564,  13.48200165,  46.09033482]), 'distance': 27.499999999999993, 'localFrame': array([[ 0.10569432, -0.88683579,  0.4498344 ],
       [ 0.99297267,  0.11834387,  0.        ],
       [-0.05323514,  0.44667327,  0.89311198]]), 'currentState': array([ 5.13409659, 26.89302569, 54.22317927,  0.10569432, -0.88683579,
        0.4498344 ]), 'targetState': array([-31.5929046 ,   5.08806237,  41.        ]), 'previousTarget': array([-17.45464564,  13.48200165,  46.09033482])}
episode index:19583
target thresh 86.59568203022032
target distance 30.361610217973293
model initialize at round 19583
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.92694789, -10.47837915,   3.88373796]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.62539947,  0.49943921, -0.5995298 ],
       [-0.6240241 ,  0.78140509,  0.        ],
       [ 0.46847564,  0.37412104,  0.80035244]]), 'currentState': array([-21.76750875, -17.43378963,  10.78787894,   0.62539947,
         0.49943921,  -0.5995298 ]), 'targetState': array([ 7.2158678 , -9.58807863,  3.        ]), 'previousTarget': array([  2.43927283, -10.82842221,   4.41591156])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6275129656629254
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.2158678 , -9.58807863,  3.        ]), 'distance': 2.448457107159142, 'localFrame': array([[ 0.83073043, -0.26203254, -0.49114753],
       [ 0.30081464,  0.95368263,  0.        ],
       [ 0.46839886, -0.14774437,  0.87107641]]), 'currentState': array([  5.36095574, -10.10145805,   4.51350092,   0.83073043,
        -0.26203254,  -0.49114753]), 'targetState': array([ 7.2158678 , -9.58807863,  3.        ]), 'previousTarget': array([ 7.2158678 , -9.58807863,  3.        ])}
episode index:19584
target thresh 86.59702239499795
target distance 69.0
model initialize at round 19584
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([30.79685136,  7.32991443, 48.54743194]), 'distance': 27.499999999999996, 'localFrame': array([[-0.31335818, -0.69289338, -0.64938849],
       [ 0.91115413, -0.41206571,  0.        ],
       [-0.26759073, -0.59169301,  0.76045683]]), 'currentState': array([26.35416997, 11.50287702, 21.73141126, -0.31335818, -0.69289338,
       -0.64938849]), 'targetState': array([37.99575376,  0.56806324, 92.        ]), 'previousTarget': array([30.65925932,  8.05783135, 49.69259976])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274809251745075
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([36.65517219, -3.29386445, 82.52572147]), 'distance': 27.5, 'localFrame': array([[-0.90677184,  0.02297826, -0.42099504],
       [-0.0253326 , -0.99967908,  0.        ],
       [-0.42085993,  0.0106649 ,  0.90706294]]), 'currentState': array([ 3.30824029e+01, -1.35862457e+01,  5.72759253e+01, -9.06771844e-01,
        2.29782606e-02, -4.20995037e-01]), 'targetState': array([37.99575376,  0.56806324, 92.        ]), 'previousTarget': array([37.10710333, -2.70463387, 83.82254926])}
episode index:19585
target thresh 86.59836262574579
target distance 16.336724870627023
model initialize at round 19585
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.50426637, 17.93703383, 73.        ]), 'distance': 18.925202903978473, 'localFrame': array([[-0.38993679,  0.63906862, -0.66297858],
       [-0.85364129, -0.52086135,  0.        ],
       [-0.34531992,  0.56594589,  0.74863837]]), 'currentState': array([14.8016356 ,  9.0628261 , 69.32177994, -0.38993679,  0.63906862,
       -0.66297858]), 'targetState': array([-1.50426637, 17.93703383, 73.        ]), 'previousTarget': array([-1.50426637, 17.93703383, 73.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6274955292959365
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.50426637, 17.93703383, 73.        ]), 'distance': 3.720204363008495, 'localFrame': array([[-0.98365528, -0.03430931,  0.17676303],
       [ 0.03485821, -0.99939227,  0.        ],
       [ 0.17665561,  0.00616164,  0.98425344]]), 'currentState': array([ 1.03326314e+00,  1.58959889e+01,  7.12013894e+01, -9.83655276e-01,
       -3.43093099e-02,  1.76763030e-01]), 'targetState': array([-1.50426637, 17.93703383, 73.        ]), 'previousTarget': array([-1.50426637, 17.93703383, 73.        ])}
episode index:19586
target thresh 86.59970272247726
target distance 38.0453272925134
model initialize at round 19586
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.8277881 ,   4.04890665,  61.56707608]), 'distance': 27.500000000000004, 'localFrame': array([[-0.17383847, -0.70917666, -0.68326324],
       [ 0.97124585, -0.23807875,  0.        ],
       [-0.16267046, -0.66361659,  0.73017213]]), 'currentState': array([ 0.43195602, 21.79727736, 67.1157791 , -0.17383847, -0.70917666,
       -0.68326324]), 'targetState': array([-36.50335942, -10.55958099,  57.        ]), 'previousTarget': array([-18.62586638,   5.18574499,  62.16889819])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6275008801837886
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-36.50335942, -10.55958099,  57.        ]), 'distance': 2.370693197227012, 'localFrame': array([[-0.5370522 , -0.60896002,  0.58373164],
       [ 0.75000004, -0.66143778,  0.        ],
       [ 0.38610216,  0.43779875,  0.81194665]]), 'currentState': array([-37.37883046,  -8.44666197,  57.62394702,  -0.5370522 ,
        -0.60896002,   0.58373164]), 'targetState': array([-36.50335942, -10.55958099,  57.        ]), 'previousTarget': array([-36.50335942, -10.55958099,  57.        ])}
episode index:19587
target thresh 86.60104268520577
target distance 43.0
model initialize at round 19587
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.75236148, 12.90036665, 66.33990716]), 'distance': 27.5, 'localFrame': array([[-0.22288343,  0.57968247,  0.78376732],
       [-0.9333841 , -0.35887897,  0.        ],
       [ 0.28127761, -0.73155595,  0.62105458]]), 'currentState': array([-3.1078936 ,  5.84387235, 39.81158996, -0.22288343,  0.57968247,
        0.78376732]), 'targetState': array([-5.72311734, 17.06592886, 82.        ]), 'previousTarget': array([-4.29319478, 12.21654759, 65.31789795])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6275081572036794
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.72311734, 17.06592886, 82.        ]), 'distance': 1.9535139189787651, 'localFrame': array([[-0.71858148, -0.61195456,  0.330382  ],
       [ 0.64836183, -0.76133234,  0.        ],
       [ 0.2515305 ,  0.21420708,  0.9438473 ]]), 'currentState': array([-6.29225352, 17.37126687, 80.15634348, -0.71858148, -0.61195456,
        0.330382  ]), 'targetState': array([-5.72311734, 17.06592886, 82.        ]), 'previousTarget': array([-5.72311734, 17.06592886, 82.        ])}
episode index:19588
target thresh 86.6023825139447
target distance 85.0
model initialize at round 19588
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.58475995, -23.38124065,  37.65223229]), 'distance': 27.5, 'localFrame': array([[ 0.71630581, -0.61438189, -0.33081852],
       [ 0.65103903,  0.75904425,  0.        ],
       [ 0.25110589, -0.21537577,  0.94369439]]), 'currentState': array([ -3.77546253, -34.31387895,  12.62131467,   0.71630581,
        -0.61438189,  -0.33081852]), 'targetState': array([ 7.23526821,  3.41334059, 99.        ]), 'previousTarget': array([ -1.29265497, -22.75275101,  38.99628192])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274761235032759
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 5.11096168,  1.67111878, 90.75554705]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.78544511,  0.15569571, -0.59902824],
       [-0.19444272,  0.98091387,  0.        ],
       [ 0.58759511,  0.11647668,  0.8007279 ]]), 'currentState': array([-1.61139739, -3.84213481, 64.66601038,  0.78544511,  0.15569571,
       -0.59902824]), 'targetState': array([ 7.23526821,  3.41334059, 99.        ]), 'previousTarget': array([ 5.02641587,  1.53946138, 91.51187431])}
episode index:19589
target thresh 86.60372220870744
target distance 40.0
model initialize at round 19589
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.01999264,  8.18851476, 48.6286447 ]), 'distance': 27.5, 'localFrame': array([[-0.28849431,  0.55450403, -0.78057435],
       [-0.88711711, -0.46154441,  0.        ],
       [-0.36026973,  0.69246086,  0.62506294]]), 'currentState': array([ 6.47716524, 12.54811371, 74.06578073, -0.28849431,  0.55450403,
       -0.78057435]), 'targetState': array([-8.10835598,  5.85273982, 35.        ]), 'previousTarget': array([-2.2818776 ,  7.97532235, 49.67000831])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274826188153586
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.10835598,  5.85273982, 35.        ]), 'distance': 2.2229749442320212, 'localFrame': array([[ 0.38337434, -0.81213671, -0.4398387 ],
       [ 0.90430653,  0.4268837 ,  0.        ],
       [ 0.18775997, -0.39774901,  0.89807679]]), 'currentState': array([-7.285494  ,  5.48534564, 37.0321263 ,  0.38337434, -0.81213671,
       -0.4398387 ]), 'targetState': array([-8.10835598,  5.85273982, 35.        ]), 'previousTarget': array([-8.10835598,  5.85273982, 35.        ])}
episode index:19590
target thresh 86.60506176950742
target distance 27.0
model initialize at round 19590
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.73146055, -3.88397818, 46.13155966]), 'distance': 27.5, 'localFrame': array([[ 0.64292882,  0.42542707,  0.63690999],
       [-0.55183034,  0.8339564 ,  0.        ],
       [-0.53115516, -0.35146626,  0.77093817]]), 'currentState': array([11.64375817, -8.90236326, 22.4719346 ,  0.64292882,  0.42542707,
        0.63690999]), 'targetState': array([25.76501839, -3.48766788, 48.        ]), 'previousTarget': array([23.99713991, -4.27059319, 44.70960862])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6274840570574983
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([25.76501839, -3.48766788, 48.        ]), 'distance': 4.174366350311097, 'localFrame': array([[ 0.75234645, -0.31416958,  0.57902702],
       [ 0.38533836,  0.92277535,  0.        ],
       [-0.53431186,  0.22312132,  0.81530835]]), 'currentState': array([24.07165736, -5.96946902, 45.10197897,  0.75234645, -0.31416958,
        0.57902702]), 'targetState': array([25.76501839, -3.48766788, 48.        ]), 'previousTarget': array([25.76501839, -3.48766788, 48.        ])}
episode index:19591
target thresh 86.60640119635801
target distance 83.0
model initialize at round 19591
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.33786923, -9.94003259, 29.53642662]), 'distance': 27.5, 'localFrame': array([[ 0.20045251,  0.67872792,  0.70650351],
       [-0.9590487 ,  0.28324121,  0.        ],
       [-0.20011091, -0.67757127,  0.70770954]]), 'currentState': array([ 25.94760499, -21.52136846,   4.85662786,   0.20045251,
         0.67872792,   0.70650351]), 'targetState': array([13.93308775, 17.02554157, 87.        ]), 'previousTarget': array([ 22.63798459, -11.23899115,  28.53872064])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6274780597978613
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([13.93308775, 17.02554157, 87.        ]), 'distance': 2.580845175766704, 'localFrame': array([[-0.44117495,  0.82626832,  0.35020755],
       [-0.88213185, -0.47100254,  0.        ],
       [ 0.16494865, -0.30892923,  0.93667213]]), 'currentState': array([14.50233213, 15.90488251, 84.74592681, -0.44117495,  0.82626832,
        0.35020755]), 'targetState': array([13.93308775, 17.02554157, 87.        ]), 'previousTarget': array([13.93308775, 17.02554157, 87.        ])}
episode index:19592
target thresh 86.60774048927262
target distance 29.364116325770695
model initialize at round 19592
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.08299578,  20.7258976 ,  67.1216201 ]), 'distance': 27.500000000000007, 'localFrame': array([[-0.48623711, -0.78053161,  0.39286625],
       [ 0.84877699, -0.528751  ,  0.        ],
       [ 0.20772842,  0.33345583,  0.91959562]]), 'currentState': array([-24.63090681,  -5.88819466,  61.81888526,  -0.48623711,
        -0.78053161,   0.39286625]), 'targetState': array([-29.8204691 ,  25.13443102,  68.        ]), 'previousTarget': array([-29.29130742,  22.13525162,  67.28503703])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.627473681040967
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([-29.8204691 ,  25.13443102,  68.        ]), 'distance': 3.053922165439599, 'localFrame': array([[ 0.34605509,  0.00185994,  0.93821235],
       [-0.00537461,  0.99998556,  0.        ],
       [-0.9381988 , -0.00504253,  0.34606009]]), 'currentState': array([-3.07418408e+01,  2.45278108e+01,  6.51522769e+01,  3.46055088e-01,
        1.85993837e-03,  9.38212352e-01]), 'targetState': array([-29.8204691 ,  25.13443102,  68.        ]), 'previousTarget': array([-29.8204691 ,  25.13443102,  68.        ])}
episode index:19593
target thresh 86.60907964826463
target distance 29.8789434753102
model initialize at round 19593
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.3038676 ,  1.29926483, 70.61015472]), 'distance': 27.499999999999996, 'localFrame': array([[ 1.73654215e-01,  5.36436571e-04, -9.84806542e-01],
       [-3.08909295e-03,  9.99995229e-01,  0.00000000e+00],
       [ 9.84801844e-01,  3.04215895e-03,  1.73655044e-01]]), 'currentState': array([-2.05946265e+01,  4.00578114e-01,  8.22509172e+01,  1.73654215e-01,
        5.36436571e-04, -9.84806542e-01]), 'targetState': array([ 9.88674262,  1.50077323, 68.        ]), 'previousTarget': array([ 4.24135607,  1.32274011, 71.02307158])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6274829825713937
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.88674262,  1.50077323, 68.        ]), 'distance': 2.8186479904167556, 'localFrame': array([[ 0.53635996, -0.27442544,  0.79812823],
       [ 0.45548727,  0.8902423 ,  0.        ],
       [-0.71052751,  0.36353725,  0.60248762]]), 'currentState': array([ 7.25351168,  1.29312895, 67.01625455,  0.53635996, -0.27442544,
        0.79812823]), 'targetState': array([ 9.88674262,  1.50077323, 68.        ]), 'previousTarget': array([ 9.88674262,  1.50077323, 68.        ])}
episode index:19594
target thresh 86.61041867334743
target distance 39.885689479186816
model initialize at round 19594
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.13027198, 21.11273202, 48.43883197]), 'distance': 27.499999999999996, 'localFrame': array([[-0.66921141, -0.57154879, -0.47485584],
       [ 0.64944033, -0.76041256,  0.        ],
       [-0.36108635, -0.30839053,  0.88006359]]), 'currentState': array([26.55854877, 21.60580486, 67.89516617, -0.66921141, -0.57154879,
       -0.47485584]), 'targetState': array([-12.28052781,  20.62010273,  29.        ]), 'previousTarget': array([ 7.96215802, 21.88248398, 48.79318291])}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.6274724620936996
{'scaleFactor': 20, 'timeStep': 87, 'trapCount': 46, 'trapConfig': [], 'currentTarget': array([-12.28052781,  20.62010273,  29.        ]), 'distance': 3.279468855181404, 'localFrame': array([[-0.46992266,  0.43446731, -0.76838197],
       [-0.67886416, -0.73426389,  0.        ],
       [-0.56419513,  0.52162698,  0.63999152]]), 'currentState': array([-9.76544391, 21.5732274 , 30.87638544, -0.46992266,  0.43446731,
       -0.76838197]), 'targetState': array([-12.28052781,  20.62010273,  29.        ]), 'previousTarget': array([-12.28052781,  20.62010273,  29.        ])}
episode index:19595
target thresh 86.61175756453441
target distance 37.657386070987144
model initialize at round 19595
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.31100257, 19.3242202 , 46.53456546]), 'distance': 27.500000000000004, 'localFrame': array([[-0.80012494, -0.13359194,  0.58476771],
       [ 0.16468419, -0.98634635,  0.        ],
       [ 0.5767835 ,  0.096302  ,  0.81120079]]), 'currentState': array([19.74555677, 26.93941557, 31.98234949, -0.80012494, -0.13359194,
        0.58476771]), 'targetState': array([-16.65760923,  14.37094481,  56.        ]), 'previousTarget': array([-0.93554239, 20.06118457, 45.5624282 ])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6274713156150837
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-16.65760923,  14.37094481,  56.        ]), 'distance': 2.762660455012183, 'localFrame': array([[-0.75441307,  0.10689635,  0.64763732],
       [-0.14029336, -0.99010998,  0.        ],
       [ 0.64123217, -0.09085922,  0.76194876]]), 'currentState': array([-16.8321054 ,  16.06262169,  53.82282908,  -0.75441307,
         0.10689635,   0.64763732]), 'targetState': array([-16.65760923,  14.37094481,  56.        ]), 'previousTarget': array([-16.65760923,  14.37094481,  56.        ])}
episode index:19596
target thresh 86.61309632183898
target distance 27.0
model initialize at round 19596
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.89382507, -3.47137958, 17.        ]), 'distance': 26.63687470638322, 'localFrame': array([[-0.66556212,  0.27404086, -0.69421083],
       [-0.38073302, -0.92468501,  0.        ],
       [-0.64192634,  0.26430899,  0.71977172]]), 'currentState': array([ 1.0292025 ,  2.07336302, 42.3711779 , -0.66556212,  0.27404086,
       -0.69421083]), 'targetState': array([-4.89382507, -3.47137958, 17.        ]), 'previousTarget': array([-4.67133586, -3.27993709, 17.88835456])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274392968716221
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-4.89382507, -3.47137958, 17.        ]), 'distance': 27.439322193908755, 'localFrame': array([[-0.75535008, -0.5808625 ,  0.30338921],
       [ 0.6095947 , -0.79271325,  0.        ],
       [ 0.24050065,  0.18494446,  0.95286672]]), 'currentState': array([-9.44315649, -4.50554455, 44.03979454, -0.75535008, -0.5808625 ,
        0.30338921]), 'targetState': array([-4.89382507, -3.47137958, 17.        ]), 'previousTarget': array([-4.89382507, -3.47137958, 17.        ])}
episode index:19597
target thresh 86.6144349452745
target distance 75.0
model initialize at round 19597
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.71194753, -14.77686653,  37.1616756 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.10168189, -0.3145393 ,  0.94378272],
       [ 0.9515161 ,  0.30759893,  0.        ],
       [-0.29030656,  0.89802446,  0.33056645]]), 'currentState': array([-15.99119769,  -8.04130838,  11.6416388 ,   0.10168189,
        -0.3145393 ,   0.94378272]), 'targetState': array([-38.1848003 , -27.40293827,  85.        ]), 'previousTarget': array([-23.05162481, -14.25882052,  35.49018091])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274072813957128
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-39.03719333, -26.00862674,  79.24953317]), 'distance': 27.5, 'localFrame': array([[ 0.61826478,  0.65738531, -0.43080531],
       [-0.72844926,  0.68509976,  0.        ],
       [ 0.29514462,  0.31381981,  0.90244489]]), 'currentState': array([-42.9582592 , -19.59469895,  52.79699506,   0.61826478,
         0.65738531,  -0.43080531]), 'targetState': array([-38.1848003 , -27.40293827,  85.        ]), 'previousTarget': array([-38.95966789, -26.46566765,  80.21365826])}
episode index:19598
target thresh 86.61577343485438
target distance 62.0
model initialize at round 19598
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.70091245,  0.06430753, 44.58298575]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.14274478, -0.78185704, -0.60689661],
       [ 0.98373923,  0.17960271,  0.        ],
       [ 0.10900027, -0.597028  ,  0.79478079]]), 'currentState': array([ 5.76867403,  2.63671315, 71.07028081,  0.14274478, -0.78185704,
       -0.60689661]), 'targetState': array([21.75195001, -3.2943392 , 10.        ]), 'previousTarget': array([11.92901909,  0.76774032, 45.6467257 ])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6274058297259882
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([21.75195001, -3.2943392 , 10.        ]), 'distance': 2.081950013673056, 'localFrame': array([[ 0.40787761,  0.30902424, -0.85915067],
       [-0.60389004,  0.79706764,  0.        ],
       [ 0.6848012 ,  0.51883253,  0.51172271]]), 'currentState': array([20.67790743, -4.49199213, 11.32158082,  0.40787761,  0.30902424,
       -0.85915067]), 'targetState': array([21.75195001, -3.2943392 , 10.        ]), 'previousTarget': array([21.75195001, -3.2943392 , 10.        ])}
episode index:19599
target thresh 86.61711179059199
target distance 22.0
model initialize at round 19599
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.9749688 , -0.49968532, 53.        ]), 'distance': 22.092000848544508, 'localFrame': array([[-0.54038334,  0.38294484, -0.74922566],
       [-0.57819157, -0.81590104,  0.        ],
       [-0.61129399,  0.43319596,  0.66231481]]), 'currentState': array([-0.88731038,  1.41023267, 74.21420273, -0.54038334,  0.38294484,
       -0.74922566]), 'targetState': array([ 4.9749688 , -0.49968532, 53.        ]), 'previousTarget': array([ 4.9749688 , -0.49968532, 53.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6274190429424162
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.9749688 , -0.49968532, 53.        ]), 'distance': 2.2177373958499604, 'localFrame': array([[ 0.30580389, -0.6261086 , -0.71726704],
       [ 0.89855058,  0.43886997,  0.        ],
       [ 0.31478697, -0.64450071,  0.69679839]]), 'currentState': array([ 4.45903688e+00, -4.53020466e-02,  5.51084851e+01,  3.05803891e-01,
       -6.26108597e-01, -7.17267039e-01]), 'targetState': array([ 4.9749688 , -0.49968532, 53.        ]), 'previousTarget': array([ 4.9749688 , -0.49968532, 53.        ])}
episode index:19600
target thresh 86.61845001250073
target distance 82.0
model initialize at round 19600
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.55873936, -12.95663004,  64.69978839]), 'distance': 27.499999999999996, 'localFrame': array([[-0.50183998, -0.68775823, -0.52454289],
       [ 0.807812  , -0.58944021,  0.        ],
       [-0.30918667, -0.42373205,  0.85138402]]), 'currentState': array([ -0.86142274, -15.03789872,  91.5225195 ,  -0.50183998,
        -0.68775823,  -0.52454289]), 'targetState': array([-17.96491182,  -8.78987732,  11.        ]), 'previousTarget': array([ -6.31029747, -12.28405266,  66.14586374])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6274160932030934
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-17.96491182,  -8.78987732,  11.        ]), 'distance': 3.264383221741473, 'localFrame': array([[ 0.16106584, -0.70571835, -0.6899416 ],
       [ 0.97493084,  0.22250811,  0.        ],
       [ 0.1535176 , -0.67264534,  0.72386503]]), 'currentState': array([-16.63050306, -10.62041168,  13.35046698,   0.16106584,
        -0.70571835,  -0.6899416 ]), 'targetState': array([-17.96491182,  -8.78987732,  11.        ]), 'previousTarget': array([-17.96491182,  -8.78987732,  11.        ])}
episode index:19601
target thresh 86.61978810059395
target distance 28.094938167385017
model initialize at round 19601
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.60819616, 36.40376343, 81.12255089]), 'distance': 27.5, 'localFrame': array([[ 0.67679327,  0.22948207, -0.69949184],
       [-0.32111537,  0.94704008,  0.        ],
       [ 0.66244682,  0.22461758,  0.71464058]]), 'currentState': array([23.87160446, 21.60633371, 74.6706733 ,  0.67679327,  0.22948207,
       -0.69949184]), 'targetState': array([-4.87029255, 40.70970708, 83.        ]), 'previousTarget': array([ 1.33171096, 36.25572896, 81.45473856])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273840854440279
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 83, 'trapConfig': [], 'currentTarget': array([-4.87029255, 40.70970708, 83.        ]), 'distance': 14.930283615181777, 'localFrame': array([[ 0.01396901,  0.0598959 ,  0.99810688],
       [-0.9738653 ,  0.22712637,  0.        ],
       [-0.22669639, -0.97202165,  0.06150327]]), 'currentState': array([8.29953027e+00, 3.78710377e+01, 7.65648550e+01, 1.39690147e-02,
       5.98959017e-02, 9.98106882e-01]), 'targetState': array([-4.87029255, 40.70970708, 83.        ]), 'previousTarget': array([-4.87029255, 40.70970708, 83.        ])}
episode index:19602
target thresh 86.62112605488507
target distance 43.0
model initialize at round 19602
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.0044338 , 21.00059492, 40.85635287]), 'distance': 27.5, 'localFrame': array([[-0.63815229, -0.44096191, -0.63112142],
       [ 0.56848133, -0.82269616,  0.        ],
       [-0.51922117, -0.35878075,  0.77568406]]), 'currentState': array([19.92382826, 29.33693427, 63.06008914, -0.63815229, -0.44096191,
       -0.63112142]), 'targetState': array([-6.44339953, 13.54557502, 21.        ]), 'previousTarget': array([ 6.67970983, 21.94605796, 41.93833963])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6273868998351704
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-6.44339953, 13.54557502, 21.        ]), 'distance': 3.7692890458142987, 'localFrame': array([[-0.09471159, -0.60258727, -0.79241296],
       [ 0.98787227, -0.15526872,  0.        ],
       [-0.12303695, -0.78280279,  0.609985  ]]), 'currentState': array([-3.46617609, 14.92055427, 22.85825524, -0.09471159, -0.60258727,
       -0.79241296]), 'targetState': array([-6.44339953, 13.54557502, 21.        ]), 'previousTarget': array([-6.44339953, 13.54557502, 21.        ])}
episode index:19603
target thresh 86.62246387538744
target distance 48.0
model initialize at round 19603
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.18596637, -3.89796921, 72.91832369]), 'distance': 27.500000000000004, 'localFrame': array([[-0.96566064,  0.05228754,  0.25449076],
       [-0.05406771, -0.99853727,  0.        ],
       [ 0.25411851, -0.01375973,  0.96707521]]), 'currentState': array([26.87324297, -5.88944454, 49.14980491, -0.96566064,  0.05228754,
        0.25449076]), 'targetState': array([-0.6816443 , -1.88025558, 97.        ]), 'previousTarget': array([14.26732798, -3.5331146 , 72.44951367])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6273801385497065
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-0.6816443 , -1.88025558, 97.        ]), 'distance': 2.6819856357031187, 'localFrame': array([[-0.58108217,  0.65962315,  0.47669782],
       [-0.75036712, -0.66102131,  0.        ],
       [ 0.31510742, -0.35769837,  0.87906723]]), 'currentState': array([-0.52718866, -3.96201429, 95.31611429, -0.58108217,  0.65962315,
        0.47669782]), 'targetState': array([-0.6816443 , -1.88025558, 97.        ]), 'previousTarget': array([-0.6816443 , -1.88025558, 97.        ])}
episode index:19604
target thresh 86.62380156211444
target distance 68.0
model initialize at round 19604
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.33416332, -29.68684732,  43.11612801]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.59880414,  0.75892734,  0.25585718],
       [-0.78505832,  0.61942185,  0.        ],
       [-0.15848353, -0.20086281,  0.96671459]]), 'currentState': array([-17.44926331, -37.48122174,  19.69131041,   0.59880414,
         0.75892734,   0.25585718]), 'targetState': array([ 17.36217122, -15.0849266 ,  87.        ]), 'previousTarget': array([ -5.67617194, -30.92606923,  42.31299894])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273481375224916
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([ -2.70632901, -30.20384179,  44.51824759]), 'distance': 27.500000000000004, 'localFrame': array([[-0.38897749, -0.55727106,  0.733584  ],
       [ 0.82000007, -0.57236342,  0.        ],
       [ 0.41987664,  0.60153893,  0.67959879]]), 'currentState': array([-13.88799947, -38.62772626,  20.84846905,  -0.38897749,
        -0.55727106,   0.733584  ]), 'targetState': array([ 17.36217122, -15.0849266 ,  87.        ]), 'previousTarget': array([ -2.96482582, -29.81461248,  43.07800667])}
episode index:19605
target thresh 86.62513911507948
target distance 42.0
model initialize at round 19605
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.82518377, 37.82057835, 45.13594584]), 'distance': 27.5, 'localFrame': array([[-0.8260886 ,  0.08178234, -0.55757446],
       [-0.09851787, -0.99513528,  0.        ],
       [-0.55486201,  0.05493105,  0.83012693]]), 'currentState': array([ 3.59491127, 25.80700271, 69.31379211, -0.8260886 ,  0.08178234,
       -0.55757446]), 'targetState': array([12.53211718, 46.33514906, 28.        ]), 'previousTarget': array([ 9.46547665, 38.10038432, 45.44096879])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6273550228931345
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.53211718, 46.33514906, 28.        ]), 'distance': 2.3566739811800894, 'localFrame': array([[ 0.30157848,  0.19269582, -0.93376589],
       [-0.53843031,  0.84267004,  0.        ],
       [ 0.78685654,  0.50276786,  0.35788442]]), 'currentState': array([13.5146577 , 45.41180802, 29.93286516,  0.30157848,  0.19269582,
       -0.93376589]), 'targetState': array([12.53211718, 46.33514906, 28.        ]), 'previousTarget': array([12.53211718, 46.33514906, 28.        ])}
episode index:19606
target thresh 86.62647653429589
target distance 44.0
model initialize at round 19606
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.63867177, -2.82785976, 27.43563296]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.78197903, -0.57638931,  0.23724281],
       [ 0.59332862,  0.80496034,  0.        ],
       [-0.19097105,  0.14076295,  0.97145039]]), 'currentState': array([ 1.24562657, -1.56474102,  0.        ,  0.78197903, -0.57638931,
        0.23724281]), 'targetState': array([ 3.47972807, -3.59047247, 44.        ]), 'previousTarget': array([ 2.63867177, -2.82785976, 27.43563296])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273230264111183
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 3.47972807, -3.59047247, 44.        ]), 'distance': 21.29767520150494, 'localFrame': array([[-0.63510928, -0.20046878, -0.74595474],
       [ 0.30100569, -0.95362234,  0.        ],
       [-0.71135911, -0.22453662,  0.66599664]]), 'currentState': array([ -0.050741  , -12.90551691,  25.17563536,  -0.63510928,
        -0.20046878,  -0.74595474]), 'targetState': array([ 3.47972807, -3.59047247, 44.        ]), 'previousTarget': array([ 3.47972807, -3.59047247, 44.        ])}
episode index:19607
target thresh 86.62781381977707
target distance 31.225734898698185
model initialize at round 19607
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.55684648, -11.20786269,  30.95282603]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.68856504,  0.45370178, -0.56571449],
       [-0.55020753,  0.83502795,  0.        ],
       [ 0.47238741,  0.31126037,  0.82460119]]), 'currentState': array([-20.67747444,  -7.41972574,  30.51489642,   0.68856504,
         0.45370178,  -0.56571449]), 'targetState': array([  9.49053974, -11.6159225 ,  31.        ]), 'previousTarget': array([  5.51772877, -11.09506472,  31.12722874])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6273315071973908
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.49053974, -11.6159225 ,  31.        ]), 'distance': 3.9937230401941055, 'localFrame': array([[ 0.33911106,  0.66202098,  0.66838006],
       [-0.89002856,  0.45590477,  0.        ],
       [-0.30471766, -0.59487735,  0.74381993]]), 'currentState': array([ 11.76617844, -13.43722528,  28.26977137,   0.33911106,
         0.66202098,   0.66838006]), 'targetState': array([  9.49053974, -11.6159225 ,  31.        ]), 'previousTarget': array([  9.49053974, -11.6159225 ,  31.        ])}
episode index:19608
target thresh 86.62915097153639
target distance 25.529685626552904
model initialize at round 19608
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.51350029, -11.44300875,   4.04203352]), 'distance': 27.5, 'localFrame': array([[-0.60928802, -0.39684048, -0.68650254],
       [ 0.54576472, -0.83793847,  0.        ],
       [-0.57524689, -0.37466886,  0.7271274 ]]), 'currentState': array([-0.72861292, -3.87975773, 17.45457025, -0.60928802, -0.39684048,
       -0.68650254]), 'targetState': array([-25.28368119, -12.03060536,   3.        ]), 'previousTarget': array([-22.26841695, -11.08123101,   4.88973059])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6273353888939487
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-25.28368119, -12.03060536,   3.        ]), 'distance': 2.6784995416834763, 'localFrame': array([[-0.63684739, -0.77033982,  0.03165379],
       [ 0.77072604, -0.63716668,  0.        ],
       [ 0.02016874,  0.0243964 ,  0.99949889]]), 'currentState': array([-23.84823247, -14.22763403,   3.53564145,  -0.63684739,
        -0.77033982,   0.03165379]), 'targetState': array([-25.28368119, -12.03060536,   3.        ]), 'previousTarget': array([-25.28368119, -12.03060536,   3.        ])}
episode index:19609
target thresh 86.63048798958722
target distance 30.0
model initialize at round 19609
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.54236205,   5.66351937,  70.2805831 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.91643669, -0.16322248,  0.36537956],
       [ 0.17534615, -0.98450684,  0.        ],
       [ 0.35971867,  0.0640679 ,  0.93085862]]), 'currentState': array([-14.51435225,  16.60722251,  89.76360811,  -0.91643669,
        -0.16322248,   0.36537956]), 'targetState': array([-38.99984167,  -0.11113059,  60.        ]), 'previousTarget': array([-29.39037137,   6.04101765,  70.95458664])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.627342273510239
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-38.99984167,  -0.11113059,  60.        ]), 'distance': 2.773294008567247, 'localFrame': array([[-0.92931573, -0.08986551, -0.35818495],
       [ 0.09625175, -0.99535702,  0.        ],
       [-0.35652191, -0.03447593,  0.93365065]]), 'currentState': array([-40.69347233,  -0.87720278,  62.05813222,  -0.92931573,
        -0.08986551,  -0.35818495]), 'targetState': array([-38.99984167,  -0.11113059,  60.        ]), 'previousTarget': array([-38.99984167,  -0.11113059,  60.        ])}
episode index:19610
target thresh 86.63182487394293
target distance 43.722707108606635
model initialize at round 19610
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.12829964, 22.54936858, 75.38941354]), 'distance': 27.499999999999996, 'localFrame': array([[-0.73748764, -0.33415085, -0.58690305],
       [ 0.41270656, -0.91086404,  0.        ],
       [-0.53458888, -0.24221874,  0.80965722]]), 'currentState': array([-2.08920953, 43.24805048, 93.38856352, -0.73748764, -0.33415085,
       -0.58690305]), 'targetState': array([ 1.98407193,  0.2519099 , 56.        ]), 'previousTarget': array([ 0.1354344 , 23.48851538, 76.72670413])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273472522498467
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.98407193,  0.2519099 , 56.        ]), 'distance': 3.5867835761433886, 'localFrame': array([[ 0.53537453, -0.5712679 , -0.62211502],
       [ 0.72965772,  0.68381256,  0.        ],
       [ 0.42541006, -0.45393103,  0.78292586]]), 'currentState': array([ 1.64311047,  2.80265872, 58.49848798,  0.53537453, -0.5712679 ,
       -0.62211502]), 'targetState': array([ 1.98407193,  0.2519099 , 56.        ]), 'previousTarget': array([ 1.98407193,  0.2519099 , 56.        ])}
episode index:19611
target thresh 86.63316162461689
target distance 26.9971291232629
model initialize at round 19611
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.82454101, -18.83164925,  23.0069462 ]), 'distance': 27.5, 'localFrame': array([[-0.83942028, -0.26391604, -0.475102  ],
       [ 0.2999282 , -0.95396178,  0.        ],
       [-0.45322915, -0.14249649,  0.87993073]]), 'currentState': array([ 2.45314995,  8.44295914, 24.27283695, -0.83942028, -0.26391604,
       -0.475102  ]), 'targetState': array([ -0.84252636, -18.98131053,  23.        ]), 'previousTarget': array([ -0.83943896, -18.96442009,  23.00125128])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6273518608201275
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.84252636, -18.98131053,  23.        ]), 'distance': 1.866976714158053, 'localFrame': array([[ 0.04273733, -0.64135167,  0.76605584],
       [ 0.99778717,  0.06648889,  0.        ],
       [-0.0509342 ,  0.76436069,  0.64277403]]), 'currentState': array([ -0.97522366, -19.52905321,  21.22012039,   0.04273733,
        -0.64135167,   0.76605584]), 'targetState': array([ -0.84252636, -18.98131053,  23.        ]), 'previousTarget': array([ -0.84252636, -18.98131053,  23.        ])}
episode index:19612
target thresh 86.63449824162245
target distance 70.63056878960307
model initialize at round 19612
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.23935497,  -9.00773808,  48.66978777]), 'distance': 27.5, 'localFrame': array([[ 0.82214472,  0.21109754, -0.52869262],
       [-0.24869724,  0.96858127,  0.        ],
       [ 0.51208177,  0.1314844 ,  0.84881335]]), 'currentState': array([-42.90797922, -18.82749412,  49.64144626,   0.82214472,
         0.21109754,  -0.52869262]), 'targetState': array([26.87198191,  7.86743849, 47.        ]), 'previousTarget': array([-18.17970735,  -9.73417474,  49.55139892])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6273529698176062
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.87198191,  7.86743849, 47.        ]), 'distance': 2.868874938548454, 'localFrame': array([[ 0.80954096, -0.15383119,  0.56655044],
       [ 0.18668221,  0.98242035,  0.        ],
       [-0.55659069,  0.10576489,  0.82402706]]), 'currentState': array([24.12897333,  7.0289402 , 47.05716625,  0.80954096, -0.15383119,
        0.56655044]), 'targetState': array([26.87198191,  7.86743849, 47.        ]), 'previousTarget': array([26.87198191,  7.86743849, 47.        ])}
episode index:19613
target thresh 86.63583472497302
target distance 48.0
model initialize at round 19613
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.58974272,   2.25789022,  46.99935239]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.16075287, -0.09666663,  0.9822495 ],
       [ 0.51533794,  0.85698705,  0.        ],
       [-0.8417751 ,  0.50619044,  0.18757911]]), 'currentState': array([-10.88547573, -15.67636522,  28.54834473,   0.16075287,
        -0.09666663,   0.9822495 ]), 'targetState': array([-35.3166219 ,  29.47433151,  75.        ]), 'previousTarget': array([-21.0139151 ,   1.71539628,  45.78703006])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6273506150426649
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-35.3166219 ,  29.47433151,  75.        ]), 'distance': 3.797928432204704, 'localFrame': array([[ 0.17915561,  0.80778707,  0.56158999],
       [-0.97627723,  0.21652431,  0.        ],
       [-0.12159789, -0.54826752,  0.82741566]]), 'currentState': array([-33.5012032 ,  27.93725332,  72.03927273,   0.17915561,
         0.80778707,   0.56158999]), 'targetState': array([-35.3166219 ,  29.47433151,  75.        ]), 'previousTarget': array([-35.3166219 ,  29.47433151,  75.        ])}
episode index:19614
target thresh 86.63717107468192
target distance 30.0
model initialize at round 19614
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.7495009 ,  34.11769053,  75.78673868]), 'distance': 27.499999999999996, 'localFrame': array([[-0.5998156 ,  0.44273103, -0.66649118],
       [-0.59386097, -0.80456767,  0.        ],
       [-0.53623726,  0.3958031 ,  0.74551292]]), 'currentState': array([-0.65350477, 34.95670927, 95.55765113, -0.5998156 ,  0.44273103,
       -0.66649118]), 'targetState': array([-28.23628816,  33.74480747,  67.        ]), 'previousTarget': array([-18.53642506,  34.16795298,  77.13850029])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6273595000783262
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.23628816,  33.74480747,  67.        ]), 'distance': 2.680415096337562, 'localFrame': array([[-0.59816851, -0.641999  , -0.47961622],
       [ 0.73164083, -0.68169032,  0.        ],
       [-0.32694973, -0.35090681,  0.87747837]]), 'currentState': array([-28.26404924,  33.01854285,  69.57999886,  -0.59816851,
        -0.641999  ,  -0.47961622]), 'targetState': array([-28.23628816,  33.74480747,  67.        ]), 'previousTarget': array([-28.23628816,  33.74480747,  67.        ])}
episode index:19615
target thresh 86.63850729076255
target distance 41.0
model initialize at round 19615
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.40933182, 12.64969008, 31.19614635]), 'distance': 27.5, 'localFrame': array([[ 0.10325668, -0.88683135, -0.45040894],
       [ 0.9932898 ,  0.11565199,  0.        ],
       [ 0.05209069, -0.4473866 ,  0.89282237]]), 'currentState': array([24.24530354, 25.61437114, 52.89289178,  0.10325668, -0.88683135,
       -0.45040894]), 'targetState': array([ 3.82223039,  1.1792179 , 12.        ]), 'previousTarget': array([13.17233677, 13.59247148, 31.49596946])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.627358986754553
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ 3.82223039,  1.1792179 , 12.        ]), 'distance': 2.734629397901648, 'localFrame': array([[ 0.01890587, -0.85673691,  0.51540705],
       [ 0.99975661,  0.02206193,  0.        ],
       [-0.01137087,  0.51528161,  0.85694549]]), 'currentState': array([ 5.16071296,  3.17400218, 10.69328732,  0.01890587, -0.85673691,
        0.51540705]), 'targetState': array([ 3.82223039,  1.1792179 , 12.        ]), 'previousTarget': array([ 3.82223039,  1.1792179 , 12.        ])}
episode index:19616
target thresh 86.63984337322823
target distance 59.0
model initialize at round 19616
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.43278795, -1.76168064, 38.60569602]), 'distance': 27.5, 'localFrame': array([[ 0.57645922,  0.76151436, -0.29629487],
       [-0.79731666,  0.60356122,  0.        ],
       [ 0.17883209,  0.23624083,  0.95509651]]), 'currentState': array([ 1.98943807, -6.7533496 , 64.29699968,  0.57645922,  0.76151436,
       -0.29629487]), 'targetState': array([21.47717298,  4.76770813,  5.        ]), 'previousTarget': array([ 9.62359235, -2.48566596, 38.58895969])}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.627354898344278
{'scaleFactor': 20, 'timeStep': 61, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([21.47717298,  4.76770813,  5.        ]), 'distance': 3.2687855237881864, 'localFrame': array([[ 0.70773611,  0.34666107, -0.61557754],
       [-0.43988261,  0.89805528,  0.        ],
       [ 0.55282266,  0.27078185,  0.78807633]]), 'currentState': array([19.21322784,  5.5798391 ,  7.21358408,  0.70773611,  0.34666107,
       -0.61557754]), 'targetState': array([21.47717298,  4.76770813,  5.        ]), 'previousTarget': array([21.47717298,  4.76770813,  5.        ])}
episode index:19617
target thresh 86.64117932209234
target distance 36.38757969998824
model initialize at round 19617
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.96434903, -3.34844547, 82.14249511]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.45792316,  0.63026454,  0.62695533],
       [-0.8090114 ,  0.58779295,  0.        ],
       [-0.36851993, -0.50721401,  0.7790552 ]]), 'currentState': array([ 31.65375986, -29.89577291,  76.71121241,   0.45792316,
         0.63026454,   0.62695533]), 'targetState': array([25.36056517,  5.73077085, 84.        ]), 'previousTarget': array([26.64429614, -4.01769229, 81.85674931])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273229198093436
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([25.36056517,  5.73077085, 84.        ]), 'distance': 10.42432450129708, 'localFrame': array([[-0.09400706,  0.9769598 ,  0.19160434],
       [-0.99540237, -0.09578168,  0.        ],
       [ 0.01835219, -0.19072342,  0.98147225]]), 'currentState': array([29.50692714, -3.09961151, 87.67404015, -0.09400706,  0.9769598 ,
        0.19160434]), 'targetState': array([25.36056517,  5.73077085, 84.        ]), 'previousTarget': array([25.36056517,  5.73077085, 84.        ])}
episode index:19618
target thresh 86.64251513736826
target distance 59.0
model initialize at round 19618
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.90703234,  8.71401079, 69.6978018 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.08477797,  0.70838491, -0.70071643],
       [-0.99291463, -0.11882987,  0.        ],
       [-0.08326604,  0.69575159,  0.7134399 ]]), 'currentState': array([-2.79365578e-01,  5.79385158e+00,  9.56455577e+01, -8.47779716e-02,
        7.08384913e-01, -7.00716427e-01]), 'targetState': array([-19.44659776,  12.28127989,  38.        ]), 'previousTarget': array([-8.17494791,  8.16951706, 71.14228968])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6273154178233485
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 34, 'trapConfig': [], 'currentTarget': array([-19.44659776,  12.28127989,  38.        ]), 'distance': 2.805225325586749, 'localFrame': array([[ 0.23619691, -0.0420475 , -0.97079505],
       [ 0.17526337,  0.98452158,  0.        ],
       [ 0.95576868, -0.17014481,  0.23991035]]), 'currentState': array([-19.65479651,  11.17256771,  40.56840412,   0.23619691,
        -0.0420475 ,  -0.97079505]), 'targetState': array([-19.44659776,  12.28127989,  38.        ]), 'previousTarget': array([-19.44659776,  12.28127989,  38.        ])}
episode index:19619
target thresh 86.64385081906933
target distance 41.43792244177494
model initialize at round 19619
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([24.3280853 , -3.93523324, 23.53183592]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.70176865, -0.26667959,  0.66060787],
       [ 0.35522649,  0.93478026,  0.        ],
       [-0.6175232 ,  0.23466542,  0.75073113]]), 'currentState': array([-2.1221199 ,  2.12052059, 28.        ,  0.70176865, -0.26667959,
        0.66060787]), 'targetState': array([39.31580254, -7.3666594 , 21.        ]), 'previousTarget': array([24.3280853 , -3.93523324, 23.53183592])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272834445604625
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 45, 'trapConfig': [], 'currentTarget': array([39.31580254, -7.3666594 , 21.        ]), 'distance': 16.939689054472414, 'localFrame': array([[-0.23029464, -0.9313707 ,  0.28198051],
       [ 0.97076418, -0.24003524,  0.        ],
       [ 0.06768526,  0.27373658,  0.95942013]]), 'currentState': array([22.73849982, -9.42793875, 18.18979941, -0.23029464, -0.9313707 ,
        0.28198051]), 'targetState': array([39.31580254, -7.3666594 , 21.        ]), 'previousTarget': array([39.31580254, -7.3666594 , 21.        ])}
episode index:19620
target thresh 86.64518636720891
target distance 56.0
model initialize at round 19620
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.47322293,  1.46313885, 71.79413102]), 'distance': 27.5, 'localFrame': array([[ 0.02362844, -0.60343341, -0.79706325],
       [ 0.99923426,  0.03912668,  0.        ],
       [ 0.03118644, -0.7964529 ,  0.60389584]]), 'currentState': array([ 2.78915023e+00,  1.46111451e+01,  9.46924925e+01,  2.36284368e-02,
       -6.03433408e-01, -7.97063247e-01]), 'targetState': array([ 21.14247602, -16.79272782,  40.        ]), 'previousTarget': array([10.81782257,  2.29662451, 73.01941113])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6272855692388178
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 21.14247602, -16.79272782,  40.        ]), 'distance': 1.787474022399917, 'localFrame': array([[-0.16021973, -0.93349726,  0.32079979],
       [ 0.98558854, -0.16916035,  0.        ],
       [ 0.05426661,  0.3161766 ,  0.94714703]]), 'currentState': array([ 21.39876023, -15.16310642,  39.3117298 ,  -0.16021973,
        -0.93349726,   0.32079979]), 'targetState': array([ 21.14247602, -16.79272782,  40.        ]), 'previousTarget': array([ 21.14247602, -16.79272782,  40.        ])}
episode index:19621
target thresh 86.64652178180033
target distance 31.730883167813708
model initialize at round 19621
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.47619995, 35.87798787,  8.0965381 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.67318361, -0.14595895, -0.72492746],
       [ 0.21189548, -0.97729233,  0.        ],
       [-0.70846605, -0.15360885,  0.68882522]]), 'currentState': array([17.55763118, 10.82014728, 14.87018052, -0.67318361, -0.14595895,
       -0.72492746]), 'targetState': array([ 5.66536858, 43.63374381,  6.        ]), 'previousTarget': array([ 8.60269902, 36.37292159,  8.28825091])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6272944544197526
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.66536858, 43.63374381,  6.        ]), 'distance': 2.75294656560393, 'localFrame': array([[ 0.15063937,  0.74686547,  0.647688  ],
       [-0.98025976,  0.19771394,  0.        ],
       [-0.12805694, -0.63490248,  0.76190567]]), 'currentState': array([ 4.91484307, 41.0692239 ,  5.33768293,  0.15063937,  0.74686547,
        0.647688  ]), 'targetState': array([ 5.66536858, 43.63374381,  6.        ]), 'previousTarget': array([ 5.66536858, 43.63374381,  6.        ])}
episode index:19622
target thresh 86.64785706285699
target distance 36.0
model initialize at round 19622
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.27212211, -11.49272455,  75.91593451]), 'distance': 27.500000000000004, 'localFrame': array([[-0.05805894,  0.90125644,  0.42937861],
       [-0.99793147, -0.06428675,  0.        ],
       [ 0.02760336, -0.42849043,  0.90312458]]), 'currentState': array([  6.63101442, -19.65917202,  51.5954283 ,  -0.05805894,
         0.90125644,   0.42937861]), 'targetState': array([-7.78547446, -7.77086787, 87.        ]), 'previousTarget': array([ -3.35300885, -12.14582234,  75.28291757])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6273033386950989
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.78547446, -7.77086787, 87.        ]), 'distance': 2.7450892636582114, 'localFrame': array([[ 0.38455896,  0.36204483,  0.84913953],
       [-0.6854721 ,  0.7280989 ,  0.        ],
       [-0.61825756, -0.58206146,  0.52816858]]), 'currentState': array([-5.53801267, -8.1865055 , 85.47958035,  0.38455896,  0.36204483,
        0.84913953]), 'targetState': array([-7.78547446, -7.77086787, 87.        ]), 'previousTarget': array([-7.78547446, -7.77086787, 87.        ])}
episode index:19623
target thresh 86.64919221039221
target distance 53.0
model initialize at round 19623
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.10596021,  1.81259603, 58.15110578]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.45853185, -0.55882534, -0.69098681],
       [ 0.77306761,  0.63432363,  0.        ],
       [ 0.43830927, -0.53417952,  0.72286736]]), 'currentState': array([-12.05690605,  19.87841816,  76.34051885,   0.45853185,
        -0.55882534,  -0.69098681]), 'targetState': array([ 16.03013119, -31.11325914,  25.        ]), 'previousTarget': array([-2.32460343,  2.75984887, 59.6194169 ])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6273022024704985
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.03013119, -31.11325914,  25.        ]), 'distance': 2.7060021034398343, 'localFrame': array([[ 0.30307526, -0.92787401,  0.21724456],
       [ 0.95057643,  0.31049065,  0.        ],
       [-0.0674524 ,  0.20650756,  0.97611721]]), 'currentState': array([ 14.53805037, -29.15366151,  23.8792327 ,   0.30307526,
        -0.92787401,   0.21724456]), 'targetState': array([ 16.03013119, -31.11325914,  25.        ]), 'previousTarget': array([ 16.03013119, -31.11325914,  25.        ])}
episode index:19624
target thresh 86.65052722441936
target distance 9.524469259821812
model initialize at round 19624
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.97146047,   0.86093751,  28.        ]), 'distance': 9.436363895225984, 'localFrame': array([[-0.01650788, -0.59563179,  0.80308796],
       [ 0.99961616, -0.02770427,  0.        ],
       [ 0.02224896,  0.8027797 ,  0.5958605 ]]), 'currentState': array([-4.22110934e+00,  4.34719644e+00,  2.85677301e+01, -1.65078790e-02,
       -5.95631787e-01,  8.03087956e-01]), 'targetState': array([-12.97146047,   0.86093751,  28.        ]), 'previousTarget': array([-12.97146047,   0.86093751,  28.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6273158603584877
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-12.97146047,   0.86093751,  28.        ]), 'distance': 3.059275925028296, 'localFrame': array([[ 0.11714752,  0.9618185 ,  0.24734923],
       [-0.99266415,  0.12090446,  0.        ],
       [-0.02990562, -0.24553471,  0.9689264 ]]), 'currentState': array([-9.97368439,  0.54016312, 27.48075863,  0.11714752,  0.9618185 ,
        0.24734923]), 'targetState': array([-12.97146047,   0.86093751,  28.        ]), 'previousTarget': array([-12.97146047,   0.86093751,  28.        ])}
episode index:19625
target thresh 86.65186210495179
target distance 30.0
model initialize at round 19625
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.67077998, -6.74589899, 88.05180277]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.47247609,  0.28210975,  0.83497331],
       [-0.5126561 ,  0.85859404,  0.        ],
       [-0.71690311, -0.42805416,  0.55029044]]), 'currentState': array([ -2.43656912, -24.5040843 ,  67.12863755,   0.47247609,
         0.28210975,   0.83497331]), 'targetState': array([ 0.,  0., 96.]), 'previousTarget': array([-0.9404567 , -7.93386474, 86.78145002])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.627325154764269
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([1.11022302e-16, 0.00000000e+00, 9.60000000e+01]), 'distance': 3.19617209546269, 'localFrame': array([[ 0.07585146,  0.96842902,  0.23746955],
       [-0.9969467 ,  0.07808508,  0.        ],
       [-0.01854283, -0.23674448,  0.97139498]]), 'currentState': array([ 8.72869130e-01, -2.64162071e+00,  9.44266419e+01,  7.58514560e-02,
        9.68429022e-01,  2.37469549e-01]), 'targetState': array([ 0.,  0., 96.]), 'previousTarget': array([ 0.,  0., 96.])}
episode index:19626
target thresh 86.65319685200282
target distance 41.0
model initialize at round 19626
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.21723328, -4.32389072, 53.42162352]), 'distance': 27.499999999999996, 'localFrame': array([[-0.00893473,  0.14916988, -0.98877122],
       [-0.99821103, -0.0597892 ,  0.        ],
       [-0.05911784,  0.98700233,  0.14943722]]), 'currentState': array([ 3.58231901e+00, -1.27252156e+01,  7.94998541e+01, -8.93473181e-03,
        1.49169879e-01, -9.88771216e-01]), 'targetState': array([ 0.,  0., 40.]), 'previousTarget': array([ 1.10493294, -4.9927402 , 54.97537891])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6273265983912019
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 40.]), 'distance': 2.1629578202173736, 'localFrame': array([[ 0.03219537,  0.88823659, -0.45825671],
       [-0.99934375,  0.0362226 ,  0.        ],
       [ 0.01659925,  0.45795598,  0.88881988]]), 'currentState': array([ 9.23603606e-01, -1.14877623e+00,  4.15829264e+01,  3.21953709e-02,
        8.88236592e-01, -4.58256712e-01]), 'targetState': array([ 0.,  0., 40.]), 'previousTarget': array([ 0.,  0., 40.])}
episode index:19627
target thresh 86.65453146558583
target distance 36.18764710796689
model initialize at round 19627
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.75188007, -8.86960854, 60.94601565]), 'distance': 27.500000000000004, 'localFrame': array([[-0.34013455,  0.57401052,  0.74486268],
       [-0.86030457, -0.50978039,  0.        ],
       [ 0.37971639, -0.64080877,  0.6672178 ]]), 'currentState': array([ -0.07263256, -35.74869848,  66.71690353,  -0.34013455,
         0.57401052,   0.74486268]), 'targetState': array([-0.98093083,  0.19435716, 59.        ]), 'previousTarget': array([-0.56011779, -8.89193686, 60.50652967])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.627334666026336
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.98093083,  0.19435716, 59.        ]), 'distance': 1.524175360500539, 'localFrame': array([[-0.66380853,  0.69514152,  0.27592845],
       [-0.72321816, -0.69061964,  0.        ],
       [ 0.19056161, -0.19955647,  0.96117818]]), 'currentState': array([-1.23787127, -1.21701076, 58.48508959, -0.66380853,  0.69514152,
        0.27592845]), 'targetState': array([-0.98093083,  0.19435716, 59.        ]), 'previousTarget': array([-0.98093083,  0.19435716, 59.        ])}
episode index:19628
target thresh 86.65586594571415
target distance 21.71771889945302
model initialize at round 19628
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.50096159, -10.2439479 ,  53.68387693]), 'distance': 27.500000000000004, 'localFrame': array([[-0.11309275,  0.69282223,  0.71218494],
       [-0.98693766, -0.16110265,  0.        ],
       [ 0.11473488, -0.70288214,  0.70199189]]), 'currentState': array([ -5.71311021, -30.19018003,  43.2367925 ,  -0.11309275,
         0.69282223,   0.71218494]), 'targetState': array([-21.97869335,  -9.64038581,  54.        ]), 'previousTarget': array([-21.015345  , -10.98139764,  53.25903167])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6273465221417069
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.97869335,  -9.64038581,  54.        ]), 'distance': 4.141467824539713, 'localFrame': array([[ 0.2263227 ,  0.88431798,  0.40836227],
       [-0.96877592,  0.24793794,  0.        ],
       [-0.1012485 , -0.39561153,  0.91281995]]), 'currentState': array([-20.48411143, -12.61454252,  51.53578157,   0.2263227 ,
         0.88431798,   0.40836227]), 'targetState': array([-21.97869335,  -9.64038581,  54.        ]), 'previousTarget': array([-21.97869335,  -9.64038581,  54.        ])}
episode index:19629
target thresh 86.65720029240114
target distance 38.92268948380278
model initialize at round 19629
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.20278306, 12.57134898, 64.24524341]), 'distance': 27.5, 'localFrame': array([[ 0.18103556,  0.55433098,  0.81236894],
       [-0.95059052,  0.31044753,  0.        ],
       [-0.25219793, -0.77223021,  0.58314381]]), 'currentState': array([11.8452947 ,  3.55566664, 49.01547341,  0.18103556,  0.55433098,
        0.81236894]), 'targetState': array([-28.2123404 ,  20.71385645,  78.        ]), 'previousTarget': array([-9.71410242, 12.06125927, 63.74232288])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.627342158339049
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-28.2123404 ,  20.71385645,  78.        ]), 'distance': 2.7993838047997377, 'localFrame': array([[-0.38784475, -0.33426519,  0.85897802],
       [ 0.6528458 , -0.75749084,  0.        ],
       [ 0.65066798,  0.56078019,  0.51201247]]), 'currentState': array([-25.43250355,  20.56455899,  77.70543759,  -0.38784475,
        -0.33426519,   0.85897802]), 'targetState': array([-28.2123404 ,  20.71385645,  78.        ]), 'previousTarget': array([-28.2123404 ,  20.71385645,  78.        ])}
episode index:19630
target thresh 86.65853450566013
target distance 4.0
model initialize at round 19630
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.99991796, -0.02864232, 22.        ]), 'distance': 5.726967452523903, 'localFrame': array([[-0.89390488,  0.25857619, -0.366159  ],
       [-0.2778739 , -0.96061756,  0.        ],
       [-0.35173877,  0.10174603,  0.9305523 ]]), 'currentState': array([ 3.08904004,  1.75106452, 16.90300624, -0.89390488,  0.25857619,
       -0.366159  ]), 'targetState': array([ 4.99991796, -0.02864232, 22.        ]), 'previousTarget': array([ 4.99991796, -0.02864232, 22.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6273562707080913
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.99991796, -0.02864232, 22.        ]), 'distance': 2.7916950606741686, 'localFrame': array([[-0.36244333,  0.4312624 ,  0.82622489],
       [-0.76554493, -0.64338244,  0.        ],
       [ 0.53157859, -0.63251228,  0.56334042]]), 'currentState': array([ 2.76970451,  0.35941316, 20.36625515, -0.36244333,  0.4312624 ,
        0.82622489]), 'targetState': array([ 4.99991796, -0.02864232, 22.        ]), 'previousTarget': array([ 4.99991796, -0.02864232, 22.        ])}
episode index:19631
target thresh 86.65986858550446
target distance 40.38190391548101
model initialize at round 19631
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.59071258, 12.17597778, 71.63528649]), 'distance': 27.5, 'localFrame': array([[-0.91651577, -0.39454037,  0.06585398],
       [ 0.39539867, -0.9185096 ,  0.        ],
       [ 0.06048751,  0.02603858,  0.99782927]]), 'currentState': array([ 2.04657722e+01,  3.50242801e+01,  7.52321853e+01, -9.16515767e-01,
       -3.94540369e-01,  6.58539809e-02]), 'targetState': array([-5.3075767, -4.5639489, 69.       ]), 'previousTarget': array([ 6.7107933 , 13.25203039, 72.08831043])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6273619932072091
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.3075767, -4.5639489, 69.       ]), 'distance': 3.2127394886150764, 'localFrame': array([[-0.8049021 , -0.08636712,  0.58708886],
       [ 0.10668897, -0.99429244,  0.        ],
       [ 0.58373802,  0.0626359 ,  0.8095225 ]]), 'currentState': array([-3.84123972, -1.72167559, 68.69498637, -0.8049021 , -0.08636712,
        0.58708886]), 'targetState': array([-5.3075767, -4.5639489, 69.       ]), 'previousTarget': array([-5.3075767, -4.5639489, 69.       ])}
episode index:19632
target thresh 86.66120253194747
target distance 28.0
model initialize at round 19632
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.9864209 ,  -5.80402232,  43.78601341]), 'distance': 27.499999999999996, 'localFrame': array([[-0.31831096, -0.65834577,  0.68209895],
       [ 0.90028974, -0.43529115,  0.        ],
       [ 0.29691164,  0.61408669,  0.73125988]]), 'currentState': array([-38.42143974,   6.76232818,  26.6291794 ,  -0.31831096,
        -0.65834577,   0.68209895]), 'targetState': array([-10.6068224 , -13.28515407,  54.        ]), 'previousTarget': array([-20.58046266,  -5.31192585,  43.48316945])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6273673383595774
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.6068224 , -13.28515407,  54.        ]), 'distance': 3.367854593282758, 'localFrame': array([[ 0.69738676, -0.4109155 ,  0.58719687],
       [ 0.50765145,  0.86156254,  0.        ],
       [-0.50590682,  0.29809134,  0.80944415]]), 'currentState': array([-12.97603888, -11.87878242,  55.93684707,   0.69738676,
        -0.4109155 ,   0.58719687]), 'targetState': array([-10.6068224 , -13.28515407,  54.        ]), 'previousTarget': array([-10.6068224 , -13.28515407,  54.        ])}
episode index:19633
target thresh 86.66253634500251
target distance 81.0
model initialize at round 19633
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.40694181, -11.97907849,  39.80363917]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.27647445, -0.70273703,  0.65553226],
       [ 0.93057148,  0.36611026,  0.        ],
       [-0.23999708,  0.61001962,  0.75516717]]), 'currentState': array([ 0.37846172,  0.67898153, 16.35194807,  0.27647445, -0.70273703,
        0.65553226]), 'targetState': array([-22.66653621, -42.31108763,  96.        ]), 'previousTarget': array([ -6.0174337 , -10.90516031,  38.40221289])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273353852507682
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-22.66653621, -42.31108763,  96.        ]), 'distance': 10.259339932431715, 'localFrame': array([[-0.78672069, -0.54792149,  0.28434591],
       [ 0.57151252, -0.82059335,  0.        ],
       [ 0.23333236,  0.16250725,  0.95872176]]), 'currentState': array([-23.46223547, -41.60861333,  85.79571421,  -0.78672069,
        -0.54792149,   0.28434591]), 'targetState': array([-22.66653621, -42.31108763,  96.        ]), 'previousTarget': array([-22.66653621, -42.31108763,  96.        ])}
episode index:19634
target thresh 86.66387002468291
target distance 49.291595678019696
model initialize at round 19634
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.38280638, -14.55971132,  32.00136566]), 'distance': 27.5, 'localFrame': array([[ 0.72499312,  0.417709  ,  0.54763507],
       [-0.49922357,  0.86647321,  0.        ],
       [-0.47451111, -0.27339233,  0.8367173 ]]), 'currentState': array([-10.54604859,  -7.51385253,  20.42484745,   0.72499312,
         0.417709  ,   0.54763507]), 'targetState': array([ 38.18418239, -21.8624842 ,  44.        ]), 'previousTarget': array([ 12.73376258, -14.97450308,  31.09190712])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6273371650651677
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 38.18418239, -21.8624842 ,  44.        ]), 'distance': 2.557988757577799, 'localFrame': array([[ 0.82950019, -0.03012775,  0.55769324],
       [ 0.03629643,  0.99934107,  0.        ],
       [-0.55732575,  0.02024227,  0.83004714]]), 'currentState': array([ 3.82635090e+01, -2.20078009e+01,  4.14473745e+01,  8.29500194e-01,
       -3.01277483e-02,  5.57693237e-01]), 'targetState': array([ 38.18418239, -21.8624842 ,  44.        ]), 'previousTarget': array([ 38.18418239, -21.8624842 ,  44.        ])}
episode index:19635
target thresh 86.66520357100201
target distance 26.95462895300694
model initialize at round 19635
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.92734485, 10.33834741, 94.28127465]), 'distance': 27.5, 'localFrame': array([[ 0.0434428 ,  0.78311489, -0.62035779],
       [-0.99846484,  0.0553892 ,  0.        ],
       [ 0.03436112,  0.61940544,  0.78431895]]), 'currentState': array([ 3.71294433e+01, -5.03634744e+00,  9.94712249e+01,  4.34427955e-02,
        7.83114893e-01, -6.20357791e-01]), 'targetState': array([ 9.44617747, 14.13399205, 93.        ]), 'previousTarget': array([15.06281916,  9.80693359, 94.45861744])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6273477159205549
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.44617747, 14.13399205, 93.        ]), 'distance': 3.7456563982965276, 'localFrame': array([[-0.32325614,  0.93888645, -0.11831189],
       [-0.94552738, -0.3255426 ,  0.        ],
       [-0.03851556,  0.11186713,  0.99297648]]), 'currentState': array([11.61439349, 13.51005105, 95.98989612, -0.32325614,  0.93888645,
       -0.11831189]), 'targetState': array([ 9.44617747, 14.13399205, 93.        ]), 'previousTarget': array([ 9.44617747, 14.13399205, 93.        ])}
episode index:19636
target thresh 86.66653698397315
target distance 44.0
model initialize at round 19636
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.93017598, 14.62920326, 41.52433215]), 'distance': 27.5, 'localFrame': array([[-0.790968  , -0.31554615, -0.52421394],
       [ 0.37053912, -0.92881686,  0.        ],
       [-0.48689875, -0.19424177,  0.8515866 ]]), 'currentState': array([32.13101894, 27.19748815, 58.91453675, -0.790968  , -0.31554615,
       -0.52421394]), 'targetState': array([-10.31622216,  -3.81779523,  16.        ]), 'previousTarget': array([16.16359594, 14.6870604 , 42.54228352])}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.6273347870905164
{'scaleFactor': 20, 'timeStep': 99, 'trapCount': 37, 'trapConfig': [], 'currentTarget': array([-10.31622216,  -3.81779523,  16.        ]), 'distance': 2.4348078698254967, 'localFrame': array([[-0.82126893,  0.22530201,  0.52417207],
       [-0.26455935, -0.96436941,  0.        ],
       [ 0.5054955 , -0.13867462,  0.85161238]]), 'currentState': array([-8.74933154, -5.63454795, 16.41539466, -0.82126893,  0.22530201,
        0.52417207]), 'targetState': array([-10.31622216,  -3.81779523,  16.        ]), 'previousTarget': array([-10.31622216,  -3.81779523,  16.        ])}
episode index:19637
target thresh 86.66787026360967
target distance 55.0
model initialize at round 19637
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.38744829,  5.3115916 , 37.19115371]), 'distance': 27.499999999999996, 'localFrame': array([[-0.72688589,  0.17000453, -0.66538362],
       [-0.22773498, -0.97372315,  0.        ],
       [-0.64789943,  0.15153113,  0.7465016 ]]), 'currentState': array([17.14214313, -1.53944481, 61.55608423, -0.72688589,  0.17000453,
       -0.66538362]), 'targetState': array([-6.49754433, 13.5196863 ,  8.        ]), 'previousTarget': array([ 7.08143638,  4.62798661, 38.69037429])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273028421476968
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 57, 'trapConfig': [], 'currentTarget': array([-6.49754433, 13.5196863 ,  8.        ]), 'distance': 9.098027131232143, 'localFrame': array([[-0.88837554,  0.191271  ,  0.41737788],
       [-0.21048097, -0.97759796,  0.        ],
       [ 0.40802776, -0.0878501 ,  0.90873302]]), 'currentState': array([ 0.91710187,  8.24832987,  8.09960294, -0.88837554,  0.191271  ,
        0.41737788]), 'targetState': array([-6.49754433, 13.5196863 ,  8.        ]), 'previousTarget': array([-6.49754433, 13.5196863 ,  8.        ])}
episode index:19638
target thresh 86.66920340992488
target distance 44.0
model initialize at round 19638
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.26948794, 12.70465702, 64.31156711]), 'distance': 27.499999999999996, 'localFrame': array([[-0.43271316,  0.0015523 ,  0.90153032],
       [-0.00358735, -0.99999357,  0.        ],
       [ 0.90152452, -0.0032341 ,  0.43271594]]), 'currentState': array([ 2.95674500e+01,  5.42498859e+00,  3.78923355e+01, -4.32713161e-01,
        1.55230214e-03,  9.01530316e-01]), 'targetState': array([25.90489777, 17.02751513, 80.        ]), 'previousTarget': array([27.35361021, 12.3248674 , 62.47600381])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6273105064135581
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([25.90489777, 17.02751513, 80.        ]), 'distance': 2.960592257342998, 'localFrame': array([[ 0.80026424, -0.09772136,  0.59163137],
       [ 0.12121101,  0.99262676,  0.        ],
       [-0.58726913,  0.07171224,  0.80620861]]), 'currentState': array([26.47084693, 17.14527342, 77.09639172,  0.80026424, -0.09772136,
        0.59163137]), 'targetState': array([25.90489777, 17.02751513, 80.        ]), 'previousTarget': array([25.90489777, 17.02751513, 80.        ])}
episode index:19639
target thresh 86.67053642293212
target distance 48.0
model initialize at round 19639
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.77149237,  6.13647136, 33.00473451]), 'distance': 27.5, 'localFrame': array([[ 0.22930141, -0.93833774, -0.25873374],
       [ 0.97141573,  0.23738468,  0.        ],
       [ 0.06141942, -0.25133802,  0.96594868]]), 'currentState': array([24.7464994 , -3.92999045,  7.91159217,  0.22930141, -0.93833774,
       -0.25873374]), 'targetState': array([34.57663151, 15.76250466, 57.        ]), 'previousTarget': array([29.46127451,  6.94972817, 34.12109293])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6273116159920691
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.57663151, 15.76250466, 57.        ]), 'distance': 2.0815338639314063, 'localFrame': array([[-0.12593449, -0.57247598,  0.81019242],
       [ 0.97664813, -0.21484514,  0.        ],
       [ 0.1740659 ,  0.79127291,  0.58616401]]), 'currentState': array([34.82037839, 16.47239076, 55.05849738, -0.12593449, -0.57247598,
        0.81019242]), 'targetState': array([34.57663151, 15.76250466, 57.        ]), 'previousTarget': array([34.57663151, 15.76250466, 57.        ])}
episode index:19640
target thresh 86.67186930264474
target distance 19.0
model initialize at round 19640
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([29.49029781, 29.90522254, 74.        ]), 'distance': 22.212812126534637, 'localFrame': array([[-0.6162908 ,  0.22345543,  0.75515119],
       [-0.34086676, -0.94011162,  0.        ],
       [ 0.70992641, -0.25740594,  0.65555067]]), 'currentState': array([26.71992111, 41.0736618 , 55.        , -0.6162908 ,  0.22345543,
        0.75515119]), 'targetState': array([29.49029781, 29.90522254, 74.        ]), 'previousTarget': array([29.49029781, 29.90522254, 74.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6273252622747566
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([29.49029781, 29.90522254, 74.        ]), 'distance': 3.1531484858459744, 'localFrame': array([[-0.4321621 , -0.67976331,  0.59258566],
       [ 0.84389451, -0.53650914,  0.        ],
       [ 0.31792762,  0.50007978,  0.80550744]]), 'currentState': array([30.41798621, 32.16746531, 72.00902103, -0.4321621 , -0.67976331,
        0.59258566]), 'targetState': array([29.49029781, 29.90522254, 74.        ]), 'previousTarget': array([29.49029781, 29.90522254, 74.        ])}
episode index:19641
target thresh 86.67320204907604
target distance 30.503860992449916
model initialize at round 19641
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 24.17260755, -13.96029002,  21.34956364]), 'distance': 27.5, 'localFrame': array([[-0.35340948, -0.61021923,  0.70903754],
       [ 0.86534962, -0.50116866,  0.        ],
       [ 0.35534739,  0.61356537,  0.70517074]]), 'currentState': array([ 3.16814312,  3.77228118, 22.13594336, -0.35340948, -0.61021923,
        0.70903754]), 'targetState': array([ 33.50956873, -21.84282041,  21.        ]), 'previousTarget': array([ 23.58586469, -13.04740853,  21.        ])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6273317480717696
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 33.50956873, -21.84282041,  21.        ]), 'distance': 2.836916123837359, 'localFrame': array([[ 0.51489465, -0.85484573,  0.06420501],
       [ 0.85661315,  0.51595921,  0.        ],
       [-0.03312717,  0.05499886,  0.99793673]]), 'currentState': array([ 30.85857344, -21.01902495,  20.41546761,   0.51489465,
        -0.85484573,   0.06420501]), 'targetState': array([ 33.50956873, -21.84282041,  21.        ]), 'previousTarget': array([ 33.50956873, -21.84282041,  21.        ])}
episode index:19642
target thresh 86.67453466223935
target distance 16.0
model initialize at round 19642
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.18434894, 15.44316107, 63.        ]), 'distance': 19.05162353893381, 'localFrame': array([[-0.25979575,  0.67640859, -0.68918618],
       [-0.93351262, -0.35854454,  0.        ],
       [-0.24710394,  0.64336399,  0.72458431]]), 'currentState': array([ 5.96729606,  9.78093579, 78.09462382, -0.25979575,  0.67640859,
       -0.68918618]), 'targetState': array([-4.18434894, 15.44316107, 63.        ]), 'previousTarget': array([-4.18434894, 15.44316107, 63.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6273458523494735
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.18434894, 15.44316107, 63.        ]), 'distance': 3.361898147540531, 'localFrame': array([[-0.15917369,  0.71972524, -0.67576572],
       [-0.97640641, -0.21594103,  0.        ],
       [-0.14592554,  0.65982198,  0.73711647]]), 'currentState': array([-4.4664847 , 13.04668833, 65.34087098, -0.15917369,  0.71972524,
       -0.67576572]), 'targetState': array([-4.18434894, 15.44316107, 63.        ]), 'previousTarget': array([-4.18434894, 15.44316107, 63.        ])}
episode index:19643
target thresh 86.67586714214804
target distance 54.0
model initialize at round 19643
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.10477257,  6.38606691, 40.23183664]), 'distance': 27.5, 'localFrame': array([[-0.65492206,  0.72396872,  0.21667113],
       [-0.74158534, -0.67085854,  0.        ],
       [ 0.14535568, -0.16068013,  0.97624465]]), 'currentState': array([ 6.88028115, -7.66959785, 59.98219532, -0.65492206,  0.72396872,
        0.21667113]), 'targetState': array([-28.61080626,  30.74771154,   6.        ]), 'previousTarget': array([-4.79112015,  5.72892089, 40.55233713])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6273463023410085
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-28.61080626,  30.74771154,   6.        ]), 'distance': 4.193896032116133, 'localFrame': array([[-0.26048036,  0.54457723, -0.79723624],
       [-0.90211463, -0.43149646,  0.        ],
       [-0.34400461,  0.71919848,  0.60366744]]), 'currentState': array([-26.5173061 ,  28.40239502,   8.7758803 ,  -0.26048036,
         0.54457723,  -0.79723624]), 'targetState': array([-28.61080626,  30.74771154,   6.        ]), 'previousTarget': array([-28.61080626,  30.74771154,   6.        ])}
episode index:19644
target thresh 86.67719948881538
target distance 44.05185993281331
model initialize at round 19644
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.05377127,  5.10687075, 87.02237663]), 'distance': 27.499999999999996, 'localFrame': array([[-0.55871478, -0.81090205, -0.173999  ],
       [ 0.8234633 , -0.56736954,  0.        ],
       [-0.09872173, -0.14328179,  0.98474583]]), 'currentState': array([-1.34876066, 29.2123436 , 96.33734159, -0.55871478, -0.81090205,
       -0.173999  ]), 'targetState': array([ 15.14216299, -13.06579121,  80.        ]), 'previousTarget': array([ 7.93156055,  6.69849154, 87.62721046])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6273524018977401
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.14216299, -13.06579121,  80.        ]), 'distance': 1.9814014391744323, 'localFrame': array([[ 0.32193809, -0.87362445, -0.36487833],
       [ 0.93831648,  0.34577765,  0.        ],
       [ 0.12616677, -0.34237135,  0.93105521]]), 'currentState': array([ 14.93259246, -11.25701936,  79.21873418,   0.32193809,
        -0.87362445,  -0.36487833]), 'targetState': array([ 15.14216299, -13.06579121,  80.        ]), 'previousTarget': array([ 15.14216299, -13.06579121,  80.        ])}
episode index:19645
target thresh 86.6785317022547
target distance 43.0
model initialize at round 19645
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.12994409, -15.42383293,  64.53972384]), 'distance': 27.5, 'localFrame': array([[ 0.42044051,  0.69475903,  0.58355777],
       [-0.8555391 ,  0.51773821,  0.        ],
       [-0.30213015, -0.49925648,  0.81207163]]), 'currentState': array([ -3.57299737, -33.07845373,  43.59704362,   0.42044051,
         0.69475903,   0.58355777]), 'targetState': array([ 1.37348898,  2.6671198 , 86.        ]), 'previousTarget': array([ -1.27893122, -16.80667453,  63.65086892])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6273545203623981
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.37348898,  2.6671198 , 86.        ]), 'distance': 1.9258013940209255, 'localFrame': array([[ 0.67963689, -0.61716994,  0.39647819],
       [ 0.67226609,  0.7403096 ,  0.        ],
       [-0.29351661,  0.26653884,  0.91804414]]), 'currentState': array([ 0.33007389,  2.98772061, 84.41342788,  0.67963689, -0.61716994,
        0.39647819]), 'targetState': array([ 1.37348898,  2.6671198 , 86.        ]), 'previousTarget': array([ 1.37348898,  2.6671198 , 86.        ])}
episode index:19646
target thresh 86.67986378247936
target distance 33.35847550946194
model initialize at round 19646
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.4387557 ,  13.37379906,  59.08030241]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.77395718,  0.25497453,  0.57963633],
       [-0.31290007,  0.94978605,  0.        ],
       [-0.5505305 , -0.18136825,  0.81487528]]), 'currentState': array([-41.9904387 ,  20.52472052,  59.43023358,   0.77395718,
         0.25497453,   0.57963633]), 'targetState': array([-9.34566039, 11.73280154, 59.        ]), 'previousTarget': array([-15.95526895,  13.29764301,  58.80186119])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6273642191671639
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.34566039, 11.73280154, 59.        ]), 'distance': 1.6501803376979383, 'localFrame': array([[ 0.92451641,  0.00372411, -0.38112406],
       [-0.00402814,  0.99999189,  0.        ],
       [ 0.38112097,  0.00153522,  0.92452391]]), 'currentState': array([-1.07678416e+01,  1.25339825e+01,  5.87579156e+01,  9.24516405e-01,
        3.72411238e-03, -3.81124057e-01]), 'targetState': array([-9.34566039, 11.73280154, 59.        ]), 'previousTarget': array([-9.34566039, 11.73280154, 59.        ])}
episode index:19647
target thresh 86.68119572950266
target distance 73.0
model initialize at round 19647
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-32.48873983,   7.15211723,  44.28574411]), 'distance': 27.5, 'localFrame': array([[-0.30628042, -0.04680652,  0.95078991],
       [ 0.15106853, -0.98852329,  0.        ],
       [ 0.93987797,  0.14363444,  0.30983633]]), 'currentState': array([-43.93804608,  12.41982086,  19.84367618,  -0.30628042,
        -0.04680652,   0.95078991]), 'targetState': array([-10.60655607,  -2.91564201,  91.        ]), 'previousTarget': array([-32.89708545,   7.55209223,  42.51833357])}
done in step count: 87
reward sum = 0.41712087993322033
running average episode reward sum: 0.6273535186714781
{'scaleFactor': 20, 'timeStep': 88, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-10.60655607,  -2.91564201,  91.        ]), 'distance': 2.1222371630916292, 'localFrame': array([[-0.31046574,  0.61850588,  0.7218459 ],
       [-0.89372511, -0.44861501,  0.        ],
       [ 0.32383091, -0.6451318 ,  0.69205383]]), 'currentState': array([-10.2304095 ,  -2.57116228,  88.93996649,  -0.31046574,
         0.61850588,   0.7218459 ]), 'targetState': array([-10.60655607,  -2.91564201,  91.        ]), 'previousTarget': array([-10.60655607,  -2.91564201,  91.        ])}
episode index:19648
target thresh 86.68252754333791
target distance 74.97213239463184
model initialize at round 19648
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.01475981, 24.76685733, 42.69763357]), 'distance': 27.5, 'localFrame': array([[ 0.63688661, -0.31283333, -0.70463519],
       [ 0.44087749,  0.89756729,  0.        ],
       [ 0.6324575 , -0.31065779,  0.70956976]]), 'currentState': array([ 1.49408829, 48.97721481, 54.        ,  0.63688661, -0.31283333,
       -0.70463519]), 'targetState': array([-18.66183967, -25.99491758,  19.        ]), 'previousTarget': array([-5.01475981, 24.76685733, 42.69763357])}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.627347028952471
{'scaleFactor': 20, 'timeStep': 70, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-18.66183967, -25.99491758,  19.        ]), 'distance': 2.8805189431707645, 'localFrame': array([[ 0.83196853,  0.53371192, -0.15159142],
       [-0.53995201,  0.8416958 ,  0.        ],
       [ 0.12759387,  0.08185209,  0.98844324]]), 'currentState': array([-18.06379712, -26.36872134,  16.20715105,   0.83196853,
         0.53371192,  -0.15159142]), 'targetState': array([-18.66183967, -25.99491758,  19.        ]), 'previousTarget': array([-18.66183967, -25.99491758,  19.        ])}
episode index:19649
target thresh 86.68385922399843
target distance 42.84060908748926
model initialize at round 19649
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.94539359,   0.53382706,  67.14970416]), 'distance': 27.5, 'localFrame': array([[-0.57521997, -0.09753121,  0.81216356],
       [ 0.16716871, -0.98592831,  0.        ],
       [ 0.80073504,  0.13576833,  0.58342981]]), 'currentState': array([ 3.44842754,  2.38685273, 65.60217076, -0.57521997, -0.09753121,
        0.81216356]), 'targetState': array([-38.9969926 ,  -0.48432221,  68.        ]), 'previousTarget': array([-23.43757165,   0.85303882,  66.54722692])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6273542908413693
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-38.9969926 ,  -0.48432221,  68.        ]), 'distance': 2.650415357587293, 'localFrame': array([[-0.50564795,  0.71538385,  0.48223034],
       [-0.81660679, -0.57719438,  0.        ],
       [ 0.27834064, -0.39379256,  0.87604446]]), 'currentState': array([-38.09683029,  -1.57601031,  65.7588783 ,  -0.50564795,
         0.71538385,   0.48223034]), 'targetState': array([-38.9969926 ,  -0.48432221,  68.        ]), 'previousTarget': array([-38.9969926 ,  -0.48432221,  68.        ])}
episode index:19650
target thresh 86.68519077149755
target distance 68.79057590651864
model initialize at round 19650
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.37288535,  4.50892264, 84.00913192]), 'distance': 27.499999999999996, 'localFrame': array([[-0.22038609, -0.67539097, -0.7037592 ],
       [ 0.95066763, -0.31021132,  0.        ],
       [-0.21831407, -0.66904108,  0.71043859]]), 'currentState': array([ 2.35360647, 30.92080856, 78.92291788, -0.22038609, -0.67539097,
       -0.7037592 ]), 'targetState': array([-12.36968343, -36.98636143,  92.        ]), 'previousTarget': array([-2.57251154,  5.38975234, 84.6078048 ])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6273507750837848
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([-12.36968343, -36.98636143,  92.        ]), 'distance': 2.629164575899122, 'localFrame': array([[-0.60503499, -0.62901835,  0.48812762],
       [ 0.72071304, -0.69323352,  0.        ],
       [ 0.33838643,  0.35179994,  0.87277226]]), 'currentState': array([-11.69332709, -35.93742791,  89.68595874,  -0.60503499,
        -0.62901835,   0.48812762]), 'targetState': array([-12.36968343, -36.98636143,  92.        ]), 'previousTarget': array([-12.36968343, -36.98636143,  92.        ])}
episode index:19651
target thresh 86.68652218584857
target distance 54.09505299338188
model initialize at round 19651
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.65208968, -6.0188838 , 28.28920282]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.41159219, -0.76704958, -0.49216542],
       [ 0.88115806,  0.47282182,  0.        ],
       [ 0.23270655, -0.43367553,  0.87050169]]), 'currentState': array([18.24719638, 20.23690047, 34.42686694,  0.41159219, -0.76704958,
       -0.49216542]), 'targetState': array([ 29.1904296 , -32.92292241,  22.        ]), 'previousTarget': array([22.7538809 , -4.92271949, 28.72894503])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.627353934981545
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 29.1904296 , -32.92292241,  22.        ]), 'distance': 4.0296095519311725, 'localFrame': array([[ 0.36656858, -0.65277497,  0.66295725],
       [ 0.87192766,  0.48963471,  0.        ],
       [-0.32460688,  0.57805076,  0.74865726]]), 'currentState': array([ 26.79525184, -30.20104252,  20.24152153,   0.36656858,
        -0.65277497,   0.66295725]), 'targetState': array([ 29.1904296 , -32.92292241,  22.        ]), 'previousTarget': array([ 29.1904296 , -32.92292241,  22.        ])}
episode index:19652
target thresh 86.68785346706481
target distance 16.389429384332043
model initialize at round 19652
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.82762509,  21.35987553,  25.        ]), 'distance': 16.323710684098106, 'localFrame': array([[-0.35131234, -0.41379642, -0.83985246],
       [ 0.76231612, -0.64720487,  0.        ],
       [-0.5435566 , -0.64023307,  0.54281474]]), 'currentState': array([-7.75711372, 21.83505201, 22.17574751, -0.35131234, -0.41379642,
       -0.83985246]), 'targetState': array([-23.82762509,  21.35987553,  25.        ]), 'previousTarget': array([-23.82762509,  21.35987553,  25.        ])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6273684957769707
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.82762509,  21.35987553,  25.        ]), 'distance': 4.166685270049613, 'localFrame': array([[-0.36943832,  0.75090369,  0.54741116],
       [-0.89728303, -0.44145573,  0.        ],
       [ 0.2416578 , -0.49118275,  0.8368638 ]]), 'currentState': array([-20.96354681,  23.98581705,  23.49575505,  -0.36943832,
         0.75090369,   0.54741116]), 'targetState': array([-23.82762509,  21.35987553,  25.        ]), 'previousTarget': array([-23.82762509,  21.35987553,  25.        ])}
episode index:19653
target thresh 86.68918461515959
target distance 20.44981891929809
model initialize at round 19653
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.54993694, -1.2639998 , 70.        ]), 'distance': 23.16175697751985, 'localFrame': array([[-0.84062512, -0.46983759, -0.26944766],
       [ 0.48788189, -0.87290965,  0.        ],
       [-0.23520346, -0.13145863,  0.96301504]]), 'currentState': array([20.90785455, -0.6248487 , 82.70155494, -0.84062512, -0.46983759,
       -0.26944766]), 'targetState': array([ 1.54993694, -1.2639998 , 70.        ]), 'previousTarget': array([ 1.54993694, -1.2639998 , 70.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6273807771098797
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.54993694, -1.2639998 , 70.        ]), 'distance': 2.0714332478125432, 'localFrame': array([[-0.88086487,  0.07236512, -0.46780376],
       [-0.08187653, -0.99664248,  0.        ],
       [-0.4662331 ,  0.03830215,  0.88383236]]), 'currentState': array([ 3.16848592, -0.3113866 , 70.87387813, -0.88086487,  0.07236512,
       -0.46780376]), 'targetState': array([ 1.54993694, -1.2639998 , 70.        ]), 'previousTarget': array([ 1.54993694, -1.2639998 , 70.        ])}
episode index:19654
target thresh 86.69051563014621
target distance 34.0
model initialize at round 19654
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.41153033, -2.51934383, 51.64588744]), 'distance': 27.5, 'localFrame': array([[-0.74701093,  0.12358632,  0.65322361],
       [-0.16322244, -0.98658929,  0.        ],
       [ 0.64446343, -0.10662075,  0.75716505]]), 'currentState': array([ -6.89024096, -15.61275115,  33.78349853,  -0.74701093,
         0.12358632,   0.65322361]), 'targetState': array([23.42417066,  8.73545813, 67.        ]), 'previousTarget': array([10.33009369, -2.11421767, 51.51527331])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273857427450285
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.42417066,  8.73545813, 67.        ]), 'distance': 3.1437996992202915, 'localFrame': array([[ 0.88657816,  0.25958218, -0.38287891],
       [-0.28099437,  0.95970942,  0.        ],
       [ 0.3674525 ,  0.10758682,  0.92379854]]), 'currentState': array([22.42363545,  6.28683344, 68.69901235,  0.88657816,  0.25958218,
       -0.38287891]), 'targetState': array([23.42417066,  8.73545813, 67.        ]), 'previousTarget': array([23.42417066,  8.73545813, 67.        ])}
episode index:19655
target thresh 86.691846512038
target distance 49.441060486665975
model initialize at round 19655
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.00386322, 19.50963312, 50.41702369]), 'distance': 27.5, 'localFrame': array([[-0.82878938,  0.13227394, -0.5437019 ],
       [-0.15760436, -0.98750234,  0.        ],
       [-0.5369069 ,  0.08568979,  0.8392784 ]]), 'currentState': array([24.59228195, 39.22028032, 62.86364676, -0.82878938,  0.13227394,
       -0.5437019 ]), 'targetState': array([-12.75441189, -11.23943848,  31.        ]), 'previousTarget': array([10.70838814, 18.98583884, 51.17420625])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627353824463448
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 67, 'trapConfig': [], 'currentTarget': array([ 0.83129144, -1.9406088 , 41.03578711]), 'distance': 27.499999999999996, 'localFrame': array([[-0.01625705, -0.70182471, -0.71216415],
       [ 0.99973182, -0.02315776,  0.        ],
       [-0.01649213, -0.71197317,  0.70201298]]), 'currentState': array([ 2.02082443e+01,  1.13220821e+01,  5.53495822e+01, -1.62570472e-02,
       -7.01824714e-01, -7.12164151e-01]), 'targetState': array([-12.75441189, -11.23943848,  31.        ]), 'previousTarget': array([ 0.83129144, -1.9406088 , 41.03578711])}
episode index:19656
target thresh 86.69317726084824
target distance 33.983754605692255
model initialize at round 19656
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.45607369, -4.54915535, 62.41777479]), 'distance': 27.5, 'localFrame': array([[ 0.02199796, -0.66659565, -0.74509484],
       [ 0.99945593,  0.03298251,  0.        ],
       [ 0.0245751 , -0.74468946,  0.66695853]]), 'currentState': array([ 2.74002141e+01,  1.51371138e+01,  8.06761428e+01,  2.19979649e-02,
       -6.66595654e-01, -7.45094842e-01]), 'targetState': array([ 17.4133796 , -17.93806598,  50.        ]), 'previousTarget': array([21.25613251, -3.60451294, 63.49685173])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6273603038582051
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.4133796 , -17.93806598,  50.        ]), 'distance': 2.2738186327065675, 'localFrame': array([[ 0.53809024, -0.70982661, -0.45453832],
       [ 0.79690687,  0.60410218,  0.        ],
       [ 0.27458759, -0.36222471,  0.89072718]]), 'currentState': array([ 18.55428571, -17.62536714,  51.94185577,   0.53809024,
        -0.70982661,  -0.45453832]), 'targetState': array([ 17.4133796 , -17.93806598,  50.        ]), 'previousTarget': array([ 17.4133796 , -17.93806598,  50.        ])}
episode index:19657
target thresh 86.69450787659027
target distance 47.479787306511064
model initialize at round 19657
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.57883136, -8.39396345,  9.50891262]), 'distance': 27.5, 'localFrame': array([[ 0.37361424, -0.69842156, -0.61042586],
       [ 0.88176368,  0.47169144,  0.        ],
       [ 0.28793265, -0.53825135,  0.7920734 ]]), 'currentState': array([ 8.68392111, 15.21355585, 14.30833235,  0.37361424, -0.69842156,
       -0.61042586]), 'targetState': array([-17.03879433, -30.57252832,   5.        ]), 'previousTarget': array([-4.20597033, -6.87118253,  9.99188121])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273652697770218
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.03879433, -30.57252832,   5.        ]), 'distance': 3.2728152989618704, 'localFrame': array([[-0.95437856,  0.04088582,  0.29578695],
       [-0.042801  , -0.99908362,  0.        ],
       [ 0.29551589, -0.01265998,  0.95525394]]), 'currentState': array([-14.85658657, -28.28699578,   4.14816062,  -0.95437856,
         0.04088582,   0.29578695]), 'targetState': array([-17.03879433, -30.57252832,   5.        ]), 'previousTarget': array([-17.03879433, -30.57252832,   5.        ])}
episode index:19658
target thresh 86.69583835927736
target distance 29.642760790762303
model initialize at round 19658
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.79907526, -17.1249781 ,  18.64917069]), 'distance': 27.5, 'localFrame': array([[-0.74101302, -0.41030936,  0.5315505 ],
       [ 0.48441142, -0.87484032,  0.        ],
       [ 0.4650218 ,  0.25748913,  0.84702661]]), 'currentState': array([ 27.4525409 , -33.80229494,   7.24016888,  -0.74101302,
        -0.41030936,   0.5315505 ]), 'targetState': array([-1.5843902 , -7.84153733, 25.        ]), 'previousTarget': array([  9.11559427, -16.77274857,  18.14167441])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627333357407635
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 84, 'trapConfig': [], 'currentTarget': array([-1.5843902 , -7.84153733, 25.        ]), 'distance': 24.781756634483656, 'localFrame': array([[-0.87196947, -0.07203067,  0.48423221],
       [ 0.08232645, -0.99660542,  0.        ],
       [ 0.48258844,  0.03986512,  0.87493952]]), 'currentState': array([ 15.56626814, -19.16899787,  11.15518082,  -0.87196947,
        -0.07203067,   0.48423221]), 'targetState': array([-1.5843902 , -7.84153733, 25.        ]), 'previousTarget': array([-1.5843902 , -7.84153733, 25.        ])}
episode index:19659
target thresh 86.69716870892283
target distance 34.48212495008502
model initialize at round 19659
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.15245225, -17.39538715,  82.29520074]), 'distance': 27.5, 'localFrame': array([[-0.81676085, -0.26164236, -0.51424214],
       [ 0.30507069, -0.95232971,  0.        ],
       [-0.48972807, -0.1568802 ,  0.85764504]]), 'currentState': array([-5.26403419,  6.09145556, 95.73847371, -0.81676085, -0.26164236,
       -0.51424214]), 'targetState': array([-12.44159516, -28.39377942,  76.        ]), 'previousTarget': array([ -9.50256   , -17.11742245,  82.54040723])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6273422229840404
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.44159516, -28.39377942,  76.        ]), 'distance': 2.221918862743803, 'localFrame': array([[-0.10411186, -0.27614384, -0.95546078],
       [ 0.93570618, -0.35278032,  0.        ],
       [-0.33706776, -0.89403055,  0.29511811]]), 'currentState': array([-12.13763834, -26.41643649,  76.96677227,  -0.10411186,
        -0.27614384,  -0.95546078]), 'targetState': array([-12.44159516, -28.39377942,  76.        ]), 'previousTarget': array([-12.44159516, -28.39377942,  76.        ])}
episode index:19660
target thresh 86.69849892554001
target distance 54.44692350582051
model initialize at round 19660
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.47839403, 16.85728637, 61.90515504]), 'distance': 27.500000000000004, 'localFrame': array([[-0.18053571,  0.97244297,  0.14751786],
       [-0.98319978, -0.18253273,  0.        ],
       [ 0.02692684, -0.14503952,  0.98905939]]), 'currentState': array([-17.77724306,  15.81150218,  45.7855101 ,  -0.18053571,
         0.97244297,   0.14751786]), 'targetState': array([37.74501539, 18.42047266, 86.        ]), 'previousTarget': array([ 5.4194781 , 15.98565776, 62.25170701])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6273429996169229
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([37.74501539, 18.42047266, 86.        ]), 'distance': 2.4388014987292483, 'localFrame': array([[ 0.11265356,  0.84680195,  0.51984192],
       [-0.99126671,  0.1318723 ,  0.        ],
       [-0.06855275, -0.515302  ,  0.85426247]]), 'currentState': array([37.10376361, 16.08635257, 85.70262425,  0.11265356,  0.84680195,
        0.51984192]), 'targetState': array([37.74501539, 18.42047266, 86.        ]), 'previousTarget': array([37.74501539, 18.42047266, 86.        ])}
episode index:19661
target thresh 86.69982900914218
target distance 30.548959302697654
model initialize at round 19661
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.00556985, -16.34601522,  88.71085517]), 'distance': 27.5, 'localFrame': array([[ 0.77669218,  0.62121155, -0.10414155],
       [-0.62460787,  0.78093855,  0.        ],
       [ 0.08132815,  0.06504763,  0.99456248]]), 'currentState': array([  8.34610528, -29.75901982,  72.44787208,   0.77669218,
         0.62121155,  -0.10414155]), 'targetState': array([37.17823033, -7.85997387, 99.        ]), 'previousTarget': array([ 24.68427538, -17.03052408,  87.95750326])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6273502572787165
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([37.17823033, -7.85997387, 99.        ]), 'distance': 3.226910902569327, 'localFrame': array([[ 0.15841878,  0.91968642, -0.35927759],
       [-0.98548663,  0.16975307,  0.        ],
       [ 0.06098847,  0.35406326,  0.93323074]]), 'currentState': array([34.63800459, -9.84993668, 99.01597362,  0.15841878,  0.91968642,
       -0.35927759]), 'targetState': array([37.17823033, -7.85997387, 99.        ]), 'previousTarget': array([37.17823033, -7.85997387, 99.        ])}
episode index:19662
target thresh 86.70115895974261
target distance 35.0
model initialize at round 19662
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.35882718, -0.86943712, 57.87260461]), 'distance': 27.5, 'localFrame': array([[-0.85206384, -0.31345102, -0.41920839],
       [ 0.34525218, -0.93850995,  0.        ],
       [-0.39343124, -0.14473261,  0.90789004]]), 'currentState': array([ -2.04090117, -15.96003463,  80.85213119,  -0.85206384,
        -0.31345102,  -0.41920839]), 'targetState': array([-1.00642676,  6.92727256, 46.        ]), 'previousTarget': array([-0.73416509, -0.93296039, 57.99990341])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6273583093502991
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.00642676,  6.92727256, 46.        ]), 'distance': 3.5457102641168206, 'localFrame': array([[ 0.25790629,  0.93107157, -0.25805052],
       [-0.9637111 ,  0.26694742,  0.        ],
       [ 0.06888592,  0.24868615,  0.96613143]]), 'currentState': array([-2.37490077,  4.9802939 , 48.62842428,  0.25790629,  0.93107157,
       -0.25805052]), 'targetState': array([-1.00642676,  6.92727256, 46.        ]), 'previousTarget': array([-1.00642676,  6.92727256, 46.        ])}
episode index:19663
target thresh 86.70248877735465
target distance 9.31098567221715
model initialize at round 19663
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.82285291,  6.38279639, 66.        ]), 'distance': 10.295790224954274, 'localFrame': array([[-0.67551832,  0.4342336 , -0.59591625],
       [-0.54073276, -0.84119444,  0.        ],
       [-0.50128144,  0.32223144,  0.80304659]]), 'currentState': array([-0.42537265, -2.91448281, 65.52453904, -0.67551832,  0.4342336 ,
       -0.59591625]), 'targetState': array([-4.82285291,  6.38279639, 66.        ]), 'previousTarget': array([-4.82285291,  6.38279639, 66.        ])}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.627375255938005
{'scaleFactor': 20, 'timeStep': 5, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.82285291,  6.38279639, 66.        ]), 'distance': 3.1616023587316047, 'localFrame': array([[-0.82401334,  0.45515248,  0.33739923],
       [-0.48350438, -0.87534194,  0.        ],
       [ 0.2953397 , -0.16313401,  0.94136165]]), 'currentState': array([-3.24921908,  3.69887574, 65.43788264, -0.82401334,  0.45515248,
        0.33739923]), 'targetState': array([-4.82285291,  6.38279639, 66.        ]), 'previousTarget': array([-4.82285291,  6.38279639, 66.        ])}
episode index:19664
target thresh 86.70381846199157
target distance 41.801206280331
model initialize at round 19664
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.70516889,  20.51220677,  73.21994527]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.45107605,  0.88490756,  0.11605607],
       [-0.89092786,  0.45414486,  0.        ],
       [-0.05270627, -0.10339759,  0.99324266]]), 'currentState': array([-35.32961859,  -2.2271599 ,  65.90246114,   0.45107605,
         0.88490756,   0.11605607]), 'targetState': array([-10.94326446,  38.47395175,  79.        ]), 'previousTarget': array([-22.84434323,  19.27131442,  73.02806025])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6273817316070244
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.94326446,  38.47395175,  79.        ]), 'distance': 2.9695358565295003, 'localFrame': array([[ 9.42293360e-01,  3.34783996e-01,  1.70297078e-03],
       [-3.34784482e-01,  9.42294726e-01,  0.00000000e+00],
       [-1.60470039e-03, -5.70128191e-04,  9.99998550e-01]]), 'currentState': array([-1.28865578e+01,  3.76285904e+01,  8.10801727e+01,  9.42293360e-01,
        3.34783996e-01,  1.70297078e-03]), 'targetState': array([-10.94326446,  38.47395175,  79.        ]), 'previousTarget': array([-10.94326446,  38.47395175,  79.        ])}
episode index:19665
target thresh 86.70514801366669
target distance 39.0
model initialize at round 19665
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.67847878, 23.12751406, 39.23181961]), 'distance': 27.5, 'localFrame': array([[ 0.57326861,  0.45334239,  0.68252749],
       [-0.62028629,  0.7843755 ,  0.        ],
       [-0.53535784, -0.42336245,  0.73085992]]), 'currentState': array([-8.61622592, 12.11468672, 14.03333238,  0.57326861,  0.45334239,
        0.68252749]), 'targetState': array([-8.71002257, 28.70776039, 52.        ]), 'previousTarget': array([-8.83581607, 22.21535623, 37.95974149])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627349829759592
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 72, 'trapConfig': [], 'currentTarget': array([-8.71002257, 28.70776039, 52.        ]), 'distance': 13.565886808462318, 'localFrame': array([[ 0.46012386, -0.60517146,  0.64965648],
       [ 0.79603953,  0.60524464,  0.        ],
       [-0.3932011 ,  0.51715224,  0.7602279 ]]), 'currentState': array([-6.84426406, 26.88988599, 38.68656458,  0.46012386, -0.60517146,
        0.64965648]), 'targetState': array([-8.71002257, 28.70776039, 52.        ]), 'previousTarget': array([-8.71002257, 28.70776039, 52.        ])}
episode index:19666
target thresh 86.70647743239327
target distance 68.0
model initialize at round 19666
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.10588731, 11.25810522, 27.65461773]), 'distance': 27.5, 'localFrame': array([[-0.54667774, -0.48273315,  0.68418723],
       [ 0.66190718, -0.74958581,  0.        ],
       [ 0.51285704,  0.45286844,  0.72930641]]), 'currentState': array([ 3.58127456,  2.68507684,  4.72238583, -0.54667774, -0.48273315,
        0.68418723]), 'targetState': array([40.32545488, 27.83626571, 72.        ]), 'previousTarget': array([16.57632103, 12.29209454, 27.22617118])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6273480814815954
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([40.32545488, 27.83626571, 72.        ]), 'distance': 2.4379359894098034, 'localFrame': array([[ 0.77876414,  0.61788511,  0.10837164],
       [-0.62154573,  0.78337788,  0.        ],
       [-0.08489595, -0.06735793,  0.99411045]]), 'currentState': array([38.61643073, 29.46468516, 71.39088717,  0.77876414,  0.61788511,
        0.10837164]), 'targetState': array([40.32545488, 27.83626571, 72.        ]), 'previousTarget': array([40.32545488, 27.83626571, 72.        ])}
episode index:19667
target thresh 86.70780671818463
target distance 29.0
model initialize at round 19667
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.84766478, 22.85347336,  8.18394204]), 'distance': 27.5, 'localFrame': array([[-0.60084422,  0.57218835, -0.55819953],
       [-0.68962721, -0.72416456,  0.        ],
       [-0.40422832,  0.38494959,  0.82970675]]), 'currentState': array([30.6944251 , 13.96499844, 33.5425778 , -0.60084422,  0.57218835,
       -0.55819953]), 'targetState': array([24.1135659 , 23.96947933,  5.        ]), 'previousTarget': array([25.26521886, 22.4849509 ,  9.05178982])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.627357770258091
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.1135659 , 23.96947933,  5.        ]), 'distance': 2.337303116861246, 'localFrame': array([[-0.80951678,  0.338124  ,  0.47995285],
       [-0.38541685, -0.92274257,  0.        ],
       [ 0.44287293, -0.18498192,  0.87729428]]), 'currentState': array([25.97728599, 25.35204732,  5.27935471, -0.80951678,  0.338124  ,
        0.47995285]), 'targetState': array([24.1135659 , 23.96947933,  5.        ]), 'previousTarget': array([24.1135659 , 23.96947933,  5.        ])}
episode index:19668
target thresh 86.70913587105406
target distance 44.74803211344641
model initialize at round 19668
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.44281515, 14.26536836, 74.67188762]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.93294163, -0.2801345 , -0.22615169],
       [ 0.28758523,  0.95775505,  0.        ],
       [ 0.21659793, -0.06503789,  0.9740921 ]]), 'currentState': array([-29.92726431,  14.9821272 ,  74.08766885,   0.93294163,
        -0.2801345 ,  -0.22615169]), 'targetState': array([12.99316265, 13.86281805, 75.        ]), 'previousTarget': array([-4.26675401, 14.1927885 , 74.61428658])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6273623649381632
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([12.99316265, 13.86281805, 75.        ]), 'distance': 3.09750764076705, 'localFrame': array([[ 0.26002318,  0.85953873,  0.43997854],
       [-0.95716125,  0.28955543,  0.        ],
       [-0.12739818, -0.42113041,  0.89800829]]), 'currentState': array([10.52422004, 12.70956844, 73.52728438,  0.26002318,  0.85953873,
        0.43997854]), 'targetState': array([12.99316265, 13.86281805, 75.        ]), 'previousTarget': array([12.99316265, 13.86281805, 75.        ])}
episode index:19669
target thresh 86.71046489101487
target distance 45.45798754384068
model initialize at round 19669
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.42950699, -18.09163967,  81.99561573]), 'distance': 27.5, 'localFrame': array([[-0.89979907,  0.37652642, -0.22043022],
       [-0.38602149, -0.92248979,  0.        ],
       [-0.20334463,  0.0850908 ,  0.97540275]]), 'currentState': array([ 21.62920099, -24.66781998,  72.77254646,  -0.89979907,
         0.37652642,  -0.22043022]), 'targetState': array([-22.46013713, -13.0974135 ,  89.        ]), 'previousTarget': array([ -2.34022676, -18.38411596,  82.36093233])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6273688396164686
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.46013713, -13.0974135 ,  89.        ]), 'distance': 2.901195142434167, 'localFrame': array([[-0.86923141, -0.48819766, -0.07810121],
       [ 0.48969346, -0.87189467,  0.        ],
       [-0.06809602, -0.03824565,  0.99694544]]), 'currentState': array([-1.98326215e+01, -1.41399566e+01,  8.96528389e+01, -8.69231413e-01,
       -4.88197657e-01, -7.81012050e-02]), 'targetState': array([-22.46013713, -13.0974135 ,  89.        ]), 'previousTarget': array([-22.46013713, -13.0974135 ,  89.        ])}
episode index:19670
target thresh 86.71179377808029
target distance 30.71701381814054
model initialize at round 19670
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.55303242, -16.62544443,  43.40692547]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.17802015, -0.63474369, -0.75193701],
       [ 0.96284896,  0.27004052,  0.        ],
       [ 0.20305346, -0.72400177,  0.65923496]]), 'currentState': array([-10.05007426,   3.59512882,  58.73508733,   0.17802015,
        -0.63474369,  -0.75193701]), 'targetState': array([  5.67670097, -26.39649723,  36.        ]), 'previousTarget': array([ -0.27188798, -15.56650364,  44.46175503])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6273749299654451
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([  5.67670097, -26.39649723,  36.        ]), 'distance': 2.008258527017404, 'localFrame': array([[ 0.45198731, -0.82420256,  0.34117096],
       [ 0.87681005,  0.48083691,  0.        ],
       [-0.16404759,  0.29914212,  0.94000127]]), 'currentState': array([  6.81797466, -24.7633909 ,  36.25211189,   0.45198731,
        -0.82420256,   0.34117096]), 'targetState': array([  5.67670097, -26.39649723,  36.        ]), 'previousTarget': array([  5.67670097, -26.39649723,  36.        ])}
episode index:19671
target thresh 86.71312253226367
target distance 41.174450930042994
model initialize at round 19671
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.16091794, -10.0208954 ,  13.98471709]), 'distance': 27.5, 'localFrame': array([[ 0.65105764,  0.23066765, -0.72312958],
       [-0.33395615,  0.94258861,  0.        ],
       [ 0.68161371,  0.24149357,  0.69071239]]), 'currentState': array([ -3.58835387, -27.00354165,  22.80548744,   0.65105764,
         0.23066765,  -0.72312958]), 'targetState': array([36.27725845,  7.27739783,  5.        ]), 'previousTarget': array([ 15.11676399, -10.10839072,  14.76453567])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.627378797023448
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([36.27725845,  7.27739783,  5.        ]), 'distance': 2.9712506982183022, 'localFrame': array([[ 0.74895555, -0.16069761,  0.64283891],
       [ 0.20978762,  0.97774698,  0.        ],
       [-0.6285338 ,  0.13485964,  0.76600139]]), 'currentState': array([33.86435987,  5.69746134,  5.71417914,  0.74895555, -0.16069761,
        0.64283891]), 'targetState': array([36.27725845,  7.27739783,  5.        ]), 'previousTarget': array([36.27725845,  7.27739783,  5.        ])}
episode index:19672
target thresh 86.71445115357828
target distance 52.0
model initialize at round 19672
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.79645674, -7.46753986, 74.15577739]), 'distance': 27.500000000000004, 'localFrame': array([[-0.64061071, -0.72580048,  0.2506623 ],
       [ 0.74973611, -0.66173693,  0.        ],
       [ 0.1658725 ,  0.18793058,  0.96807459]]), 'currentState': array([-1.31011894, -4.35344668, 47.52380117, -0.64061071, -0.72580048,
        0.2506623 ]), 'targetState': array([ 10.72240455, -10.48952052, 100.        ]), 'previousTarget': array([ 5.43045621, -6.83769925, 74.6556049 ])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.627378601150824
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.72240455, -10.48952052, 100.        ]), 'distance': 3.2159789751872414, 'localFrame': array([[-0.7486732 ,  0.26101329,  0.60939355],
       [-0.32920149, -0.9442597 ,  0.        ],
       [ 0.57542577, -0.20061327,  0.79286789]]), 'currentState': array([  9.37734813, -11.30861036,  97.19599646,  -0.7486732 ,
         0.26101329,   0.60939355]), 'targetState': array([ 10.72240455, -10.48952052, 100.        ]), 'previousTarget': array([ 10.72240455, -10.48952052, 100.        ])}
episode index:19673
target thresh 86.71577964203738
target distance 35.96148253104714
model initialize at round 19673
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.03685356,  -0.40744476,  85.51642919]), 'distance': 27.499999999999996, 'localFrame': array([[-0.9134245 , -0.39395926,  0.10223393],
       [ 0.39603433, -0.9182357 ,  0.        ],
       [ 0.09387484,  0.04048815,  0.99476038]]), 'currentState': array([ 8.96636593, 12.96451744, 84.38237424, -0.9134245 , -0.39395926,
        0.10223393]), 'targetState': array([-25.27203322,  -6.10936469,  86.        ]), 'previousTarget': array([-13.52628716,   0.20359175,  85.67337982])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6273753748922748
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([-25.27203322,  -6.10936469,  86.        ]), 'distance': 4.232104724692365, 'localFrame': array([[-0.73155559, -0.03457291,  0.68090464],
       [ 0.04720676, -0.99888514,  0.        ],
       [ 0.68014553,  0.0321433 ,  0.73237208]]), 'currentState': array([-2.30432538e+01, -3.24104725e+00,  8.38283629e+01, -7.31555592e-01,
       -3.45729097e-02,  6.80904641e-01]), 'targetState': array([-25.27203322,  -6.10936469,  86.        ]), 'previousTarget': array([-25.27203322,  -6.10936469,  86.        ])}
episode index:19674
target thresh 86.71710799765428
target distance 76.0
model initialize at round 19674
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.79514702,  11.44884031,  34.52296006]), 'distance': 27.499999999999993, 'localFrame': array([[ 0.63600986, -0.27337575, -0.72163506],
       [ 0.39489549,  0.91872605,  0.        ],
       [ 0.66298492, -0.28497043,  0.69227368]]), 'currentState': array([-18.18182296,  13.03689864,   7.19351425,   0.63600986,
        -0.27337575,  -0.72163506]), 'targetState': array([-25.62191387,   8.51572251,  85.        ]), 'previousTarget': array([-21.28967956,  11.49618101,  36.34081351])}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.6273636494758342
{'scaleFactor': 20, 'timeStep': 93, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-25.62191387,   8.51572251,  85.        ]), 'distance': 3.1826515826376283, 'localFrame': array([[ 0.25508793,  0.20685982,  0.94453119],
       [-0.62986032,  0.77670843,  0.        ],
       [-0.73362533, -0.59492271,  0.32842173]]), 'currentState': array([-25.81242583,   7.13610536,  82.13824655,   0.25508793,
         0.20685982,   0.94453119]), 'targetState': array([-25.62191387,   8.51572251,  85.        ]), 'previousTarget': array([-25.62191387,   8.51572251,  85.        ])}
episode index:19675
target thresh 86.71843622044229
target distance 17.871857069133718
model initialize at round 19675
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.8949863 , -42.36164894,  77.        ]), 'distance': 22.086018869355602, 'localFrame': array([[-0.54791543, -0.44020389,  0.71134325],
       [ 0.62631736, -0.77956819,  0.        ],
       [ 0.55454057,  0.44552663,  0.70284478]]), 'currentState': array([  7.16606758, -25.32987798,  63.75808416,  -0.54791543,
        -0.44020389,   0.71134325]), 'targetState': array([ 11.8949863 , -42.36164894,  77.        ]), 'previousTarget': array([ 11.8949863 , -42.36164894,  77.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6273768137989812
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.8949863 , -42.36164894,  77.        ]), 'distance': 1.991038838178183, 'localFrame': array([[ 0.69910101, -0.20689888,  0.68443453],
       [ 0.28378297,  0.95888854,  0.        ],
       [-0.65629643,  0.19423087,  0.72907432]]), 'currentState': array([ 10.8743412 , -42.34436163,  75.29054969,   0.69910101,
        -0.20689888,   0.68443453]), 'targetState': array([ 11.8949863 , -42.36164894,  77.        ]), 'previousTarget': array([ 11.8949863 , -42.36164894,  77.        ])}
episode index:19676
target thresh 86.71976431041463
target distance 40.0
model initialize at round 19676
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.64337508,  1.23632383, 23.60369532]), 'distance': 27.499999999999996, 'localFrame': array([[-0.24470016, -0.38621173, -0.88936063],
       [ 0.84472079, -0.53520724,  0.        ],
       [-0.47599225, -0.75126141,  0.45720638]]), 'currentState': array([18.34855754,  3.16564441,  0.83098624, -0.24470016, -0.38621173,
       -0.88936063]), 'targetState': array([45.99887141, -0.32222457, 42.        ]), 'previousTarget': array([33.95902762,  1.82869709, 24.52858827])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6273763011866492
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([45.99887141, -0.32222457, 42.        ]), 'distance': 2.5567519687162603, 'localFrame': array([[ 0.77033944,  0.12110184,  0.62602834],
       [-0.15529853,  0.98786759,  0.        ],
       [-0.61843311, -0.09722128,  0.7798003 ]]), 'currentState': array([47.7356746 , -0.24794382, 40.12517262,  0.77033944,  0.12110184,
        0.62602834]), 'targetState': array([45.99887141, -0.32222457, 42.        ]), 'previousTarget': array([45.99887141, -0.32222457, 42.        ])}
episode index:19677
target thresh 86.72109226758462
target distance 31.0
model initialize at round 19677
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.2410312 ,  19.77796011,  27.19604431]), 'distance': 27.5, 'localFrame': array([[-0.89693828,  0.35044136, -0.26961559],
       [-0.36391796, -0.93143101,  0.        ],
       [-0.25112832,  0.09811795,  0.96296803]]), 'currentState': array([-0.16891639, 34.91524805, 45.33682981, -0.89693828,  0.35044136,
       -0.26961559]), 'targetState': array([-24.47742645,   8.76673227,  14.        ]), 'previousTarget': array([-13.5724269 ,  19.40472143,  27.08329607])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273812612453326
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.47742645,   8.76673227,  14.        ]), 'distance': 3.735257564400714, 'localFrame': array([[ 0.22747123, -0.84058286, -0.49160685],
       [ 0.96528043,  0.26121581,  0.        ],
       [ 0.12841548, -0.47453847,  0.87081726]]), 'currentState': array([-24.95946695,  11.21887857,  11.223894  ,   0.22747123,
        -0.84058286,  -0.49160685]), 'targetState': array([-24.47742645,   8.76673227,  14.        ]), 'previousTarget': array([-24.47742645,   8.76673227,  14.        ])}
episode index:19678
target thresh 86.72242009196553
target distance 79.0
model initialize at round 19678
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.05434421, 34.86126861, 64.25783157]), 'distance': 27.5, 'localFrame': array([[-0.59495762,  0.19715183, -0.77920254],
       [-0.314551  , -0.94924057,  0.        ],
       [-0.73965066,  0.24509894,  0.62677221]]), 'currentState': array([ 5.76521909, 42.59895472, 90.61530433, -0.59495762,  0.19715183,
       -0.77920254]), 'targetState': array([ 9.56132805, 19.81365706, 13.        ]), 'previousTarget': array([ 7.3938901 , 34.24308448, 65.50414516])}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.6273685499991147
{'scaleFactor': 20, 'timeStep': 98, 'trapCount': 24, 'trapConfig': [], 'currentTarget': array([ 9.56132805, 19.81365706, 13.        ]), 'distance': 3.036010015131982, 'localFrame': array([[-0.32421216,  0.74854431, -0.57841844],
       [-0.91762584, -0.39744535,  0.        ],
       [-0.22988972,  0.53077171,  0.81574022]]), 'currentState': array([ 9.11749731, 16.81999814, 12.7583859 , -0.32421216,  0.74854431,
       -0.57841844]), 'targetState': array([ 9.56132805, 19.81365706, 13.        ]), 'previousTarget': array([ 9.56132805, 19.81365706, 13.        ])}
episode index:19679
target thresh 86.72374778357066
target distance 58.47391478787651
model initialize at round 19679
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.20036983,   6.47382127,  97.78213177]), 'distance': 27.5, 'localFrame': array([[-0.67685109, -0.5985347 , -0.42851934],
       [ 0.6624384 , -0.74911639,  0.        ],
       [-0.32101086, -0.28386767,  0.90353261]]), 'currentState': array([12.298315  ,  6.65662599, 97.58486209, -0.67685109, -0.5985347 ,
       -0.42851934]), 'targetState': array([-45.57041709,   6.27192842,  98.        ]), 'previousTarget': array([-14.58486777,   7.00138598,  98.52990379])}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6273644741908013
{'scaleFactor': 20, 'timeStep': 61, 'trapCount': 28, 'trapConfig': [], 'currentTarget': array([-45.57041709,   6.27192842,  98.        ]), 'distance': 2.587169243710633, 'localFrame': array([[-0.89366349, -0.16405381, -0.41767441],
       [ 0.18055733, -0.98356446,  0.        ],
       [-0.4108097 , -0.07541418,  0.90859677]]), 'currentState': array([-43.61796011,   5.67689782,  99.58974684,  -0.89366349,
        -0.16405381,  -0.41767441]), 'targetState': array([-45.57041709,   6.27192842,  98.        ]), 'previousTarget': array([-45.57041709,   6.27192842,  98.        ])}
episode index:19680
target thresh 86.72507534241325
target distance 60.21324332050878
model initialize at round 19680
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.64351563,   1.72030657,  17.83053954]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.24903605, -0.67413142, -0.69536169],
       [ 0.9380395 ,  0.34652835,  0.        ],
       [ 0.24096254, -0.65227673,  0.71865995]]), 'currentState': array([-31.65386603,  28.55623902,  23.49103603,   0.24903605,
        -0.67413142,  -0.69536169]), 'targetState': array([-27.21761927, -30.66270049,  11.        ]), 'previousTarget': array([-29.45290539,   2.82146386,  18.78530229])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6273627264123455
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-27.21761927, -30.66270049,  11.        ]), 'distance': 3.2130080054677386, 'localFrame': array([[ 0.86815635,  0.20904444, -0.45011661],
       [-0.23410024,  0.97221246,  0.        ],
       [ 0.43760898,  0.10537241,  0.89296978]]), 'currentState': array([-27.96973281, -27.81057634,  12.27402256,   0.86815635,
         0.20904444,  -0.45011661]), 'targetState': array([-27.21761927, -30.66270049,  11.        ]), 'previousTarget': array([-27.21761927, -30.66270049,  11.        ])}
episode index:19681
target thresh 86.72640276850659
target distance 36.69671826033687
model initialize at round 19681
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.09451822, -16.96158959,  28.87228388]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.55666852, -0.82909006, -0.05224777],
       [ 0.83022402,  0.55742988,  0.        ],
       [ 0.02912447, -0.04337735,  0.99863415]]), 'currentState': array([-1.96861081, 10.25704106, 25.04709011,  0.55666852, -0.82909006,
       -0.05224777]), 'targetState': array([ -0.83682453, -24.98599057,  30.        ]), 'previousTarget': array([ -1.2928564 , -15.59554238,  28.97642638])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273676861526943
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.83682453, -24.98599057,  30.        ]), 'distance': 3.6121304280038418, 'localFrame': array([[ 0.08199332,  0.72008684,  0.68902253],
       [-0.99357965,  0.11313482,  0.        ],
       [-0.07795244, -0.68459876,  0.72473992]]), 'currentState': array([  0.18262803, -23.10216494,  27.09149459,   0.08199332,
         0.72008684,   0.68902253]), 'targetState': array([ -0.83682453, -24.98599057,  30.        ]), 'previousTarget': array([ -0.83682453, -24.98599057,  30.        ])}
episode index:19682
target thresh 86.72773006186397
target distance 14.387749801220043
model initialize at round 19682
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.36950744, -14.20177313,  11.        ]), 'distance': 18.981582738100812, 'localFrame': array([[-0.8286349 ,  0.28475225, -0.48195473],
       [-0.32498689, -0.94571852,  0.        ],
       [-0.45579352,  0.15662897,  0.87619612]]), 'currentState': array([  5.70390175, -19.61943964,  23.6505077 ,  -0.8286349 ,
         0.28475225,  -0.48195473]), 'targetState': array([ -7.36950744, -14.20177313,  11.        ]), 'previousTarget': array([ -7.36950744, -14.20177313,  11.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6273808455890385
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.36950744, -14.20177313,  11.        ]), 'distance': 2.336676789674678, 'localFrame': array([[-0.16440191,  0.79328921, -0.58622882],
       [-0.97919348, -0.20292887,  0.        ],
       [-0.11896275,  0.57403144,  0.81014552]]), 'currentState': array([ -6.76765226, -15.05192467,  13.09166707,  -0.16440191,
         0.79328921,  -0.58622882]), 'targetState': array([ -7.36950744, -14.20177313,  11.        ]), 'previousTarget': array([ -7.36950744, -14.20177313,  11.        ])}
episode index:19683
target thresh 86.72905722249865
target distance 45.09944515363516
model initialize at round 19683
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.59640012, -11.42123925,  38.32835506]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.56978389, -0.48976046,  0.65990985],
       [ 0.65184516,  0.75835209,  0.        ],
       [-0.50044401,  0.43015904,  0.75134479]]), 'currentState': array([-1.15345022, 11.60157547, 26.09560715,  0.56978389, -0.48976046,
        0.65990985]), 'targetState': array([ 15.94490424, -33.38802223,  50.        ]), 'previousTarget': array([  6.69775419, -10.9190123 ,  37.54474149])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6273833018074632
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.94490424, -33.38802223,  50.        ]), 'distance': 4.0179835432527495, 'localFrame': array([[ 0.79775701,  0.03254225,  0.60210029],
       [-0.04075829,  0.99916904,  0.        ],
       [-0.60159996, -0.02454058,  0.79842047]]), 'currentState': array([ 1.29656201e+01, -3.19923908e+01,  4.76934288e+01,  7.97757010e-01,
        3.25422500e-02,  6.02100286e-01]), 'targetState': array([ 15.94490424, -33.38802223,  50.        ]), 'previousTarget': array([ 15.94490424, -33.38802223,  50.        ])}
episode index:19684
target thresh 86.7303842504239
target distance 43.520138422134636
model initialize at round 19684
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.38394836, 22.92218928,  2.04079308]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.02643032,  0.80976459,  0.58615932],
       [-0.99946776,  0.03262214,  0.        ],
       [-0.01912177, -0.58584735,  0.81019581]]), 'currentState': array([ 0.76346059, -2.45342334,  5.09993933,  0.02643032,  0.80976459,
        0.58615932]), 'targetState': array([-16.15340633,  39.85056416,   0.        ]), 'previousTarget': array([-8.75381674, 21.70319025,  1.66795186])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273514306719891
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 64, 'trapConfig': [], 'currentTarget': array([-16.15340633,  39.85056416,   0.        ]), 'distance': 13.18800242581762, 'localFrame': array([[-0.3383438 ,  0.78640948, -0.51680132],
       [-0.9185896 , -0.3952128 ,  0.        ],
       [-0.2042465 ,  0.47472832,  0.85610536]]), 'currentState': array([-9.53325488, 28.8839534 ,  3.13535504, -0.3383438 ,  0.78640948,
       -0.51680132]), 'targetState': array([-16.15340633,  39.85056416,   0.        ]), 'previousTarget': array([-16.15340633,  39.85056416,   0.        ])}
episode index:19685
target thresh 86.731711145653
target distance 58.12148699337865
model initialize at round 19685
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.52084811,  3.58018131, 36.65312277]), 'distance': 27.500000000000004, 'localFrame': array([[-0.91389304, -0.02802183, -0.40498678],
       [ 0.03064764, -0.99953025,  0.        ],
       [-0.40479653, -0.01241189,  0.91432254]]), 'currentState': array([-8.23533146, 27.83594567, 25.57054144, -0.91389304, -0.02802183,
       -0.40498678]), 'targetState': array([  7.77719795, -30.00858531,  52.        ]), 'previousTarget': array([-0.81611796,  3.52295966, 37.57695841])}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.6273425254713183
{'scaleFactor': 20, 'timeStep': 80, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([  7.77719795, -30.00858531,  52.        ]), 'distance': 2.4652916110514544, 'localFrame': array([[ 0.78547   ,  0.36386305,  0.50064015],
       [-0.4203324 ,  0.9073702 ,  0.        ],
       [-0.45426596, -0.21043528,  0.8656555 ]]), 'currentState': array([  7.85457011, -28.91886581,  49.78998016,   0.78547   ,
         0.36386305,   0.50064015]), 'targetState': array([  7.77719795, -30.00858531,  52.        ]), 'previousTarget': array([  7.77719795, -30.00858531,  52.        ])}
episode index:19686
target thresh 86.7330379081992
target distance 47.0
model initialize at round 19686
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.3013342 ,  -9.37565382,  39.24260651]), 'distance': 27.499999999999996, 'localFrame': array([[-0.29978875, -0.65193585,  0.69649576],
       [ 0.90854428, -0.41778858,  0.        ],
       [ 0.29098798,  0.63279724,  0.7175609 ]]), 'currentState': array([  4.37397339, -10.60108834,  16.0179718 ,  -0.29978875,
        -0.65193585,   0.69649576]), 'targetState': array([-24.68139889,  -8.17487301,  62.        ]), 'previousTarget': array([-10.23469136,  -8.64030683,  38.4401858 ])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6273390167403321
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.68139889,  -8.17487301,  62.        ]), 'distance': 2.9148873020942885, 'localFrame': array([[ 0.45451527, -0.48276636,  0.74856697],
       [ 0.72808939,  0.68548219,  0.        ],
       [-0.51312933,  0.54502367,  0.66305919]]), 'currentState': array([-23.17625709,  -8.80798404,  59.58540136,   0.45451527,
        -0.48276636,   0.74856697]), 'targetState': array([-24.68139889,  -8.17487301,  62.        ]), 'previousTarget': array([-24.68139889,  -8.17487301,  62.        ])}
episode index:19687
target thresh 86.73436453807577
target distance 54.526121835840065
model initialize at round 19687
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.76529461, -14.03018303,  80.58021051]), 'distance': 27.5, 'localFrame': array([[-0.99049741, -0.0817896 , -0.11056829],
       [ 0.08229418, -0.99660808,  0.        ],
       [-0.11019326, -0.00909913,  0.99386853]]), 'currentState': array([ 1.92601978e+01,  1.19467000e+01,  8.56079310e+01, -9.90497408e-01,
       -8.17895957e-02, -1.10568294e-01]), 'targetState': array([  3.44678562, -42.861634  ,  75.        ]), 'previousTarget': array([ 12.68416621, -14.12590391,  80.27008508])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.627342525182089
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.44678562, -42.861634  ,  75.        ]), 'distance': 3.9482040563267913, 'localFrame': array([[ 0.03036023, -0.76159568,  0.64734093],
       [ 0.99920638,  0.03983233,  0.        ],
       [-0.0257851 ,  0.64682718,  0.76220058]]), 'currentState': array([ 5.59653461e+00, -3.98979287e+01,  7.64776148e+01,  3.03602264e-02,
       -7.61595681e-01,  6.47340927e-01]), 'targetState': array([  3.44678562, -42.861634  ,  75.        ]), 'previousTarget': array([  3.44678562, -42.861634  ,  75.        ])}
episode index:19688
target thresh 86.73569103529599
target distance 6.984586917898518
model initialize at round 19688
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.2355173 , 14.27253377, 24.        ]), 'distance': 10.432004440393564, 'localFrame': array([[-0.39105081,  0.59949562, -0.69834395],
       [-0.83756239, -0.54634169,  0.        ],
       [-0.38153441,  0.58490663,  0.71576234]]), 'currentState': array([-16.5698123 ,   9.48301888,  18.33472149,  -0.39105081,
         0.59949562,  -0.69834395]), 'targetState': array([-9.2355173 , 14.27253377, 24.        ]), 'previousTarget': array([-9.2355173 , 14.27253377, 24.        ])}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6273575285936003
{'scaleFactor': 20, 'timeStep': 9, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.2355173 , 14.27253377, 24.        ]), 'distance': 3.3499609809646556, 'localFrame': array([[-0.67076093,  0.46299106,  0.57941268],
       [-0.56806327, -0.82298489,  0.        ],
       [ 0.47684788, -0.32914306,  0.81503432]]), 'currentState': array([-9.78955217, 16.50845952, 21.56773357, -0.67076093,  0.46299106,
        0.57941268]), 'targetState': array([-9.2355173 , 14.27253377, 24.        ]), 'previousTarget': array([-9.2355173 , 14.27253377, 24.        ])}
episode index:19689
target thresh 86.73701739987314
target distance 34.0
model initialize at round 19689
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.4366762 , 33.16187767, 22.24537562]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.1198883 , -0.44057867, -0.88967254],
       [ 0.9649135 ,  0.26256796,  0.        ],
       [ 0.2335995 , -0.85845704,  0.45659914]]), 'currentState': array([-14.81656075,  23.59875504,  45.38181685,   0.1198883 ,
        -0.44057867,  -0.88967254]), 'targetState': array([ 1.11075208, 36.98332367, 13.        ]), 'previousTarget': array([-3.49595422, 33.11188717, 23.27181938])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6273576538400507
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.11075208, 36.98332367, 13.        ]), 'distance': 2.9513600695216176, 'localFrame': array([[-0.49579456, -0.20159898, -0.84471629],
       [ 0.37666962, -0.92634767,  0.        ],
       [-0.78250096, -0.31817896,  0.53521435]]), 'currentState': array([-0.08080435, 38.76726145, 15.02689061, -0.49579456, -0.20159898,
       -0.84471629]), 'targetState': array([ 1.11075208, 36.98332367, 13.        ]), 'previousTarget': array([ 1.11075208, 36.98332367, 13.        ])}
episode index:19690
target thresh 86.73834363182044
target distance 16.02658752158576
model initialize at round 19690
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.65849587, -10.55445242,  67.        ]), 'distance': 19.156028054618666, 'localFrame': array([[-0.00917295,  0.98700282,  0.16044091],
       [-0.99995682, -0.00929334,  0.        ],
       [ 0.00149103, -0.16043398,  0.98704545]]), 'currentState': array([-2.75802001e+01, -1.95181651e+01,  6.64889302e+01, -9.17294924e-03,
        9.87002822e-01,  1.60440913e-01]), 'targetState': array([-10.65849587, -10.55445242,  67.        ]), 'previousTarget': array([-10.65849587, -10.55445242,  67.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6273717224206798
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.65849587, -10.55445242,  67.        ]), 'distance': 2.392763771457836, 'localFrame': array([[ 0.44569344,  0.74592642,  0.49492539],
       [-0.85843706,  0.51291891,  0.        ],
       [-0.25385659, -0.4248623 ,  0.86893548]]), 'currentState': array([-12.74426119,  -9.44586199,  66.61799379,   0.44569344,
         0.74592642,   0.49492539]), 'targetState': array([-10.65849587, -10.55445242,  67.        ]), 'previousTarget': array([-10.65849587, -10.55445242,  67.        ])}
episode index:19691
target thresh 86.7396697311512
target distance 11.0
model initialize at round 19691
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.64500273, -6.80396693, 85.        ]), 'distance': 11.735680699210906, 'localFrame': array([[ 0.86329063,  0.2319365 , -0.44825745],
       [-0.25946452,  0.96575264,  0.        ],
       [ 0.43290582,  0.1163069 ,  0.8939045 ]]), 'currentState': array([ 1.12616106e+00, -1.82295619e-02,  9.45608981e+01,  8.63290633e-01,
        2.31936499e-01, -4.48257451e-01]), 'targetState': array([ 1.64500273, -6.80396693, 85.        ]), 'previousTarget': array([ 1.64500273, -6.80396693, 85.        ])}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6273876734884729
{'scaleFactor': 20, 'timeStep': 7, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.64500273, -6.80396693, 85.        ]), 'distance': 2.9702445690227406, 'localFrame': array([[ 0.90014107, -0.43438726, -0.03246184],
       [ 0.43461631,  0.90061571,  0.        ],
       [ 0.02923565, -0.01410845,  0.99947298]]), 'currentState': array([ 3.34039239e+00, -4.87955093e+00,  8.64982088e+01,  9.00141066e-01,
       -4.34387258e-01, -3.24618443e-02]), 'targetState': array([ 1.64500273, -6.80396693, 85.        ]), 'previousTarget': array([ 1.64500273, -6.80396693, 85.        ])}
episode index:19692
target thresh 86.74099569787863
target distance 11.0
model initialize at round 19692
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.42231511, -3.18314145, 74.        ]), 'distance': 13.52025198528462, 'localFrame': array([[ 0.30881393,  0.70932982, -0.63362857],
       [-0.91687632,  0.39917141,  0.        ],
       [ 0.25292641,  0.58095904,  0.7736374 ]]), 'currentState': array([  1.61564719, -11.9050085 ,  84.29929783,   0.30881393,
         0.70932982,  -0.63362857]), 'targetState': array([ 2.42231511, -3.18314145, 74.        ]), 'previousTarget': array([ 2.42231511, -3.18314145, 74.        ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273558150782008
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 93, 'trapConfig': [], 'currentTarget': array([ 2.42231511, -3.18314145, 74.        ]), 'distance': 8.74802176363532, 'localFrame': array([[ 0.85100684,  0.49830127,  0.16578058],
       [-0.50529318,  0.86294774,  0.        ],
       [-0.14305998, -0.0837678 ,  0.98616266]]), 'currentState': array([-2.83493543, -4.82772452, 80.79592143,  0.85100684,  0.49830127,
        0.16578058]), 'targetState': array([ 2.42231511, -3.18314145, 74.        ]), 'previousTarget': array([ 2.42231511, -3.18314145, 74.        ])}
episode index:19693
target thresh 86.74232153201604
target distance 40.25838624447744
model initialize at round 19693
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.95305618,  1.4108744 , 52.69787031]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.83845032,  0.33617544,  0.42893722],
       [-0.37214953,  0.92817279,  0.        ],
       [-0.39812786, -0.15962879,  0.9033343 ]]), 'currentState': array([ 12.01537174, -17.24845031,  35.18132365,   0.83845032,
         0.33617544,   0.42893722]), 'targetState': array([-9.70942727, 23.03751337, 73.        ]), 'previousTarget': array([ 0.87635887,  1.6006123 , 52.76565093])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6273589679811555
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.70942727, 23.03751337, 73.        ]), 'distance': 3.487446594266722, 'localFrame': array([[ 0.24868055,  0.69146625,  0.6782569 ],
       [-0.94099462,  0.33842152,  0.        ],
       [-0.22953673, -0.63823609,  0.73482487]]), 'currentState': array([-6.86461652, 21.04116482, 72.71029665,  0.24868055,  0.69146625,
        0.6782569 ]), 'targetState': array([-9.70942727, 23.03751337, 73.        ]), 'previousTarget': array([-9.70942727, 23.03751337, 73.        ])}
episode index:19694
target thresh 86.74364723357665
target distance 76.0
model initialize at round 19694
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.55282921, -10.37326442,  56.11181467]), 'distance': 27.5, 'localFrame': array([[-0.8295895 , -0.34575051, -0.43844936],
       [ 0.38469902, -0.92304207,  0.        ],
       [-0.40470721, -0.16867104,  0.89875589]]), 'currentState': array([-2.37061164, -1.55873617, 82.14804565, -0.8295895 , -0.34575051,
       -0.43844936]), 'targetState': array([-1.02564901e-02, -2.69999981e+01,  7.00000000e+00]), 'previousTarget': array([ -0.50109639, -10.4897531 ,  56.89343125])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6273543429117078
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.02564901e-02, -2.69999981e+01,  7.00000000e+00]), 'distance': 3.1656058561377476, 'localFrame': array([[-0.22387198, -0.31385371, -0.92270103],
       [ 0.81411269, -0.58070692,  0.        ],
       [-0.53581887, -0.75118261,  0.3855163 ]]), 'currentState': array([  1.01167469, -28.78357072,   9.40740226,  -0.22387198,
        -0.31385371,  -0.92270103]), 'targetState': array([-1.02564901e-02, -2.69999981e+01,  7.00000000e+00]), 'previousTarget': array([-1.02564901e-02, -2.69999981e+01,  7.00000000e+00])}
episode index:19695
target thresh 86.74497280257374
target distance 22.0
model initialize at round 19695
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.92344241,   4.56250485,  28.37786876]), 'distance': 27.5, 'localFrame': array([[-0.50276255,  0.54276579,  0.67278162],
       [-0.73362479, -0.67955475,  0.        ],
       [ 0.45719195, -0.49356928,  0.73984112]]), 'currentState': array([-3.36833198, -1.06393825,  8.87656073, -0.50276255,  0.54276579,
        0.67278162]), 'targetState': array([-23.46686833,   5.03051598,  30.        ]), 'previousTarget': array([-21.15937966,   4.16650541,  27.51765257])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6273564558999114
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-23.46686833,   5.03051598,  30.        ]), 'distance': 4.166464974928269, 'localFrame': array([[-0.79261797,  0.60645684,  0.06298289],
       [-0.60766329, -0.79419476,  0.        ],
       [ 0.05002068, -0.03827239,  0.99801461]]), 'currentState': array([-21.98552489,   2.25796162,  32.73459218,  -0.79261797,
         0.60645684,   0.06298289]), 'targetState': array([-23.46686833,   5.03051598,  30.        ]), 'previousTarget': array([-23.46686833,   5.03051598,  30.        ])}
episode index:19696
target thresh 86.74629823902055
target distance 61.0
model initialize at round 19696
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.28577465, 29.40592009, 73.01306014]), 'distance': 27.500000000000007, 'localFrame': array([[ 0.65285251, -0.16748425, -0.73873719],
       [ 0.24849531,  0.9686331 ,  0.        ],
       [ 0.71556529, -0.18357272,  0.6739936 ]]), 'currentState': array([ 0.66696358, 32.61881712, 97.7300689 ,  0.65285251, -0.16748425,
       -0.73873719]), 'targetState': array([28.74448672, 24.85466723, 38.        ]), 'previousTarget': array([11.29062932, 29.10943014, 74.34694412])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.627356581156311
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([28.74448672, 24.85466723, 38.        ]), 'distance': 1.9447756037455233, 'localFrame': array([[ 0.48469181, -0.24929499, -0.83840674],
       [ 0.45738416,  0.88926921,  0.        ],
       [ 0.7455693 , -0.38347396,  0.54504509]]), 'currentState': array([28.85606275, 26.13193485, 39.46228943,  0.48469181, -0.24929499,
       -0.83840674]), 'targetState': array([28.74448672, 24.85466723, 38.        ]), 'previousTarget': array([28.74448672, 24.85466723, 38.        ])}
episode index:19697
target thresh 86.74762354293036
target distance 28.51217455340561
model initialize at round 19697
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.6198979 ,   4.43533234,   8.79549151]), 'distance': 27.499999999999996, 'localFrame': array([[-0.76784388, -0.16357248, -0.6194028 ],
       [ 0.20835311, -0.97805367,  0.        ],
       [-0.60580918, -0.1290545 ,  0.78507336]]), 'currentState': array([  2.64147237, -11.990595  ,  17.50845803,  -0.76784388,
        -0.16357248,  -0.6194028 ]), 'targetState': array([-24.12061018,   9.70547085,   6.        ]), 'previousTarget': array([-16.28264383,   3.67384604,   9.29878719])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6273658395219859
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.12061018,   9.70547085,   6.        ]), 'distance': 4.0769594699341045, 'localFrame': array([[-0.88222458,  0.09818471, -0.46047753],
       [-0.1106093 , -0.99386397,  0.        ],
       [-0.45765202,  0.0509331 ,  0.88767136]]), 'currentState': array([-22.85737586,   6.81229385,   8.5797993 ,  -0.88222458,
         0.09818471,  -0.46047753]), 'targetState': array([-24.12061018,   9.70547085,   6.        ]), 'previousTarget': array([-24.12061018,   9.70547085,   6.        ])}
episode index:19698
target thresh 86.74894871431637
target distance 44.86547615848801
model initialize at round 19698
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.22622014, 15.30439488, 40.41790815]), 'distance': 27.5, 'localFrame': array([[-0.50479313, -0.69533553, -0.51155879],
       [ 0.80923703, -0.58748227,  0.        ],
       [-0.30053172, -0.41397232,  0.85924828]]), 'currentState': array([ 3.43577876, 42.02793032, 34.55991444, -0.50479313, -0.69533553,
       -0.51155879]), 'targetState': array([ 7.93254108, -1.03672173, 44.        ]), 'previousTarget': array([ 6.30523312, 16.97111311, 40.38763416])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6273730823925013
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.93254108, -1.03672173, 44.        ]), 'distance': 2.3802036247415908, 'localFrame': array([[ 0.05316343, -0.99824291,  0.02616768],
       [ 0.99858486,  0.05318164,  0.        ],
       [-0.00139164,  0.02613065,  0.99965757]]), 'currentState': array([ 7.42162393e+00,  9.11779152e-01,  4.27320579e+01,  5.31634308e-02,
       -9.98242907e-01,  2.61676822e-02]), 'targetState': array([ 7.93254108, -1.03672173, 44.        ]), 'previousTarget': array([ 7.93254108, -1.03672173, 44.        ])}
episode index:19699
target thresh 86.7502737531919
target distance 19.0
model initialize at round 19699
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 33.57796091, -19.83735217,  40.        ]), 'distance': 23.037632507349475, 'localFrame': array([[ 0.69467662,  0.13830246, -0.70590143],
       [-0.19525694,  0.98075212,  0.        ],
       [ 0.69231432,  0.13783216,  0.70831009]]), 'currentState': array([ 19.47801063, -23.22133833,  57.90174716,   0.69467662,
         0.13830246,  -0.70590143]), 'targetState': array([ 33.57796091, -19.83735217,  40.        ]), 'previousTarget': array([ 33.57796091, -19.83735217,  40.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6273857802575068
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 33.57796091, -19.83735217,  40.        ]), 'distance': 2.523637443934404, 'localFrame': array([[ 0.84560473,  0.52846209,  0.07536879],
       [-0.52996947,  0.84801672,  0.        ],
       [-0.06391399, -0.03994316,  0.99715573]]), 'currentState': array([ 31.39145685, -19.01250197,  40.95266369,   0.84560473,
         0.52846209,   0.07536879]), 'targetState': array([ 33.57796091, -19.83735217,  40.        ]), 'previousTarget': array([ 33.57796091, -19.83735217,  40.        ])}
episode index:19700
target thresh 86.75159865957016
target distance 32.86803607896452
model initialize at round 19700
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([23.42389432,  2.10111734, 89.25329719]), 'distance': 27.5, 'localFrame': array([[-0.73721299, -0.54718527, -0.3963651 ],
       [ 0.59600202, -0.80298293,  0.        ],
       [-0.31827441, -0.2362344 ,  0.91809297]]), 'currentState': array([-0.        ,  0.        , 75.        , -0.73721299, -0.54718527,
       -0.3963651 ]), 'targetState': array([32.86803608,  2.94825445, 95.        ]), 'previousTarget': array([23.42389432,  2.10111734, 89.25329719])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6273907340444059
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([32.86803608,  2.94825445, 95.        ]), 'distance': 2.0503233241179655, 'localFrame': array([[ 0.69595565, -0.43568684,  0.57080882],
       [ 0.5306246 ,  0.84760695,  0.        ],
       [-0.48382152,  0.3028852 ,  0.821083  ]]), 'currentState': array([30.89646613,  2.60078248, 95.4427199 ,  0.69595565, -0.43568684,
        0.57080882]), 'targetState': array([32.86803608,  2.94825445, 95.        ]), 'previousTarget': array([32.86803608,  2.94825445, 95.        ])}
episode index:19701
target thresh 86.7529234334644
target distance 27.88994917712579
model initialize at round 19701
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.31699293,  0.93272144, 89.55707454]), 'distance': 27.500000000000004, 'localFrame': array([[-0.97107541, -0.22311706, -0.08503714],
       [ 0.22392818, -0.97460565,  0.        ],
       [-0.08287767, -0.01904221,  0.99637778]]), 'currentState': array([ 2.63650510e+01, -1.08781463e+01,  9.57574220e+01, -9.71075415e-01,
       -2.23117063e-01, -8.50371368e-02]), 'targetState': array([ 0.15637841,  1.99387708, 89.        ]), 'previousTarget': array([ 3.4670514 ,  0.49309298, 89.71222926])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6274012468363765
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.15637841,  1.99387708, 89.        ]), 'distance': 3.0613136104169185, 'localFrame': array([[-0.17685062,  0.61643334, -0.7672899 ],
       [-0.96122401, -0.27576876,  0.        ],
       [-0.21159458,  0.73753748,  0.6413004 ]]), 'currentState': array([-1.84997903,  1.47826221, 91.25395478, -0.17685062,  0.61643334,
       -0.7672899 ]), 'targetState': array([ 0.15637841,  1.99387708, 89.        ]), 'previousTarget': array([ 0.15637841,  1.99387708, 89.        ])}
episode index:19702
target thresh 86.75424807488788
target distance 40.8937352103689
model initialize at round 19702
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.75616324,   1.87225083,  44.44441094]), 'distance': 27.5, 'localFrame': array([[-0.61472603,  0.25930496, -0.74489788],
       [-0.38865905, -0.92138165,  0.        ],
       [-0.68633523,  0.2895113 ,  0.6671785 ]]), 'currentState': array([-16.17144218, -23.86085318,  53.45591099,  -0.61472603,
         0.25930496,  -0.74489788]), 'targetState': array([-21.9219175 ,  17.41922883,  39.        ]), 'previousTarget': array([-19.38584175,   1.8455771 ,  45.09331543])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6274092799731562
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.9219175 ,  17.41922883,  39.        ]), 'distance': 2.252629964616238, 'localFrame': array([[-0.16835503,  0.98408789,  0.05681212],
       [-0.98567987, -0.16862739,  0.        ],
       [ 0.00958008, -0.05599856,  0.99838489]]), 'currentState': array([-21.38661189,  15.2435215 ,  38.76743331,  -0.16835503,
         0.98408789,   0.05681212]), 'targetState': array([-21.9219175 ,  17.41922883,  39.        ]), 'previousTarget': array([-21.9219175 ,  17.41922883,  39.        ])}
episode index:19703
target thresh 86.75557258385385
target distance 23.325708458095043
model initialize at round 19703
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.73263711, -12.63673294,  48.81665255]), 'distance': 27.5, 'localFrame': array([[-0.54344726, -0.47904302, -0.68933509],
       [ 0.66125735, -0.75015913,  0.        ],
       [-0.51711101, -0.45582789,  0.72444264]]), 'currentState': array([16.03745716,  5.23297772, 55.60390631, -0.54344726, -0.47904302,
       -0.68933509]), 'targetState': array([ -6.11140449, -14.78684331,  48.        ]), 'previousTarget': array([ -2.8022683 , -11.94286667,  49.27679833])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6274197907568284
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.11140449, -14.78684331,  48.        ]), 'distance': 2.919562505456289, 'localFrame': array([[-0.137549  , -0.97911556, -0.14970969],
       [ 0.99027597, -0.13911685,  0.        ],
       [-0.02082714, -0.14825391,  0.98873   ]]), 'currentState': array([ -5.57005453, -14.2670624 ,  50.82145587,  -0.137549  ,
        -0.97911556,  -0.14970969]), 'targetState': array([ -6.11140449, -14.78684331,  48.        ]), 'previousTarget': array([ -6.11140449, -14.78684331,  48.        ])}
episode index:19704
target thresh 86.75689696037553
target distance 80.0
model initialize at round 19704
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.86721311,   4.78457984,  72.14769569]), 'distance': 27.500000000000004, 'localFrame': array([[-0.88804708, -0.04368551, -0.45767233],
       [ 0.04913337, -0.99879223,  0.        ],
       [-0.45711956, -0.02248698,  0.88912094]]), 'currentState': array([-4.78298974e+00,  8.83929484e-01,  9.86810172e+01, -8.88047081e-01,
       -4.36855085e-02, -4.57672327e-01]), 'targetState': array([-22.82494062,  12.45078655,  20.        ]), 'previousTarget': array([-10.02763839,   5.16843484,  73.48631885])}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.6274089067111738
{'scaleFactor': 20, 'timeStep': 89, 'trapCount': 23, 'trapConfig': [], 'currentTarget': array([-22.82494062,  12.45078655,  20.        ]), 'distance': 2.962757670004486, 'localFrame': array([[ 0.72908323, -0.41341527, -0.54545895],
       [ 0.49325469,  0.86988494,  0.        ],
       [ 0.47448652, -0.26905018,  0.83813754]]), 'currentState': array([-23.7123219 ,  13.37137222,  22.67264092,   0.72908323,
        -0.41341527,  -0.54545895]), 'targetState': array([-22.82494062,  12.45078655,  20.        ]), 'previousTarget': array([-22.82494062,  12.45078655,  20.        ])}
episode index:19705
target thresh 86.75822120446618
target distance 32.0
model initialize at round 19705
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.83316356, 29.96026698, 54.53094013]), 'distance': 27.5, 'localFrame': array([[-0.74039454,  0.25850507, -0.62047647],
       [-0.32963123, -0.94410977,  0.        ],
       [-0.5857979 ,  0.20452842,  0.78422506]]), 'currentState': array([ 7.26885411, 12.63493392, 74.89425817, -0.74039454,  0.25850507,
       -0.62047647]), 'targetState': array([-2.49506968, 38.92010569, 44.        ]), 'previousTarget': array([ 1.39861033, 28.80765954, 55.76918519])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6274169382362978
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.49506968, 38.92010569, 44.        ]), 'distance': 2.238829317608915, 'localFrame': array([[-0.59769645, -0.22888202, -0.76835667],
       [ 0.35761591, -0.93386876,  0.        ],
       [-0.71754429, -0.27477657,  0.6400219 ]]), 'currentState': array([-0.92882857, 38.44840677, 45.52864175, -0.59769645, -0.22888202,
       -0.76835667]), 'targetState': array([-2.49506968, 38.92010569, 44.        ]), 'previousTarget': array([-2.49506968, 38.92010569, 44.        ])}
episode index:19706
target thresh 86.75954531613905
target distance 27.237733603549145
model initialize at round 19706
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-20.47413229,   8.16939527,  35.68917405]), 'distance': 27.499999999999996, 'localFrame': array([[-0.03923322, -0.94078853,  0.33671605],
       [ 0.99913158, -0.04166627,  0.        ],
       [ 0.0140297 ,  0.33642364,  0.94160623]]), 'currentState': array([-38.16227722,  25.68346786,  24.        ,  -0.03923322,
        -0.94078853,   0.33671605]), 'targetState': array([-10.92454362,  -1.28621411,  42.        ]), 'previousTarget': array([-20.47413229,   8.16939527,  35.68917405])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6274218889339038
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-10.92454362,  -1.28621411,  42.        ]), 'distance': 3.783689335656351, 'localFrame': array([[ 0.45949419, -0.71581137, -0.52581287],
       [ 0.84153672,  0.54019991,  0.        ],
       [ 0.28404406, -0.44249084,  0.85060028]]), 'currentState': array([-13.78420092,   1.17862232,  42.25148853,   0.45949419,
        -0.71581137,  -0.52581287]), 'targetState': array([-10.92454362,  -1.28621411,  42.        ]), 'previousTarget': array([-10.92454362,  -1.28621411,  42.        ])}
episode index:19707
target thresh 86.76086929540736
target distance 48.0
model initialize at round 19707
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.83020216,  5.40757405, 50.22142058]), 'distance': 27.5, 'localFrame': array([[ 0.37206347, -0.59515565, -0.71229104],
       [ 0.84793991,  0.53009236,  0.        ],
       [ 0.37758004, -0.60398   ,  0.70188423]]), 'currentState': array([24.2932714 , 23.20067254, 67.77857553,  0.37206347, -0.59515565,
       -0.71229104]), 'targetState': array([ -6.24846795, -24.20654144,  21.        ]), 'previousTarget': array([12.11347216,  5.82528431, 51.12072452])}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6274149104674978
{'scaleFactor': 20, 'timeStep': 72, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ -6.24846795, -24.20654144,  21.        ]), 'distance': 2.1475551430261945, 'localFrame': array([[-0.7314056 ,  0.24421485,  0.63671418],
       [-0.31670975, -0.9485225 ,  0.        ],
       [ 0.60393773, -0.20165359,  0.7710999 ]]), 'currentState': array([ -7.11906249, -24.79223663,  19.12622857,  -0.7314056 ,
         0.24421485,   0.63671418]), 'targetState': array([ -6.24846795, -24.20654144,  21.        ]), 'previousTarget': array([ -6.24846795, -24.20654144,  21.        ])}
episode index:19708
target thresh 86.76219314228439
target distance 86.0
model initialize at round 19708
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -1.55107759, -13.36010685,  37.83092101]), 'distance': 27.500000000000004, 'localFrame': array([[-0.2807643 , -0.62776696,  0.72600279],
       [ 0.91286096, -0.40827057,  0.        ],
       [ 0.29640558,  0.66273961,  0.68769175]]), 'currentState': array([-4.93157122, -9.2267851 , 10.8543031 , -0.2807643 , -0.62776696,
        0.72600279]), 'targetState': array([  5.73820536, -22.27269627,  96.        ]), 'previousTarget': array([ -0.69768449, -12.63684239,  36.98475513])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6274134665127562
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.73820536, -22.27269627,  96.        ]), 'distance': 2.648992538976977, 'localFrame': array([[ 0.13199733, -0.73231063,  0.66805528],
       [ 0.98414079,  0.17738914,  0.        ],
       [-0.11850575,  0.65746045,  0.74411165]]), 'currentState': array([  6.49410916, -22.37705316,  93.46329336,   0.13199733,
        -0.73231063,   0.66805528]), 'targetState': array([  5.73820536, -22.27269627,  96.        ]), 'previousTarget': array([  5.73820536, -22.27269627,  96.        ])}
episode index:19709
target thresh 86.76351685678333
target distance 26.0
model initialize at round 19709
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-2.38701754,  7.30165013, 55.78775782]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.73595275,  0.13332547, -0.66377546],
       [-0.17825883,  0.98398363,  0.        ],
       [ 0.65314419,  0.11832384,  0.7479319 ]]), 'currentState': array([-3.32796431, 24.77750206, 77.        ,  0.73595275,  0.13332547,
       -0.66377546]), 'targetState': array([-2.17463899,  3.35722285, 51.        ]), 'previousTarget': array([-2.38701754,  7.30165013, 55.78775782])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6274223055347261
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-2.17463899,  3.35722285, 51.        ]), 'distance': 3.625785152382465, 'localFrame': array([[-5.23431561e-02, -9.98625778e-01, -2.59802311e-03],
       [ 9.98629148e-01, -5.23433327e-02,  0.00000000e+00],
       [-1.35989188e-04, -2.59446161e-03,  9.99996625e-01]]), 'currentState': array([-2.52105876e+00,  6.09740253e+00,  5.33489842e+01, -5.23431561e-02,
       -9.98625778e-01, -2.59802311e-03]), 'targetState': array([-2.17463899,  3.35722285, 51.        ]), 'previousTarget': array([-2.17463899,  3.35722285, 51.        ])}
episode index:19710
target thresh 86.76484043891743
target distance 69.0
model initialize at round 19710
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 7.60309096, -3.13367431, 27.35940244]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.77569615,  0.53412178, -0.33616872],
       [-0.5671276 ,  0.82362994,  0.        ],
       [ 0.27687862,  0.19065056,  0.94180178]]), 'currentState': array([10.19925976, -4.12008476,  0.        ,  0.77569615,  0.53412178,
       -0.33616872]), 'targetState': array([ 3.65176123, -1.63237249, 69.        ]), 'previousTarget': array([ 7.60309096, -3.13367431, 27.35940244])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6274237380807685
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 3.65176123, -1.63237249, 69.        ]), 'distance': 3.2820968318438264, 'localFrame': array([[-0.7896736 ,  0.10687801,  0.60414626],
       [-0.13412168, -0.99096487,  0.        ],
       [ 0.59868772, -0.08102911,  0.79687345]]), 'currentState': array([ 5.14984696, -1.33237003, 66.09519411, -0.7896736 ,  0.10687801,
        0.60414626]), 'targetState': array([ 3.65176123, -1.63237249, 69.        ]), 'previousTarget': array([ 3.65176123, -1.63237249, 69.        ])}
episode index:19711
target thresh 86.76616388869995
target distance 19.0
model initialize at round 19711
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.31002801,  1.97582454, 88.        ]), 'distance': 25.366484338172114, 'localFrame': array([[-0.08324194, -0.63243158,  0.77013056],
       [ 0.99144877, -0.13049652,  0.        ],
       [ 0.10049936,  0.76354499,  0.6378863 ]]), 'currentState': array([12.30163235, 15.58835674, 70.7053905 , -0.08324194, -0.63243158,
        0.77013056]), 'targetState': array([-0.31002801,  1.97582454, 88.        ]), 'previousTarget': array([-0.31002801,  1.97582454, 88.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.627436425645953
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.31002801,  1.97582454, 88.        ]), 'distance': 3.2868094681766467, 'localFrame': array([[-0.38974673, -0.90293374, -0.18113019],
       [ 0.91812025, -0.39630191,  0.        ],
       [-0.07178224, -0.16629929,  0.98345913]]), 'currentState': array([ 2.00971494,  3.52633706, 86.26281261, -0.38974673, -0.90293374,
       -0.18113019]), 'targetState': array([-0.31002801,  1.97582454, 88.        ]), 'previousTarget': array([-0.31002801,  1.97582454, 88.        ])}
episode index:19712
target thresh 86.76748720614411
target distance 35.0
model initialize at round 19712
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.03804074, 15.23809011, 75.61097344]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.00256684, -0.89773741,  0.4405235 ],
       [ 0.99999591,  0.00285922,  0.        ],
       [-0.00125955,  0.4405217 ,  0.89774108]]), 'currentState': array([-1.15787011e+01,  2.92356636e+01,  5.39477849e+01,  2.56683793e-03,
       -8.97737412e-01,  4.40523496e-01]), 'targetState': array([ 3.41819648,  7.23297538, 88.        ]), 'previousTarget': array([-2.74231418, 16.2913198 , 74.41011623])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6274444529231387
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.41819648,  7.23297538, 88.        ]), 'distance': 1.9577508551489418, 'localFrame': array([[-0.2876262 ,  0.26127494,  0.92141553],
       [-0.67238613, -0.74020058,  0.        ],
       [ 0.6820323 , -0.61954702,  0.38857872]]), 'currentState': array([ 3.08489936,  7.42194898, 86.08010666, -0.2876262 ,  0.26127494,
        0.92141553]), 'targetState': array([ 3.41819648,  7.23297538, 88.        ]), 'previousTarget': array([ 3.41819648,  7.23297538, 88.        ])}
episode index:19713
target thresh 86.76881039126314
target distance 63.0
model initialize at round 19713
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.06739149,   3.94604875,  71.42808634]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.45703867, -0.55144629, -0.69787007],
       [ 0.7699351 ,  0.63812219,  0.        ],
       [ 0.44532638, -0.53731467,  0.71622438]]), 'currentState': array([-6.46981042, 13.4761457 , 96.97186567,  0.45703867, -0.55144629,
       -0.69787007]), 'targetState': array([-15.19791631,  -9.6448608 ,  35.        ]), 'previousTarget': array([-10.8767861 ,   4.33128203,  72.39033546])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6274427040134034
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-15.19791631,  -9.6448608 ,  35.        ]), 'distance': 3.157402315307971, 'localFrame': array([[ 0.39688076, -0.5713405 , -0.71837016],
       [ 0.82129148,  0.57050881,  0.        ],
       [ 0.4098365 , -0.5899913 ,  0.69566106]]), 'currentState': array([-15.3670469 ,  -7.51195638,  37.32191795,   0.39688076,
        -0.5713405 ,  -0.71837016]), 'targetState': array([-15.19791631,  -9.6448608 ,  35.        ]), 'previousTarget': array([-15.19791631,  -9.6448608 ,  35.        ])}
episode index:19714
target thresh 86.77013344407027
target distance 78.0
model initialize at round 19714
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.2283078 ,  -3.84625232,  54.95728211]), 'distance': 27.5, 'localFrame': array([[ 0.12895481,  0.39626664, -0.90903433],
       [-0.95091539,  0.309451  ,  0.        ],
       [ 0.28130158,  0.86441473,  0.41672124]]), 'currentState': array([-16.23959526, -20.83695967,  76.3730114 ,   0.12895481,
         0.39626664,  -0.90903433]), 'targetState': array([-26.89797507,  39.7554894 ,   0.        ]), 'previousTarget': array([-19.82306537,  -4.08784484,  56.39490945])}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.627436746267638
{'scaleFactor': 20, 'timeStep': 68, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-26.89797507,  39.7554894 ,   0.        ]), 'distance': 2.793854257417964, 'localFrame': array([[-0.72789311,  0.63715982, -0.25337517],
       [-0.65865292, -0.75244689,  0.        ],
       [-0.19065136,  0.16688629,  0.96736809]]), 'currentState': array([-26.37897502,  37.46927267,   1.51969525,  -0.72789311,
         0.63715982,  -0.25337517]), 'targetState': array([-26.89797507,  39.7554894 ,   0.        ]), 'previousTarget': array([-26.89797507,  39.7554894 ,   0.        ])}
episode index:19715
target thresh 86.77145636457873
target distance 40.0
model initialize at round 19715
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.98629032, -18.84379588,  21.32984135]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83417996, -0.27670494, -0.47705155],
       [ 0.31483981, -0.94914482,  0.        ],
       [-0.452791  , -0.15019482,  0.87887532]]), 'currentState': array([-1.42912741, -3.62843229, 43.5528171 , -0.83417996, -0.27670494,
       -0.47705155]), 'targetState': array([-11.06979335, -30.02431806,   5.        ]), 'previousTarget': array([ -6.40025886, -18.01115973,  22.7483592 ])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6274424403043148
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.06979335, -30.02431806,   5.        ]), 'distance': 3.3893690560772023, 'localFrame': array([[ 0.42256639, -0.56052434, -0.71221493],
       [ 0.79851156,  0.60197948,  0.        ],
       [ 0.42873877, -0.56871185,  0.70196146]]), 'currentState': array([ -9.40718412, -27.87568117,   7.02655191,   0.42256639,
        -0.56052434,  -0.71221493]), 'targetState': array([-11.06979335, -30.02431806,   5.        ]), 'previousTarget': array([-11.06979335, -30.02431806,   5.        ])}
episode index:19716
target thresh 86.77277915280176
target distance 51.065071050499284
model initialize at round 19716
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.13430903,  12.82383376,  54.96838704]), 'distance': 27.499999999999996, 'localFrame': array([[-0.22440076,  0.85617204,  0.4654178 ],
       [-0.96732645, -0.25353408,  0.        ],
       [ 0.11799927, -0.45021095,  0.88509111]]), 'currentState': array([-1.7476484 , -9.38989321, 41.09523826, -0.22440076,  0.85617204,
        0.4654178 ]), 'targetState': array([-20.43033917,  40.09490294,  72.        ]), 'previousTarget': array([-9.39083908, 11.3634657 , 54.55804729])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6274462951125866
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.43033917,  40.09490294,  72.        ]), 'distance': 2.9667917440131144, 'localFrame': array([[ 0.343275  ,  0.40125602,  0.84920898],
       [-0.75987266,  0.65007195,  0.        ],
       [-0.55204694, -0.64529068,  0.52805692]]), 'currentState': array([-20.20718185,  37.29679828,  71.03944578,   0.343275  ,
         0.40125602,   0.84920898]), 'targetState': array([-20.43033917,  40.09490294,  72.        ]), 'previousTarget': array([-20.43033917,  40.09490294,  72.        ])}
episode index:19717
target thresh 86.77410180875258
target distance 85.0
model initialize at round 19717
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.49270748, 11.89003002, 62.04069958]), 'distance': 27.499999999999996, 'localFrame': array([[-0.27025383,  0.40154842, -0.87505527],
       [-0.82960607, -0.55834915,  0.        ],
       [-0.48858637,  0.72595116,  0.484023  ]]), 'currentState': array([15.95684984,  5.51431992, 88.67094078, -0.27025383,  0.40154842,
       -0.87505527]), 'targetState': array([23.92439293, 25.54649531,  5.        ]), 'previousTarget': array([19.32504628, 11.60691598, 63.33142932])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.627440599343356
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([23.92439293, 25.54649531,  5.        ]), 'distance': 2.0496321552105865, 'localFrame': array([[ 0.55674487,  0.55445762, -0.6185563 ],
       [-0.70564981,  0.70856076,  0.        ],
       [ 0.43828472,  0.43648414,  0.78574048]]), 'currentState': array([24.32064054, 23.56386422,  5.33638335,  0.55674487,  0.55445762,
       -0.6185563 ]), 'targetState': array([23.92439293, 25.54649531,  5.        ]), 'previousTarget': array([23.92439293, 25.54649531,  5.        ])}
episode index:19718
target thresh 86.77542433244442
target distance 67.0
model initialize at round 19718
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.41878735, 30.82908446, 47.62937962]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.59185232, -0.40820534,  0.69503901],
       [ 0.56776248,  0.82319242,  0.        ],
       [-0.57215085,  0.39461708,  0.71897202]]), 'currentState': array([39.4915525 , 24.01347593, 25.66101074,  0.59185232, -0.40820534,
        0.69503901]), 'targetState': array([-6.02443509, 44.59491206, 92.        ]), 'previousTarget': array([23.21405471, 31.12274577, 47.3128711 ])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6274324062390316
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-6.02443509, 44.59491206, 92.        ]), 'distance': 3.332324827243584, 'localFrame': array([[-0.78983823, -0.13015797,  0.59934503],
       [ 0.16259771, -0.98669245,  0.        ],
       [ 0.59136922,  0.09745213,  0.80049081]]), 'currentState': array([-5.61311287, 42.86575501, 89.18127357, -0.78983823, -0.13015797,
        0.59934503]), 'targetState': array([-6.02443509, 44.59491206, 92.        ]), 'previousTarget': array([-6.02443509, 44.59491206, 92.        ])}
episode index:19719
target thresh 86.7767467238905
target distance 56.0
model initialize at round 19719
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.84390936,  19.16281133,  29.18573258]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.76161536,  0.38137419,  0.52392343],
       [-0.4477456 ,  0.89416099,  0.        ],
       [-0.4684719 , -0.23458441,  0.85176536]]), 'currentState': array([-20.10152035,   2.47677142,   7.3280175 ,   0.76161536,
         0.38137419,   0.52392343]), 'targetState': array([-19.44538065,  44.97640683,  63.        ]), 'previousTarget': array([-20.93261952,  18.92384938,  28.85075362])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6274306584723056
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.44538065,  44.97640683,  63.        ]), 'distance': 2.6109271879464053, 'localFrame': array([[-0.39814763,  0.71458334,  0.57519485],
       [-0.87355622, -0.48672326,  0.        ],
       [ 0.27996071, -0.50246504,  0.81801643]]), 'currentState': array([-20.25826641,  42.59824576,  63.70746551,  -0.39814763,
         0.71458334,   0.57519485]), 'targetState': array([-19.44538065,  44.97640683,  63.        ]), 'previousTarget': array([-19.44538065,  44.97640683,  63.        ])}
episode index:19720
target thresh 86.77806898310405
target distance 39.0
model initialize at round 19720
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.53702956,  4.81007985, 39.16217172]), 'distance': 27.500000000000004, 'localFrame': array([[-0.22629584,  0.77540695,  0.58952036],
       [-0.9599549 , -0.28015458,  0.        ],
       [ 0.16515683, -0.56591296,  0.80775352]]), 'currentState': array([37.59714429, -6.03704727, 21.48670598, -0.22629584,  0.77540695,
        0.58952036]), 'targetState': array([-0.73250449, 16.98421141, 59.        ]), 'previousTarget': array([19.7630211 ,  4.03686337, 38.00375693])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273988431151496
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 76, 'trapConfig': [], 'currentTarget': array([-0.73250449, 16.98421141, 59.        ]), 'distance': 24.146359142083718, 'localFrame': array([[-0.50368644,  0.45262976,  0.73581673],
       [-0.6684032 , -0.74379914,  0.        ],
       [ 0.54729986, -0.49182226,  0.67718073]]), 'currentState': array([19.78709998,  9.06931685, 49.03270666, -0.50368644,  0.45262976,
        0.73581673]), 'targetState': array([-0.73250449, 16.98421141, 59.        ]), 'previousTarget': array([-0.73250449, 16.98421141, 59.        ])}
episode index:19721
target thresh 86.77939111009829
target distance 37.0
model initialize at round 19721
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-30.26531228,  -5.33304823,  57.78143278]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.51659061,  0.50395507, -0.69221632],
       [-0.69829846,  0.71580672,  0.        ],
       [ 0.4954931 ,  0.48337359,  0.72169008]]), 'currentState': array([-31.75989914,  -8.9615202 ,  85.        ,   0.51659061,
         0.50395507,  -0.69221632]), 'targetState': array([-29.7282083 ,  -4.02909809,  48.        ]), 'previousTarget': array([-30.26531228,  -5.33304823,  57.78143278])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6274076775004263
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-29.7282083 ,  -4.02909809,  48.        ]), 'distance': 2.19472079881874, 'localFrame': array([[ 0.22729759,  0.78666856, -0.57401078],
       [-0.96070188,  0.27758224,  0.        ],
       [ 0.1593352 ,  0.55145324,  0.81884774]]), 'currentState': array([-29.13635701,  -4.60402291,  50.03370915,   0.22729759,
         0.78666856,  -0.57401078]), 'targetState': array([-29.7282083 ,  -4.02909809,  48.        ]), 'previousTarget': array([-29.7282083 ,  -4.02909809,  48.        ])}
episode index:19722
target thresh 86.78071310488643
target distance 22.64853654088044
model initialize at round 19722
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.82002234,  8.14907537, 97.        ]), 'distance': 26.151015605518662, 'localFrame': array([[-0.83307223,  0.44198043, -0.33263186],
       [-0.46866791, -0.88337444,  0.        ],
       [-0.29383848,  0.15589388,  0.94305676]]), 'currentState': array([ 18.14927531, -13.71702594,  97.64941539,  -0.83307223,
         0.44198043,  -0.33263186]), 'targetState': array([ 3.82002234,  8.14907537, 97.        ]), 'previousTarget': array([ 3.82002234,  8.14907537, 97.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6274203588037522
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.82002234,  8.14907537, 97.        ]), 'distance': 4.689223623782718, 'localFrame': array([[-0.6799919 ,  0.22094617,  0.69913791],
       [-0.30902131, -0.95105512,  0.        ],
       [ 0.66491868, -0.21604851,  0.71498684]]), 'currentState': array([ 6.51260355,  5.34328716, 99.6203773 , -0.6799919 ,  0.22094617,
        0.69913791]), 'targetState': array([ 3.82002234,  8.14907537, 97.        ]), 'previousTarget': array([ 3.82002234,  8.14907537, 97.        ])}
episode index:19723
target thresh 86.7820349674817
target distance 50.0
model initialize at round 19723
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.21225902, 10.53982859, 54.82524299]), 'distance': 27.5, 'localFrame': array([[ 0.69559862,  0.21800037, -0.68455708],
       [-0.299057  ,  0.95423525,  0.        ],
       [ 0.6532285 ,  0.20472159,  0.72895926]]), 'currentState': array([-8.22890175, -1.24281417, 78.344443  ,  0.69559862,  0.21800037,
       -0.68455708]), 'targetState': array([ 8.59041056, 23.47775216, 29.        ]), 'previousTarget': array([-1.26060934, 10.41028534, 55.58195592])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6274171386066142
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([ 8.59041056, 23.47775216, 29.        ]), 'distance': 2.9433425313825334, 'localFrame': array([[-0.52224498, -0.32286805, -0.78931388],
       [ 0.52585238, -0.85057585,  0.        ],
       [-0.67137132, -0.41506258,  0.6139899 ]]), 'currentState': array([ 9.07533872, 25.78404706, 30.76326792, -0.52224498, -0.32286805,
       -0.78931388]), 'targetState': array([ 8.59041056, 23.47775216, 29.        ]), 'previousTarget': array([ 8.59041056, 23.47775216, 29.        ])}
episode index:19724
target thresh 86.78335669789735
target distance 30.0
model initialize at round 19724
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.83226598, 10.79722772, 50.05519   ]), 'distance': 27.5, 'localFrame': array([[-5.22642402e-04,  9.99870882e-01, -1.60606842e-02],
       [-9.99999863e-01, -5.22709822e-04,  0.00000000e+00],
       [-8.39507737e-06,  1.60606820e-02,  9.99871019e-01]]), 'currentState': array([ 1.58824230e+01, -3.78844516e+00,  3.00976886e+01, -5.22642402e-04,
        9.99870882e-01, -1.60606842e-02]), 'targetState': array([-2.77611027, 18.79609565, 61.        ]), 'previousTarget': array([ 4.01384454, 10.10770305, 50.22213199])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274235924544519
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-2.77611027, 18.79609565, 61.        ]), 'distance': 2.361036368298476, 'localFrame': array([[ 0.23728277,  0.81584878,  0.52734016],
       [-0.96021267,  0.2792698 ,  0.        ],
       [-0.14727018, -0.50635871,  0.84965426]]), 'currentState': array([-2.43540985, 16.98962615, 59.51841844,  0.23728277,  0.81584878,
        0.52734016]), 'targetState': array([-2.77611027, 18.79609565, 61.        ]), 'previousTarget': array([-2.77611027, 18.79609565, 61.        ])}
episode index:19725
target thresh 86.78467829614654
target distance 33.13393721918266
model initialize at round 19725
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.62093018,  5.04903563, 10.28401856]), 'distance': 27.5, 'localFrame': array([[ 0.2183285 , -0.76990863,  0.59964437],
       [ 0.96206518,  0.2728197 ,  0.        ],
       [-0.1635948 ,  0.57689697,  0.8002666 ]]), 'currentState': array([-24.90945117,  25.03976848,   0.72867011,   0.2183285 ,
        -0.76990863,   0.59964437]), 'targetState': array([ 1.12281535, -6.90936218, 16.        ]), 'previousTarget': array([-9.74461364,  6.31723046,  9.61302942])}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.6274109093486255
{'scaleFactor': 20, 'timeStep': 98, 'trapCount': 66, 'trapConfig': [], 'currentTarget': array([ 1.12281535, -6.90936218, 16.        ]), 'distance': 2.475395396879552, 'localFrame': array([[-0.1213684 ,  0.90539949,  0.40684331],
       [-0.99113466, -0.13286116,  0.        ],
       [ 0.05405368, -0.40323651,  0.91349796]]), 'currentState': array([ 0.46303707, -7.61856496, 13.72199508, -0.1213684 ,  0.90539949,
        0.40684331]), 'targetState': array([ 1.12281535, -6.90936218, 16.        ]), 'previousTarget': array([ 1.12281535, -6.90936218, 16.        ])}
episode index:19726
target thresh 86.78599976224251
target distance 38.0
model initialize at round 19726
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.07900026,  14.5410015 ,  23.56245214]), 'distance': 27.499999999999996, 'localFrame': array([[-0.54317429,  0.67296299, -0.50207819],
       [-0.77815181, -0.62807623,  0.        ],
       [-0.31534338,  0.39069305,  0.86482223]]), 'currentState': array([-15.89505638,   7.95298959,  49.9318039 ,  -0.54317429,
         0.67296299,  -0.50207819]), 'targetState': array([-21.91357895,  17.42971766,  12.        ]), 'previousTarget': array([-19.58221144,  14.05213269,  23.98345832])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6274189322224257
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.91357895,  17.42971766,  12.        ]), 'distance': 3.4902645622274435, 'localFrame': array([[ 0.0171948 ,  0.5318681 , -0.84665262],
       [-0.99947783,  0.03231218,  0.        ],
       [ 0.0273572 ,  0.84621052,  0.53214598]]), 'currentState': array([-1.94867058e+01,  1.49618245e+01,  1.24491514e+01,  1.71947990e-02,
        5.31868104e-01, -8.46652620e-01]), 'targetState': array([-21.91357895,  17.42971766,  12.        ]), 'previousTarget': array([-21.91357895,  17.42971766,  12.        ])}
episode index:19727
target thresh 86.78732109619848
target distance 70.26564072313852
model initialize at round 19727
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.35824219, -9.86590076,  7.28226089]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.78047759,  0.19157573, -0.59510795],
       [-0.2383833 ,  0.97117115,  0.        ],
       [ 0.57795168,  0.1418638 ,  0.80364577]]), 'currentState': array([-12.99142416, -34.56222701,   1.51364659,   0.78047759,
         0.19157573,  -0.59510795]), 'targetState': array([17.3975717 , 36.01839112, 18.        ]), 'previousTarget': array([-2.99134548, -9.60408921,  8.26071352])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6274137709062915
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([17.3975717 , 36.01839112, 18.        ]), 'distance': 1.6333280453556158, 'localFrame': array([[-0.36629263,  0.13635584, -0.92045467],
       [-0.3488706 , -0.9371709 ,  0.        ],
       [-0.86262333,  0.32111957,  0.39084934]]), 'currentState': array([17.66526255, 35.83403339, 19.6006606 , -0.36629263,  0.13635584,
       -0.92045467]), 'targetState': array([17.3975717 , 36.01839112, 18.        ]), 'previousTarget': array([17.3975717 , 36.01839112, 18.        ])}
episode index:19728
target thresh 86.78864229802768
target distance 32.0
model initialize at round 19728
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.47996444,  8.54007128, 55.4830817 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.00590842, -0.70303834, -0.7111274 ],
       [ 0.99996469,  0.00840383,  0.        ],
       [ 0.00597619, -0.71110229,  0.70306317]]), 'currentState': array([-4.50943524e+00,  2.64769093e+01,  3.47312900e+01,  5.90842136e-03,
       -7.03038342e-01, -7.11127401e-01]), 'targetState': array([-7.66853425, -2.27894329, 68.        ]), 'previousTarget': array([-6.80812577,  8.78106722, 56.10849701])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6274148702452069
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-7.66853425, -2.27894329, 68.        ]), 'distance': 3.141235318852049, 'localFrame': array([[-0.21405433, -0.45924281,  0.86213501],
       [ 0.90637892, -0.42246569,  0.        ],
       [ 0.36422246,  0.781421  ,  0.50667861]]), 'currentState': array([-7.21109947, -1.46546056, 65.00060696, -0.21405433, -0.45924281,
        0.86213501]), 'targetState': array([-7.66853425, -2.27894329, 68.        ]), 'previousTarget': array([-7.66853425, -2.27894329, 68.        ])}
episode index:19729
target thresh 86.7899633677433
target distance 42.0
model initialize at round 19729
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  3.37686027, -21.63241187,  29.20517146]), 'distance': 27.499999999999996, 'localFrame': array([[-0.51681092, -0.47983088, -0.70899139],
       [ 0.68040162, -0.73283943,  0.        ],
       [-0.51957685, -0.48239889,  0.70521714]]), 'currentState': array([16.31758881, -7.59844208, 49.        , -0.51681092, -0.47983088,
       -0.70899139]), 'targetState': array([-11.13961283, -37.37524617,   7.        ]), 'previousTarget': array([  3.37686027, -21.63241187,  29.20517146])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6274180144021062
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-11.13961283, -37.37524617,   7.        ]), 'distance': 2.6120884543145735, 'localFrame': array([[-0.93523425, -0.33601222, -0.11150199],
       [ 0.33812067, -0.94110276,  0.        ],
       [-0.10493483, -0.03770113,  0.99376421]]), 'currentState': array([ -8.6035957 , -36.91144418,   7.420132  ,  -0.93523425,
        -0.33601222,  -0.11150199]), 'targetState': array([-11.13961283, -37.37524617,   7.        ]), 'previousTarget': array([-11.13961283, -37.37524617,   7.        ])}
episode index:19730
target thresh 86.79128430535854
target distance 20.0
model initialize at round 19730
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.21867086,  2.15342355, 51.90949558]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.58221365,  0.64050611, -0.50077858],
       [-0.73997724,  0.67263191,  0.        ],
       [ 0.33683966,  0.37056475,  0.86557542]]), 'currentState': array([ -3.64375049, -15.41283745,  31.57950566,   0.58221365,
         0.64050611,  -0.50077858]), 'targetState': array([ 2.53313223,  3.09568104, 53.        ]), 'previousTarget': array([ 2.2482363 ,  2.2328885 , 52.12097483])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6274280874145959
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.53313223,  3.09568104, 53.        ]), 'distance': 4.124421922554707, 'localFrame': array([[-0.56197058,  0.6281164 ,  0.53819964],
       [-0.745258  , -0.66677621,  0.        ],
       [ 0.35885872, -0.40109759,  0.84281738]]), 'currentState': array([ 0.66407586,  0.4782865 , 50.41799878, -0.56197058,  0.6281164 ,
        0.53819964]), 'targetState': array([ 2.53313223,  3.09568104, 53.        ]), 'previousTarget': array([ 2.53313223,  3.09568104, 53.        ])}
episode index:19731
target thresh 86.79260511088663
target distance 70.0
model initialize at round 19731
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.28772875,  1.60137801, 31.70236644]), 'distance': 27.499999999999996, 'localFrame': array([[-0.47921632, -0.58439226, -0.65485678],
       [ 0.77325827, -0.63409119,  0.        ],
       [-0.41523892, -0.50637342,  0.755753  ]]), 'currentState': array([ 0.0103468 , -1.33656916,  4.69639544, -0.47921632, -0.58439226,
       -0.65485678]), 'targetState': array([11.30387708,  6.42046439, 76.        ]), 'previousTarget': array([ 4.36615324,  2.47992182, 33.03769024])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273962899238492
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 81, 'trapConfig': [], 'currentTarget': array([ 5.45332791,  2.60962022, 55.86415398]), 'distance': 27.5, 'localFrame': array([[ 0.40023836,  0.72807839,  0.55651695],
       [-0.87631995,  0.48172953,  0.        ],
       [-0.26809065, -0.48768691,  0.83083625]]), 'currentState': array([-2.09592487, -2.30770039, 29.88187655,  0.40023836,  0.72807839,
        0.55651695]), 'targetState': array([11.30387708,  6.42046439, 76.        ]), 'previousTarget': array([ 5.45332791,  2.60962022, 55.86415398])}
episode index:19732
target thresh 86.79392578434076
target distance 60.0
model initialize at round 19732
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.60864403,   7.32513283,  54.52917745]), 'distance': 27.499999999999996, 'localFrame': array([[-0.19243811, -0.67052165, -0.71649723],
       [ 0.96119743, -0.27586137,  0.        ],
       [-0.19765391, -0.6886953 ,  0.69758994]]), 'currentState': array([-24.15482399,  16.40534652,  29.41051412,  -0.19243811,
        -0.67052165,  -0.71649723]), 'targetState': array([-8.10397555, -5.85880365, 91.        ]), 'previousTarget': array([-17.74266442,   7.65774663,  55.86427464])}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6273960937603144
{'scaleFactor': 20, 'timeStep': 48, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.10397555, -5.85880365, 91.        ]), 'distance': 2.405149040504034, 'localFrame': array([[ 0.16166237,  0.47611748,  0.86439425],
       [-0.94690454,  0.32151484,  0.        ],
       [-0.27791558, -0.81849884,  0.50281465]]), 'currentState': array([-9.3268285 , -6.25482722, 88.96713554,  0.16166237,  0.47611748,
        0.86439425]), 'targetState': array([-8.10397555, -5.85880365, 91.        ]), 'previousTarget': array([-8.10397555, -5.85880365, 91.        ])}
episode index:19733
target thresh 86.79524632573417
target distance 41.28104114532607
model initialize at round 19733
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.32556907, -14.23047552,  13.70924051]), 'distance': 27.499999999999996, 'localFrame': array([[-0.54043733,  0.48121248, -0.69018986],
       [-0.66499951, -0.7468438 ,  0.        ],
       [-0.51546402,  0.45897592,  0.72362833]]), 'currentState': array([  7.68503569, -38.70515559,  26.02479785,  -0.54043733,
         0.48121248,  -0.69018986]), 'targetState': array([3.84860013, 1.09008121, 6.        ]), 'previousTarget': array([  5.58823925, -15.78302234,  14.58348445])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274025457311994
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([3.84860013, 1.09008121, 6.        ]), 'distance': 3.0423602313625513, 'localFrame': array([[ 0.26362948,  0.40470152, -0.87562331],
       [-0.83790065,  0.54582278,  0.        ],
       [ 0.47793515,  0.73368534,  0.48299464]]), 'currentState': array([ 3.4546917 , -0.2976962 ,  8.67859399,  0.26362948,  0.40470152,
       -0.87562331]), 'targetState': array([3.84860013, 1.09008121, 6.        ]), 'previousTarget': array([3.84860013, 1.09008121, 6.        ])}
episode index:19734
target thresh 86.79656673508002
target distance 27.0
model initialize at round 19734
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.19101395, -15.68700552,  83.24657568]), 'distance': 27.5, 'localFrame': array([[ 0.69682624, -0.50983088,  0.50448555],
       [ 0.59047833,  0.80705349,  0.        ],
       [-0.40714682,  0.29788779,  0.86342014]]), 'currentState': array([-0.97188174,  0.69099083, 63.14551655,  0.69682624, -0.50983088,
        0.50448555]), 'targetState': array([ 11.26950459, -21.18957919,  90.        ]), 'previousTarget': array([  7.53895664, -14.71239295,  82.49052757])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.627410167662472
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 11.26950459, -21.18957919,  90.        ]), 'distance': 3.562805293343848, 'localFrame': array([[ 0.70987086,  0.23100139,  0.66537337],
       [-0.30944149,  0.95091849,  0.        ],
       [-0.63271584, -0.20589413,  0.74651074]]), 'currentState': array([  9.05466217, -20.69150622,  87.25409798,   0.70987086,
         0.23100139,   0.66537337]), 'targetState': array([ 11.26950459, -21.18957919,  90.        ]), 'previousTarget': array([ 11.26950459, -21.18957919,  90.        ])}
episode index:19735
target thresh 86.79788701239156
target distance 5.384239658183155
model initialize at round 19735
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.92410565, -26.16096543,  56.        ]), 'distance': 6.3212004055139825, 'localFrame': array([[ 0.78873564, -0.3654104 , -0.49433928],
       [ 0.42036514,  0.90735503,  0.        ],
       [ 0.44854123, -0.207803  ,  0.86926905]]), 'currentState': array([ 28.33871906, -20.55009629,  58.85184933,   0.78873564,
        -0.3654104 ,  -0.49433928]), 'targetState': array([ 28.92410565, -26.16096543,  56.        ]), 'previousTarget': array([ 28.92410565, -26.16096543,  56.        ])}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6274280380431134
{'scaleFactor': 20, 'timeStep': 3, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 28.92410565, -26.16096543,  56.        ]), 'distance': 3.136269289058275, 'localFrame': array([[ 0.84120142, -0.42638118, -0.3325346 ],
       [ 0.4521103 ,  0.89196204,  0.        ],
       [ 0.29660824, -0.15034232,  0.94309105]]), 'currentState': array([ 29.836617  , -23.75163608,  57.78847426,   0.84120142,
        -0.42638118,  -0.3325346 ]), 'targetState': array([ 28.92410565, -26.16096543,  56.        ]), 'previousTarget': array([ 28.92410565, -26.16096543,  56.        ])}
episode index:19736
target thresh 86.79920715768195
target distance 39.0
model initialize at round 19736
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([10.21282215, 24.59615635, 76.91192261]), 'distance': 27.5, 'localFrame': array([[-0.15750102,  0.80820794,  0.56744458],
       [-0.98153578, -0.19127861,  0.        ],
       [ 0.10854001, -0.55696716,  0.82341159]]), 'currentState': array([13.76340008,  5.96396017, 57.        , -0.15750102,  0.80820794,
        0.56744458]), 'targetState': array([ 6.80914749, 42.4574553 , 96.        ]), 'previousTarget': array([10.21282215, 24.59615635, 76.91192261])}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6274231476091525
{'scaleFactor': 20, 'timeStep': 64, 'trapCount': 27, 'trapConfig': [], 'currentTarget': array([ 6.80914749, 42.4574553 , 96.        ]), 'distance': 2.515778039494358, 'localFrame': array([[ 0.82253423, -0.20615026,  0.53003727],
       [ 0.24310907,  0.96999896,  0.        ],
       [-0.5141356 ,  0.12885687,  0.84797435]]), 'currentState': array([ 7.23900136, 42.63464377, 93.52755808,  0.82253423, -0.20615026,
        0.53003727]), 'targetState': array([ 6.80914749, 42.4574553 , 96.        ]), 'previousTarget': array([ 6.80914749, 42.4574553 , 96.        ])}
episode index:19737
target thresh 86.8005271709644
target distance 46.0
model initialize at round 19737
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.9438907 , -20.57897303,  45.84260391]), 'distance': 27.5, 'localFrame': array([[ 0.5253184 ,  0.6343555 ,  0.56712757],
       [-0.77019477,  0.63780876,  0.        ],
       [-0.36171893, -0.43679869,  0.82362997]]), 'currentState': array([  2.4939532 , -38.63046512,  25.15466201,   0.5253184 ,
         0.6343555 ,   0.56712757]), 'targetState': array([-0.86612376,  0.49982961, 70.        ]), 'previousTarget': array([  0.2126342 , -21.0824761 ,  44.85605543])}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6274266429009977
{'scaleFactor': 20, 'timeStep': 37, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.86612376,  0.49982961, 70.        ]), 'distance': 2.7926541957506554, 'localFrame': array([[-0.79992151,  0.09502349,  0.59253364],
       [-0.11796163, -0.99301815,  0.        ],
       [ 0.58839666, -0.06989624,  0.8055457 ]]), 'currentState': array([-5.65143968e-02, -2.91239586e-02,  6.73801416e+01, -7.99921508e-01,
        9.50234853e-02,  5.92533643e-01]), 'targetState': array([-0.86612376,  0.49982961, 70.        ]), 'previousTarget': array([-0.86612376,  0.49982961, 70.        ])}
episode index:19738
target thresh 86.80184705225214
target distance 57.0
model initialize at round 19738
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-42.54069128,   8.92374866,  59.92959191]), 'distance': 27.5, 'localFrame': array([[-0.57272683,  0.44239902,  0.69012107],
       [-0.61130679, -0.79139371,  0.        ],
       [ 0.54615748, -0.42187569,  0.72369393]]), 'currentState': array([-38.981804  ,  23.28140653,  36.74679695,  -0.57272683,
         0.44239902,   0.69012107]), 'targetState': array([-47.61746717, -11.55754385,  93.        ]), 'previousTarget': array([-42.09870771,   8.03954639,  59.50034822])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273948567597089
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 39, 'trapConfig': [], 'currentTarget': array([-47.2175963 , -12.87983732,  82.38318686]), 'distance': 27.499999999999996, 'localFrame': array([[-0.76617671, -0.60528977, -0.21586466],
       [ 0.61990509, -0.7846768 ,  0.        ],
       [-0.16938399, -0.1338156 ,  0.97642329]]), 'currentState': array([-46.19049651, -16.27625212,  55.11306708,  -0.76617671,
        -0.60528977,  -0.21586466]), 'targetState': array([-47.61746717, -11.55754385,  93.        ]), 'previousTarget': array([-47.2175963 , -12.87983732,  82.38318686])}
episode index:19739
target thresh 86.80316680155835
target distance 76.0
model initialize at round 19739
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.19118916,  0.98772926, 67.77250879]), 'distance': 27.499999999999996, 'localFrame': array([[-0.60188222, -0.60745555, -0.51839709],
       [ 0.710358  , -0.70384054,  0.        ],
       [-0.36486889, -0.36824752,  0.85514002]]), 'currentState': array([-6.89105771, -5.62926737, 92.48715331, -0.60188222, -0.60745555,
       -0.51839709]), 'targetState': array([23.90364437, 14.58135062, 17.        ]), 'previousTarget': array([ 4.24057513,  1.79580204, 68.01424405])}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6273871538815552
{'scaleFactor': 20, 'timeStep': 75, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([23.90364437, 14.58135062, 17.        ]), 'distance': 2.196222680458768, 'localFrame': array([[-0.81756295, -0.16207456, -0.5525601 ],
       [ 0.19445687, -0.98091107,  0.        ],
       [-0.54201232, -0.10744911,  0.83347306]]), 'currentState': array([23.2774021 , 15.4732812 , 18.90674448, -0.81756295, -0.16207456,
       -0.5525601 ]), 'targetState': array([23.90364437, 14.58135062, 17.        ]), 'previousTarget': array([23.90364437, 14.58135062, 17.        ])}
episode index:19740
target thresh 86.80448641889623
target distance 19.0
model initialize at round 19740
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.15360789, -9.93323657, 64.        ]), 'distance': 17.20360226561144, 'localFrame': array([[ 0.17901165,  0.15441765, -0.97165324],
       [-0.65317562,  0.75720645,  0.        ],
       [ 0.7357421 ,  0.63466021,  0.23641062]]), 'currentState': array([-2.86090521, -8.87206134, 81.08575354,  0.17901165,  0.15441765,
       -0.97165324]), 'targetState': array([-1.15360789, -9.93323657, 64.        ]), 'previousTarget': array([-1.15360789, -9.93323657, 64.        ])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6273928432194563
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([-1.15360789, -9.93323657, 64.        ]), 'distance': 3.6183230758937834, 'localFrame': array([[ 0.34738814,  0.01795569, -0.9375495 ],
       [-0.05161877,  0.99866686,  0.        ],
       [ 0.93629962,  0.04839516,  0.34785188]]), 'currentState': array([-3.79900729e+00, -1.08515367e+01,  6.62914731e+01,  3.47388141e-01,
        1.79556871e-02, -9.37549504e-01]), 'targetState': array([-1.15360789, -9.93323657, 64.        ]), 'previousTarget': array([-1.15360789, -9.93323657, 64.        ])}
episode index:19741
target thresh 86.80580590427897
target distance 20.25620727767862
model initialize at round 19741
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 46.67273141, -11.20964508,  65.        ]), 'distance': 19.921666190906407, 'localFrame': array([[ 0.34972532, -0.61952784,  0.70276415],
       [ 0.87082921,  0.49158569,  0.        ],
       [-0.3454688 ,  0.61198755,  0.7114229 ]]), 'currentState': array([46.25250243,  8.55601637, 67.45251271,  0.34972532, -0.61952784,
        0.70276415]), 'targetState': array([ 46.67273141, -11.20964508,  65.        ]), 'previousTarget': array([ 46.67273141, -11.20964508,  65.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6274050685750207
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 46.67273141, -11.20964508,  65.        ]), 'distance': 2.6488944052603944, 'localFrame': array([[ 0.57255864, -0.72198954, -0.38846841],
       [ 0.78352608,  0.6213589 ,  0.        ],
       [ 0.2413783 , -0.30437513,  0.92146204]]), 'currentState': array([46.54226519, -8.73863655, 64.05462337,  0.57255864, -0.72198954,
       -0.38846841]), 'targetState': array([ 46.67273141, -11.20964508,  65.        ]), 'previousTarget': array([ 46.67273141, -11.20964508,  65.        ])}
episode index:19742
target thresh 86.80712525771978
target distance 15.0
model initialize at round 19742
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 32.76805074, -36.43150904,  77.        ]), 'distance': 15.21267877577062, 'localFrame': array([[-0.64583859,  0.2527105 ,  0.72043731],
       [-0.36438816, -0.93124716,  0.        ],
       [ 0.6709052 , -0.26251883,  0.69352006]]), 'currentState': array([ 35.35322992, -30.50477912,  63.22987594,  -0.64583859,
         0.2527105 ,   0.72043731]), 'targetState': array([ 32.76805074, -36.43150904,  77.        ]), 'previousTarget': array([ 32.76805074, -36.43150904,  77.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.627419097699593
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 32.76805074, -36.43150904,  77.        ]), 'distance': 1.8540166922931263, 'localFrame': array([[-0.39383915, -0.32886194,  0.85833591],
       [ 0.64094628, -0.76758574,  0.        ],
       [ 0.65884641,  0.55014721,  0.51308816]]), 'currentState': array([ 33.4286673 , -35.76407803,  75.40140703,  -0.39383915,
        -0.32886194,   0.85833591]), 'targetState': array([ 32.76805074, -36.43150904,  77.        ]), 'previousTarget': array([ 32.76805074, -36.43150904,  77.        ])}
episode index:19743
target thresh 86.80844447923182
target distance 47.347319825903305
model initialize at round 19743
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([26.8421377 ,  4.7365201 , 88.56558347]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.67017576,  0.2724827 , -0.69037499],
       [-0.37664256,  0.92635867,  0.        ],
       [ 0.63953486,  0.2600246 ,  0.72345171]]), 'currentState': array([ 0.37648113, -2.71259707, 87.99140946,  0.67017576,  0.2724827 ,
       -0.69037499]), 'targetState': array([46.86589093, 10.37247641, 89.        ]), 'previousTarget': array([25.83740145,  4.0021049 , 89.        ])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274255452375542
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([46.86589093, 10.37247641, 89.        ]), 'distance': 2.153537998161201, 'localFrame': array([[ 0.83851331, -0.47031466,  0.2751355 ],
       [ 0.48919491,  0.87217449,  0.        ],
       [-0.23996617,  0.13459489,  0.96140546]]), 'currentState': array([45.24769767, 10.13137852, 90.40037433,  0.83851331, -0.47031466,
        0.2751355 ]), 'targetState': array([46.86589093, 10.37247641, 89.        ]), 'previousTarget': array([46.86589093, 10.37247641, 89.        ])}
episode index:19744
target thresh 86.80976356882833
target distance 47.0
model initialize at round 19744
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.60762926, -4.51493576, 57.43348578]), 'distance': 27.499999999999996, 'localFrame': array([[-0.23946418,  0.87284434,  0.42520544],
       [-0.96436567, -0.26457298,  0.        ],
       [ 0.11249787, -0.41005353,  0.90509686]]), 'currentState': array([ 20.01397335, -16.29736391,  34.05063382,  -0.23946418,
         0.87284434,   0.42520544]), 'targetState': array([ 3.13525695,  7.36003831, 81.        ]), 'previousTarget': array([11.73497333, -5.62338618, 57.05364954])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6274266430893208
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([ 3.13525695,  7.36003831, 81.        ]), 'distance': 3.4519754187486322, 'localFrame': array([[-0.61691009,  0.57520724,  0.53717648],
       [-0.6819535 , -0.73139553,  0.        ],
       [ 0.39288847, -0.36632938,  0.84346988]]), 'currentState': array([ 4.14319149,  5.59929145, 78.20715687, -0.61691009,  0.57520724,
        0.53717648]), 'targetState': array([ 3.13525695,  7.36003831, 81.        ]), 'previousTarget': array([ 3.13525695,  7.36003831, 81.        ])}
episode index:19745
target thresh 86.81108252652245
target distance 19.0
model initialize at round 19745
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.7809598 ,  18.09918517, 100.        ]), 'distance': 23.519529773599935, 'localFrame': array([[-0.61538121, -0.49806569,  0.61093088],
       [ 0.62912189, -0.77730667,  0.        ],
       [ 0.47488064,  0.38434999,  0.79168394]]), 'currentState': array([ 9.66707058, 20.78518278, 82.46968183, -0.61538121, -0.49806569,
        0.61093088]), 'targetState': array([ -5.7809598 ,  18.09918517, 100.        ]), 'previousTarget': array([ -5.7809598 ,  18.09918517, 100.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6274397575544594
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.7809598 ,  18.09918517, 100.        ]), 'distance': 4.523370239666186, 'localFrame': array([[-0.16356164,  0.57052037,  0.80483172],
       [-0.96127618, -0.27558685,  0.        ],
       [ 0.22180104, -0.77366556,  0.59350308]]), 'currentState': array([-2.8384286 , 20.52547192, 97.56781558, -0.16356164,  0.57052037,
        0.80483172]), 'targetState': array([ -5.7809598 ,  18.09918517, 100.        ]), 'previousTarget': array([ -5.7809598 ,  18.09918517, 100.        ])}
episode index:19746
target thresh 86.81240135232741
target distance 82.0
model initialize at round 19746
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.10819667, -12.45924343,  29.6440588 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.57318025, -0.44334322,  0.68913801],
       [ 0.61182002,  0.790997  ,  0.        ],
       [-0.5451061 ,  0.42162843,  0.72463012]]), 'currentState': array([-10.30138318,  -8.74554187,   2.39665343,   0.57318025,
        -0.44334322,   0.68913801]), 'targetState': array([ -9.72989834, -19.73142362,  83.        ]), 'previousTarget': array([-10.27805499, -11.62152663,  28.20235843])}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6274354148856192
{'scaleFactor': 20, 'timeStep': 62, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.72989834, -19.73142362,  83.        ]), 'distance': 2.969082117276287, 'localFrame': array([[-0.93505278,  0.03954245,  0.35229631],
       [-0.04225124, -0.99910702,  0.        ],
       [ 0.35198172, -0.01488496,  0.93588851]]), 'currentState': array([-7.99948865e+00, -2.00285753e+01,  8.06056667e+01, -9.35052780e-01,
        3.95424519e-02,  3.52296313e-01]), 'targetState': array([ -9.72989834, -19.73142362,  83.        ]), 'previousTarget': array([ -9.72989834, -19.73142362,  83.        ])}
episode index:19747
target thresh 86.81372004625639
target distance 38.70012070892438
model initialize at round 19747
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.84319203,  16.42221277,  37.76095231]), 'distance': 27.5, 'localFrame': array([[-0.79906921, -0.16465669, -0.57825303],
       [ 0.20182041, -0.97942254,  0.        ],
       [-0.56635406, -0.11670327,  0.81585748]]), 'currentState': array([-0.7753526 , -0.61603589, 42.46140166, -0.79906921, -0.16465669,
       -0.57825303]), 'targetState': array([-38.70012071,  30.05496061,  34.        ]), 'previousTarget': array([-21.28081065,  16.52692328,  38.50109967])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6274342791074266
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 21, 'trapConfig': [], 'currentTarget': array([-38.70012071,  30.05496061,  34.        ]), 'distance': 2.5021205663677897, 'localFrame': array([[ 0.79391376,  0.52064231, -0.31405815],
       [-0.54838872,  0.83622354,  0.        ],
       [ 0.26262282,  0.17222595,  0.94940375]]), 'currentState': array([-38.19693331,  29.89526823,  36.44579396,   0.79391376,
         0.52064231,  -0.31405815]), 'targetState': array([-38.70012071,  30.05496061,  34.        ]), 'previousTarget': array([-38.70012071,  30.05496061,  34.        ])}
episode index:19748
target thresh 86.81503860832255
target distance 44.0
model initialize at round 19748
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.01217454, 32.31811729, 64.57320509]), 'distance': 27.5, 'localFrame': array([[-0.49103886, -0.81656383,  0.30348698],
       [ 0.85698293, -0.51534479,  0.        ],
       [ 0.15640044,  0.26008316,  0.95283558]]), 'currentState': array([-20.10394888,  41.46587318,  41.63072146,  -0.49103886,
        -0.81656383,   0.30348698]), 'targetState': array([ 3.28076013, 23.77470532, 86.        ]), 'previousTarget': array([-7.46055354, 32.99471406, 64.85266596])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6274399637544608
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.28076013, 23.77470532, 86.        ]), 'distance': 3.61515287048901, 'localFrame': array([[-0.20260741, -0.71802032,  0.66588066],
       [ 0.96241855, -0.27157048,  0.        ],
       [ 0.18083353,  0.6408559 ,  0.74605827]]), 'currentState': array([ 4.33431251, 25.68425758, 83.11677823, -0.20260741, -0.71802032,
        0.66588066]), 'targetState': array([ 3.28076013, 23.77470532, 86.        ]), 'previousTarget': array([ 3.28076013, 23.77470532, 86.        ])}
episode index:19749
target thresh 86.81635703853912
target distance 38.09893179904893
model initialize at round 19749
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([19.36217326, -6.12505439, 13.31174634]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.25842787,  0.65572703,  0.70939207],
       [-0.9303546 ,  0.36666105,  0.        ],
       [-0.26010644, -0.65998617,  0.70481409]]), 'currentState': array([  2.17227789, -27.512225  ,  15.14107149,   0.25842787,
         0.65572703,   0.70939207]), 'targetState': array([31.6884578 ,  9.21095229, 12.        ]), 'previousTarget': array([19.23957101, -7.06417495, 12.85436134])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6274379180237358
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([31.6884578 ,  9.21095229, 12.        ]), 'distance': 3.87620357959974, 'localFrame': array([[ 0.37680762, -0.46886958, -0.79886002],
       [ 0.77947856,  0.62642891,  0.        ],
       [ 0.50042901, -0.62269426,  0.60151697]]), 'currentState': array([28.97526828,  7.22479011, 10.07160258,  0.37680762, -0.46886958,
       -0.79886002]), 'targetState': array([31.6884578 ,  9.21095229, 12.        ]), 'previousTarget': array([31.6884578 ,  9.21095229, 12.        ])}
episode index:19750
target thresh 86.81767533691925
target distance 67.0
model initialize at round 19750
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.07097521,  3.89066023, 46.55265077]), 'distance': 27.499999999999996, 'localFrame': array([[-0.80212785,  0.58336494,  0.12757845],
       [-0.58817119, -0.80873646,  0.        ],
       [ 0.10317734, -0.07503797,  0.99182848]]), 'currentState': array([21.38936933, 21.52846683, 25.71647937, -0.80212785,  0.58336494,
        0.12757845]), 'targetState': array([ 10.67371362, -35.42699306,  93.        ]), 'previousTarget': array([18.73259774,  2.58035331, 47.00760049])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6274297383293986
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([ 10.67371362, -35.42699306,  93.        ]), 'distance': 2.903393141327901, 'localFrame': array([[-0.33364412,  0.61936311,  0.71068343],
       [-0.88038748, -0.47425509,  0.        ],
       [ 0.33704523, -0.62567679,  0.70351195]]), 'currentState': array([ 11.19109717, -35.00365129,  90.17461724,  -0.33364412,
         0.61936311,   0.71068343]), 'targetState': array([ 10.67371362, -35.42699306,  93.        ]), 'previousTarget': array([ 10.67371362, -35.42699306,  93.        ])}
episode index:19751
target thresh 86.81899350347614
target distance 37.0
model initialize at round 19751
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.66728818, -12.77907646,  33.12804571]), 'distance': 27.499999999999996, 'localFrame': array([[-0.96181675, -0.0783882 ,  0.26222859],
       [ 0.08123081, -0.99669532,  0.        ],
       [ 0.26136201,  0.02130104,  0.96500579]]), 'currentState': array([-2.06263341, -6.04105283,  7.29729711, -0.96181675, -0.0783882 ,
        0.26222859]), 'targetState': array([-11.1914446 , -15.35420359,  43.        ]), 'previousTarget': array([ -7.985246  , -12.48513869,  31.70011403])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.627437352323985
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.1914446 , -15.35420359,  43.        ]), 'distance': 2.431946476612551, 'localFrame': array([[ 0.31934947, -0.73185419,  0.60200113],
       [ 0.91654171,  0.39993911,  0.        ],
       [-0.24076379,  0.55175915,  0.79849524]]), 'currentState': array([-11.78180322, -15.78372835,  40.68022655,   0.31934947,
        -0.73185419,   0.60200113]), 'targetState': array([-11.1914446 , -15.35420359,  43.        ]), 'previousTarget': array([-11.1914446 , -15.35420359,  43.        ])}
episode index:19752
target thresh 86.82031153822295
target distance 42.0
model initialize at round 19752
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.10614755,  -2.33526739,  26.07206169]), 'distance': 27.5, 'localFrame': array([[ 0.74237376,  0.23119061,  0.62883392],
       [-0.29733611,  0.95477287,  0.        ],
       [-0.60039357, -0.18697503,  0.77753965]]), 'currentState': array([-17.40496893,   9.76954059,   1.48671963,   0.74237376,
         0.23119061,   0.62883392]), 'targetState': array([-13.61682578, -10.17752699,  42.        ]), 'previousTarget': array([-15.60170631,  -1.79976384,  24.63729215])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6274412003644181
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.61682578, -10.17752699,  42.        ]), 'distance': 2.794608701442099, 'localFrame': array([[ 0.57906684,  0.54625477,  0.60521676],
       [-0.68619736,  0.72741541,  0.        ],
       [-0.440244  , -0.41529814,  0.79606072]]), 'currentState': array([-13.81219364, -11.11557018,  39.37478683,   0.57906684,
         0.54625477,   0.60521676]), 'targetState': array([-13.61682578, -10.17752699,  42.        ]), 'previousTarget': array([-13.61682578, -10.17752699,  42.        ])}
episode index:19753
target thresh 86.82162944117289
target distance 49.71601038893743
model initialize at round 19753
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.48731146, -38.73431205,  33.9927581 ]), 'distance': 27.5, 'localFrame': array([[-0.70303467,  0.15117741,  0.69490117],
       [-0.21022987, -0.97765198,  0.        ],
       [ 0.67937151, -0.14608899,  0.71910525]]), 'currentState': array([ 14.73140032, -43.76017649,  24.24559118,  -0.70303467,
         0.15117741,   0.69490117]), 'targetState': array([-33.79162532, -34.08997006,  43.        ]), 'previousTarget': array([ -9.23113768, -38.56857918,  33.11968682])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6274450480152551
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-33.79162532, -34.08997006,  43.        ]), 'distance': 3.7772359486058953, 'localFrame': array([[-0.90416084, -0.4199703 ,  0.07821844],
       [ 0.42126094, -0.90693948,  0.        ],
       [ 0.07093939,  0.03295037,  0.99693624]]), 'currentState': array([-31.1206969 , -35.15859446,  40.55220616,  -0.90416084,
        -0.4199703 ,   0.07821844]), 'targetState': array([-33.79162532, -34.08997006,  43.        ]), 'previousTarget': array([-33.79162532, -34.08997006,  43.        ])}
episode index:19754
target thresh 86.8229472123391
target distance 26.0
model initialize at round 19754
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.04834758, -3.43573459, 73.        ]), 'distance': 24.773008903162975, 'localFrame': array([[-0.6391082 ,  0.23236763,  0.73317528],
       [-0.34169715, -0.93981012,  0.        ],
       [ 0.68904554, -0.2505239 ,  0.68003971]]), 'currentState': array([-0.19945287, -0.96879775, 48.41956508, -0.6391082 ,  0.23236763,
        0.73317528]), 'targetState': array([-2.04834758, -3.43573459, 73.        ]), 'previousTarget': array([-2.04834758, -3.43573459, 73.        ])}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6274572626831749
{'scaleFactor': 20, 'timeStep': 15, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.04834758, -3.43573459, 73.        ]), 'distance': 2.34818853997135, 'localFrame': array([[-0.34893258, -0.84384611,  0.40763929],
       [ 0.92411164, -0.38212259,  0.        ],
       [ 0.15576818,  0.37670422,  0.91314304]]), 'currentState': array([-1.55025809, -2.92241609, 70.76339534, -0.34893258, -0.84384611,
        0.40763929]), 'targetState': array([-2.04834758, -3.43573459, 73.        ]), 'previousTarget': array([-2.04834758, -3.43573459, 73.        ])}
episode index:19755
target thresh 86.8242648517348
target distance 40.6771338142693
model initialize at round 19755
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.27390843, -17.67133368,  24.4079306 ]), 'distance': 27.5, 'localFrame': array([[-0.28155905,  0.77489724,  0.5659141 ],
       [-0.93987978, -0.34150549,  0.        ],
       [ 0.19326277, -0.53189122,  0.82446421]]), 'currentState': array([-11.76324089, -43.8000231 ,  23.19433119,  -0.28155905,
         0.77489724,   0.5659141 ]), 'targetState': array([ 0.86771684, -4.92413114, 25.        ]), 'previousTarget': array([ -3.46059917, -19.29829129,  24.293256  ])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6274633223527258
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.86771684, -4.92413114, 25.        ]), 'distance': 3.9830272076599225, 'localFrame': array([[ 0.97279469, -0.0461521 , -0.22702528],
       [ 0.0473895 ,  0.99887649,  0.        ],
       [ 0.22677022, -0.01075861,  0.97388886]]), 'currentState': array([ 1.41343406, -7.60330789, 22.10367294,  0.97279469, -0.0461521 ,
       -0.22702528]), 'targetState': array([ 0.86771684, -4.92413114, 25.        ]), 'previousTarget': array([ 0.86771684, -4.92413114, 25.        ])}
episode index:19756
target thresh 86.82558235937316
target distance 19.14938535264899
model initialize at round 19756
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.89460912, -32.91576717,  44.26354469]), 'distance': 27.5, 'localFrame': array([[-0.98537293, -0.05575075, -0.16103432],
       [ 0.05648799, -0.99840328,  0.        ],
       [-0.16077719, -0.0090965 ,  0.98694881]]), 'currentState': array([ 2.64874509e+01, -1.49408193e+01,  6.15481409e+01, -9.85372926e-01,
       -5.57507531e-02, -1.61034315e-01]), 'targetState': array([ 14.04714509, -34.22977819,  43.        ]), 'previousTarget': array([ 15.16801486, -32.71573992,  44.42316259])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6274742288603449
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 14.04714509, -34.22977819,  43.        ]), 'distance': 4.167299329575809, 'localFrame': array([[-0.39317924, -0.85024694,  0.34998604],
       [ 0.90765143, -0.41972477,  0.        ],
       [ 0.14689781,  0.31766533,  0.93675492]]), 'currentState': array([ 16.84729813, -31.51105409,  44.46084424,  -0.39317924,
        -0.85024694,   0.34998604]), 'targetState': array([ 14.04714509, -34.22977819,  43.        ]), 'previousTarget': array([ 14.04714509, -34.22977819,  43.        ])}
episode index:19757
target thresh 86.82689973526733
target distance 36.04177323509761
model initialize at round 19757
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.3806362 , -3.68802436, 22.16012933]), 'distance': 27.5, 'localFrame': array([[-7.02003284e-01, -8.48350640e-04, -7.12173202e-01],
       [ 1.20847016e-03, -9.99999270e-01,  0.00000000e+00],
       [-7.12172682e-01, -8.60640063e-04,  7.02003796e-01]]), 'currentState': array([ 2.48580980e+01,  1.14714772e+01,  3.57624423e+01, -7.02003284e-01,
       -8.48350640e-04, -7.12173202e-01]), 'targetState': array([-10.13775483, -17.2402415 ,  10.        ]), 'previousTarget': array([ 7.18883576, -3.84234431, 22.97988151])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.627473400993692
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 9, 'trapConfig': [], 'currentTarget': array([-10.13775483, -17.2402415 ,  10.        ]), 'distance': 3.700046517035662, 'localFrame': array([[-0.49080982, -0.48197341,  0.72581496],
       [ 0.70065477, -0.71350045,  0.        ],
       [ 0.5178693 ,  0.50854571,  0.68789   ]]), 'currentState': array([ -8.40145651, -14.80872698,   7.81749009,  -0.49080982,
        -0.48197341,   0.72581496]), 'targetState': array([-10.13775483, -17.2402415 ,  10.        ]), 'previousTarget': array([-10.13775483, -17.2402415 ,  10.        ])}
episode index:19758
target thresh 86.82821697943048
target distance 46.043051854491395
model initialize at round 19758
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.46194231, 17.76635622, 50.81569883]), 'distance': 27.499999999999996, 'localFrame': array([[-0.09572555, -0.45086319, -0.88744521],
       [ 0.9781954 , -0.2076867 ,  0.        ],
       [-0.18431057, -0.86809482,  0.46091322]]), 'currentState': array([-27.36685915,  33.67596657,  64.32674067,  -0.09572555,
        -0.45086319,  -0.88744521]), 'targetState': array([19.44835772, -7.92220816, 29.        ]), 'previousTarget': array([-9.08433773, 17.92629598, 51.92875231])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6274679790149308
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([19.44835772, -7.92220816, 29.        ]), 'distance': 3.5833935087027933, 'localFrame': array([[ 0.15335482, -0.75743745, -0.63464227],
       [ 0.98011325,  0.19843895,  0.        ],
       [ 0.12593775, -0.62202129,  0.77280605]]), 'currentState': array([18.22675719, -9.68751079, 31.869165  ,  0.15335482, -0.75743745,
       -0.63464227]), 'targetState': array([19.44835772, -7.92220816, 29.        ]), 'previousTarget': array([19.44835772, -7.92220816, 29.        ])}
episode index:19759
target thresh 86.82953409187583
target distance 24.145223997653005
model initialize at round 19759
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  7.55514211, -21.90538821,  28.26280248]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.38392358, -0.59407979, -0.70687474],
       [ 0.83988017,  0.54277187,  0.        ],
       [ 0.38367173, -0.59369007,  0.70733875]]), 'currentState': array([-3.19300546, -4.35167529, 46.49997569,  0.38392358, -0.59407979,
       -0.70687474]), 'targetState': array([ 10.65679482, -26.97096076,  23.        ]), 'previousTarget': array([  6.82756561, -20.71611071,  29.21722959])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274744188584627
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.65679482, -26.97096076,  23.        ]), 'distance': 3.799708578767382, 'localFrame': array([[ 0.17859829, -0.92578518,  0.33320331],
       [ 0.9818956 ,  0.18942286,  0.        ],
       [-0.06311633,  0.32717087,  0.942855  ]]), 'currentState': array([ 13.53636555, -24.8828351 ,  24.3362593 ,   0.17859829,
        -0.92578518,   0.33320331]), 'targetState': array([ 10.65679482, -26.97096076,  23.        ]), 'previousTarget': array([ 10.65679482, -26.97096076,  23.        ])}
episode index:19760
target thresh 86.8308510726165
target distance 37.0
model initialize at round 19760
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.52986804, -7.89003221, 14.79119528]), 'distance': 27.5, 'localFrame': array([[ 0.53619268, -0.31536916, -0.78296852],
       [ 0.50697438,  0.86196113,  0.        ],
       [ 0.67488842, -0.39694498,  0.62206133]]), 'currentState': array([-2.13119149, -2.51947056, 40.33318258,  0.53619268, -0.31536916,
       -0.78296852]), 'targetState': array([ 9.84997483, -9.94876856,  5.        ]), 'previousTarget': array([ 5.9288585 , -7.74235865, 16.41549244])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6274789862444118
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 9.84997483, -9.94876856,  5.        ]), 'distance': 4.09449034459816, 'localFrame': array([[ 0.69032117, -0.11973637, -0.71352638],
       [ 0.17089855,  0.98528863,  0.        ],
       [ 0.70302943, -0.12194062,  0.70062837]]), 'currentState': array([ 6.87462144, -7.85608091,  6.87956959,  0.69032117, -0.11973637,
       -0.71352638]), 'targetState': array([ 9.84997483, -9.94876856,  5.        ]), 'previousTarget': array([ 9.84997483, -9.94876856,  5.        ])}
episode index:19761
target thresh 86.83216792166569
target distance 24.262962277765418
model initialize at round 19761
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.44260358, -16.29986412,  50.27547366]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.07318863,  0.80434568, -0.58963671],
       [-0.9958858 ,  0.09061715,  0.        ],
       [ 0.0534312 ,  0.58721082,  0.80766859]]), 'currentState': array([-12.62980673, -26.88620485,  37.74598474,   0.07318863,
         0.80434568,  -0.58963671]), 'targetState': array([ 10.71895593, -15.68770167,  51.        ]), 'previousTarget': array([  8.92973133, -16.58714997,  50.11508352])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6274846648896473
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([ 10.71895593, -15.68770167,  51.        ]), 'distance': 3.3469675705198934, 'localFrame': array([[ 0.93258157,  0.17876269,  0.31358494],
       [-0.18825842,  0.98211953,  0.        ],
       [-0.30797789, -0.059035  ,  0.94956015]]), 'currentState': array([  7.99397416, -16.83580578,  52.56796786,   0.93258157,
         0.17876269,   0.31358494]), 'targetState': array([ 10.71895593, -15.68770167,  51.        ]), 'previousTarget': array([ 10.71895593, -15.68770167,  51.        ])}
episode index:19762
target thresh 86.83348463903656
target distance 30.0
model initialize at round 19762
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.14152304, 18.89432353, 64.5450041 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.20711396, -0.79528004,  0.56976616],
       [ 0.96772133,  0.25202267,  0.        ],
       [-0.14359399,  0.55137486,  0.82180687]]), 'currentState': array([31.15947214, 34.49879236, 46.76165078,  0.20711396, -0.79528004,
        0.56976616]), 'targetState': array([ 8.11198027,  8.84283756, 76.        ]), 'previousTarget': array([17.83232834, 20.10522706, 63.60388013])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.627492271867055
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.11198027,  8.84283756, 76.        ]), 'distance': 4.012174749713722, 'localFrame': array([[ 0.05920113, -0.96091045,  0.27045577],
       [ 0.99810753,  0.06149282,  0.        ],
       [-0.01663109,  0.26994394,  0.9627324 ]]), 'currentState': array([ 1.05394937e+01,  1.16069470e+01,  7.43986182e+01,  5.92011337e-02,
       -9.60910454e-01,  2.70455773e-01]), 'targetState': array([ 8.11198027,  8.84283756, 76.        ]), 'previousTarget': array([ 8.11198027,  8.84283756, 76.        ])}
episode index:19763
target thresh 86.83480122474228
target distance 49.50339920125454
model initialize at round 19763
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 12.38440225, -25.81841756,  74.8481723 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.11546086, -0.62852471,  0.76917194],
       [ 0.98354229, -0.18067808,  0.        ],
       [ 0.13897251,  0.75651313,  0.63904188]]), 'currentState': array([ 39.30462311, -25.82581273,  80.4652582 ,  -0.11546086,
        -0.62852471,   0.76917194]), 'targetState': array([-10.85075409, -25.81203471,  70.        ]), 'previousTarget': array([ 11.60023157, -25.4162685 ,  74.0817171 ])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6274987091780921
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.85075409, -25.81203471,  70.        ]), 'distance': 3.1478745834133752, 'localFrame': array([[-0.89111122,  0.42092858,  0.16952851],
       [-0.42711089, -0.90419925,  0.        ],
       [ 0.15328755, -0.07240747,  0.98552528]]), 'currentState': array([ -7.98053972, -25.3575911 ,  71.21015076,  -0.89111122,
         0.42092858,   0.16952851]), 'targetState': array([-10.85075409, -25.81203471,  70.        ]), 'previousTarget': array([-10.85075409, -25.81203471,  70.        ])}
episode index:19764
target thresh 86.836117678796
target distance 41.48018053422825
model initialize at round 19764
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.36233686, -11.82195751,  19.89196538]), 'distance': 27.5, 'localFrame': array([[ 0.31619562, -0.51224582, -0.79851396],
       [ 0.8509402 ,  0.52526259,  0.        ],
       [ 0.41942951, -0.67948763,  0.60197629]]), 'currentState': array([14.5013604 ,  7.19575808, 19.79184077,  0.31619562, -0.51224582,
       -0.79851396]), 'targetState': array([-26.79530123, -32.34210618,  20.        ]), 'previousTarget': array([ -4.89252691, -10.74208352,  20.52802987])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627466961203937
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 61, 'trapConfig': [], 'currentTarget': array([-26.79530123, -32.34210618,  20.        ]), 'distance': 15.028673630053891, 'localFrame': array([[-0.02921055, -0.9840668 , -0.17538322],
       [ 0.99955974, -0.02967043,  0.        ],
       [-0.0052037 , -0.17530601,  0.98450024]]), 'currentState': array([-1.62553818e+01, -2.91120867e+01,  3.02146025e+01, -2.92105485e-02,
       -9.84066801e-01, -1.75383222e-01]), 'targetState': array([-26.79530123, -32.34210618,  20.        ]), 'previousTarget': array([-26.79530123, -32.34210618,  20.        ])}
episode index:19765
target thresh 86.8374340012109
target distance 76.0
model initialize at round 19765
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 15.77509524, -20.05291388,  67.55116688]), 'distance': 27.500000000000004, 'localFrame': array([[-0.90055967,  0.12052029, -0.41769265],
       [-0.13264564, -0.99116353,  0.        ],
       [-0.41400172,  0.05540511,  0.90858838]]), 'currentState': array([ 32.09173103, -27.21595792,  88.49652906,  -0.90055967,
         0.12052029,  -0.41769265]), 'targetState': array([-25.94177162,  -1.7391047 ,  14.        ]), 'previousTarget': array([ 16.85553394, -20.28950643,  69.02316955])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6274587862476466
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-25.94177162,  -1.7391047 ,  14.        ]), 'distance': 1.9887524633715659, 'localFrame': array([[ 0.4399098 ,  0.05280679,  0.89648804],
       [-0.11918441,  0.99287214,  0.        ],
       [-0.890098  , -0.1068474 ,  0.44306793]]), 'currentState': array([-26.37492544,  -1.86126422,  12.0628394 ,   0.4399098 ,
         0.05280679,   0.89648804]), 'targetState': array([-25.94177162,  -1.7391047 ,  14.        ]), 'previousTarget': array([-25.94177162,  -1.7391047 ,  14.        ])}
episode index:19766
target thresh 86.83875019200015
target distance 67.0
model initialize at round 19766
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.37533788, -1.17738401, 31.9801086 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.26821261, -0.70321624,  0.65844431],
       [ 0.9343459 , -0.35636741,  0.        ],
       [ 0.23464809,  0.61521475,  0.75262945]]), 'currentState': array([ 2.86989075, -2.46935392,  4.55116175, -0.26821261, -0.70321624,
        0.65844431]), 'targetState': array([-0.75078426,  0.6605475 , 71.        ]), 'previousTarget': array([ 1.41196601, -0.15188485, 31.45319301])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274270435053869
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-4.75192896e+00, -1.76819412e-02,  6.21316467e+01]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.74636931, -0.17631141,  0.64175318],
       [ 0.22989806,  0.97321472,  0.        ],
       [-0.62456364,  0.14753781,  0.76691125]]), 'currentState': array([-16.0339845 ,  -1.93009022,  37.12548907,   0.74636931,
        -0.17631141,   0.64175318]), 'targetState': array([-0.75078426,  0.6605475 , 71.        ]), 'previousTarget': array([-5.58185066, -0.25649322, 60.62456123])}
episode index:19767
target thresh 86.84006625117688
target distance 69.0
model initialize at round 19767
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.78141291,  23.08765798,  33.57331883]), 'distance': 27.5, 'localFrame': array([[ 0.73154514,  0.34137568,  0.59017316],
       [-0.4228732 ,  0.90618886,  0.        ],
       [-0.53480834, -0.24956841,  0.80727668]]), 'currentState': array([-25.79338661,  32.54489034,  10.21636716,   0.73154514,
         0.34137568,   0.59017316]), 'targetState': array([ 6.16419044,  5.09928977, 78.        ]), 'previousTarget': array([-15.33209673,  22.39296314,  32.51305617])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6274216263402284
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([ 6.16419044,  5.09928977, 78.        ]), 'distance': 1.9746821046554865, 'localFrame': array([[ 0.20838804,  0.66326623,  0.71878531],
       [-0.9540213 ,  0.29973881,  0.        ],
       [-0.21544785, -0.6857365 ,  0.6952321 ]]), 'currentState': array([ 7.37334845,  5.02766794, 76.44045627,  0.20838804,  0.66326623,
        0.71878531]), 'targetState': array([ 6.16419044,  5.09928977, 78.        ]), 'previousTarget': array([ 6.16419044,  5.09928977, 78.        ])}
episode index:19768
target thresh 86.8413821787543
target distance 64.0
model initialize at round 19768
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.37292694, -21.04309824,  40.18148389]), 'distance': 27.5, 'localFrame': array([[-0.67102916, -0.62387061, -0.40063116],
       [ 0.68090348, -0.73237316,  0.        ],
       [-0.29341151, -0.27279115,  0.91623942]]), 'currentState': array([ -1.2617641 , -23.17348838,  16.67432496,  -0.67102916,
        -0.62387061,  -0.40063116]), 'targetState': array([-40.47625366, -17.25319941,  82.        ]), 'previousTarget': array([-15.22697444, -20.26223424,  41.31930629])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6274223947137295
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-40.47625366, -17.25319941,  82.        ]), 'distance': 3.233099728710256, 'localFrame': array([[ 0.22373262,  0.02403501,  0.97435416],
       [-0.10681282,  0.99427915,  0.        ],
       [-0.96878002, -0.10407352,  0.22501993]]), 'currentState': array([-4.01513395e+01, -1.90999078e+01,  7.93661753e+01,  2.23732622e-01,
        2.40350135e-02,  9.74354162e-01]), 'targetState': array([-40.47625366, -17.25319941,  82.        ]), 'previousTarget': array([-40.47625366, -17.25319941,  82.        ])}
episode index:19769
target thresh 86.84269797474553
target distance 26.868437176885696
model initialize at round 19769
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.7908786 , -16.90033959,  28.9354017 ]), 'distance': 27.5, 'localFrame': array([[-0.28555726, -0.79496771,  0.53524144],
       [ 0.94112529, -0.33805795,  0.        ],
       [ 0.18094262,  0.50372925,  0.84469912]]), 'currentState': array([-0.66588746, -2.19201166, 14.39580798, -0.28555726, -0.79496771,
        0.53524144]), 'targetState': array([-26.35097916, -23.03531848,  35.        ]), 'previousTarget': array([-17.64589583, -15.84936454,  28.19622691])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6274194700201414
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-26.35097916, -23.03531848,  35.        ]), 'distance': 2.2735241229413177, 'localFrame': array([[ 0.16928216, -0.70754597,  0.68609202],
       [ 0.97255203,  0.23268553,  0.        ],
       [-0.15964368,  0.66726018,  0.72751477]]), 'currentState': array([-25.26486508, -23.38913704,  33.03427352,   0.16928216,
        -0.70754597,   0.68609202]), 'targetState': array([-26.35097916, -23.03531848,  35.        ]), 'previousTarget': array([-26.35097916, -23.03531848,  35.        ])}
episode index:19770
target thresh 86.84401363916375
target distance 45.08433429538936
model initialize at round 19770
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.87298642, -8.52298444, 30.13498314]), 'distance': 27.5, 'localFrame': array([[ 0.89111553, -0.38727467, -0.23649829],
       [ 0.39858167,  0.91713284,  0.        ],
       [ 0.21690035, -0.09426389,  0.9716319 ]]), 'currentState': array([ -4.15635634, -11.14033066,  48.79576896,   0.89111553,
        -0.38727467,  -0.23649829]), 'targetState': array([39.63131944, -5.41835022,  8.        ]), 'previousTarget': array([14.59529343, -8.34548076, 31.32324762])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6274208983621855
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([39.63131944, -5.41835022,  8.        ]), 'distance': 2.0965765059668864, 'localFrame': array([[ 0.97382489, -0.19683197,  0.1136761 ],
       [ 0.19811619,  0.98017854,  0.        ],
       [-0.11142288,  0.02252108,  0.99351786]]), 'currentState': array([38.19890518, -4.05991682,  8.70603196,  0.97382489, -0.19683197,
        0.1136761 ]), 'targetState': array([39.63131944, -5.41835022,  8.        ]), 'previousTarget': array([39.63131944, -5.41835022,  8.        ])}
episode index:19771
target thresh 86.84532917202208
target distance 35.34717415373626
model initialize at round 19771
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.39366489, -5.38953289, 84.34216164]), 'distance': 27.500000000000004, 'localFrame': array([[-0.58053752, -0.39778313,  0.71045392],
       [ 0.56523862, -0.82492746,  0.        ],
       [ 0.58607294,  0.40157599,  0.70374372]]), 'currentState': array([ 0.19144451, 15.98684202, 71.15842717, -0.58053752, -0.39778313,
        0.71045392]), 'targetState': array([ 18.75023093, -19.42752789,  93.        ]), 'previousTarget': array([11.96049449, -5.43242509, 83.89354745])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.627428111706685
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.75023093, -19.42752789,  93.        ]), 'distance': 3.259267826786388, 'localFrame': array([[ 0.04357975, -0.96809794, -0.24675327],
       [ 0.99898832,  0.04497031,  0.        ],
       [ 0.01109657, -0.24650364,  0.96907834]]), 'currentState': array([ 1.72918703e+01, -1.84656883e+01,  9.02484776e+01,  4.35797538e-02,
       -9.68097944e-01, -2.46753271e-01]), 'targetState': array([ 18.75023093, -19.42752789,  93.        ]), 'previousTarget': array([ 18.75023093, -19.42752789,  93.        ])}
episode index:19772
target thresh 86.84664457333372
target distance 22.533723051026975
model initialize at round 19772
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.49722628, 14.62142437, 73.        ]), 'distance': 25.122147979327064, 'localFrame': array([[-0.34704153,  0.61022645, -0.71216982],
       [-0.8692595 , -0.49435606,  0.        ],
       [-0.35206547,  0.61906039,  0.70200723]]), 'currentState': array([ 4.84521585, -7.39622277, 68.79351217, -0.34704153,  0.61022645,
       -0.71216982]), 'targetState': array([-6.49722628, 14.62142437, 73.        ]), 'previousTarget': array([-6.49722628, 14.62142437, 73.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6274398767520971
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.49722628, 14.62142437, 73.        ]), 'distance': 3.6202863297493284, 'localFrame': array([[-0.42574118,  0.76052367, -0.49025319],
       [-0.87258039, -0.48847054,  0.        ],
       [-0.23947424,  0.42778532,  0.87158007]]), 'currentState': array([-4.59045641, 13.40182222, 70.17453148, -0.42574118,  0.76052367,
       -0.49025319]), 'targetState': array([-6.49722628, 14.62142437, 73.        ]), 'previousTarget': array([-6.49722628, 14.62142437, 73.        ])}
episode index:19773
target thresh 86.8479598431118
target distance 76.0
model initialize at round 19773
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.71517451, -18.22533593,  30.41620218]), 'distance': 27.5, 'localFrame': array([[-0.6982996 , -0.3431278 ,  0.62820456],
       [ 0.44101098, -0.89750171,  0.        ],
       [ 0.56381467,  0.27704511,  0.77804822]]), 'currentState': array([ 17.86707173, -19.31952037,   5.77103725,  -0.6982996 ,
        -0.3431278 ,   0.62820456]), 'targetState': array([-19.22639573, -15.97954027,  81.        ]), 'previousTarget': array([  6.50375263, -17.31390205,  29.585723  ])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6274363785353375
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.22639573, -15.97954027,  81.        ]), 'distance': 2.7615472408837003, 'localFrame': array([[-0.55339277,  0.47548474,  0.68386454],
       [-0.65169802, -0.75847854,  0.        ],
       [ 0.51869658, -0.44567317,  0.729609  ]]), 'currentState': array([-20.8177683 , -17.05870616,  79.01780992,  -0.55339277,
         0.47548474,   0.68386454]), 'targetState': array([-19.22639573, -15.97954027,  81.        ]), 'previousTarget': array([-19.22639573, -15.97954027,  81.        ])}
episode index:19774
target thresh 86.84927498136948
target distance 31.0
model initialize at round 19774
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.67725046, -8.38167047, 45.8085671 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13580134,  0.95505779,  0.26348171],
       [-0.99004151, -0.14077573,  0.        ],
       [ 0.03709183, -0.26085783,  0.96466439]]), 'currentState': array([ -0.09277395, -18.70096678,  71.24970618,  -0.13580134,
         0.95505779,   0.26348171]), 'targetState': array([-2.10128865, -5.62001655, 39.        ]), 'previousTarget': array([-1.64341946, -8.45418982, 45.1103601 ])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6274455968154733
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.10128865, -5.62001655, 39.        ]), 'distance': 1.8547630988129973, 'localFrame': array([[ 0.11244513,  0.11824413, -0.9865974 ],
       [-0.72465291,  0.68911404,  0.        ],
       [ 0.67987812,  0.71494067,  0.16317346]]), 'currentState': array([-2.19018039, -6.40318997, 40.67895319,  0.11244513,  0.11824413,
       -0.9865974 ]), 'targetState': array([-2.10128865, -5.62001655, 39.        ]), 'previousTarget': array([-2.10128865, -5.62001655, 39.        ])}
episode index:19775
target thresh 86.8505899881199
target distance 65.0
model initialize at round 19775
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.1303674 , -6.39735624, 56.02693783]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.22787053, -0.77716176, -0.58659579],
       [ 0.9596013 ,  0.28136338,  0.        ],
       [ 0.16504657, -0.56289808,  0.80987986]]), 'currentState': array([-1.56782163,  0.90701039, 82.50013094,  0.22787053, -0.77716176,
       -0.58659579]), 'targetState': array([  1.93443766, -16.88958114,  18.        ]), 'previousTarget': array([-4.74140845e-02, -5.25339615e+00,  5.66965014e+01])}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6274381481837863
{'scaleFactor': 20, 'timeStep': 74, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([  1.93443766, -16.88958114,  18.        ]), 'distance': 1.7908472053254214, 'localFrame': array([[-0.72371882, -0.25886389, -0.6397035 ],
       [ 0.33678969, -0.9415799 ,  0.        ],
       [-0.60233195, -0.21544554,  0.76862178]]), 'currentState': array([  1.27752649, -17.29159699,  19.6167822 ,  -0.72371882,
        -0.25886389,  -0.6397035 ]), 'targetState': array([  1.93443766, -16.88958114,  18.        ]), 'previousTarget': array([  1.93443766, -16.88958114,  18.        ])}
episode index:19776
target thresh 86.85190486337623
target distance 36.465682627490956
model initialize at round 19776
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.7070614 , 15.40040325, 30.93624704]), 'distance': 27.499999999999996, 'localFrame': array([[-0.37976557, -0.43672808, -0.81550395],
       [ 0.75460379, -0.65618071,  0.        ],
       [-0.53511796, -0.61538237,  0.57875151]]), 'currentState': array([33.26342175, 33.97958056, 41.07717314, -0.37976557, -0.43672808,
       -0.81550395]), 'targetState': array([ 0.23628767, -0.97168315, 22.        ]), 'previousTarget': array([16.51669636, 16.72621738, 31.70660591])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6274412836915824
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.23628767, -0.97168315, 22.        ]), 'distance': 2.360523429572299, 'localFrame': array([[ 0.14412312, -0.4108794 ,  0.90022589],
       [ 0.94363233,  0.3309955 ,  0.        ],
       [-0.29797072,  0.84948225,  0.43542319]]), 'currentState': array([ 1.19025126, -1.88653926, 20.04422327,  0.14412312, -0.4108794 ,
        0.90022589]), 'targetState': array([ 0.23628767, -0.97168315, 22.        ]), 'previousTarget': array([ 0.23628767, -0.97168315, 22.        ])}
episode index:19777
target thresh 86.85321960715162
target distance 38.0
model initialize at round 19777
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.41875964, -3.90435232, 81.4041515 ]), 'distance': 27.5, 'localFrame': array([[-0.56425829,  0.09578744,  0.82002277],
       [-0.16736375, -0.98589522,  0.        ],
       [ 0.80845653, -0.13724208,  0.57233089]]), 'currentState': array([  9.16852681, -11.38251104,  55.20746491,  -0.56425829,
         0.09578744,   0.82002277]), 'targetState': array([ 3.90208063, -0.87964014, 92.        ]), 'previousTarget': array([ 5.85513037, -4.52401533, 79.99992132])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6274401493394459
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 3.90208063, -0.87964014, 92.        ]), 'distance': 2.5036123953476963, 'localFrame': array([[ 0.48712978,  0.87285713,  0.02872286],
       [-0.87321741,  0.48733084,  0.        ],
       [-0.01399754, -0.0250813 ,  0.99958741]]), 'currentState': array([ 5.14631903e+00, -2.84037628e+00,  9.10643400e+01,  4.87129778e-01,
        8.72857134e-01,  2.87228599e-02]), 'targetState': array([ 3.90208063, -0.87964014, 92.        ]), 'previousTarget': array([ 3.90208063, -0.87964014, 92.        ])}
episode index:19778
target thresh 86.8545342194592
target distance 35.94933114837074
model initialize at round 19778
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([8.35997393, 0.17169518, 4.28057415]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.83643555, -0.31284397,  0.4500047 ],
       [ 0.35031892,  0.93663048,  0.        ],
       [-0.42148811,  0.15764516,  0.89302619]]), 'currentState': array([35.82715435,  1.28926985,  5.02557523,  0.83643555, -0.31284397,
        0.4500047 ]), 'targetState': array([-1.98441492, -0.24919356,  4.        ]), 'previousTarget': array([6.50966452, 0.17454099, 4.23627921])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6274485508832199
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.98441492, -0.24919356,  4.        ]), 'distance': 3.9248248526435807, 'localFrame': array([[-0.84934945, -0.12188368,  0.51356585],
       [ 0.14204726, -0.98985988,  0.        ],
       [ 0.50835823,  0.07295062,  0.85805018]]), 'currentState': array([ 0.33738887, -2.60636886,  6.11120863, -0.84934945, -0.12188368,
        0.51356585]), 'targetState': array([-1.98441492, -0.24919356,  4.        ]), 'previousTarget': array([-1.98441492, -0.24919356,  4.        ])}
episode index:19779
target thresh 86.85584870031211
target distance 54.02976374747625
model initialize at round 19779
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.32756964, 12.64531755, 36.6896216 ]), 'distance': 27.5, 'localFrame': array([[-0.06228562, -0.88610341, -0.4592834 ],
       [ 0.99753866, -0.07011859,  0.        ],
       [-0.0322043 , -0.45815295,  0.88828979]]), 'currentState': array([ 6.54963849, 34.3536156 , 47.60645751, -0.06228562, -0.88610341,
       -0.4592834 ]), 'targetState': array([-24.83462834, -18.55373912,  21.        ]), 'previousTarget': array([-6.34823672, 13.71356401, 37.72197739])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6274459172229112
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.83462834, -18.55373912,  21.        ]), 'distance': 3.091315335541687, 'localFrame': array([[-0.75590983, -0.19741796,  0.62420067],
       [ 0.25269045, -0.96754718,  0.        ],
       [ 0.60394359,  0.15772954,  0.78126406]]), 'currentState': array([-22.03098431, -18.64925845,  19.70127496,  -0.75590983,
        -0.19741796,   0.62420067]), 'targetState': array([-24.83462834, -18.55373912,  21.        ]), 'previousTarget': array([-24.83462834, -18.55373912,  21.        ])}
episode index:19780
target thresh 86.85716304972351
target distance 42.0
model initialize at round 19780
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.5475807 , -6.92677148, 20.93736795]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.30539403, -0.45919567, -0.83419052],
       [ 0.8326661 ,  0.55377538,  0.        ],
       [ 0.46195417, -0.69460217,  0.55147636]]), 'currentState': array([ 1.07486448,  3.61530704, 45.73986696,  0.30539403, -0.45919567,
       -0.83419052]), 'targetState': array([ 10.06418973, -13.700806  ,   5.        ]), 'previousTarget': array([ 5.82062757, -6.42057405, 22.28190713])}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6274389632951917
{'scaleFactor': 20, 'timeStep': 72, 'trapCount': 36, 'trapConfig': [], 'currentTarget': array([ 10.06418973, -13.700806  ,   5.        ]), 'distance': 2.591740507325225, 'localFrame': array([[-0.07246554, -0.96091569,  0.26718866],
       [ 0.99716851, -0.07519947,  0.        ],
       [ 0.02009245,  0.26643212,  0.96364424]]), 'currentState': array([ 10.06141446, -11.64368481,   3.42349641,  -0.07246554,
        -0.96091569,   0.26718866]), 'targetState': array([ 10.06418973, -13.700806  ,   5.        ]), 'previousTarget': array([ 10.06418973, -13.700806  ,   5.        ])}
episode index:19781
target thresh 86.85847726770655
target distance 17.0
model initialize at round 19781
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.73153565, -1.00089176, 64.        ]), 'distance': 18.716533505383474, 'localFrame': array([[ 0.52095582, -0.64872144, -0.55476618],
       [ 0.77970737,  0.62614408,  0.        ],
       [ 0.34736356, -0.43255528,  0.8320063 ]]), 'currentState': array([-6.02733786,  5.15262295, 79.88215241,  0.52095582, -0.64872144,
       -0.55476618]), 'targetState': array([ 1.73153565, -1.00089176, 64.        ]), 'previousTarget': array([ 1.73153565, -1.00089176, 64.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6274525058738472
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 1.73153565, -1.00089176, 64.        ]), 'distance': 3.487232807016024, 'localFrame': array([[ 0.90454045, -0.170055  , -0.3910088 ],
       [ 0.18476468,  0.98278279,  0.        ],
       [ 0.38427672, -0.07224461,  0.92038694]]), 'currentState': array([ 2.67808171,  0.97962882, 66.70968291,  0.90454045, -0.170055  ,
       -0.3910088 ]), 'targetState': array([ 1.73153565, -1.00089176, 64.        ]), 'previousTarget': array([ 1.73153565, -1.00089176, 64.        ])}
episode index:19782
target thresh 86.85979135427434
target distance 71.365534655461
model initialize at round 19782
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-1.9991137 , 24.47560141, 26.57524163]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.97394042,  0.19217107,  0.12045884],
       [-0.19358066,  0.98108436,  0.        ],
       [-0.11818029, -0.0233185 ,  0.99271832]]), 'currentState': array([-5.13012552, 48.73070526, 14.        ,  0.97394042,  0.19217107,
        0.12045884]), 'targetState': array([  4.08221733, -22.63482939,  51.        ]), 'previousTarget': array([-1.9991137 , 24.47560141, 26.57524163])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274207891217937
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 52, 'trapConfig': [], 'currentTarget': array([  4.08221733, -22.63482939,  51.        ]), 'distance': 9.66235282466162, 'localFrame': array([[ 0.2112959 , -0.96286529,  0.16806095],
       [ 0.9767581 ,  0.2143446 ,  0.        ],
       [-0.03602296,  0.1641549 ,  0.98577661]]), 'currentState': array([  3.60830199, -16.35091318,  43.67546156,   0.2112959 ,
        -0.96286529,   0.16806095]), 'targetState': array([  4.08221733, -22.63482939,  51.        ]), 'previousTarget': array([  4.08221733, -22.63482939,  51.        ])}
episode index:19783
target thresh 86.86110530944006
target distance 44.0
model initialize at round 19783
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.22212005e+01, -2.32904113e-02,  3.19148766e+01]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.47448284, -0.67943547,  0.55967265],
       [ 0.81986747,  0.57255334,  0.        ],
       [-0.32044244,  0.4588574 ,  0.82871378]]), 'currentState': array([-26.51587098,  -0.05053231,   8.4220766 ,   0.47448284,
        -0.67943547,   0.55967265]), 'targetState': array([-0.,  0., 52.]), 'previousTarget': array([-13.22827447,   0.44991869,  31.20069107])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6274187478759796
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([0.00000000e+00, 2.22044605e-16, 5.20000000e+01]), 'distance': 3.1334586268121543, 'localFrame': array([[ 0.9633397 , -0.2588488 , -0.07052599],
       [ 0.25949496,  0.96574446,  0.        ],
       [ 0.06811009, -0.01830114,  0.99750994]]), 'currentState': array([-2.65295914,  1.6666677 , 52.05088785,  0.9633397 , -0.2588488 ,
       -0.07052599]), 'targetState': array([-0.,  0., 52.]), 'previousTarget': array([ 0.,  0., 52.])}
episode index:19784
target thresh 86.86241913321683
target distance 29.0
model initialize at round 19784
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.90863291,  7.83430799, 23.44705442]), 'distance': 27.5, 'localFrame': array([[-0.51989323, -0.16479778,  0.83818418],
       [ 0.30216654, -0.95325515,  0.        ],
       [ 0.79900339,  0.25327121,  0.54538727]]), 'currentState': array([ 4.5626928 , -7.94911794,  0.98825766, -0.51989323, -0.16479778,
        0.83818418]), 'targetState': array([ 2.49966601, 11.73676573, 29.        ]), 'previousTarget': array([ 3.1685164 ,  7.8897174 , 23.02274387])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6274229497703135
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 2.49966601, 11.73676573, 29.        ]), 'distance': 3.598725679257965, 'localFrame': array([[-0.35736044,  0.61990738,  0.69857595],
       [-0.86635397, -0.49943047,  0.        ],
       [ 0.34889012, -0.60521405,  0.71553591]]), 'currentState': array([ 4.90665937,  9.67829549, 27.29122566, -0.35736044,  0.61990738,
        0.69857595]), 'targetState': array([ 2.49966601, 11.73676573, 29.        ]), 'previousTarget': array([ 2.49966601, 11.73676573, 29.        ])}
episode index:19785
target thresh 86.8637328256178
target distance 29.851745117692133
model initialize at round 19785
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.98405946, -32.720467  ,  14.93173638]), 'distance': 27.5, 'localFrame': array([[ 0.86188976, -0.2693871 ,  0.42962382],
       [ 0.29832196,  0.9544653 ,  0.        ],
       [-0.41006103,  0.12816622,  0.90300796]]), 'currentState': array([-4.21069012, -6.47688125, 14.32322501,  0.86188976, -0.2693871 ,
        0.42962382]), 'targetState': array([  4.90335737, -35.66450738,  15.        ]), 'previousTarget': array([  3.53267177, -31.7301019 ,  14.73640365])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6274334163027951
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.90335737, -35.66450738,  15.        ]), 'distance': 3.9190364240210394, 'localFrame': array([[ 0.24343051,  0.37546055, -0.89429914],
       [-0.839075  ,  0.54401576,  0.        ],
       [ 0.48651283,  0.75038405,  0.4474696 ]]), 'currentState': array([  5.42367438, -33.11226   ,  17.92816496,   0.24343051,
         0.37546055,  -0.89429914]), 'targetState': array([  4.90335737, -35.66450738,  15.        ]), 'previousTarget': array([  4.90335737, -35.66450738,  15.        ])}
episode index:19786
target thresh 86.86504638665609
target distance 42.72675167394533
model initialize at round 19786
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.51840336, -20.67158291,  84.75437529]), 'distance': 27.499999999999996, 'localFrame': array([[-0.53813199,  0.49308761, -0.68357777],
       [-0.67557568, -0.73729065,  0.        ],
       [-0.5039955 ,  0.46180852,  0.72987768]]), 'currentState': array([40.67800936, -8.49595973, 89.68566704, -0.53813199,  0.49308761,
       -0.68357777]), 'targetState': array([ -1.87520062, -29.94133635,  81.        ]), 'previousTarget': array([ 16.52573038, -21.24757565,  85.30665339])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6274351774906802
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ -1.87520062, -29.94133635,  81.        ]), 'distance': 1.7681217700288097, 'localFrame': array([[-0.72423271, -0.21751906, -0.65434887],
       [ 0.28765026, -0.95773552,  0.        ],
       [-0.62669315, -0.18822362,  0.75619281]]), 'currentState': array([ -0.34553959, -29.05525208,  81.03530435,  -0.72423271,
        -0.21751906,  -0.65434887]), 'targetState': array([ -1.87520062, -29.94133635,  81.        ]), 'previousTarget': array([ -1.87520062, -29.94133635,  81.        ])}
episode index:19787
target thresh 86.86635981634485
target distance 43.354588002884476
model initialize at round 19787
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-17.34042644,   1.80863781,  87.09247244]), 'distance': 27.500000000000004, 'localFrame': array([[-0.55198547, -0.23442326, -0.80022358],
       [ 0.39089972, -0.92043327,  0.        ],
       [-0.73655241, -0.31280717,  0.59970178]]), 'currentState': array([-30.4309597 , -21.04653549,  95.        ,  -0.55198547,
        -0.23442326,  -0.80022358]), 'targetState': array([-5.5991779 , 22.30805251, 80.        ]), 'previousTarget': array([-17.34042644,   1.80863781,  87.09247244])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6274316819863875
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 21, 'trapConfig': [], 'currentTarget': array([-5.5991779 , 22.30805251, 80.        ]), 'distance': 3.613572306528869, 'localFrame': array([[ 0.93703599, -0.06590087, -0.34295863],
       [ 0.07015579,  0.99753605,  0.        ],
       [ 0.34211359, -0.02406053,  0.93935051]]), 'currentState': array([-6.81931277e+00,  1.99228379e+01,  8.24248561e+01,  9.37035995e-01,
       -6.59008732e-02, -3.42958626e-01]), 'targetState': array([-5.5991779 , 22.30805251, 80.        ]), 'previousTarget': array([-5.5991779 , 22.30805251, 80.        ])}
episode index:19788
target thresh 86.8676731146972
target distance 22.0
model initialize at round 19788
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 7.5984604 ,  7.95382924, 35.        ]), 'distance': 23.668358596898834, 'localFrame': array([[-0.15765082,  0.95420062, -0.25425851],
       [-0.98662477, -0.16300786,  0.        ],
       [-0.04144613,  0.25085774,  0.96713629]]), 'currentState': array([ 1.24963021, 13.94411755, 57.        , -0.15765082,  0.95420062,
       -0.25425851]), 'targetState': array([ 7.5984604 ,  7.95382924, 35.        ]), 'previousTarget': array([ 7.5984604 ,  7.95382924, 35.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6274443197821838
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 7.5984604 ,  7.95382924, 35.        ]), 'distance': 3.4895587052863752, 'localFrame': array([[-0.42254351,  0.33659294, -0.84152372],
       [-0.62306623, -0.78216908,  0.        ],
       [-0.65821384,  0.52432501,  0.54022017]]), 'currentState': array([ 4.66380437,  8.7441342 , 36.71471049, -0.42254351,  0.33659294,
       -0.84152372]), 'targetState': array([ 7.5984604 ,  7.95382924, 35.        ]), 'previousTarget': array([ 7.5984604 ,  7.95382924, 35.        ])}
episode index:19789
target thresh 86.8689862817263
target distance 56.0
model initialize at round 19789
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.45331961, 18.78095869, 63.79033567]), 'distance': 27.500000000000004, 'localFrame': array([[-0.15346566,  0.92709872, -0.34195944],
       [-0.98657465, -0.16331091,  0.        ],
       [-0.05584571,  0.33736852,  0.93971471]]), 'currentState': array([ 6.70965053, 13.54580781, 37.64906328, -0.15346566,  0.92709872,
       -0.34195944]), 'targetState': array([21.50448275, 25.03112506, 95.        ]), 'previousTarget': array([13.55257884, 18.20530108, 64.96487172])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6274126146624374
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([21.50448275, 25.03112506, 95.        ]), 'distance': 15.919989623407195, 'localFrame': array([[ 0.94707907, -0.280007  ,  0.1569628 ],
       [ 0.28352139,  0.95896591,  0.        ],
       [-0.15052198,  0.04450231,  0.98760452]]), 'currentState': array([14.25650347, 24.03776696, 80.86047718,  0.94707907, -0.280007  ,
        0.1569628 ]), 'targetState': array([21.50448275, 25.03112506, 95.        ]), 'previousTarget': array([21.50448275, 25.03112506, 95.        ])}
episode index:19790
target thresh 86.87029931744523
target distance 49.0
model initialize at round 19790
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.43191878,   3.96261508,  44.13367528]), 'distance': 27.5, 'localFrame': array([[ 0.26278421,  0.73465246, -0.62548399],
       [-0.94157603,  0.3368005 ,  0.        ],
       [ 0.21066332,  0.58894074,  0.780237  ]]), 'currentState': array([-29.78335382, -15.0715681 ,  24.33156696,   0.26278421,
         0.73465246,  -0.62548399]), 'targetState': array([-26.39363087,  32.67072466,  74.        ]), 'previousTarget': array([-29.1681145 ,   3.12147927,  44.4146234 ])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6274143765454308
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.39363087,  32.67072466,  74.        ]), 'distance': 2.8135848718840824, 'localFrame': array([[-0.42406395,  0.62827459,  0.65225824],
       [-0.82886176, -0.55945346,  0.        ],
       [ 0.36490813, -0.54063191,  0.75799683]]), 'currentState': array([-27.76038819,  30.23454439,  74.33654115,  -0.42406395,
         0.62827459,   0.65225824]), 'targetState': array([-26.39363087,  32.67072466,  74.        ]), 'previousTarget': array([-26.39363087,  32.67072466,  74.        ])}
episode index:19791
target thresh 86.87161222186717
target distance 79.0
model initialize at round 19791
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-30.41199659,  -5.86091787,  34.10064842]), 'distance': 27.5, 'localFrame': array([[-0.24322369, -0.69343909,  0.67822154],
       [ 0.9436375 , -0.33098076,  0.        ],
       [ 0.22447828,  0.63999528,  0.7348575 ]]), 'currentState': array([-32.67610612,   4.72595574,   8.82137004,  -0.24322369,
        -0.69343909,   0.67822154]), 'targetState': array([-25.67412694, -28.01498181,  87.        ]), 'previousTarget': array([-29.61673066,  -4.98021742,  33.2215205 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273826761424122
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 26, 'trapConfig': [], 'currentTarget': array([-25.67412694, -28.01498181,  87.        ]), 'distance': 11.350176132909494, 'localFrame': array([[ 0.77594106, -0.18970843,  0.60160301],
       [ 0.2374932 ,  0.9713892 ,  0.        ],
       [-0.58439067,  0.14287662,  0.79879523]]), 'currentState': array([-26.69689966, -27.04246824,  75.73791087,   0.77594106,
        -0.18970843,   0.60160301]), 'targetState': array([-25.67412694, -28.01498181,  87.        ]), 'previousTarget': array([-25.67412694, -28.01498181,  87.        ])}
episode index:19792
target thresh 86.87292499500523
target distance 11.504832259449675
model initialize at round 19792
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.39127495, -10.99303888,   0.        ]), 'distance': 13.126061260024425, 'localFrame': array([[-0.05848169, -0.94226694, -0.3297164 ],
       [ 0.99807952, -0.0619457 ,  0.        ],
       [-0.02042451, -0.32908319,  0.94408003]]), 'currentState': array([ 7.12530337, -0.73283669,  4.6555984 , -0.05848169, -0.94226694,
       -0.3297164 ]), 'targetState': array([  0.39127495, -10.99303888,   0.        ]), 'previousTarget': array([  0.39127495, -10.99303888,   0.        ])}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6273990257293246
{'scaleFactor': 20, 'timeStep': 6, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.39127495, -10.99303888,   0.        ]), 'distance': 4.43496491746917, 'localFrame': array([[-0.72512013, -0.68011322,  0.10792038],
       [ 0.68410873, -0.72938004,  0.        ],
       [ 0.07871497,  0.07382928,  0.99415954]]), 'currentState': array([ 2.12738832, -8.09833328,  2.87671752, -0.72512013, -0.68011322,
        0.10792038]), 'targetState': array([  0.39127495, -10.99303888,   0.        ]), 'previousTarget': array([ 3.91274948e-01, -1.09930389e+01, -4.44089210e-16])}
episode index:19793
target thresh 86.87423763687254
target distance 39.0
model initialize at round 19793
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.77992067, -0.06403384, 21.0155051 ]), 'distance': 27.5, 'localFrame': array([[-0.0720805 , -0.62143522, -0.78014272],
       [ 0.99334024, -0.11521791,  0.        ],
       [-0.08988641, -0.77494716,  0.62560158]]), 'currentState': array([-10.58471561,   1.94907681,  43.73389716,  -0.0720805 ,
        -0.62143522,  -0.78014272]), 'targetState': array([14.93503084, -1.39458017,  6.        ]), 'previousTarget': array([ 4.98768697,  0.31949825, 21.81791377])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6274039555722178
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.93503084, -1.39458017,  6.        ]), 'distance': 2.6085314891701574, 'localFrame': array([[ 0.86446002,  0.36457787, -0.3461096 ],
       [-0.38859536,  0.92140851,  0.        ],
       [ 0.31890833,  0.13449658,  0.93819409]]), 'currentState': array([13.19403422,  0.28288161,  6.97953524,  0.86446002,  0.36457787,
       -0.3461096 ]), 'targetState': array([14.93503084, -1.39458017,  6.        ]), 'previousTarget': array([14.93503084, -1.39458017,  6.        ])}
episode index:19794
target thresh 86.87555014748224
target distance 58.896267307752424
model initialize at round 19794
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  4.54951596, -14.34168462,  23.7849876 ]), 'distance': 27.5, 'localFrame': array([[-0.60502986,  0.68554713,  0.40492468],
       [-0.74976442, -0.66170485,  0.        ],
       [ 0.26794063, -0.30359812,  0.91435004]]), 'currentState': array([ 27.42006995, -29.6032193 ,  24.3077869 ,  -0.60502986,
         0.68554713,   0.40492468]), 'targetState': array([-29.79081572,   8.57363975,  23.        ]), 'previousTarget': array([  6.15277478, -15.13815011,  23.61028639])}
done in step count: 82
reward sum = 0.43861750180991077
running average episode reward sum: 0.627394418494483
{'scaleFactor': 20, 'timeStep': 83, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-29.79081572,   8.57363975,  23.        ]), 'distance': 3.7289567698453654, 'localFrame': array([[ 0.73775809, -0.42906064,  0.52117172],
       [ 0.50273562,  0.86444022,  0.        ],
       [-0.4505218 ,  0.26201159,  0.85345184]]), 'currentState': array([-30.76929843,   6.30705368,  20.20531258,   0.73775809,
        -0.42906064,   0.52117172]), 'targetState': array([-29.79081572,   8.57363975,  23.        ]), 'previousTarget': array([-29.79081572,   8.57363975,  23.        ])}
episode index:19795
target thresh 86.87686252684743
target distance 17.223264915143208
model initialize at round 19795
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.72664092, -19.64973657,  36.        ]), 'distance': 24.049957392177944, 'localFrame': array([[-0.76871418, -0.57672494, -0.27652641],
       [ 0.60012606, -0.79990544,  0.        ],
       [-0.22119498, -0.16595071,  0.96100632]]), 'currentState': array([ 12.72728809, -36.76496553,  39.83895932,  -0.76871418,
        -0.57672494,  -0.27652641]), 'targetState': array([ -3.72664092, -19.64973657,  36.        ]), 'previousTarget': array([ -3.72664092, -19.64973657,  36.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6274070537038438
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.72664092, -19.64973657,  36.        ]), 'distance': 2.7996430558353493, 'localFrame': array([[-0.06716915,  0.75317489, -0.65438207],
       [-0.99604691, -0.08882881,  0.        ],
       [-0.05812798,  0.65179523,  0.75616408]]), 'currentState': array([ -2.53729877, -21.93603388,  37.09376007,  -0.06716915,
         0.75317489,  -0.65438207]), 'targetState': array([ -3.72664092, -19.64973657,  36.        ]), 'previousTarget': array([ -3.72664092, -19.64973657,  36.        ])}
episode index:19796
target thresh 86.87817477498125
target distance 56.0
model initialize at round 19796
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.22877024, -4.15133748, 42.40534974]), 'distance': 27.5, 'localFrame': array([[-0.28344402,  0.84588642, -0.45181373],
       [-0.94818379, -0.31772235,  0.        ],
       [-0.14355132,  0.42840246,  0.8921123 ]]), 'currentState': array([ 0.85634307,  2.09622439, 16.57863605, -0.28344402,  0.84588642,
       -0.45181373]), 'targetState': array([-14.89621743, -11.79418103,  74.        ]), 'previousTarget': array([-5.98627217, -4.77318734, 43.74913331])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6273988945747568
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-14.89621743, -11.79418103,  74.        ]), 'distance': 2.6308903852666923, 'localFrame': array([[ 0.12288024, -0.19985465,  0.97208979],
       [ 0.85186227,  0.52376585,  0.        ],
       [-0.50914744,  0.82808662,  0.23460911]]), 'currentState': array([-15.65248663, -10.61048375,  71.77547716,   0.12288024,
        -0.19985465,   0.97208979]), 'targetState': array([-14.89621743, -11.79418103,  74.        ]), 'previousTarget': array([-14.89621743, -11.79418103,  74.        ])}
episode index:19797
target thresh 86.8794868918968
target distance 36.11908798401967
model initialize at round 19797
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.54476611, -11.07734626,  38.32336893]), 'distance': 27.5, 'localFrame': array([[ 0.65745378,  0.64704194, -0.38612337],
       [-0.70144066,  0.71272786,  0.        ],
       [ 0.27520088,  0.27084263,  0.92244715]]), 'currentState': array([ 15.95178605, -37.48834857,  45.96256597,   0.65745378,
         0.64704194,  -0.38612337]), 'targetState': array([16.72511378, -3.04476089, 36.        ]), 'previousTarget': array([ 16.31067664, -12.68358123,  38.66862229])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6274081040390284
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.72511378, -3.04476089, 36.        ]), 'distance': 2.4994122372312466, 'localFrame': array([[-0.84189375,  0.41049044, -0.35030345],
       [-0.43826025, -0.89884813,  0.        ],
       [-0.3148696 ,  0.15352408,  0.93663627]]), 'currentState': array([16.59660014, -5.51116419, 36.3839278 , -0.84189375,  0.41049044,
       -0.35030345]), 'targetState': array([16.72511378, -3.04476089, 36.        ]), 'previousTarget': array([16.72511378, -3.04476089, 36.        ])}
episode index:19798
target thresh 86.88079887760723
target distance 29.177068697678592
model initialize at round 19798
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-4.2548229 ,  3.34892194, 36.88100013]), 'distance': 27.499999999999996, 'localFrame': array([[-0.35549765,  0.71806274,  0.59833713],
       [-0.89618435, -0.44368188,  0.        ],
       [ 0.26547134, -0.53622038,  0.80124445]]), 'currentState': array([-10.68105578, -23.24570351,  34.10982201,  -0.35549765,
         0.71806274,   0.59833713]), 'targetState': array([-3.97886776,  4.49094771, 37.        ]), 'previousTarget': array([-4.61354261,  1.82921441, 36.63509243])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273764151605982
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 77, 'trapConfig': [], 'currentTarget': array([-3.97886776,  4.49094771, 37.        ]), 'distance': 5.415529428791373, 'localFrame': array([[ 0.31691886,  0.50477507,  0.80297233],
       [-0.84691487,  0.5317285 ,  0.        ],
       [-0.42696328, -0.68004921,  0.5960163 ]]), 'currentState': array([-9.04733663,  3.98481336, 35.16086697,  0.31691886,  0.50477507,
        0.80297233]), 'targetState': array([-3.97886776,  4.49094771, 37.        ]), 'previousTarget': array([-3.97886776,  4.49094771, 37.        ])}
episode index:19799
target thresh 86.88211073212567
target distance 31.0
model initialize at round 19799
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.66751297, -12.98946883,  68.99409655]), 'distance': 27.5, 'localFrame': array([[ 0.39578138,  0.67299075, -0.62485242],
       [-0.86198769,  0.50692921,  0.        ],
       [ 0.31675595,  0.53861509,  0.78074288]]), 'currentState': array([-20.53945387, -28.60153816,  87.61737308,   0.39578138,
         0.67299075,  -0.62485242]), 'targetState': array([ 0.62250525, -2.93470394, 57.        ]), 'previousTarget': array([ -8.62288064, -14.05667032,  69.78582834])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6273771846144832
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([ 0.62250525, -2.93470394, 57.        ]), 'distance': 2.2792961995756125, 'localFrame': array([[ 0.52866485,  0.54437078, -0.65128636],
       [-0.71737981,  0.69668229,  0.        ],
       [ 0.45373968,  0.46721969,  0.75883204]]), 'currentState': array([ 0.92862845, -5.09450595, 57.66085932,  0.52866485,  0.54437078,
       -0.65128636]), 'targetState': array([ 0.62250525, -2.93470394, 57.        ]), 'previousTarget': array([ 0.62250525, -2.93470394, 57.        ])}
episode index:19800
target thresh 86.8834224554652
target distance 70.73043168271214
model initialize at round 19800
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.80910415,  2.44561237, 22.86759276]), 'distance': 27.5, 'localFrame': array([[-0.4805761 , -0.61513502,  0.62502441],
       [ 0.78802321, -0.61564553,  0.        ],
       [ 0.38479348,  0.49253375,  0.7806052 ]]), 'currentState': array([43.26788921, 13.45538828,  5.71218287, -0.4805761 , -0.61513502,
        0.62502441]), 'targetState': array([-25.90416825, -27.80241117,  70.        ]), 'previousTarget': array([26.26874992,  3.12688785, 22.05402257])}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.627368329832687
{'scaleFactor': 20, 'timeStep': 80, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.90416825, -27.80241117,  70.        ]), 'distance': 2.501599319574095, 'localFrame': array([[-0.91637915, -0.27954066,  0.28654193],
       [ 0.29177548, -0.95648684,  0.        ],
       [ 0.27407359,  0.08360591,  0.9580677 ]]), 'currentState': array([-26.23863904, -27.7542448 ,  67.5213293 ,  -0.91637915,
        -0.27954066,   0.28654193]), 'targetState': array([-25.90416825, -27.80241117,  70.        ]), 'previousTarget': array([-25.90416825, -27.80241117,  70.        ])}
episode index:19801
target thresh 86.88473404763893
target distance 41.24442535751576
model initialize at round 19801
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.42510493,   3.74773077,  84.76130712]), 'distance': 27.5, 'localFrame': array([[-0.25925912,  0.59014033, -0.76453849],
       [-0.91554514, -0.40221523,  0.        ],
       [-0.30750903,  0.6999695 ,  0.64457808]]), 'currentState': array([-38.85592566, -20.76156883,  77.92447948,  -0.25925912,
         0.59014033,  -0.76453849]), 'targetState': array([-21.95821007,  18.9429937 ,  89.        ]), 'previousTarget': array([-28.79362341,   2.48218419,  85.00896136])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6273743799167442
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.95821007,  18.9429937 ,  89.        ]), 'distance': 3.546689984325361, 'localFrame': array([[-0.83538147,  0.14763399, -0.52947333],
       [-0.17402966, -0.98474041,  0.        ],
       [-0.52139379,  0.09214406,  0.84832658]]), 'currentState': array([-24.20562885,  18.2563232 ,  91.65642661,  -0.83538147,
         0.14763399,  -0.52947333]), 'targetState': array([-21.95821007,  18.9429937 ,  89.        ]), 'previousTarget': array([-21.95821007,  18.9429937 ,  89.        ])}
episode index:19802
target thresh 86.88604550866003
target distance 34.0
model initialize at round 19802
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.77287213, -4.48794887, 35.03543598]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.15852011, -0.97745534,  0.13947198],
       [ 0.98710327,  0.16008477,  0.        ],
       [-0.02232734,  0.13767325,  0.99022602]]), 'currentState': array([17.38200775,  6.30004802, 13.10648657,  0.15852011, -0.97745534,
        0.13947198]), 'targetState': array([-1.53174703, -9.88199125, 46.        ]), 'previousTarget': array([ 5.18529309, -3.59051097, 33.89282313])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6273811954666321
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.53174703, -9.88199125, 46.        ]), 'distance': 3.242635630135425, 'localFrame': array([[-0.74470225, -0.05021616,  0.66550499],
       [ 0.06727841, -0.99773424,  0.        ],
       [ 0.66399712,  0.04477412,  0.7463934 ]]), 'currentState': array([ 1.24040274, -9.81835493, 44.31898189, -0.74470225, -0.05021616,
        0.66550499]), 'targetState': array([-1.53174703, -9.88199125, 46.        ]), 'previousTarget': array([-1.53174703, -9.88199125, 46.        ])}
episode index:19803
target thresh 86.88735683854158
target distance 57.0
model initialize at round 19803
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -3.01979677, -13.59619222,  64.79477458]), 'distance': 27.5, 'localFrame': array([[ 0.95675142,  0.21852096, -0.19202944],
       [-0.22266494,  0.97489503,  0.        ],
       [ 0.18720854,  0.04275822,  0.98138917]]), 'currentState': array([-11.16610239, -22.45836116,  89.52026075,   0.95675142,
         0.21852096,  -0.19202944]), 'targetState': array([ 7.78509721, -1.84180929, 32.        ]), 'previousTarget': array([ -4.04012954, -13.6686187 ,  64.54123124])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.627380065637894
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 7, 'trapConfig': [], 'currentTarget': array([ 7.78509721, -1.84180929, 32.        ]), 'distance': 3.7238432830994292, 'localFrame': array([[ 0.74253306, -0.27140261, -0.61236041],
       [ 0.34329612,  0.93922722,  0.        ],
       [ 0.57514557, -0.21022096,  0.79057873]]), 'currentState': array([ 7.81018897, -4.28913515, 34.80659498,  0.74253306, -0.27140261,
       -0.61236041]), 'targetState': array([ 7.78509721, -1.84180929, 32.        ]), 'previousTarget': array([ 7.78509721, -1.84180929, 32.        ])}
episode index:19804
target thresh 86.8886680372967
target distance 40.7607787092601
model initialize at round 19804
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.98113278, -2.67113537, 22.5535568 ]), 'distance': 27.5, 'localFrame': array([[-0.46443145,  0.62374348, -0.62868713],
       [-0.80207907, -0.59721786,  0.        ],
       [-0.37546318,  0.50425678,  0.77765834]]), 'currentState': array([42.20872777, -7.04849624, 39.47921552, -0.46443145,  0.62374348,
       -0.62868713]), 'targetState': array([1.47438212, 1.3513687 , 7.        ]), 'previousTarget': array([21.44267595, -3.26607487, 23.65625662])}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.6273682166584807
{'scaleFactor': 20, 'timeStep': 94, 'trapCount': 52, 'trapConfig': [], 'currentTarget': array([1.47438212, 1.3513687 , 7.        ]), 'distance': 3.7979272962544957, 'localFrame': array([[-0.7660529 , -0.60427657, -0.21911821],
       [ 0.61932726, -0.78513294,  0.        ],
       [-0.17203692, -0.13570588,  0.97569832]]), 'currentState': array([ 3.90223115, -1.43113642,  6.11260713, -0.7660529 , -0.60427657,
       -0.21911821]), 'targetState': array([1.47438212, 1.3513687 , 7.        ]), 'previousTarget': array([1.47438212, 1.3513687 , 7.        ])}
episode index:19805
target thresh 86.88997910493849
target distance 29.92366264940997
model initialize at round 19805
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.34502842,   5.80020936,  38.49965008]), 'distance': 27.499999999999996, 'localFrame': array([[-0.00214021, -0.70791163,  0.70629777],
       [ 0.99999543, -0.00302326,  0.        ],
       [ 0.00213532,  0.70629454,  0.70791487]]), 'currentState': array([-8.88124072e-01,  2.52458645e+01,  2.35684067e+01, -2.14020855e-03,
       -7.07911630e-01,  7.06297773e-01]), 'targetState': array([-19.6024538 ,  -3.96784638,  46.        ]), 'previousTarget': array([-13.24362387,   6.5521911 ,  37.56250014])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6273750314872037
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.6024538 ,  -3.96784638,  46.        ]), 'distance': 3.1460294821723154, 'localFrame': array([[-0.65961368, -0.74586876, -0.09268006],
       [ 0.7490929 , -0.66246497,  0.        ],
       [-0.0613973 , -0.06942598,  0.99569594]]), 'currentState': array([-19.36949059,  -1.75849751,  43.77244367,  -0.65961368,
        -0.74586876,  -0.09268006]), 'targetState': array([-19.6024538 ,  -3.96784638,  46.        ]), 'previousTarget': array([-19.6024538 ,  -3.96784638,  46.        ])}
episode index:19806
target thresh 86.89129004148008
target distance 41.0
model initialize at round 19806
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.78359072,  2.81523121, 53.39248878]), 'distance': 27.5, 'localFrame': array([[-0.35150255, -0.61618899,  0.70480997],
       [ 0.86861055, -0.49549542,  0.        ],
       [ 0.34923011,  0.61220538,  0.70939616]]), 'currentState': array([-2.11543481,  4.31987507, 26.08714685, -0.35150255, -0.61618899,
        0.70480997]), 'targetState': array([ 2.12213822,  2.12050215, 66.        ]), 'previousTarget': array([ 1.15556234,  3.05737179, 52.36692084])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6273818456278035
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 2.12213822,  2.12050215, 66.        ]), 'distance': 2.090521694825334, 'localFrame': array([[-0.39052691, -0.23508107,  0.89007057],
       [ 0.51572913, -0.85675169,  0.        ],
       [ 0.76256947,  0.45903532,  0.45582275]]), 'currentState': array([ 1.68485927,  2.89242895, 64.10706653, -0.39052691, -0.23508107,
        0.89007057]), 'targetState': array([ 2.12213822,  2.12050215, 66.        ]), 'previousTarget': array([ 2.12213822,  2.12050215, 66.        ])}
episode index:19807
target thresh 86.89260084693456
target distance 69.28314589132785
model initialize at round 19807
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.88409156, -13.3815976 ,  63.1173246 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.78662329,  0.15582005,  0.59744783],
       [-0.19431168, -0.98093984,  0.        ],
       [ 0.58606038, -0.11609109,  0.80190778]]), 'currentState': array([ -5.93318906, -40.3612225 ,  65.07650935,  -0.78662329,
         0.15582005,   0.59744783]), 'targetState': array([-18.76163866,  29.54658888,  60.        ]), 'previousTarget': array([-10.08788357, -12.83791366,  62.44703106])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6273804105591867
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-18.76163866,  29.54658888,  60.        ]), 'distance': 2.9576042604692874, 'localFrame': array([[-0.47504406,  0.37510021, -0.79601066],
       [-0.61971088, -0.78483019,  0.        ],
       [-0.6247332 ,  0.49329647,  0.6052826 ]]), 'currentState': array([-17.06900679,  27.19424931,  60.59069351,  -0.47504406,
         0.37510021,  -0.79601066]), 'targetState': array([-18.76163866,  29.54658888,  60.        ]), 'previousTarget': array([-18.76163866,  29.54658888,  60.        ])}
episode index:19808
target thresh 86.89391152131505
target distance 17.443519285444808
model initialize at round 19808
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.27921517,  3.31858804, 75.        ]), 'distance': 25.403039552432013, 'localFrame': array([[-0.77762547, -0.5353425 , -0.3297075 ],
       [ 0.56705017, -0.82368326,  0.        ],
       [-0.27157455, -0.18696069,  0.94408313]]), 'currentState': array([23.66293065, 16.37317939, 60.63107763, -0.77762547, -0.5353425 ,
       -0.3297075 ]), 'targetState': array([ 7.27921517,  3.31858804, 75.        ]), 'previousTarget': array([ 7.27921517,  3.31858804, 75.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6273930381836221
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.27921517,  3.31858804, 75.        ]), 'distance': 2.586631278362061, 'localFrame': array([[-0.3533115 , -0.93102509,  0.09145085],
       [ 0.93494289, -0.35479825,  0.        ],
       [ 0.0324466 ,  0.08550132,  0.99580959]]), 'currentState': array([ 8.75777778,  5.18514654, 73.9898152 , -0.3533115 , -0.93102509,
        0.09145085]), 'targetState': array([ 7.27921517,  3.31858804, 75.        ]), 'previousTarget': array([ 7.27921517,  3.31858804, 75.        ])}
episode index:19809
target thresh 86.89522206463467
target distance 17.0
model initialize at round 19809
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.58111533,  5.40390868, 53.        ]), 'distance': 20.59863850932335, 'localFrame': array([[ 0.83138591, -0.19847458,  0.51904269],
       [ 0.23220237,  0.9726675 ,  0.        ],
       [-0.50485595,  0.12052294,  0.85474832]]), 'currentState': array([-3.8078734 ,  7.71455489, 37.51775845,  0.83138591, -0.19847458,
        0.51904269]), 'targetState': array([ 9.58111533,  5.40390868, 53.        ]), 'previousTarget': array([ 9.58111533,  5.40390868, 53.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6274061119763295
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.58111533,  5.40390868, 53.        ]), 'distance': 3.3121534597303652, 'localFrame': array([[ 0.94792226, -0.31844465,  0.00603306],
       [ 0.31845044,  0.94793951,  0.        ],
       [-0.00571897,  0.00192123,  0.9999818 ]]), 'currentState': array([ 1.15494972e+01,  5.06313614e+00,  5.03580864e+01,  9.47922258e-01,
       -3.18444649e-01,  6.03305716e-03]), 'targetState': array([ 9.58111533,  5.40390868, 53.        ]), 'previousTarget': array([ 9.58111533,  5.40390868, 53.        ])}
episode index:19810
target thresh 86.8965324769065
target distance 39.91568183632002
model initialize at round 19810
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.1409405 ,  8.68491344, 10.2164689 ]), 'distance': 27.5, 'localFrame': array([[ 0.00309498, -0.65339804, -0.75700821],
       [ 0.99998878,  0.0047367 ,  0.        ],
       [ 0.00358572, -0.75699972,  0.65340537]]), 'currentState': array([ 2.10587236e+01,  3.61099920e+01,  8.40739722e+00,  3.09498438e-03,
       -6.53398036e-01, -7.57008209e-01]), 'targetState': array([19.74343741, -3.19322395, 11.        ]), 'previousTarget': array([19.94252305,  9.23460844, 10.68864787])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.627414101074751
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([19.74343741, -3.19322395, 11.        ]), 'distance': 3.0935353097748024, 'localFrame': array([[ 0.05753454, -0.87799145,  0.47520604],
       [ 0.99785982,  0.06538948,  0.        ],
       [-0.03107348,  0.47418902,  0.87987455]]), 'currentState': array([21.15534399, -0.44757225, 10.80537918,  0.05753454, -0.87799145,
        0.47520604]), 'targetState': array([19.74343741, -3.19322395, 11.        ]), 'previousTarget': array([19.74343741, -3.19322395, 11.        ])}
episode index:19811
target thresh 86.89784275814365
target distance 56.0
model initialize at round 19811
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.96414515, 23.85313175, 74.68963541]), 'distance': 27.500000000000004, 'localFrame': array([[-0.65980294, -0.40880611, -0.63050586],
       [ 0.52668679, -0.85005943,  0.        ],
       [-0.53596745, -0.33207911,  0.77618449]]), 'currentState': array([27.63692018, 30.66585816, 94.62728275, -0.65980294, -0.40880611,
       -0.63050586]), 'targetState': array([-20.78482498,  11.9996271 ,  40.        ]), 'previousTarget': array([11.15652298, 23.90555146, 76.02500453])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627382432686851
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 57, 'trapConfig': [], 'currentTarget': array([-14.37398975,  13.61168369,  43.03719492]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.41742005, -0.81374323,  0.40445328],
       [ 0.88976603,  0.45641692,  0.        ],
       [-0.18459932,  0.35986879,  0.91455866]]), 'currentState': array([ 9.86021786, 19.70557207, 54.51838413,  0.41742005, -0.81374323,
        0.40445328]), 'targetState': array([-20.78482498,  11.9996271 ,  40.        ]), 'previousTarget': array([-14.37398975,  13.61168369,  43.03719492])}
episode index:19812
target thresh 86.89915290835924
target distance 34.01478803274385
model initialize at round 19812
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.13030785,   9.01970595,  91.63744905]), 'distance': 27.5, 'localFrame': array([[ 0.36623625,  0.88097503, -0.29958973],
       [-0.92338797,  0.38386803,  0.        ],
       [ 0.11500292,  0.27663755,  0.95406813]]), 'currentState': array([-28.84920406, -10.53833542,  80.38359106,   0.36623625,
         0.88097503,  -0.29958973]), 'targetState': array([-2.84661896, 21.81505811, 99.        ]), 'previousTarget': array([-14.02323908,   7.5959563 ,  91.05748587])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6273881015881129
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-2.84661896, 21.81505811, 99.        ]), 'distance': 3.1985690761720953, 'localFrame': array([[ 0.71267951, -0.51708561,  0.47403626],
       [ 0.58726007,  0.8093983 ,  0.        ],
       [-0.38368415,  0.27838257,  0.88050532]]), 'currentState': array([-4.13089924, 21.80166551, 96.07061629,  0.71267951, -0.51708561,
        0.47403626]), 'targetState': array([-2.84661896, 21.81505811, 99.        ]), 'previousTarget': array([-2.84661896, 21.81505811, 99.        ])}
episode index:19813
target thresh 86.90046292756635
target distance 34.505219553481396
model initialize at round 19813
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.48813253, 20.0057575 , 73.01763027]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69347645, -0.51173679,  0.50716454],
       [ 0.59376601, -0.80463776,  0.        ],
       [ 0.40808374,  0.30113707,  0.86184925]]), 'currentState': array([17.49371313, 42.9504918 , 59.09921909, -0.69347645, -0.51173679,
        0.50716454]), 'targetState': array([ 8.47534725,  8.49520388, 80.        ]), 'previousTarget': array([12.13961142, 20.53978267, 72.32056086])}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.627393396595081
{'scaleFactor': 20, 'timeStep': 32, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.47534725,  8.49520388, 80.        ]), 'distance': 3.7485652996248198, 'localFrame': array([[ 0.35851888, -0.71867133,  0.5957984 ],
       [ 0.89483362,  0.44639981,  0.        ],
       [-0.26596429,  0.53314044,  0.80313403]]), 'currentState': array([10.80205454,  9.85643469, 77.39515343,  0.35851888, -0.71867133,
        0.5957984 ]), 'targetState': array([ 8.47534725,  8.49520388, 80.        ]), 'previousTarget': array([ 8.47534725,  8.49520388, 80.        ])}
episode index:19814
target thresh 86.90177281577809
target distance 42.84380048365227
model initialize at round 19814
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([19.20716132, 14.17004086, 69.70468891]), 'distance': 27.5, 'localFrame': array([[-0.40196009, -0.7828857 , -0.47488742],
       [ 0.88959578, -0.45674867,  0.        ],
       [-0.2169042 , -0.42245785,  0.88004655]]), 'currentState': array([ 31.31340599, -10.41492176,  72.        ,  -0.40196009,
        -0.7828857 ,  -0.47488742]), 'targetState': array([10.2160572 , 32.42887872, 68.        ]), 'previousTarget': array([19.20716132, 14.17004086, 69.70468891])}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.6273815529226996
{'scaleFactor': 20, 'timeStep': 94, 'trapCount': 51, 'trapConfig': [], 'currentTarget': array([10.2160572 , 32.42887872, 68.        ]), 'distance': 3.182567428817731, 'localFrame': array([[ 0.09827484,  0.99429999, -0.04134723],
       [-0.995151  ,  0.09835896,  0.        ],
       [ 0.00406687,  0.04114674,  0.99914484]]), 'currentState': array([ 1.14918383e+01,  2.97661823e+01,  6.68120750e+01,  9.82748423e-02,
        9.94299986e-01, -4.13472329e-02]), 'targetState': array([10.2160572 , 32.42887872, 68.        ]), 'previousTarget': array([10.2160572 , 32.42887872, 68.        ])}
episode index:19815
target thresh 86.90308257300757
target distance 28.0
model initialize at round 19815
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.22669772,   8.39581088,  57.307199  ]), 'distance': 27.5, 'localFrame': array([[-0.56876595,  0.67001244,  0.4770625 ],
       [-0.76235725, -0.64715642,  0.        ],
       [ 0.30873405, -0.36369205,  0.87886937]]), 'currentState': array([-16.76963334,  -3.6242696 ,  37.23829382,  -0.56876595,
         0.67001244,   0.4770625 ]), 'targetState': array([-36.76837119,  13.0033411 ,  65.        ]), 'previousTarget': array([-30.57158048,   7.49582604,  56.58722362])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.627378349634323
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 34, 'trapConfig': [], 'currentTarget': array([-36.76837119,  13.0033411 ,  65.        ]), 'distance': 2.361435918814066, 'localFrame': array([[-0.32158136,  0.93552212,  0.14623198],
       [-0.94568796, -0.32507582,  0.        ],
       [ 0.04753648, -0.13828982,  0.98925033]]), 'currentState': array([-36.91639258,  13.68955764,  62.74532128,  -0.32158136,
         0.93552212,   0.14623198]), 'targetState': array([-36.76837119,  13.0033411 ,  65.        ]), 'previousTarget': array([-36.76837119,  13.0033411 ,  65.        ])}
episode index:19816
target thresh 86.90439219926786
target distance 59.0
model initialize at round 19816
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 29.18477694, -27.77513043,  66.58319441]), 'distance': 27.5, 'localFrame': array([[-0.00823042,  0.11891051,  0.99287086],
       [-0.9976132 , -0.06905002,  0.        ],
       [ 0.06855775, -0.99050108,  0.11919501]]), 'currentState': array([ 3.74201428e+01, -2.46063596e+01,  4.05373187e+01, -8.23041706e-03,
        1.18910512e-01,  9.92870863e-01]), 'targetState': array([ 19.25119316, -31.59733473,  98.        ]), 'previousTarget': array([ 29.12118124, -28.26303124,  65.20903852])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6273807894940104
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 19.25119316, -31.59733473,  98.        ]), 'distance': 1.5569290757701313, 'localFrame': array([[-0.02650048, -0.01007348,  0.99959804],
       [ 0.35531949, -0.93474492,  0.        ],
       [ 0.93436919,  0.35517667,  0.02835049]]), 'currentState': array([ 1.88441332e+01, -3.21323827e+01,  9.65957016e+01, -2.65004780e-02,
       -1.00734823e-02,  9.99598044e-01]), 'targetState': array([ 19.25119316, -31.59733473,  98.        ]), 'previousTarget': array([ 19.25119316, -31.59733473,  98.        ])}
episode index:19817
target thresh 86.90570169457207
target distance 37.553229308403814
model initialize at round 19817
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.33003547,  15.88907431,  55.43054907]), 'distance': 27.500000000000004, 'localFrame': array([[-0.74342576, -0.52008763, -0.42050802],
       [ 0.57323271, -0.81939262,  0.        ],
       [-0.34456117, -0.24104895,  0.90728882]]), 'currentState': array([-1.90749985,  1.36080205, 70.97514807, -0.74342576, -0.52008763,
       -0.42050802]), 'targetState': array([-37.74555241,  31.24537203,  39.        ]), 'previousTarget': array([-18.19664704,  16.0164234 ,  55.65808729])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6273839214092579
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-37.74555241,  31.24537203,  39.        ]), 'distance': 2.2248287925044736, 'localFrame': array([[ 0.27199163, -0.70574791, -0.65417157],
       [ 0.93310169,  0.35961262,  0.        ],
       [ 0.23524835, -0.6104086 ,  0.75634619]]), 'currentState': array([-37.70050596,  29.54911068,  40.43893413,   0.27199163,
        -0.70574791,  -0.65417157]), 'targetState': array([-37.74555241,  31.24537203,  39.        ]), 'previousTarget': array([-37.74555241,  31.24537203,  39.        ])}
episode index:19818
target thresh 86.9070110589333
target distance 25.8736904297307
model initialize at round 19818
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.93200822, -43.68573253,  30.43162439]), 'distance': 27.5, 'localFrame': array([[-0.26138741, -0.71359206, -0.64997153],
       [ 0.93898813, -0.34394956,  0.        ],
       [-0.22355742, -0.61031555,  0.75995856]]), 'currentState': array([  0.43918081, -19.57878414,  39.65069523,  -0.26138741,
        -0.71359206,  -0.64997153]), 'targetState': array([ 10.37644952, -44.81438715,  30.        ]), 'previousTarget': array([  9.77946786, -43.0742612 ,  30.73980113])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6273947978042311
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.37644952, -44.81438715,  30.        ]), 'distance': 3.5260619934910618, 'localFrame': array([[-0.15962302, -0.86240731,  0.48038956],
       [ 0.98329875, -0.18199882,  0.        ],
       [ 0.08743033,  0.47236645,  0.87705523]]), 'currentState': array([  8.46552583, -41.9409622 ,  29.27549127,  -0.15962302,
        -0.86240731,   0.48038956]), 'targetState': array([ 10.37644952, -44.81438715,  30.        ]), 'previousTarget': array([ 10.37644952, -44.81438715,  30.        ])}
episode index:19819
target thresh 86.90832029236465
target distance 33.11868826135787
model initialize at round 19819
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.18980674,   6.45799994,  86.26430939]), 'distance': 27.500000000000004, 'localFrame': array([[-0.59243732,  0.57879541,  0.56036943],
       [-0.69882337, -0.71529427,  0.        ],
       [ 0.40082904, -0.39159925,  0.82824278]]), 'currentState': array([-25.48577253, -17.44612166,  76.34322121,  -0.59243732,
         0.57879541,   0.56036943]), 'targetState': array([-12.68949996,  15.4588677 ,  90.        ]), 'previousTarget': array([-15.91743014,   6.21257234,  85.81220037])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273631431726567
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 77, 'trapConfig': [], 'currentTarget': array([-12.68949996,  15.4588677 ,  90.        ]), 'distance': 6.755311075716666, 'localFrame': array([[-0.39931917,  0.49888295,  0.76919438],
       [-0.78070633, -0.62489809,  0.        ],
       [ 0.4806681 , -0.60051492,  0.63901487]]), 'currentState': array([-14.39564744,   8.9270565 ,  89.75765548,  -0.39931917,
         0.49888295,   0.76919438]), 'targetState': array([-12.68949996,  15.4588677 ,  90.        ]), 'previousTarget': array([-12.68949996,  15.4588677 ,  90.        ])}
episode index:19820
target thresh 86.90962939487919
target distance 38.568598978218134
model initialize at round 19820
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.67880364,  22.08157642,  29.11758412]), 'distance': 27.500000000000004, 'localFrame': array([[-0.51085816, -0.10558353, -0.85315653],
       [ 0.20240107, -0.97930271,  0.        ],
       [-0.8354985 , -0.17267979,  0.521655  ]]), 'currentState': array([ 3.06399192, 13.04072966, 39.64339759, -0.51085816, -0.10558353,
       -0.85315653]), 'targetState': array([-34.4780743 ,  27.33610053,  23.        ]), 'previousTarget': array([-19.41293848,  21.47840334,  30.0309125 ])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6273703415987014
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-34.4780743 ,  27.33610053,  23.        ]), 'distance': 4.17296130895812, 'localFrame': array([[-0.64518036, -0.43498567,  0.62811605],
       [ 0.55902155, -0.82915312,  0.        ],
       [ 0.52080438,  0.35113041,  0.77811967]]), 'currentState': array([-32.17319545,  24.61361736,  20.83453817,  -0.64518036,
        -0.43498567,   0.62811605]), 'targetState': array([-34.4780743 ,  27.33610053,  23.        ]), 'previousTarget': array([-34.4780743 ,  27.33610053,  23.        ])}
episode index:19821
target thresh 86.91093836649003
target distance 30.0
model initialize at round 19821
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.94115044, 12.68467957, 71.87890671]), 'distance': 27.5, 'localFrame': array([[-0.28137687,  0.77601354, -0.56447325],
       [-0.94010813, -0.34087637,  0.        ],
       [-0.19241559,  0.53066589,  0.82545136]]), 'currentState': array([-1.66604486, -3.5297527 , 93.67490769, -0.28137687,  0.77601354,
       -0.56447325]), 'targetState': array([-7.48653293, 18.54593823, 64.        ]), 'previousTarget': array([-5.47979994, 11.72415124, 72.70371053])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6273683068111088
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-7.48653293, 18.54593823, 64.        ]), 'distance': 1.601320579801323, 'localFrame': array([[-0.49872816, -0.03655332, -0.86598734],
       [ 0.07309699, -0.99732484,  0.        ],
       [-0.86367068, -0.06330107,  0.50006592]]), 'currentState': array([-7.96533758e+00,  1.85933985e+01,  6.55273249e+01, -4.98728164e-01,
       -3.65533160e-02, -8.65987340e-01]), 'targetState': array([-7.48653293, 18.54593823, 64.        ]), 'previousTarget': array([-7.48653293, 18.54593823, 64.        ])}
episode index:19822
target thresh 86.91224720721026
target distance 38.95290094158445
model initialize at round 19822
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.97337805, -20.98702742,  78.47389292]), 'distance': 27.5, 'localFrame': array([[ 0.04403876, -0.96254177,  0.26753303],
       [ 0.99895499,  0.04570476,  0.        ],
       [-0.01222753,  0.26725345,  0.96354869]]), 'currentState': array([-5.88362901e-02, -1.31603331e+00,  5.92844030e+01,  4.40387649e-02,
       -9.62541773e-01,  2.67533029e-01]), 'targetState': array([  1.91611801, -38.95290094,  96.        ]), 'previousTarget': array([  0.96770414, -19.67252706,  77.19127999])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273366583065024
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([  1.91611801, -38.95290094,  96.        ]), 'distance': 10.146521224677356, 'localFrame': array([[-0.01759144,  0.10416527,  0.99440441],
       [-0.98603772, -0.16652213,  0.        ],
       [ 0.16559034, -0.98052026,  0.10564025]]), 'currentState': array([ 2.31072127e+00, -3.49435561e+01,  8.66875709e+01, -1.75914403e-02,
        1.04165274e-01,  9.94404413e-01]), 'targetState': array([  1.91611801, -38.95290094,  96.        ]), 'previousTarget': array([  1.91611801, -38.95290094,  96.        ])}
episode index:19823
target thresh 86.91355591705296
target distance 45.0
model initialize at round 19823
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.90339719,  2.14809641, 53.4016451 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.81130647, -0.3906251 ,  0.43496419],
       [ 0.43381207,  0.90100338,  0.        ],
       [-0.3919042 ,  0.18869271,  0.90044775]]), 'currentState': array([-21.06660321,   6.25475978,  74.49215659,   0.81130647,
        -0.3906251 ,   0.43496419]), 'targetState': array([16.76815779, -2.79801433, 28.        ]), 'previousTarget': array([-4.24367221,  2.46042369, 52.42485927])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6273390994077309
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.76815779, -2.79801433, 28.        ]), 'distance': 3.4817465986336993, 'localFrame': array([[ 0.94843784, -0.04890634, -0.31316743],
       [ 0.05149673,  0.99867316,  0.        ],
       [ 0.31275191, -0.0161271 ,  0.94969793]]), 'currentState': array([15.0173079 , -1.53255708, 30.73051313,  0.94843784, -0.04890634,
       -0.31316743]), 'targetState': array([16.76815779, -2.79801433, 28.        ]), 'previousTarget': array([16.76815779, -2.79801433, 28.        ])}
episode index:19824
target thresh 86.91486449603121
target distance 53.0
model initialize at round 19824
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.54813893, -15.39823146,  19.67946795]), 'distance': 27.500000000000004, 'localFrame': array([[-0.23967304, -0.94581528,  0.21906685],
       [ 0.96936121, -0.24563966,  0.        ],
       [ 0.05381151,  0.21235491,  0.97570985]]), 'currentState': array([40.28119787, -1.1678442 ,  0.5705912 , -0.23967304, -0.94581528,
        0.21906685]), 'targetState': array([  1.88285402, -40.95674378,  54.        ]), 'previousTarget': array([ 27.17295582, -14.18027976,  19.73209642])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6273324159050925
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  1.88285402, -40.95674378,  54.        ]), 'distance': 1.9899843212458437, 'localFrame': array([[-0.39119134,  0.53587448,  0.74820309],
       [-0.80768486, -0.58961442,  0.        ],
       [ 0.44115133, -0.60431231,  0.66346976]]), 'currentState': array([  2.58163673, -40.5198954 ,  52.18867346,  -0.39119134,
         0.53587448,   0.74820309]), 'targetState': array([  1.88285402, -40.95674378,  54.        ]), 'previousTarget': array([  1.88285402, -40.95674378,  54.        ])}
episode index:19825
target thresh 86.9161729441581
target distance 70.0
model initialize at round 19825
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.7023929 , -5.73065638, 55.76549526]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.31842688,  0.606123  , -0.72884788],
       [-0.88527025,  0.46507697,  0.        ],
       [ 0.33897036,  0.64522734,  0.68467567]]), 'currentState': array([ -4.16861774, -10.43036549,  81.36760269,   0.31842688,
         0.606123  ,  -0.72884788]), 'targetState': array([19.86693258,  2.30325635, 12.        ]), 'previousTarget': array([ 3.97037253, -6.77447099, 56.56263383])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6273007739997204
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 14, 'trapConfig': [], 'currentTarget': array([19.86693258,  2.30325635, 12.        ]), 'distance': 8.449788617509377, 'localFrame': array([[-0.48979182,  0.56290937, -0.66576048],
       [-0.75440282, -0.65641175,  0.        ],
       [-0.437013  ,  0.50225158,  0.74616552]]), 'currentState': array([15.88645029,  3.43474792, 19.36711716, -0.48979182,  0.56290937,
       -0.66576048]), 'targetState': array([19.86693258,  2.30325635, 12.        ]), 'previousTarget': array([19.86693258,  2.30325635, 12.        ])}
episode index:19826
target thresh 86.91748126144674
target distance 49.174666740850064
model initialize at round 19826
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.40709467,  25.35137557,  57.9203218 ]), 'distance': 27.5, 'localFrame': array([[-0.73620324,  0.18558734, -0.65081651],
       [-0.2444399 , -0.96966444,  0.        ],
       [-0.63107363,  0.15908552,  0.75923506]]), 'currentState': array([-13.48405319,  44.19984896,  74.70341494,  -0.73620324,
         0.18558734,  -0.65081651]), 'targetState': array([-42.57864569,  -6.0049089 ,  30.        ]), 'previousTarget': array([-23.85590589,  24.81254816,  58.82791321])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6273025383244788
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-42.57864569,  -6.0049089 ,  30.        ]), 'distance': 2.978648620360292, 'localFrame': array([[-0.37697269, -0.91191389, -0.16218708],
       [ 0.92414961, -0.38203076,  0.        ],
       [-0.06196045, -0.14988513,  0.98676003]]), 'currentState': array([-40.96051511,  -3.56005168,  30.5259983 ,  -0.37697269,
        -0.91191389,  -0.16218708]), 'targetState': array([-42.57864569,  -6.0049089 ,  30.        ]), 'previousTarget': array([-42.57864569,  -6.0049089 ,  30.        ])}
episode index:19827
target thresh 86.91878944791019
target distance 49.0
model initialize at round 19827
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.71825425, -4.91852226, 58.4666749 ]), 'distance': 27.5, 'localFrame': array([[-0.51930384,  0.54687691, -0.65669565],
       [-0.72515119, -0.68858968,  0.        ],
       [-0.45219385,  0.47620363,  0.7541557 ]]), 'currentState': array([ 7.72470455, -9.96630097, 85.20089055, -0.51930384,  0.54687691,
       -0.65669565]), 'targetState': array([ 0.50120957, -0.86532593, 37.        ]), 'previousTarget': array([ 4.52064489, -5.26413242, 59.42244592])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6273063786087573
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.50120957, -0.86532593, 37.        ]), 'distance': 2.2342652355883996, 'localFrame': array([[-0.83690182, -0.35219625, -0.41899063],
       [ 0.38788537, -0.92170762,  0.        ],
       [-0.38618685, -0.16252033,  0.90799056]]), 'currentState': array([-0.39214669,  0.05243002, 38.83073203, -0.83690182, -0.35219625,
       -0.41899063]), 'targetState': array([ 0.50120957, -0.86532593, 37.        ]), 'previousTarget': array([ 0.50120957, -0.86532593, 37.        ])}
episode index:19828
target thresh 86.92009750356151
target distance 46.71183752630131
model initialize at round 19828
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.37315267,  -6.60006916,  18.95161501]), 'distance': 27.5, 'localFrame': array([[ 0.04822018, -0.6902663 ,  0.72194684],
       [ 0.99756887,  0.06968752,  0.        ],
       [-0.05031069,  0.7201917 ,  0.69194852]]), 'currentState': array([-35.61643235,  -3.1360215 ,   1.83514802,   0.04822018,
        -0.6902663 ,   0.72194684]), 'targetState': array([ 11.7500379, -10.8598623,  40.       ]), 'previousTarget': array([-14.09052209,  -5.75662371,  18.42555778])}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6272996981044955
{'scaleFactor': 20, 'timeStep': 71, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([ 11.7500379, -10.8598623,  40.       ]), 'distance': 2.303831768979183, 'localFrame': array([[ 0.62090749,  0.60236811,  0.5016239 ],
       [-0.69631025,  0.71774093,  0.        ],
       [-0.36003601, -0.34928586,  0.86508581]]), 'currentState': array([ 11.71977125, -11.50149013,  37.78752661,   0.62090749,
         0.60236811,   0.5016239 ]), 'targetState': array([ 11.7500379, -10.8598623,  40.       ]), 'previousTarget': array([ 11.7500379, -10.8598623,  40.       ])}
episode index:19829
target thresh 86.92140542841383
target distance 32.64777578292056
model initialize at round 19829
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.30609477,  6.04548366, 87.19326528]), 'distance': 27.499999999999996, 'localFrame': array([[-0.13502705,  0.7158195 ,  0.68510594],
       [-0.98266997, -0.1853638 ,  0.        ],
       [ 0.12699384, -0.67323303,  0.72844345]]), 'currentState': array([ -9.75756046, -12.5963207 ,  80.46198164,  -0.13502705,
         0.7158195 ,   0.68510594]), 'targetState': array([22.91923876, 19.35738863, 92.        ]), 'previousTarget': array([ 9.55218878,  5.64604074, 86.54028588])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6273065081406145
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.91923876, 19.35738863, 92.        ]), 'distance': 1.7792864084201674, 'localFrame': array([[ 0.15426949,  0.89299068, -0.42281032],
       [-0.9854036 ,  0.17023438,  0.        ],
       [ 0.07197685,  0.41663882,  0.9062182 ]]), 'currentState': array([22.18287182, 17.80942475, 92.47689797,  0.15426949,  0.89299068,
       -0.42281032]), 'targetState': array([22.91923876, 19.35738863, 92.        ]), 'previousTarget': array([22.91923876, 19.35738863, 92.        ])}
episode index:19830
target thresh 86.9227132224802
target distance 84.0
model initialize at round 19830
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-28.18086421,  -9.12650492,  26.75213205]), 'distance': 27.5, 'localFrame': array([[-0.43659849,  0.58955831,  0.67956071],
       [-0.80362979, -0.59512953,  0.        ],
       [ 0.40442664, -0.54611524,  0.73361927]]), 'currentState': array([-37.24659253, -24.05978204,   5.51396347,  -0.43659849,
         0.58955831,   0.67956071]), 'targetState': array([-2.03658867, 33.9389497 , 88.        ]), 'previousTarget': array([-28.39802211, -10.15901745,  25.27454971])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272748755195595
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([-17.40672763,   0.4278826 ,  46.69230237]), 'distance': 27.5, 'localFrame': array([[ 0.28369693,  0.86221537,  0.41964355],
       [-0.94990177,  0.31254861,  0.        ],
       [-0.13115901, -0.39862015,  0.90768898]]), 'currentState': array([-25.04078942, -16.21644038,  26.17553818,   0.28369693,
         0.86221537,   0.41964355]), 'targetState': array([-2.03658867, 33.9389497 , 88.        ]), 'previousTarget': array([-18.0528508 ,  -0.65134312,  45.26756441])}
episode index:19831
target thresh 86.92402088577369
target distance 67.68448653513515
model initialize at round 19831
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.6612428 ,  13.97604224,  50.5175754 ]), 'distance': 27.5, 'localFrame': array([[-0.86688128,  0.05339094, -0.49564731],
       [-0.06147319, -0.99810873,  0.        ],
       [-0.49470991,  0.03046902,  0.86852389]]), 'currentState': array([-2.63877740e+01,  3.32911100e+01,  6.89005005e+01, -8.66881278e-01,
        5.33909363e-02, -4.95647311e-01]), 'targetState': array([ -2.63990804, -34.90029922,   4.        ]), 'previousTarget': array([-18.34929363,  13.4721136 ,  50.45387731])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6272694835090277
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ -2.63990804, -34.90029922,   4.        ]), 'distance': 3.215366954383939, 'localFrame': array([[-0.53269574, -0.54924388, -0.64386831],
       [ 0.71783794, -0.69621024,  0.        ],
       [-0.44826771, -0.4621931 ,  0.76513632]]), 'currentState': array([ -1.30719667, -32.00710274,   4.4380402 ,  -0.53269574,
        -0.54924388,  -0.64386831]), 'targetState': array([ -2.63990804, -34.90029922,   4.        ]), 'previousTarget': array([ -2.63990804, -34.90029922,   4.        ])}
episode index:19832
target thresh 86.9253284183074
target distance 32.0
model initialize at round 19832
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.15299677, -22.55409553,  32.92439645]), 'distance': 27.5, 'localFrame': array([[ 0.57110979, -0.28447936,  0.77000331],
       [ 0.44586453,  0.89510045,  0.        ],
       [-0.68923031,  0.34331717,  0.63803989]]), 'currentState': array([ 8.07641873, -5.6776815 , 13.19998566,  0.57110979, -0.28447936,
        0.77000331]), 'targetState': array([ 22.24965496, -32.03049881,  44.        ]), 'previousTarget': array([ 16.40923325, -22.22973924,  31.96082491])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.62723785594469
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 76, 'trapConfig': [], 'currentTarget': array([ 22.24965496, -32.03049881,  44.        ]), 'distance': 9.237495122371163, 'localFrame': array([[-0.4000642 , -0.24874625,  0.882085  ],
       [ 0.52802245, -0.84923041,  0.        ],
       [ 0.7490934 ,  0.46576068,  0.47109029]]), 'currentState': array([ 17.31970842, -26.85468787,  38.14868183,  -0.4000642 ,
        -0.24874625,   0.882085  ]), 'targetState': array([ 22.24965496, -32.03049881,  44.        ]), 'previousTarget': array([ 22.24965496, -32.03049881,  44.        ])}
episode index:19833
target thresh 86.92663582009439
target distance 29.21192752794104
model initialize at round 19833
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.70316276, 19.41073145, 61.80516078]), 'distance': 27.5, 'localFrame': array([[ 0.20720535, -0.2324745 ,  0.95027446],
       [ 0.74651357,  0.6653702 ,  0.        ],
       [-0.6322843 ,  0.70939277,  0.31141364]]), 'currentState': array([-4.68372815, -7.51587732, 60.32558432,  0.20720535, -0.2324745 ,
        0.95027446]), 'targetState': array([ 1.41253983, 22.95658361, 62.        ]), 'previousTarget': array([ 0.86854291, 20.4734249 , 61.74498512])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6272487314784925
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.41253983, 22.95658361, 62.        ]), 'distance': 3.9405624704628, 'localFrame': array([[ 0.39365482,  0.85629948,  0.33434575],
       [-0.9085883 ,  0.41769283,  0.        ],
       [-0.13965382, -0.30378263,  0.94245049]]), 'currentState': array([-0.60261362, 20.91623948, 64.70262557,  0.39365482,  0.85629948,
        0.33434575]), 'targetState': array([ 1.41253983, 22.95658361, 62.        ]), 'previousTarget': array([ 1.41253983, 22.95658361, 62.        ])}
episode index:19834
target thresh 86.92794309114774
target distance 40.0
model initialize at round 19834
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.51251007, -17.73461402,  32.18192221]), 'distance': 27.5, 'localFrame': array([[-0.21385669,  0.54934748,  0.80776399],
       [-0.93187765, -0.36277271,  0.        ],
       [ 0.29303473, -0.75273721,  0.58950601]]), 'currentState': array([ 21.85561663, -36.16369025,  13.55367156,  -0.21385669,
         0.54934748,   0.80776399]), 'targetState': array([ 4.63651103,  1.87156765, 52.        ]), 'previousTarget': array([ 14.24665913, -18.36826688,  30.86839417])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6272518673672945
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.63651103,  1.87156765, 52.        ]), 'distance': 1.9767567709431304, 'localFrame': array([[ 0.43520145,  0.79732681,  0.41817419],
       [-0.87775859,  0.47910318,  0.        ],
       [-0.20034858, -0.36705598,  0.90836686]]), 'currentState': array([ 5.19787818,  1.7708256 , 50.1073074 ,  0.43520145,  0.79732681,
        0.41817419]), 'targetState': array([ 4.63651103,  1.87156765, 52.        ]), 'previousTarget': array([ 4.63651103,  1.87156765, 52.        ])}
episode index:19835
target thresh 86.92925023148051
target distance 25.944170689957488
model initialize at round 19835
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.80057914, -8.9227332 , 32.19888473]), 'distance': 27.5, 'localFrame': array([[ 0.31846304, -0.63430322,  0.70444355],
       [ 0.89368689,  0.44869115,  0.        ],
       [-0.31607759,  0.62955196,  0.70976002]]), 'currentState': array([25.01924859,  9.85445261, 14.90015393,  0.31846304, -0.63430322,
        0.70444355]), 'targetState': array([ 11.37375667, -15.21964715,  38.        ]), 'previousTarget': array([14.638818  , -8.33906471, 31.63502575])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6272560668712219
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 11.37375667, -15.21964715,  38.        ]), 'distance': 2.0639249548455147, 'localFrame': array([[-0.46142121, -0.85687057, -0.22992018],
       [ 0.8804585 , -0.47412322,  0.        ],
       [-0.1090105 , -0.20243518,  0.97320949]]), 'currentState': array([ 13.35701093, -15.6037565 ,  37.57697664,  -0.46142121,
        -0.85687057,  -0.22992018]), 'targetState': array([ 11.37375667, -15.21964715,  38.        ]), 'previousTarget': array([ 11.37375667, -15.21964715,  38.        ])}
episode index:19836
target thresh 86.9305572411058
target distance 30.06906139295052
model initialize at round 19836
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.3772364 , -5.97107706, 71.47228789]), 'distance': 27.5, 'localFrame': array([[-0.83806057, -0.08566007,  0.53881058],
       [ 0.10168249, -0.9948169 ,  0.        ],
       [ 0.53601787,  0.0547876 ,  0.84242695]]), 'currentState': array([ 27.93713768, -18.02787884,  74.24623449,  -0.83806057,
        -0.08566007,   0.53881058]), 'targetState': array([-0.8042947 , -3.91830448, 71.        ]), 'previousTarget': array([ 4.18921951, -6.14193826, 71.33213635])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272244463607178
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 85, 'trapConfig': [], 'currentTarget': array([-0.8042947 , -3.91830448, 71.        ]), 'distance': 13.932039383190812, 'localFrame': array([[-0.69314203,  0.54459713, -0.47219497],
       [-0.61781141, -0.78632631,  0.        ],
       [-0.37129933,  0.29172744,  0.88149414]]), 'currentState': array([  7.97769444, -13.99288086,  74.93462819,  -0.69314203,
         0.54459713,  -0.47219497]), 'targetState': array([-0.8042947 , -3.91830448, 71.        ]), 'previousTarget': array([-0.8042947 , -3.91830448, 71.        ])}
episode index:19837
target thresh 86.93186412003665
target distance 37.93545374062859
model initialize at round 19837
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.71532804,  30.94965501,  45.76826873]), 'distance': 27.5, 'localFrame': array([[-0.56392486, -0.62397305, -0.54096801],
       [ 0.74190371, -0.67050643,  0.        ],
       [-0.36272253, -0.40134617,  0.84104317]]), 'currentState': array([ 8.53347839, 32.79080283, 42.54915037, -0.56392486, -0.62397305,
       -0.54096801]), 'targetState': array([-29.14153895,  30.24517661,  47.        ]), 'previousTarget': array([-18.49647231,  31.26436853,  46.15817008])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.62723011608181
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-29.14153895,  30.24517661,  47.        ]), 'distance': 2.52296234728604, 'localFrame': array([[-0.32073185,  0.84964433,  0.41861151],
       [-0.93556121, -0.35316457,  0.        ],
       [ 0.14783875, -0.39163669,  0.90816541]]), 'currentState': array([-30.39820034,  29.27524037,  45.03903982,  -0.32073185,
         0.84964433,   0.41861151]), 'targetState': array([-29.14153895,  30.24517661,  47.        ]), 'previousTarget': array([-29.14153895,  30.24517661,  47.        ])}
episode index:19838
target thresh 86.93317086828614
target distance 49.0
model initialize at round 19838
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.94230268,  8.75786409, 51.4405148 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.78051864,  0.44151631, -0.44255395],
       [-0.49235607,  0.87039388,  0.        ],
       [ 0.38519625,  0.21789412,  0.89674188]]), 'currentState': array([ 5.53439044,  9.14933178, 77.59854276,  0.78051864,  0.44151631,
       -0.44255395]), 'targetState': array([-9.89025245,  8.43699629, 30.        ]), 'previousTarget': array([-3.32313555,  8.69821119, 52.59443598])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6272369265358785
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.89025245,  8.43699629, 30.        ]), 'distance': 3.346938056922319, 'localFrame': array([[ 0.50495199,  0.03069691, -0.86260141],
       [-0.06067972,  0.99815729,  0.        ],
       [ 0.86101188,  0.05234241,  0.50588419]]), 'currentState': array([-9.20753744e+00,  7.07628632e+00,  3.29806649e+01,  5.04951993e-01,
        3.06969097e-02, -8.62601405e-01]), 'targetState': array([-9.89025245,  8.43699629, 30.        ]), 'previousTarget': array([-9.89025245,  8.43699629, 30.        ])}
episode index:19839
target thresh 86.93447748586736
target distance 69.0
model initialize at round 19839
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.28120892,  8.10289524, 66.18133227]), 'distance': 27.499999999999996, 'localFrame': array([[-0.25386437,  0.952559  , -0.16788159],
       [-0.96627316, -0.2575193 ,  0.        ],
       [-0.04323275,  0.16221947,  0.98580717]]), 'currentState': array([10.11506351, -1.73896594, 89.90227392, -0.25386437,  0.952559  ,
       -0.16788159]), 'targetState': array([-18.03479234,  26.43380913,  22.        ]), 'previousTarget': array([ 0.73727572,  6.89654887, 67.38728764])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272053117714361
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([-18.06294499,  26.42354923,  22.24039917]), 'distance': 27.499999999999996, 'localFrame': array([[-0.70050979,  0.39573901,  0.59386587],
       [-0.49186762, -0.87067   ,  0.        ],
       [ 0.51706119, -0.29210339,  0.80456406]]), 'currentState': array([-21.25868383,  25.25890003,  49.52924022,  -0.70050979,
         0.39573901,   0.59386587]), 'targetState': array([-18.03479234,  26.43380913,  22.        ]), 'previousTarget': array([-18.03479234,  26.43380913,  22.        ])}
episode index:19840
target thresh 86.93578397279333
target distance 29.055387827519695
model initialize at round 19840
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.6101777 , -1.69041098, 53.35859759]), 'distance': 27.499999999999996, 'localFrame': array([[-0.01177525, -0.6823872 ,  0.73089606],
       [ 0.99985115, -0.0172534 ,  0.        ],
       [ 0.01261044,  0.73078726,  0.68248879]]), 'currentState': array([ 2.17346169e+01,  1.90518906e+01,  3.72340693e+01, -1.17752500e-02,
       -6.82387203e-01,  7.30896059e-01]), 'targetState': array([10.76772363, -8.94740901, 59.        ]), 'previousTarget': array([13.64592967, -0.69109984, 52.46437528])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271737001938054
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([10.76772363, -8.94740901, 59.        ]), 'distance': 16.06466365602002, 'localFrame': array([[-0.62045521, -0.16908736,  0.76579684],
       [ 0.26293258, -0.96481421,  0.        ],
       [ 0.73885167,  0.20135294,  0.64308258]]), 'currentState': array([12.24146105, -3.43995228, 43.98103077, -0.62045521, -0.16908736,
        0.76579684]), 'targetState': array([10.76772363, -8.94740901, 59.        ]), 'previousTarget': array([10.76772363, -8.94740901, 59.        ])}
episode index:19841
target thresh 86.93709032907715
target distance 48.0465075043053
model initialize at round 19841
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.76939303, -3.06787476, 26.63550918]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.94613848, -0.11685996,  0.30193663],
       [ 0.12258107,  0.9924585 ,  0.        ],
       [-0.29965958,  0.03701171,  0.953328  ]]), 'currentState': array([ 6.89337672, 20.66875073, 12.77711079,  0.94613848, -0.11685996,
        0.30193663]), 'targetState': array([  8.67740043, -27.67133394,  41.        ]), 'previousTarget': array([ 6.85755569, -3.33514784, 26.81763283])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6271741543710996
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.67740043, -27.67133394,  41.        ]), 'distance': 2.8700054390832666, 'localFrame': array([[ 0.86016057, -0.35144357,  0.36960954],
       [ 0.37822689,  0.92571292,  0.        ],
       [-0.34215233,  0.13979627,  0.92918717]]), 'currentState': array([  8.47230643, -26.75302626,  38.28862052,   0.86016057,
        -0.35144357,   0.36960954]), 'targetState': array([  8.67740043, -27.67133394,  41.        ]), 'previousTarget': array([  8.67740043, -27.67133394,  41.        ])}
episode index:19842
target thresh 86.93839655473187
target distance 46.005495767851585
model initialize at round 19842
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.28124617,   0.44258241,  73.09846348]), 'distance': 27.5, 'localFrame': array([[ 0.99684384, -0.05634725,  0.05592266],
       [ 0.05643557,  0.99840624,  0.        ],
       [-0.05583354,  0.00315603,  0.9984351 ]]), 'currentState': array([-47.28924087,   0.63099734,  54.23329325,   0.99684384,
        -0.05634725,   0.05592266]), 'targetState': array([-2.992368  ,  0.21385449, 96.        ]), 'previousTarget': array([-28.46782358,   0.34879671,  73.29632816])}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.6271653285633031
{'scaleFactor': 20, 'timeStep': 80, 'trapCount': 35, 'trapConfig': [], 'currentTarget': array([-2.992368  ,  0.21385449, 96.        ]), 'distance': 2.924667883653475, 'localFrame': array([[ 0.41240633,  0.72262411,  0.55473905],
       [-0.86851314,  0.49566615,  0.        ],
       [-0.27496537, -0.48179815,  0.83202439]]), 'currentState': array([-5.52562822, -1.18960217, 96.40814742,  0.41240633,  0.72262411,
        0.55473905]), 'targetState': array([-2.992368  ,  0.21385449, 96.        ]), 'previousTarget': array([-2.992368  ,  0.21385449, 96.        ])}
episode index:19843
target thresh 86.93970264977055
target distance 50.0
model initialize at round 19843
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.83233967,  3.32379969, 63.06859669]), 'distance': 27.5, 'localFrame': array([[ 0.27309389, -0.74512398, -0.60844884],
       [ 0.93892446,  0.34412331,  0.        ],
       [ 0.20938143, -0.5712875 ,  0.7935931 ]]), 'currentState': array([ 3.44661412,  6.50945637, 90.34828431,  0.27309389, -0.74512398,
       -0.60844884]), 'targetState': array([ 5.95335757,  0.74668175, 41.        ]), 'previousTarget': array([ 4.24414825,  3.9170804 , 63.83580901])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6271667644578813
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 5.95335757,  0.74668175, 41.        ]), 'distance': 2.389105490559308, 'localFrame': array([[-0.43233389,  0.10796366, -0.89522693],
       [-0.24228256, -0.97020573,  0.        ],
       [-0.8685543 ,  0.21689787,  0.44561052]]), 'currentState': array([ 5.96236249,  1.04360828, 43.37056504, -0.43233389,  0.10796366,
       -0.89522693]), 'targetState': array([ 5.95335757,  0.74668175, 41.        ]), 'previousTarget': array([ 5.95335757,  0.74668175, 41.        ])}
episode index:19844
target thresh 86.94100861420625
target distance 40.55517910335678
model initialize at round 19844
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.44665967, 10.63443337, 75.77233564]), 'distance': 27.5, 'localFrame': array([[ 0.72209453, -0.68473866, -0.09855179],
       [ 0.68808833,  0.72562693,  0.        ],
       [ 0.07151183, -0.06781234,  0.99513192]]), 'currentState': array([-20.98167361,  19.02038568,  58.32787995,   0.72209453,
        -0.68473866,  -0.09855179]), 'targetState': array([17.84558247,  2.35269767, 93.        ]), 'previousTarget': array([-2.89273266, 11.17377668, 75.10238383])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6271695554798291
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.84558247,  2.35269767, 93.        ]), 'distance': 2.5059725419632657, 'localFrame': array([[ 0.98270504, -0.12464361,  0.13694809],
       [ 0.12582915,  0.99205193,  0.        ],
       [-0.13585962,  0.01723206,  0.99057823]]), 'currentState': array([15.48512829,  2.91927068, 92.37778665,  0.98270504, -0.12464361,
        0.13694809]), 'targetState': array([17.84558247,  2.35269767, 93.        ]), 'previousTarget': array([17.84558247,  2.35269767, 93.        ])}
episode index:19845
target thresh 86.94231444805207
target distance 6.241046647442431
model initialize at round 19845
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-14.34269402, -45.80706417,  10.        ]), 'distance': 8.014204442886966, 'localFrame': array([[-0.30162994,  0.34075564,  0.89045212],
       [-0.74878687, -0.66281085,  0.        ],
       [ 0.59020133, -0.66675885,  0.45507695]]), 'currentState': array([ -8.10164738, -45.28093719,   5.        ,  -0.30162994,
         0.34075564,   0.89045212]), 'targetState': array([-14.34269402, -45.80706417,  10.        ]), 'previousTarget': array([-14.34269402, -45.80706417,  10.        ])}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6271868450819917
{'scaleFactor': 20, 'timeStep': 4, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-14.34269402, -45.80706417,  10.        ]), 'distance': 2.7027296618393106, 'localFrame': array([[-0.88818758,  0.37664003,  0.26318264],
       [-0.3904033 , -0.92064394,  0.        ],
       [ 0.2422975 , -0.10274737,  0.96474603]]), 'currentState': array([-12.33619666, -45.10044491,   8.33284519,  -0.88818758,
         0.37664003,   0.26318264]), 'targetState': array([-14.34269402, -45.80706417,  10.        ]), 'previousTarget': array([-14.34269402, -45.80706417,  10.        ])}
episode index:19846
target thresh 86.94362015132101
target distance 19.0
model initialize at round 19846
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.90149573,  5.40114322, 44.        ]), 'distance': 19.096274738658426, 'localFrame': array([[-0.70359874, -0.48919636,  0.51539862],
       [ 0.57085716, -0.82104939,  0.        ],
       [ 0.42316772,  0.29421899,  0.85695056]]), 'currentState': array([-8.90811038,  2.96253777, 25.30023581, -0.70359874, -0.48919636,
        0.51539862]), 'targetState': array([-5.90149573,  5.40114322, 44.        ]), 'previousTarget': array([-5.90149573,  5.40114322, 44.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6271994582818667
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-5.90149573,  5.40114322, 44.        ]), 'distance': 3.5279140677347196, 'localFrame': array([[ 0.1142309 , -0.81444877,  0.56888004],
       [ 0.99030698,  0.13889598,  0.        ],
       [-0.07901515,  0.56336588,  0.82242051]]), 'currentState': array([-5.58010488,  3.46926693, 41.06559383,  0.1142309 , -0.81444877,
        0.56888004]), 'targetState': array([-5.90149573,  5.40114322, 44.        ]), 'previousTarget': array([-5.90149573,  5.40114322, 44.        ])}
episode index:19847
target thresh 86.94492572402616
target distance 56.637031141236676
model initialize at round 19847
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.12228057, -10.86585066,  70.93129699]), 'distance': 27.5, 'localFrame': array([[-0.35919686,  0.63613263, -0.68287107],
       [-0.87077173, -0.49168751,  0.        ],
       [-0.33575917,  0.59462482,  0.73053891]]), 'currentState': array([-14.14116727, -35.0265531 ,  83.06889287,  -0.35919686,
         0.63613263,  -0.68287107]), 'targetState': array([-2.53470133, 20.84646947, 55.        ]), 'previousTarget': array([ -8.40172309, -11.62333229,  71.6255934 ])}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.6271922934245507
{'scaleFactor': 20, 'timeStep': 73, 'trapCount': 31, 'trapConfig': [], 'currentTarget': array([-2.53470133, 20.84646947, 55.        ]), 'distance': 2.170691723926883, 'localFrame': array([[ 0.6418267 ,  0.68172338, -0.35115768],
       [-0.72809087,  0.68548062,  0.        ],
       [ 0.24071179,  0.2556747 ,  0.93631634]]), 'currentState': array([-3.32253513, 18.99195378, 55.80746023,  0.6418267 ,  0.68172338,
       -0.35115768]), 'targetState': array([-2.53470133, 20.84646947, 55.        ]), 'previousTarget': array([-2.53470133, 20.84646947, 55.        ])}
episode index:19848
target thresh 86.94623116618055
target distance 64.70006049328757
model initialize at round 19848
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.45174689,  6.44596888, 54.71978636]), 'distance': 27.500000000000004, 'localFrame': array([[-0.67393218,  0.04157756, -0.73762234],
       [-0.06157691, -0.99810234,  0.        ],
       [-0.73622259,  0.04542051,  0.67521351]]), 'currentState': array([ 1.80146581e+01,  1.88884600e+01,  5.64018807e+01, -6.73932183e-01,
        4.15775638e-02, -7.37622342e-01]), 'targetState': array([-46.01159347, -13.6723541 ,  52.        ]), 'previousTarget': array([-5.7016984 ,  6.98679298, 55.73816297])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6271893919639759
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-46.01159347, -13.6723541 ,  52.        ]), 'distance': 3.0264490665377113, 'localFrame': array([[-0.70955046,  0.2240103 , -0.66809993],
       [-0.30106016, -0.95360515,  0.        ],
       [-0.63710354,  0.20113827,  0.74407155]]), 'currentState': array([-43.31544354, -12.41179882,  51.4512105 ,  -0.70955046,
         0.2240103 ,  -0.66809993]), 'targetState': array([-46.01159347, -13.6723541 ,  52.        ]), 'previousTarget': array([-46.01159347, -13.6723541 ,  52.        ])}
episode index:19849
target thresh 86.94753647779727
target distance 26.005820544380512
model initialize at round 19849
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 6.47931517, -9.17958283, 74.84516759]), 'distance': 27.499999999999996, 'localFrame': array([[-0.3555069 ,  0.88818095,  0.29111758],
       [-0.92839214, -0.37160199,  0.        ],
       [ 0.10817987, -0.27027127,  0.95668728]]), 'currentState': array([  4.18237782, -29.70703125,  93.        ,  -0.3555069 ,
         0.88818095,   0.29111758]), 'targetState': array([ 7.09232256, -3.70121071, 70.        ]), 'previousTarget': array([ 6.47931517, -9.17958283, 74.84516759])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271577955210558
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 85, 'trapConfig': [], 'currentTarget': array([ 7.09232256, -3.70121071, 70.        ]), 'distance': 11.265732582316835, 'localFrame': array([[-0.30920432,  0.54796077, -0.77725908],
       [-0.87091164, -0.49143963,  0.        ],
       [-0.38197592,  0.67692399,  0.62918067]]), 'currentState': array([  7.45400799, -12.34948976,  77.21062991,  -0.30920432,
         0.54796077,  -0.77725908]), 'targetState': array([ 7.09232256, -3.70121071, 70.        ]), 'previousTarget': array([ 7.09232256, -3.70121071, 70.        ])}
episode index:19850
target thresh 86.94884165888934
target distance 35.0
model initialize at round 19850
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.10897721, 18.15140032, 65.14483885]), 'distance': 27.5, 'localFrame': array([[-0.22825829,  0.55772236,  0.79802502],
       [-0.92548929, -0.37877377,  0.        ],
       [ 0.30227094, -0.73856361,  0.60262432]]), 'currentState': array([ 2.1130724 ,  0.30799119, 85.19257877, -0.22825829,  0.55772236,
        0.79802502]), 'targetState': array([12.93759716, 32.52104826, 49.        ]), 'previousTarget': array([ 7.90825347, 17.49398148, 64.66610698])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6271638412768772
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.93759716, 32.52104826, 49.        ]), 'distance': 3.299296983661826, 'localFrame': array([[ 0.44906313, -0.21202322, -0.86797953],
       [ 0.42694971,  0.90427537,  0.        ],
       [ 0.78489251, -0.37058361,  0.49659998]]), 'currentState': array([13.8829736 , 30.42969589, 51.37020447,  0.44906313, -0.21202322,
       -0.86797953]), 'targetState': array([12.93759716, 32.52104826, 49.        ]), 'previousTarget': array([12.93759716, 32.52104826, 49.        ])}
episode index:19851
target thresh 86.95014670946985
target distance 33.119823437481436
model initialize at round 19851
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -0.78022041, -13.23484315,  60.46517673]), 'distance': 27.5, 'localFrame': array([[-0.87490416,  0.35930088, -0.32472386],
       [-0.37988741, -0.92503273,  0.        ],
       [-0.3003802 ,  0.12335851,  0.94580887]]), 'currentState': array([ 21.77029852, -21.88655479,  73.61325573,  -0.87490416,
         0.35930088,  -0.32472386]), 'targetState': array([-10.15364836,  -9.63864228,  55.        ]), 'previousTarget': array([  0.59011327, -13.74051881,  61.48781334])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.627173449532787
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.15364836,  -9.63864228,  55.        ]), 'distance': 4.485522906760623, 'localFrame': array([[-0.97534071, -0.21884126, -0.0286182 ],
       [ 0.21893093, -0.97574036,  0.        ],
       [-0.02792393, -0.00626541,  0.99959042]]), 'currentState': array([-7.31496087e+00, -1.16125918e+01,  5.78574976e+01, -9.75340710e-01,
       -2.18841261e-01, -2.86181997e-02]), 'targetState': array([-10.15364836,  -9.63864228,  55.        ]), 'previousTarget': array([-10.15364836,  -9.63864228,  55.        ])}
episode index:19852
target thresh 86.9514516295518
target distance 35.96177104467406
model initialize at round 19852
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([10.24926901, 32.57632492, 43.0172869 ]), 'distance': 27.5, 'localFrame': array([[ 0.97087943,  0.13884308,  0.19523248],
       [-0.14156726,  0.98992864,  0.        ],
       [-0.19326623, -0.02763853,  0.98075699]]), 'currentState': array([-7.4600688 , 45.22350951, 26.20425398,  0.97087943,  0.13884308,
        0.19523248]), 'targetState': array([27.08402256, 20.5537277 , 59.        ]), 'previousTarget': array([ 9.01903515, 32.90194765, 41.92048601])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.627179117538824
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([27.08402256, 20.5537277 , 59.        ]), 'distance': 2.2362302669010163, 'localFrame': array([[ 0.17804814, -0.73711809,  0.65188633],
       [ 0.97204518,  0.2347939 ,  0.        ],
       [-0.15305893,  0.63366296,  0.7583167 ]]), 'currentState': array([25.11920417, 21.34271452, 58.28047632,  0.17804814, -0.73711809,
        0.65188633]), 'targetState': array([27.08402256, 20.5537277 , 59.        ]), 'previousTarget': array([27.08402256, 20.5537277 , 59.        ])}
episode index:19853
target thresh 86.95275641914829
target distance 18.828709334561243
model initialize at round 19853
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.9056308 ,  5.68933839, 51.        ]), 'distance': 20.49649606766445, 'localFrame': array([[ 0.77801873,  0.57331472,  0.25689896],
       [-0.59322436,  0.80503718,  0.        ],
       [-0.20681322, -0.15239872,  0.96643827]]), 'currentState': array([-15.46549474,  -5.08066152,  52.53539919,   0.77801873,
         0.57331472,   0.25689896]), 'targetState': array([ 1.9056308 ,  5.68933839, 51.        ]), 'previousTarget': array([ 1.9056308 ,  5.68933839, 51.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6271921731323657
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.9056308 ,  5.68933839, 51.        ]), 'distance': 2.869949037015009, 'localFrame': array([[ 0.52072605,  0.84569356, -0.11681944],
       [-0.8515238 ,  0.52431595,  0.        ],
       [ 0.0612503 ,  0.09947454,  0.99315317]]), 'currentState': array([ 2.20271551,  5.20025969, 53.81232114,  0.52072605,  0.84569356,
       -0.11681944]), 'targetState': array([ 1.9056308 ,  5.68933839, 51.        ]), 'previousTarget': array([ 1.9056308 ,  5.68933839, 51.        ])}
episode index:19854
target thresh 86.95406107827233
target distance 24.48150174177802
model initialize at round 19854
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.68956306,  41.09339635,  94.        ]), 'distance': 25.053442571005252, 'localFrame': array([[-0.40828535,  0.67511166,  0.61443252],
       [-0.85568804, -0.51749201,  0.        ],
       [ 0.31796392, -0.52576255,  0.78896938]]), 'currentState': array([-22.12623775,  17.06698835,  88.5609959 ,  -0.40828535,
         0.67511166,   0.61443252]), 'targetState': array([-26.68956306,  41.09339635,  94.        ]), 'previousTarget': array([-26.68956306,  41.09339635,  94.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6272047809817672
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.68956306,  41.09339635,  94.        ]), 'distance': 2.7636963076934986, 'localFrame': array([[ 0.25716061,  0.92645145,  0.27487474],
       [-0.96356805,  0.26746329,  0.        ],
       [-0.0735189 , -0.26486052,  0.96148004]]), 'currentState': array([-26.76814879,  38.60466478,  95.19919004,   0.25716061,
         0.92645145,   0.27487474]), 'targetState': array([-26.68956306,  41.09339635,  94.        ]), 'previousTarget': array([-26.68956306,  41.09339635,  94.        ])}
episode index:19855
target thresh 86.95536560693698
target distance 81.0
model initialize at round 19855
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.58004355, 10.67954726, 40.00610347]), 'distance': 27.5, 'localFrame': array([[ 0.27356448,  0.75936703,  0.59035938],
       [-0.94081144,  0.33893043,  0.        ],
       [-0.20009076, -0.55541686,  0.80714051]]), 'currentState': array([ 8.004722  ,  5.461395  , 14.40683224,  0.27356448,  0.75936703,
        0.59035938]), 'targetState': array([-19.02235404,  21.88949626,  95.        ]), 'previousTarget': array([-1.30869008,  9.70294219, 39.63482772])}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6272033582997308
{'scaleFactor': 20, 'timeStep': 52, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.02235404,  21.88949626,  95.        ]), 'distance': 2.3026429752934643, 'localFrame': array([[ 0.7659286 ,  0.0754794 ,  0.63847963],
       [-0.09807121,  0.9951794 ,  0.        ],
       [-0.63540178, -0.06261647,  0.76963872]]), 'currentState': array([-1.79696750e+01,  2.07563986e+01,  9.32940923e+01,  7.65928598e-01,
        7.54793997e-02,  6.38479634e-01]), 'targetState': array([-19.02235404,  21.88949626,  95.        ]), 'previousTarget': array([-19.02235404,  21.88949626,  95.        ])}
episode index:19856
target thresh 86.95667000515527
target distance 31.357057636653245
model initialize at round 19856
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.66692227, 27.32607928, 66.60162815]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.65917236, -0.57384483, -0.48599785],
       [ 0.65660308,  0.7542363 ,  0.        ],
       [ 0.36655722, -0.31910768,  0.87396001]]), 'currentState': array([-4.36999657,  2.54250033, 78.49826217,  0.65917236, -0.57384483,
       -0.48599785]), 'targetState': array([-3.45407113, 34.82914574, 63.        ]), 'previousTarget': array([-3.7845887 , 27.62813897, 66.90397328])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6272117387663343
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.45407113, 34.82914574, 63.        ]), 'distance': 3.361866255378495, 'localFrame': array([[ 0.68146629,  0.49329213, -0.54061684],
       [-0.58636674,  0.81004571,  0.        ],
       [ 0.43792435,  0.31699974,  0.84126894]]), 'currentState': array([-3.48718551, 31.84323631, 64.54447178,  0.68146629,  0.49329213,
       -0.54061684]), 'targetState': array([-3.45407113, 34.82914574, 63.        ]), 'previousTarget': array([-3.45407113, 34.82914574, 63.        ])}
episode index:19857
target thresh 86.95797427294029
target distance 35.0
model initialize at round 19857
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.63708509, -17.85869045,  57.04734073]), 'distance': 27.5, 'localFrame': array([[ 0.24081118,  0.71462206,  0.65675359],
       [-0.94764235,  0.31933366,  0.        ],
       [-0.20972352, -0.62236751,  0.75410524]]), 'currentState': array([ 37.44960176, -23.37523377,  34.54384989,   0.24081118,
         0.71462206,   0.65675359]), 'targetState': array([ 14.76946287, -14.9285956 ,  69.        ]), 'previousTarget': array([ 22.32369831, -18.36727565,  56.76029513])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6272205220703313
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.76946287, -14.9285956 ,  69.        ]), 'distance': 3.0539244342895437, 'localFrame': array([[-0.90006579,  0.43496025, -0.02628975],
       [-0.43511064, -0.90037699,  0.        ],
       [-0.02367069,  0.01143895,  0.99965436]]), 'currentState': array([ 1.64196023e+01, -1.51354405e+01,  6.64386118e+01, -9.00065793e-01,
        4.34960248e-01, -2.62897524e-02]), 'targetState': array([ 14.76946287, -14.9285956 ,  69.        ]), 'previousTarget': array([ 14.76946287, -14.9285956 ,  69.        ])}
episode index:19858
target thresh 86.95927841030505
target distance 25.0
model initialize at round 19858
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.05645915, -12.62908059,  22.4501188 ]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.55598842, -0.39766943, -0.72988759],
       [ 0.58175617,  0.81336324,  0.        ],
       [ 0.59366374, -0.42461661,  0.68356719]]), 'currentState': array([  5.32761772, -13.86515864,  44.24158446,   0.55598842,
        -0.39766943,  -0.72988759]), 'targetState': array([ 23.93736288, -12.49010242,  20.        ]), 'previousTarget': array([ 20.98274318, -12.63862083,  23.64579923])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6272297122282523
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.93736288, -12.49010242,  20.        ]), 'distance': 4.1254609789578325, 'localFrame': array([[ 0.59635033, -0.59156956,  0.54259722],
       [ 0.70425533,  0.70994677,  0.        ],
       [-0.38521515,  0.38212699,  0.83999301]]), 'currentState': array([22.52390121, -9.79567925, 22.78597167,  0.59635033, -0.59156956,
        0.54259722]), 'targetState': array([ 23.93736288, -12.49010242,  20.        ]), 'previousTarget': array([ 23.93736288, -12.49010242,  20.        ])}
episode index:19859
target thresh 86.96058241726257
target distance 62.248466469224574
model initialize at round 19859
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.91280779, -10.46512808,  54.78287585]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.96754076, -0.05413749,  0.24684814],
       [ 0.05586632,  0.99843826,  0.        ],
       [-0.24646262,  0.0137905 ,  0.96905418]]), 'currentState': array([ 1.78263901e+01,  1.42112509e+01,  6.30214663e+01,  9.67540764e-01,
       -5.41374915e-02,  2.46848136e-01]), 'targetState': array([ -4.91737557, -48.75263498,  42.        ]), 'previousTarget': array([  7.71362804, -11.32338226,  54.62704693])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6272311437241409
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -4.91737557, -48.75263498,  42.        ]), 'distance': 2.542377793931874, 'localFrame': array([[ 0.07500996, -0.99161657,  0.10521451],
       [ 0.9971512 ,  0.07542862,  0.        ],
       [-0.00793619,  0.10491478,  0.99444955]]), 'currentState': array([ -5.47073966, -46.28436102,  41.74485948,   0.07500996,
        -0.99161657,   0.10521451]), 'targetState': array([ -4.91737557, -48.75263498,  42.        ]), 'previousTarget': array([ -4.91737557, -48.75263498,  42.        ])}
episode index:19860
target thresh 86.96188629382593
target distance 9.564898384796841
model initialize at round 19860
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.38507314, 33.97177611, 28.        ]), 'distance': 9.225127860007238, 'localFrame': array([[ 0.70349819, -0.37741855, -0.60220058],
       [ 0.4727513 ,  0.8811959 ,  0.        ],
       [ 0.53065668, -0.2846911 ,  0.79834483]]), 'currentState': array([-5.30237579, 42.17281043, 26.41862079,  0.70349819, -0.37741855,
       -0.60220058]), 'targetState': array([-1.38507314, 33.97177611, 28.        ]), 'previousTarget': array([-1.38507314, 33.97177611, 28.        ])}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6272479286224982
{'scaleFactor': 20, 'timeStep': 5, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.38507314, 33.97177611, 28.        ]), 'distance': 2.902931816798716, 'localFrame': array([[-0.26729371, -0.86969188, -0.41495795],
       [ 0.95587281, -0.29378081,  0.        ],
       [-0.12190668, -0.39664702,  0.90984059]]), 'currentState': array([-1.66726215, 36.30662467, 26.29827595, -0.26729371, -0.86969188,
       -0.41495795]), 'targetState': array([-1.38507314, 33.97177611, 28.        ]), 'previousTarget': array([-1.38507314, 33.97177611, 28.        ])}
episode index:19861
target thresh 86.96319004000816
target distance 36.0
model initialize at round 19861
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.93522253, -9.20343511, 80.77030312]), 'distance': 27.5, 'localFrame': array([[ 0.26451868,  0.10819002,  0.95829264],
       [-0.37856641,  0.92557413,  0.        ],
       [-0.88697088, -0.3627774 ,  0.28578876]]), 'currentState': array([ 0.12813813,  1.03257271, 55.53183541,  0.26451868,  0.10819002,
        0.95829264]), 'targetState': array([  5.32747167, -12.9467388 ,  90.        ]), 'previousTarget': array([ 3.79288287, -9.21740496, 79.63012847])}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6272528491948139
{'scaleFactor': 20, 'timeStep': 33, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([  5.32747167, -12.9467388 ,  90.        ]), 'distance': 3.064036284886966, 'localFrame': array([[-0.86131371,  0.14513646,  0.48690256],
       [-0.1661634 , -0.98609823,  0.        ],
       [ 0.48013375, -0.08090538,  0.87345629]]), 'currentState': array([  6.68831081, -12.56286856,  87.28171399,  -0.86131371,
         0.14513646,   0.48690256]), 'targetState': array([  5.32747167, -12.9467388 ,  90.        ]), 'previousTarget': array([  5.32747167, -12.9467388 ,  90.        ])}
episode index:19862
target thresh 86.96449365582228
target distance 39.92363711016068
model initialize at round 19862
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.16197461,  2.38830172, 20.3765825 ]), 'distance': 27.5, 'localFrame': array([[-0.79493564, -0.36432772,  0.48512127],
       [ 0.4166379 , -0.90907253,  0.        ],
       [ 0.44101042,  0.20211991,  0.87444688]]), 'currentState': array([19.85702904, 19.65567228,  4.8156285 , -0.79493564, -0.36432772,
        0.48512127]), 'targetState': array([-14.3138392 , -20.49668284,  41.        ]), 'previousTarget': array([ 6.40669439,  2.58150685, 19.611843  ])}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6272529786204801
{'scaleFactor': 20, 'timeStep': 47, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-14.3138392 , -20.49668284,  41.        ]), 'distance': 2.8114996536744123, 'localFrame': array([[ 0.81046324,  0.21391228,  0.54533556],
       [-0.25519892,  0.96688857,  0.        ],
       [-0.52727872, -0.13916905,  0.83821783]]), 'currentState': array([-12.68666819, -20.53124291,  38.70747946,   0.81046324,
         0.21391228,   0.54533556]), 'targetState': array([-14.3138392 , -20.49668284,  41.        ]), 'previousTarget': array([-14.3138392 , -20.49668284,  41.        ])}
episode index:19863
target thresh 86.96579714128134
target distance 64.23263900884815
model initialize at round 19863
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.60476897,  7.12133424, 73.94713422]), 'distance': 27.500000000000004, 'localFrame': array([[-0.79067574,  0.44822527, -0.41704433],
       [-0.49315887, -0.86993927,  0.        ],
       [-0.36280324,  0.20566911,  0.90888614]]), 'currentState': array([  3.68097132, -18.15112959,  81.9764904 ,  -0.79067574,
         0.44822527,  -0.41704433]), 'targetState': array([-14.44545349,  44.72503632,  62.        ]), 'previousTarget': array([-2.9802686 ,  5.66921054, 74.76877852])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.62725036594284
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.44545349,  44.72503632,  62.        ]), 'distance': 3.279490799333524, 'localFrame': array([[-0.37245202, -0.64568591, -0.66661023],
       [ 0.86621989, -0.49966298,  0.        ],
       [-0.33308045, -0.57743104,  0.74540647]]), 'currentState': array([-15.74648509,  44.45186325,  64.99795816,  -0.37245202,
        -0.64568591,  -0.66661023]), 'targetState': array([-14.44545349,  44.72503632,  62.        ]), 'previousTarget': array([-14.44545349,  44.72503632,  62.        ])}
episode index:19864
target thresh 86.96710049639836
target distance 40.0
model initialize at round 19864
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.23796153, -16.6199915 ,  21.0843968 ]), 'distance': 27.5, 'localFrame': array([[ 0.70081461,  0.04794185,  0.71173061],
       [-0.06824923,  0.9976683 ,  0.        ],
       [-0.71007107, -0.04857507,  0.70245251]]), 'currentState': array([ 3.10275443, -2.50034802,  2.18779425,  0.70081461,  0.04794185,
        0.71173061]), 'targetState': array([ 32.13541068, -31.50103776,  41.        ]), 'previousTarget': array([ 16.47830778, -16.88040317,  20.17070587])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.627254201700658
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 32.13541068, -31.50103776,  41.        ]), 'distance': 2.822089876990676, 'localFrame': array([[ 0.91879494, -0.35577143, -0.17100451],
       [ 0.3610902 ,  0.93253089,  0.        ],
       [ 0.15946699, -0.06174805,  0.98527025]]), 'currentState': array([ 29.60712114, -30.25174807,  41.10591757,   0.91879494,
        -0.35577143,  -0.17100451]), 'targetState': array([ 32.13541068, -31.50103776,  41.        ]), 'previousTarget': array([ 32.13541068, -31.50103776,  41.        ])}
episode index:19865
target thresh 86.9684037211864
target distance 52.0
model initialize at round 19865
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  3.35443035, -24.78409588,  44.86728559]), 'distance': 27.5, 'localFrame': array([[ 0.11431163,  0.70497542,  0.69995893],
       [-0.9871074 ,  0.16005928,  0.        ],
       [-0.11203492, -0.69093465,  0.71418309]]), 'currentState': array([ 10.84603673, -36.926544  ,  21.35799408,   0.11431163,
         0.70497542,   0.69995893]), 'targetState': array([ -5.29183687, -10.77016539,  72.        ]), 'previousTarget': array([  3.00482434, -25.47116403,  43.63235666])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.627258394745336
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -5.29183687, -10.77016539,  72.        ]), 'distance': 1.942676886801857, 'localFrame': array([[-0.80144491,  0.44604456,  0.39840972],
       [-0.48630711, -0.87378796,  0.        ],
       [ 0.34812561, -0.19374948,  0.91720755]]), 'currentState': array([ -4.32537836, -10.83298764,  70.31595582,  -0.80144491,
         0.44604456,   0.39840972]), 'targetState': array([ -5.29183687, -10.77016539,  72.        ]), 'previousTarget': array([ -5.29183687, -10.77016539,  72.        ])}
episode index:19866
target thresh 86.96970681565847
target distance 40.40701939599363
model initialize at round 19866
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.15148851, -23.35248797,  29.71269019]), 'distance': 27.5, 'localFrame': array([[-0.59778714, -0.45078425,  0.66290579],
       [ 0.60208703, -0.79843047,  0.        ],
       [ 0.52928418,  0.39912698,  0.74870282]]), 'currentState': array([ -1.22684155, -31.48164231,  13.82791762,  -0.59778714,
        -0.45078425,   0.66290579]), 'targetState': array([-40.97183438, -16.04084747,  44.        ]), 'previousTarget': array([-21.60788562, -22.72783767,  29.14410564])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272268218659507
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 42, 'trapConfig': [], 'currentTarget': array([-40.97183438, -16.04084747,  44.        ]), 'distance': 12.727150567606687, 'localFrame': array([[-0.90478279, -0.07773521,  0.41871869],
       [ 0.08560053, -0.99632954,  0.        ],
       [ 0.4171818 ,  0.03584254,  0.90811599]]), 'currentState': array([-36.59234392, -17.70930724,  32.16713615,  -0.90478279,
        -0.07773521,   0.41871869]), 'targetState': array([-40.97183438, -16.04084747,  44.        ]), 'previousTarget': array([-40.97183438, -16.04084747,  44.        ])}
episode index:19867
target thresh 86.97100977982761
target distance 19.0
model initialize at round 19867
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.85415451, -13.00239989,  94.        ]), 'distance': 20.185500487982324, 'localFrame': array([[-0.93731777,  0.27558551,  0.21327922],
       [-0.28207571, -0.95939215,  0.        ],
       [ 0.20461841, -0.06016089,  0.97699129]]), 'currentState': array([ -9.29107897, -10.4120003 ,  74.50839664,  -0.93731777,
         0.27558551,   0.21327922]), 'targetState': array([-13.85415451, -13.00239989,  94.        ]), 'previousTarget': array([-13.85415451, -13.00239989,  94.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6272403165021694
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.85415451, -13.00239989,  94.        ]), 'distance': 2.6830012606336795, 'localFrame': array([[-0.25369407, -0.52854358,  0.81011172],
       [ 0.90152762, -0.43272157,  0.        ],
       [ 0.35055282,  0.73033809,  0.58627553]]), 'currentState': array([-14.73070966, -14.19383965,  91.761559  ,  -0.25369407,
        -0.52854358,   0.81011172]), 'targetState': array([-13.85415451, -13.00239989,  94.        ]), 'previousTarget': array([-13.85415451, -13.00239989,  94.        ])}
episode index:19868
target thresh 86.97231261370686
target distance 22.93732316127953
model initialize at round 19868
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.29713256,  -4.69017435,  27.        ]), 'distance': 24.541813905237046, 'localFrame': array([[-0.24998304, -0.79191507,  0.55711668],
       [ 0.95361565, -0.30102691,  0.        ],
       [ 0.16770711,  0.53127519,  0.83043422]]), 'currentState': array([ 8.0771866 ,  1.05747379, 22.21393959, -0.24998304, -0.79191507,
        0.55711668]), 'targetState': array([-15.29713256,  -4.69017435,  27.        ]), 'previousTarget': array([-15.29713256,  -4.69017435,  27.        ])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6272407667095056
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-15.29713256,  -4.69017435,  27.        ]), 'distance': 3.052024940018555, 'localFrame': array([[-0.95397972,  0.14281501,  0.26367893],
       [-0.14805459, -0.98897919,  0.        ],
       [ 0.26077297, -0.03903887,  0.9646105 ]]), 'currentState': array([-13.3384508 ,  -7.01593277,  26.7368085 ,  -0.95397972,
         0.14281501,   0.26367893]), 'targetState': array([-15.29713256,  -4.69017435,  27.        ]), 'previousTarget': array([-15.29713256,  -4.69017435,  27.        ])}
episode index:19869
target thresh 86.97361531730921
target distance 24.332807266710784
model initialize at round 19869
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.53809323, -2.50397672, 21.31580729]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83745949, -0.42831106, -0.33942781],
       [ 0.45534387, -0.89031565,  0.        ],
       [-0.30219789, -0.15455637,  0.94063211]]), 'currentState': array([ 9.00174728,  1.8165823 , 41.16164019, -0.83745949, -0.42831106,
       -0.33942781]), 'targetState': array([-13.5698907 ,  -3.44355432,  17.        ]), 'previousTarget': array([-8.54783301, -2.26402611, 21.95336947])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6272475660022905
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-13.5698907 ,  -3.44355432,  17.        ]), 'distance': 3.211441256150392, 'localFrame': array([[ 0.02834222,  0.99867251, -0.0430108 ],
       [-0.99959753,  0.02836847,  0.        ],
       [ 0.00122015,  0.04299349,  0.99907461]]), 'currentState': array([-12.36719329,  -5.5930829 ,  14.93932031,   0.02834222,
         0.99867251,  -0.0430108 ]), 'targetState': array([-13.5698907 ,  -3.44355432,  17.        ]), 'previousTarget': array([-13.5698907 ,  -3.44355432,  17.        ])}
episode index:19870
target thresh 86.97491789064773
target distance 47.22779585207268
model initialize at round 19870
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.94462403, -21.34108067,  89.0964575 ]), 'distance': 27.5, 'localFrame': array([[ 0.00497989,  0.9174679 ,  0.39777864],
       [-0.99998527,  0.00542778,  0.        ],
       [-0.00215906, -0.39777278,  0.91748142]]), 'currentState': array([-1.02606052e+00, -4.72112483e+01,  7.99804919e+01,  4.97988920e-03,
        9.17467904e-01,  3.97778640e-01]), 'targetState': array([ 2.43702814, -1.74954103, 96.        ]), 'previousTarget': array([  0.66900835, -23.01184665,  88.79668077])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6272506962685008
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([ 2.43702814, -1.74954103, 96.        ]), 'distance': 3.304018755009643, 'localFrame': array([[ 0.6883245 ,  0.25271294, -0.67995996],
       [-0.34464807,  0.93873197,  0.        ],
       [ 0.63830015,  0.23434689,  0.73324924]]), 'currentState': array([-3.06283456e-02, -3.29603917e+00,  9.75606264e+01,  6.88324502e-01,
        2.52712939e-01, -6.79959963e-01]), 'targetState': array([ 2.43702814, -1.74954103, 96.        ]), 'previousTarget': array([ 2.43702814, -1.74954103, 96.        ])}
episode index:19871
target thresh 86.97622033373543
target distance 69.0
model initialize at round 19871
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.79281363, -5.25370445, 50.61686446]), 'distance': 27.5, 'localFrame': array([[ 0.80430028,  0.34802376,  0.48164357],
       [-0.39712093,  0.91776629,  0.        ],
       [-0.44203623, -0.19127074,  0.8763672 ]]), 'currentState': array([-25.52382119, -10.28793159,  28.62955088,   0.80430028,
         0.34802376,   0.48164357]), 'targetState': array([23.392389  ,  5.36620321, 97.        ]), 'previousTarget': array([-10.7319006 ,  -6.2891457 ,  49.84483218])}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.627248971014381
{'scaleFactor': 20, 'timeStep': 53, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([23.392389  ,  5.36620321, 97.        ]), 'distance': 2.6343490880653224, 'localFrame': array([[ 0.90965681, -0.02683252,  0.41449306],
       [ 0.02948459,  0.99956524,  0.        ],
       [-0.41431285,  0.01222116,  0.91005247]]), 'currentState': array([ 2.14057742e+01,  5.39158533e+00,  9.52701120e+01,  9.09656813e-01,
       -2.68325219e-02,  4.14493061e-01]), 'targetState': array([23.392389  ,  5.36620321, 97.        ]), 'previousTarget': array([23.392389  ,  5.36620321, 97.        ])}
episode index:19872
target thresh 86.97752264658533
target distance 45.0
model initialize at round 19872
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-18.86899898,  14.16720709,  67.84606121]), 'distance': 27.5, 'localFrame': array([[ 0.19966598, -0.6983597 ,  0.68733342],
       [ 0.96147497,  0.2748925 ,  0.        ],
       [-0.1889428 ,  0.66085387,  0.72634205]]), 'currentState': array([-33.80883339,  24.0016336 ,  46.95741829,   0.19966598,
        -0.6983597 ,   0.68733342]), 'targetState': array([-2.3089949 ,  3.26627349, 91.        ]), 'previousTarget': array([-18.90275331,  15.13927152,  66.96458623])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627217408141588
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 64, 'trapConfig': [], 'currentTarget': array([-2.3089949 ,  3.26627349, 91.        ]), 'distance': 16.901085320552824, 'localFrame': array([[ 0.62107681, -0.31297882,  0.71854565],
       [ 0.45001857,  0.8930192 ,  0.        ],
       [-0.64167506,  0.32335888,  0.6954798 ]]), 'currentState': array([-2.74143261, 11.39827015, 76.19019537,  0.62107681, -0.31297882,
        0.71854565]), 'targetState': array([-2.3089949 ,  3.26627349, 91.        ]), 'previousTarget': array([-2.3089949 ,  3.26627349, 91.        ])}
episode index:19873
target thresh 86.97882482921045
target distance 40.32887168019968
model initialize at round 19873
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.61627915, -4.52260138, 89.03302997]), 'distance': 27.499999999999996, 'localFrame': array([[-0.66226208,  0.23274502,  0.71220691],
       [-0.33155997, -0.94343414,  0.        ],
       [ 0.67192032, -0.23613931,  0.70196959]]), 'currentState': array([45.29285729, -6.66037101, 75.20915249, -0.66226208,  0.23274502,
        0.71220691]), 'targetState': array([ 6.25827508, -3.13591981, 98.        ]), 'previousTarget': array([23.00596297, -4.41497334, 88.03333114])}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6272249860801639
{'scaleFactor': 20, 'timeStep': 26, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.25827508, -3.13591981, 98.        ]), 'distance': 3.7111625083151174, 'localFrame': array([[-0.63194193, -0.73873572,  0.23434788],
       [ 0.75989671, -0.65004383,  0.        ],
       [ 0.15233639,  0.17808018,  0.9721528 ]]), 'currentState': array([ 9.24515156, -4.5530839 , 99.6861026 , -0.63194193, -0.73873572,
        0.23434788]), 'targetState': array([ 6.25827508, -3.13591981, 98.        ]), 'previousTarget': array([ 6.25827508, -3.13591981, 98.        ])}
episode index:19874
target thresh 86.98012688162382
target distance 50.23849120515773
model initialize at round 19874
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.13722653, 19.23245239, 85.10622012]), 'distance': 27.5, 'localFrame': array([[-0.04667795,  0.93263493,  0.35778941],
       [-0.99874987, -0.04998697,  0.        ],
       [ 0.01788481, -0.35734212,  0.9338023 ]]), 'currentState': array([ 1.21263702e+00, -6.06011764e+00,  8.08590535e+01, -4.66779491e-02,
        9.32634929e-01,  3.57789406e-01]), 'targetState': array([20.23603764, 42.42054668, 89.        ]), 'previousTarget': array([11.1130095 , 17.69829851, 85.06321805])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6272241756275073
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([20.23603764, 42.42054668, 89.        ]), 'distance': 3.0584752769023233, 'localFrame': array([[ 0.21088347,  0.94159523,  0.26253872],
       [-0.97582578,  0.21854988,  0.        ],
       [-0.0573778 , -0.25619205,  0.96492146]]), 'currentState': array([17.7536315 , 40.68783373, 88.56452715,  0.21088347,  0.94159523,
        0.26253872]), 'targetState': array([20.23603764, 42.42054668, 89.        ]), 'previousTarget': array([20.23603764, 42.42054668, 89.        ])}
episode index:19875
target thresh 86.98142880383847
target distance 55.93206932065486
model initialize at round 19875
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.63326299,  6.4582517 , 22.47063173]), 'distance': 27.500000000000004, 'localFrame': array([[-0.44690644, -0.88777474, -0.11013918],
       [ 0.89320887, -0.44964198,  0.        ],
       [-0.0495232 , -0.09837729,  0.99391617]]), 'currentState': array([18.55255608, 33.73834045, 25.94053724, -0.44690644, -0.88777474,
       -0.11013918]), 'targetState': array([ 18.7139868 , -20.82754662,  19.        ]), 'previousTarget': array([18.95051561,  7.88247857, 23.10641345])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6272233652564018
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 13, 'trapConfig': [], 'currentTarget': array([ 18.7139868 , -20.82754662,  19.        ]), 'distance': 3.988874839300963, 'localFrame': array([[-0.87426421, -0.48543661,  0.0036595 ],
       [ 0.48543986, -0.87427006,  0.        ],
       [ 0.0031994 ,  0.00177647,  0.9999933 ]]), 'currentState': array([ 2.01082969e+01, -1.84247915e+01,  2.18624796e+01, -8.74264209e-01,
       -4.85436609e-01,  3.65950457e-03]), 'targetState': array([ 18.7139868 , -20.82754662,  19.        ]), 'previousTarget': array([ 18.7139868 , -20.82754662,  19.        ])}
episode index:19876
target thresh 86.9827305958674
target distance 64.8802140744097
model initialize at round 19876
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.83240562, 16.94620741, 69.57872375]), 'distance': 27.500000000000007, 'localFrame': array([[-0.2577887 ,  0.94842425, -0.18448963],
       [-0.96498881, -0.26229107,  0.        ],
       [-0.04838998,  0.17803043,  0.98283446]]), 'currentState': array([23.80399954, 20.35833506, 60.2308836 , -0.2577887 ,  0.94842425,
       -0.18448963]), 'targetState': array([-41.38268409,  11.68218547,  84.        ]), 'previousTarget': array([-2.2901844 , 15.88122996, 70.14173048])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6272247958473016
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-41.38268409,  11.68218547,  84.        ]), 'distance': 3.0505480232372584, 'localFrame': array([[-0.46144167,  0.34404621, -0.81774311],
       [-0.59773481, -0.80169389,  0.        ],
       [-0.65557965,  0.48879352,  0.57558336]]), 'currentState': array([-39.81635117,   9.70778176,  85.71877119,  -0.46144167,
         0.34404621,  -0.81774311]), 'targetState': array([-41.38268409,  11.68218547,  84.        ]), 'previousTarget': array([-41.38268409,  11.68218547,  84.        ])}
episode index:19877
target thresh 86.98403225772363
target distance 12.19055824968213
model initialize at round 19877
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.20074261, -15.36642533,  17.        ]), 'distance': 15.407413251849338, 'localFrame': array([[ 0.94342471, -0.32263971,  0.07650772],
       [ 0.32358815,  0.94619803,  0.        ],
       [-0.07239146,  0.02475699,  0.99706899]]), 'currentState': array([-27.58845529,  -4.4366978 ,   7.57121474,   0.94342471,
        -0.32263971,   0.07650772]), 'targetState': array([-22.20074261, -15.36642533,  17.        ]), 'previousTarget': array([-22.20074261, -15.36642533,  17.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6272373874675427
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.20074261, -15.36642533,  17.        ]), 'distance': 1.8921537890227016, 'localFrame': array([[-0.22249394,  0.76318507, -0.60666712],
       [-0.96003438, -0.27988208,  0.        ],
       [-0.16979526,  0.58242129,  0.79495598]]), 'currentState': array([-21.89640806, -15.33023781,  18.86716815,  -0.22249394,
         0.76318507,  -0.60666712]), 'targetState': array([-22.20074261, -15.36642533,  17.        ]), 'previousTarget': array([-22.20074261, -15.36642533,  17.        ])}
episode index:19878
target thresh 86.9853337894202
target distance 55.794305044525935
model initialize at round 19878
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.71046193,   3.40569113,  38.6871939 ]), 'distance': 27.5, 'localFrame': array([[ 0.36165549, -0.65991899, -0.65856832],
       [ 0.87694436,  0.48059192,  0.        ],
       [ 0.31650262, -0.57752777,  0.75252094]]), 'currentState': array([-7.37375596, 23.71151301, 55.71940877,  0.36165549, -0.65991899,
       -0.65856832]), 'targetState': array([-27.06760448, -30.79520722,  10.        ]), 'previousTarget': array([-14.38945337,   4.70988797,  39.90877783])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6272378375957481
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-27.06760448, -30.79520722,  10.        ]), 'distance': 2.494681120200352, 'localFrame': array([[ 0.4699479 , -0.8792697 ,  0.07767737],
       [ 0.88193442,  0.47137213,  0.        ],
       [-0.03661495,  0.06850634,  0.99697855]]), 'currentState': array([-26.52344906, -28.36424388,   9.86678587,   0.4699479 ,
        -0.8792697 ,   0.07767737]), 'targetState': array([-27.06760448, -30.79520722,  10.        ]), 'previousTarget': array([-27.06760448, -30.79520722,  10.        ])}
episode index:19879
target thresh 86.98663519097009
target distance 42.0
model initialize at round 19879
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.88008182, -14.57768355,  26.53979269]), 'distance': 27.5, 'localFrame': array([[ 0.17947758, -0.85252078, -0.49091356],
       [ 0.9785499 ,  0.20600996,  0.        ],
       [ 0.10113308, -0.48038341,  0.87120829]]), 'currentState': array([-3.75147147, -6.67009701, 47.73825057,  0.17947758, -0.85252078,
       -0.49091356]), 'targetState': array([ 27.02594129, -22.23957053,   6.        ]), 'previousTarget': array([ 11.83708025, -13.59906091,  26.90189293])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.627243494664953
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 27.02594129, -22.23957053,   6.        ]), 'distance': 3.1070000698422535, 'localFrame': array([[ 0.43437452,  0.63397603, -0.6398384 ],
       [-0.82494236,  0.56521686,  0.        ],
       [ 0.36164745,  0.5278298 ,  0.76850948]]), 'currentState': array([ 25.86914128, -20.11364889,   7.94826085,   0.43437452,
         0.63397603,  -0.6398384 ]), 'targetState': array([ 27.02594129, -22.23957053,   6.        ]), 'previousTarget': array([ 27.02594129, -22.23957053,   6.        ])}
episode index:19880
target thresh 86.98793646238634
target distance 31.273037624543385
model initialize at round 19880
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.73659723, -33.46547907,  34.29362522]), 'distance': 27.5, 'localFrame': array([[ 0.6029198 , -0.75878913,  0.24642801],
       [ 0.78293395,  0.62210483,  0.        ],
       [-0.15330406,  0.19293686,  0.9691611 ]]), 'currentState': array([ -7.72577412, -14.18941365,  36.722494  ,   0.6029198 ,
        -0.75878913,   0.24642801]), 'targetState': array([ 22.10233407, -43.73198862,  33.        ]), 'previousTarget': array([ 10.75303993, -32.07801823,  34.49060933])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6272506773846924
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 22.10233407, -43.73198862,  33.        ]), 'distance': 3.899551232696678, 'localFrame': array([[ 0.14789023, -0.65651875, -0.73966993],
       [ 0.97555451,  0.21975759,  0.        ],
       [ 0.16254808, -0.72158834,  0.67296983]]), 'currentState': array([ 19.30635991, -41.28947803,  34.19296703,   0.14789023,
        -0.65651875,  -0.73966993]), 'targetState': array([ 22.10233407, -43.73198862,  33.        ]), 'previousTarget': array([ 22.10233407, -43.73198862,  33.        ])}
episode index:19881
target thresh 86.98923760368194
target distance 30.95589593301485
model initialize at round 19881
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.27582625, 27.57332214, 63.37241538]), 'distance': 27.5, 'localFrame': array([[-0.34149897,  0.79748572, -0.49738815],
       [-0.91926211, -0.393646  ,  0.        ],
       [-0.19579486,  0.45723008,  0.86752811]]), 'currentState': array([19.02293074,  7.28180747, 68.80792036, -0.34149897,  0.79748572,
       -0.49738815]), 'targetState': array([-6.47018888, 36.42988685, 61.        ]), 'previousTarget': array([ 2.00486004, 26.2241882 , 63.63748106])}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6272594481276839
{'scaleFactor': 20, 'timeStep': 23, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.47018888, 36.42988685, 61.        ]), 'distance': 2.8470838617481133, 'localFrame': array([[-0.29388038,  0.80333267, -0.51796809],
       [-0.93913108, -0.34355904,  0.        ],
       [-0.17795262,  0.48643993,  0.85539994]]), 'currentState': array([-4.32445277, 36.20054748, 62.85717703, -0.29388038,  0.80333267,
       -0.51796809]), 'targetState': array([-6.47018888, 36.42988685, 61.        ]), 'previousTarget': array([-6.47018888, 36.42988685, 61.        ])}
episode index:19882
target thresh 86.99053861486993
target distance 55.0
model initialize at round 19882
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.60303886,   3.74704131,  32.93821843]), 'distance': 27.5, 'localFrame': array([[-0.13404402,  0.67208635, -0.72823907],
       [-0.98068527, -0.19559242,  0.        ],
       [-0.14243804,  0.71417333,  0.68532318]]), 'currentState': array([ 0.33216668,  1.64825976, 55.25221271, -0.13404402,  0.67208635,
       -0.72823907]), 'targetState': array([-38.41124025,   6.75104601,   1.        ]), 'previousTarget': array([-15.66665603,   2.75352514,  33.5673455 ])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6272562617746348
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-38.41124025,   6.75104601,   1.        ]), 'distance': 3.0554357836456445, 'localFrame': array([[-0.27430531,  0.73078462, -0.62506835],
       [-0.93621917, -0.35141665,  0.        ],
       [-0.21965943,  0.58520097,  0.78057002]]), 'currentState': array([-37.33024486,   5.36088505,   3.49691596,  -0.27430531,
         0.73078462,  -0.62506835]), 'targetState': array([-38.41124025,   6.75104601,   1.        ]), 'previousTarget': array([-38.41124025,   6.75104601,   1.        ])}
episode index:19883
target thresh 86.9918394959633
target distance 60.54419551138052
model initialize at round 19883
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-7.120957  , 12.58864004, 34.98524747]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.50306414, -0.61536484, -0.60683819],
       [ 0.77421386,  0.63292408,  0.        ],
       [ 0.3840825 , -0.46982254,  0.7948254 ]]), 'currentState': array([-9.43973433, 33.64964218, 52.5154346 ,  0.50306414, -0.61536484,
       -0.60683819]), 'targetState': array([ -2.8901598 , -25.83886562,   3.        ]), 'previousTarget': array([-7.25617739, 13.74734007, 36.34583064])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6272483825943799
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([ -2.8901598 , -25.83886562,   3.        ]), 'distance': 4.131300768936576, 'localFrame': array([[-0.97026981, -0.08428711,  0.22687479],
       [ 0.08654383, -0.99624804,  0.        ],
       [ 0.22602357,  0.01963461,  0.97392393]]), 'currentState': array([ -0.73636228, -23.16140102,   0.70653415,  -0.97026981,
        -0.08428711,   0.22687479]), 'targetState': array([ -2.8901598 , -25.83886562,   3.        ]), 'previousTarget': array([ -2.8901598 , -25.83886562,   3.        ])}
episode index:19884
target thresh 86.99314024697507
target distance 67.9350824189612
model initialize at round 19884
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.64471218, 21.82234965, 22.19587166]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.03985853, -0.73325977,  0.67877935],
       [ 0.99852587,  0.05427786,  0.        ],
       [-0.03684269,  0.67777874,  0.73434229]]), 'currentState': array([-1.82583943e+00,  4.50986383e+01,  1.64505581e+01,  3.98585296e-02,
       -7.33259773e-01,  6.78779348e-01]), 'targetState': array([ 36.97623974, -21.94897936,  33.        ]), 'previousTarget': array([11.97745813, 22.6175559 , 21.19170293])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272168387984234
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 73, 'trapConfig': [], 'currentTarget': array([28.4130247 , -8.43989849, 30.62440134]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.79625404, -0.3094457 , -0.51982964],
       [ 0.36223409,  0.93208716,  0.        ],
       [ 0.48452653, -0.18830002,  0.85426995]]), 'currentState': array([13.84969169, 14.53479533, 26.58425588,  0.79625404, -0.3094457 ,
       -0.51982964]), 'targetState': array([ 36.97623974, -21.94897936,  33.        ]), 'previousTarget': array([27.33302725, -7.11727377, 30.58688971])}
episode index:19885
target thresh 86.99444086791826
target distance 38.84586199949051
model initialize at round 19885
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.91761437, 35.77775528, 57.23178112]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.73101605, -0.16359562,  0.66245906],
       [ 0.21839011,  0.97586155,  0.        ],
       [-0.64646832,  0.14467451,  0.74909812]]), 'currentState': array([-4.57257046, 39.11222543, 40.40016609,  0.73101605, -0.16359562,
        0.66245906]), 'targetState': array([33.21975669, 33.24827462, 70.        ]), 'previousTarget': array([15.74528748, 35.65212778, 56.05492277])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.627219621547906
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.21975669, 33.24827462, 70.        ]), 'distance': 3.08953761198067, 'localFrame': array([[ 0.53087173, -0.55221328,  0.64283412],
       [ 0.72089997,  0.69303913,  0.        ],
       [-0.4455092 ,  0.4634191 ,  0.76600542]]), 'currentState': array([32.67951135, 31.25684531, 67.70052464,  0.53087173, -0.55221328,
        0.64283412]), 'targetState': array([33.21975669, 33.24827462, 70.        ]), 'previousTarget': array([33.21975669, 33.24827462, 70.        ])}
episode index:19886
target thresh 86.99574135880583
target distance 24.98081966495415
model initialize at round 19886
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.29644785, -17.88129765,  27.3255131 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.5922242 ,  0.33812971,  0.73139511],
       [-0.49582484, -0.86842255,  0.        ],
       [ 0.63516001, -0.36264386,  0.68195396]]), 'currentState': array([-3.01659972, -1.84161762, 40.16396382, -0.5922242 ,  0.33812971,
        0.73139511]), 'targetState': array([-27.45526906, -23.28536452,  23.        ]), 'previousTarget': array([-21.63282149, -18.59059818,  26.72922756])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6272260327545062
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-27.45526906, -23.28536452,  23.        ]), 'distance': 3.297731780065953, 'localFrame': array([[-0.28329631, -0.9540115 ,  0.09800647],
       [ 0.95862654, -0.28466676,  0.        ],
       [ 0.02789918,  0.0939516 ,  0.99518578]]), 'currentState': array([-26.71140799, -22.12209907,  25.99474858,  -0.28329631,
        -0.9540115 ,   0.09800647]), 'targetState': array([-27.45526906, -23.28536452,  23.        ]), 'previousTarget': array([-27.45526906, -23.28536452,  23.        ])}
episode index:19887
target thresh 86.99704171965082
target distance 84.0
model initialize at round 19887
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.1333015 , -0.7819487 , 66.73873187]), 'distance': 27.5, 'localFrame': array([[-0.49827574,  0.73154755, -0.46535951],
       [-0.82649369, -0.56294598,  0.        ],
       [-0.26197226,  0.3846167 ,  0.88512176]]), 'currentState': array([-8.79267946, -7.43662914, 92.5770351 , -0.49827574,  0.73154755,
       -0.46535951]), 'targetState': array([12.74786292, 14.08871857,  9.        ]), 'previousTarget': array([-1.06590221, -1.3946643 , 67.10197328])}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6272191272959508
{'scaleFactor': 20, 'timeStep': 72, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([12.74786292, 14.08871857,  9.        ]), 'distance': 3.4063446740478422, 'localFrame': array([[-0.18425534,  0.9245786 , -0.33347322],
       [-0.98071515, -0.19544256,  0.        ],
       [-0.06517486,  0.32704223,  0.94275957]]), 'currentState': array([10.37187079, 11.67723843,  8.62236398, -0.18425534,  0.9245786 ,
       -0.33347322]), 'targetState': array([12.74786292, 14.08871857,  9.        ]), 'previousTarget': array([12.74786292, 14.08871857,  9.        ])}
episode index:19888
target thresh 86.99834195046623
target distance 7.0
model initialize at round 19888
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.92527537, -0.54158542, 90.        ]), 'distance': 7.25662487916667, 'localFrame': array([[ 0.48283036, -0.37604074,  0.79086548],
       [ 0.61445566,  0.78895135,  0.        ],
       [-0.62395439,  0.48595177,  0.61199003]]), 'currentState': array([-1.7052244 , -4.83114145, 84.15107777,  0.48283036, -0.37604074,
        0.79086548]), 'targetState': array([-1.92527537, -0.54158542, 90.        ]), 'previousTarget': array([-1.92527537, -0.54158542, 90.        ])}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6272363770255855
{'scaleFactor': 20, 'timeStep': 4, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.92527537, -0.54158542, 90.        ]), 'distance': 2.535145871517959, 'localFrame': array([[ 0.05444874,  0.80402098,  0.59210269],
       [-0.99771482,  0.06756579,  0.        ],
       [-0.04000589, -0.59074963,  0.80586252]]), 'currentState': array([-2.86131138e+00, -2.20594678e+00,  8.83324562e+01,  5.44487399e-02,
        8.04020979e-01,  5.92102694e-01]), 'targetState': array([-1.92527537, -0.54158542, 90.        ]), 'previousTarget': array([-1.92527537, -0.54158542, 90.        ])}
episode index:19889
target thresh 86.99964205126507
target distance 28.0
model initialize at round 19889
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-22.0440774 ,  12.20789402,  78.44133398]), 'distance': 27.5, 'localFrame': array([[-0.89709752, -0.2666215 ,  0.35231947],
       [ 0.28488862, -0.95856063,  0.        ],
       [ 0.33771958,  0.10037181,  0.93587979]]), 'currentState': array([-11.73821941,   5.20934465,  53.92481715,  -0.89709752,
        -0.2666215 ,   0.35231947]), 'targetState': array([-23.54001204,  13.22376018,  82.        ]), 'previousTarget': array([-21.75016378,  12.28927612,  78.23497523])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6272048417627889
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 22, 'trapConfig': [], 'currentTarget': array([-23.54001204,  13.22376018,  82.        ]), 'distance': 21.822056829232967, 'localFrame': array([[-0.31623271, -0.58277962,  0.74857517],
       [ 0.87893776, -0.47693649,  0.        ],
       [ 0.35702282,  0.65795098,  0.66304994]]), 'currentState': array([-22.83568067,   4.82537713,  61.87108439,  -0.31623271,
        -0.58277962,   0.74857517]), 'targetState': array([-23.54001204,  13.22376018,  82.        ]), 'previousTarget': array([-23.54001204,  13.22376018,  82.        ])}
episode index:19890
target thresh 87.00094202206031
target distance 53.0
model initialize at round 19890
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.65498046,  5.71495006, 72.00136083]), 'distance': 27.5, 'localFrame': array([[-0.37446248, -0.89035429, -0.25893452],
       [ 0.92179213, -0.3876845 ,  0.        ],
       [-0.1003849 , -0.2386838 ,  0.96589488]]), 'currentState': array([12.16849573,  5.25257084, 99.1244946 , -0.37446248, -0.89035429,
       -0.25893452]), 'targetState': array([ 3.32813755,  6.15820595, 46.        ]), 'previousTarget': array([ 8.3132452 ,  6.35187315, 71.99294111])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6272086747954789
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 3.32813755,  6.15820595, 46.        ]), 'distance': 1.8498729570020556, 'localFrame': array([[ 0.08576027, -0.53021277, -0.84351621],
       [ 0.98717017,  0.15967171,  0.        ],
       [ 0.13468568, -0.83269404,  0.53710372]]), 'currentState': array([ 2.96060016,  5.60942981, 47.72794415,  0.08576027, -0.53021277,
       -0.84351621]), 'targetState': array([ 3.32813755,  6.15820595, 46.        ]), 'previousTarget': array([ 3.32813755,  6.15820595, 46.        ])}
episode index:19891
target thresh 87.00224186286498
target distance 57.34386197097003
model initialize at round 19891
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.41168954, -7.32444343, 64.90787937]), 'distance': 27.499999999999996, 'localFrame': array([[-0.92307362,  0.04288004,  0.38222557],
       [-0.0464035 , -0.99892278,  0.        ],
       [ 0.38181383, -0.0177366 ,  0.92406905]]), 'currentState': array([ 3.03136219e+01, -1.01374869e+01,  7.82133074e+01, -9.23073623e-01,
        4.28800393e-02,  3.82225572e-01]), 'targetState': array([-25.75810758,  -3.53834624,  47.        ]), 'previousTarget': array([ 7.32358361, -7.01062558, 64.30700901])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6272066552955363
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-25.75810758,  -3.53834624,  47.        ]), 'distance': 3.776548599528858, 'localFrame': array([[-0.56992169,  0.69659283, -0.4358299 ],
       [-0.77396705, -0.63322588,  0.        ],
       [-0.27597877,  0.33731798,  0.90002905]]), 'currentState': array([-24.88100503,  -6.02579368,  49.70289024,  -0.56992169,
         0.69659283,  -0.4358299 ]), 'targetState': array([-25.75810758,  -3.53834624,  47.        ]), 'previousTarget': array([-25.75810758,  -3.53834624,  47.        ])}
episode index:19892
target thresh 87.00354157369208
target distance 51.69255544124355
model initialize at round 19892
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([21.86744678, 10.58053479, 75.47798526]), 'distance': 27.500000000000004, 'localFrame': array([[-0.13773872, -0.92386306, -0.35707855],
       [ 0.989068  , -0.14746012,  0.        ],
       [-0.05265485, -0.35317497,  0.93407436]]), 'currentState': array([40.41892567, 26.65847116, 87.87123315, -0.13773872, -0.92386306,
       -0.35707855]), 'targetState': array([-11.77989675, -18.5804745 ,  53.        ]), 'previousTarget': array([21.72152423, 11.883284  , 75.68314508])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6272077559828673
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.77989675, -18.5804745 ,  53.        ]), 'distance': 2.7068906398465287, 'localFrame': array([[-0.81212599,  0.17837316, -0.55554874],
       [-0.21452387, -0.97671875,  0.        ],
       [-0.54261487,  0.11917846,  0.83148398]]), 'currentState': array([-10.80457774, -16.8061102 ,  54.7965637 ,  -0.81212599,
         0.17837316,  -0.55554874]), 'targetState': array([-11.77989675, -18.5804745 ,  53.        ]), 'previousTarget': array([-11.77989675, -18.5804745 ,  53.        ])}
episode index:19893
target thresh 87.00484115455458
target distance 49.0
model initialize at round 19893
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-23.59290907,  26.72505263,  64.19310249]), 'distance': 27.5, 'localFrame': array([[-0.83107351,  0.42995929,  0.3527773 ],
       [-0.4595019 , -0.88817679,  0.        ],
       [ 0.31332861, -0.16210184,  0.93570731]]), 'currentState': array([-29.58663675,  11.53019907,  42.06977422,  -0.83107351,
         0.42995929,   0.3527773 ]), 'targetState': array([-16.33029281,  45.13669834,  91.        ]), 'previousTarget': array([-22.56747309,  26.71062383,  64.242869  ])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6272115882910515
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.33029281,  45.13669834,  91.        ]), 'distance': 4.201789582367805, 'localFrame': array([[-0.49998352,  0.7601853 ,  0.4148913 ],
       [-0.83548691, -0.54951035,  0.        ],
       [ 0.22798706, -0.34663625,  0.90987099]]), 'currentState': array([-18.49528554,  43.11744146,  88.01831527,  -0.49998352,
         0.7601853 ,   0.4148913 ]), 'targetState': array([-16.33029281,  45.13669834,  91.        ]), 'previousTarget': array([-16.33029281,  45.13669834,  91.        ])}
episode index:19894
target thresh 87.0061406054655
target distance 73.43225323345743
model initialize at round 19894
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.52546403, 13.41101321, 43.77422433]), 'distance': 27.5, 'localFrame': array([[ 0.85130293, -0.27838204,  0.44473224],
       [ 0.31081094,  0.95047176,  0.        ],
       [-0.42270544,  0.13822765,  0.89566357]]), 'currentState': array([-23.09423018,  19.42478186,  47.16191781,   0.85130293,
        -0.27838204,   0.44473224]), 'targetState': array([48.89795338,  3.16072064, 38.        ]), 'previousTarget': array([ 2.037767  , 14.25390596, 43.74327572])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271800621996572
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 59, 'trapConfig': [], 'currentTarget': array([48.89795338,  3.16072064, 38.        ]), 'distance': 12.732698017498583, 'localFrame': array([[ 0.81677373, -0.57688401,  0.00924734],
       [ 0.57690868,  0.81680865,  0.        ],
       [-0.00755331,  0.00533487,  0.99995724]]), 'currentState': array([ 3.64274674e+01,  3.71708509e+00,  4.05097880e+01,  8.16773730e-01,
       -5.76884009e-01,  9.24734301e-03]), 'targetState': array([48.89795338,  3.16072064, 38.        ]), 'previousTarget': array([48.89795338,  3.16072064, 38.        ])}
episode index:19895
target thresh 87.00743992643783
target distance 43.0
model initialize at round 19895
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([28.41269391, 17.1209707 , 66.95496668]), 'distance': 27.5, 'localFrame': array([[ 0.25941743,  0.80540812, -0.53293561],
       [-0.95184376,  0.30658353,  0.        ],
       [ 0.16338928,  0.50727143,  0.8461558 ]]), 'currentState': array([21.56075054,  8.63673145, 41.70979771,  0.25941743,  0.80540812,
       -0.53293561]), 'targetState': array([33.58182113, 23.52150696, 86.        ]), 'previousTarget': array([28.13516931, 16.69876422, 67.79600148])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6271868556582492
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([33.58182113, 23.52150696, 86.        ]), 'distance': 2.4874519815300262, 'localFrame': array([[ 0.62760988,  0.51474393,  0.58407578],
       [-0.63415607,  0.77320507,  0.        ],
       [-0.45161036, -0.3703952 ,  0.81169913]]), 'currentState': array([32.74837801, 22.97166566, 83.72174091,  0.62760988,  0.51474393,
        0.58407578]), 'targetState': array([33.58182113, 23.52150696, 86.        ]), 'previousTarget': array([33.58182113, 23.52150696, 86.        ])}
episode index:19896
target thresh 87.00873911748455
target distance 28.0
model initialize at round 19896
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.51965865, 14.1783717 , 69.73500542]), 'distance': 27.5, 'localFrame': array([[ 0.70050869,  0.6955242 ,  0.15979252],
       [-0.70457759,  0.70962696,  0.        ],
       [-0.11339308, -0.11258623,  0.98715062]]), 'currentState': array([-10.60113256,   7.87209139,  44.55549669,   0.70050869,
         0.6955242 ,   0.15979252]), 'targetState': array([-0.34207559, 14.99609897, 73.        ]), 'previousTarget': array([-1.83354988, 13.97576684, 69.47364109])}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6271972756665817
{'scaleFactor': 20, 'timeStep': 19, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.34207559, 14.99609897, 73.        ]), 'distance': 1.6652206935067748, 'localFrame': array([[-0.46453215, -0.47663458,  0.746344  ],
       [ 0.71614023, -0.69795642,  0.        ],
       [ 0.52091558,  0.53448696,  0.66556039]]), 'currentState': array([-0.77051011, 14.80332558, 71.40242614, -0.46453215, -0.47663458,
        0.746344  ]), 'targetState': array([-0.34207559, 14.99609897, 73.        ]), 'previousTarget': array([-0.34207559, 14.99609897, 73.        ])}
episode index:19897
target thresh 87.01003817861866
target distance 80.0
model initialize at round 19897
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.91813964, -17.01958506,  32.90247999]), 'distance': 27.499999999999996, 'localFrame': array([[-0.01120214,  0.66564727, -0.74618244],
       [-0.99985842, -0.01682656,  0.        ],
       [-0.01255568,  0.7460768 ,  0.66574152]]), 'currentState': array([ 1.60684713e+01, -2.45615163e+01,  6.96325571e+00, -1.12021392e-02,
        6.65647265e-01, -7.46182438e-01]), 'targetState': array([-2.16825843e-02, -9.99764905e-01,  8.80000000e+01]), 'previousTarget': array([ 10.29554644, -17.82683288,  33.83775393])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271657550476417
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 32, 'trapConfig': [], 'currentTarget': array([-2.16825843e-02, -9.99764905e-01,  8.80000000e+01]), 'distance': 14.014164329681135, 'localFrame': array([[ 0.40758553, -0.43591007,  0.80240666],
       [ 0.73043967,  0.68297722,  0.        ],
       [-0.54802547,  0.58610966,  0.59677764]]), 'currentState': array([ 5.29936805,  2.12821098, 75.41830739,  0.40758553, -0.43591007,
        0.80240666]), 'targetState': array([-2.16825843e-02, -9.99764905e-01,  8.80000000e+01]), 'previousTarget': array([-2.16825843e-02, -9.99764905e-01,  8.80000000e+01])}
episode index:19898
target thresh 87.01133710985314
target distance 56.0
model initialize at round 19898
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.19397754, -14.21736094,  62.89945727]), 'distance': 27.5, 'localFrame': array([[-0.04470312,  0.80414485, -0.59275011],
       [-0.9984584 , -0.05550518,  0.        ],
       [-0.0329007 ,  0.59183632,  0.80538644]]), 'currentState': array([ 9.10922853e+00, -2.45829638e+01,  8.66959194e+01, -4.47031176e-02,
        8.04144851e-01, -5.92750107e-01]), 'targetState': array([29.99042913, -0.75773399, 32.        ]), 'previousTarget': array([ 17.53605209, -15.12161112,  64.30038538])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6271675197738057
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.99042913, -0.75773399, 32.        ]), 'distance': 1.4966393740495283, 'localFrame': array([[ 0.13559589, -0.403745  , -0.90476722],
       [ 0.94796648,  0.31837016,  0.        ],
       [ 0.28805089, -0.857689  ,  0.42590641]]), 'currentState': array([29.46923808, -1.41653623, 33.23865609,  0.13559589, -0.403745  ,
       -0.90476722]), 'targetState': array([29.99042913, -0.75773399, 32.        ]), 'previousTarget': array([29.99042913, -0.75773399, 32.        ])}
episode index:19899
target thresh 87.012635911201
target distance 71.0
model initialize at round 19899
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.43047619,   4.6506672 ,  44.66945394]), 'distance': 27.5, 'localFrame': array([[-0.59467068,  0.38970908, -0.7032024 ],
       [-0.54812196, -0.83639842,  0.        ],
       [-0.58815738,  0.38544068,  0.71098972]]), 'currentState': array([-6.98198608, 11.50249358, 70.92804132, -0.59467068,  0.38970908,
       -0.7032024 ]), 'targetState': array([-18.82855633,  -6.74429141,   1.        ]), 'previousTarget': array([-11.03853546,   3.86074552,  45.62294362])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.627164340762282
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-18.82855633,  -6.74429141,   1.        ]), 'distance': 2.162702217955592, 'localFrame': array([[-0.47875156, -0.39980598, -0.78163426],
       [ 0.64098485, -0.76755353,  0.        ],
       [-0.59994614, -0.50101572,  0.62373702]]), 'currentState': array([-19.04103097,  -5.18247093,   2.48082821,  -0.47875156,
        -0.39980598,  -0.78163426]), 'targetState': array([-18.82855633,  -6.74429141,   1.        ]), 'previousTarget': array([-18.82855633,  -6.74429141,   1.        ])}
episode index:19900
target thresh 87.01393458267523
target distance 11.366887811610052
model initialize at round 19900
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-21.58893418,  -7.93208176,   0.        ]), 'distance': 18.153404714432742, 'localFrame': array([[-0.81268723, -0.42445243, -0.39922375],
       [ 0.46294465, -0.88638719,  0.        ],
       [-0.35386682, -0.1848185 ,  0.91685353]]), 'currentState': array([-34.75070641,  -2.16234498,  11.0916181 ,  -0.81268723,
        -0.42445243,  -0.39922375]), 'targetState': array([-21.58893418,  -7.93208176,   0.        ]), 'previousTarget': array([-21.58893418,  -7.93208176,   0.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6271778161611813
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-21.58893418,  -7.93208176,   0.        ]), 'distance': 1.9853995745289874, 'localFrame': array([[ 0.60397672, -0.77454102,  0.18787853],
       [ 0.7885839 ,  0.61492717,  0.        ],
       [-0.11553161,  0.14815798,  0.98219227]]), 'currentState': array([-23.45474086,  -7.2960233 ,   0.23665699,   0.60397672,
        -0.77454102,   0.18787853]), 'targetState': array([-21.58893418,  -7.93208176,   0.        ]), 'previousTarget': array([-21.58893418,  -7.93208176,   0.        ])}
episode index:19901
target thresh 87.0152331242888
target distance 45.918509978293656
model initialize at round 19901
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.10964953,  0.26184481, 72.03696898]), 'distance': 27.5, 'localFrame': array([[-0.50390654, -0.51235699, -0.69539091],
       [ 0.71296188, -0.70120279,  0.        ],
       [-0.48761004, -0.49578721,  0.71863168]]), 'currentState': array([19.83566859,  0.6231327 , 88.60568901, -0.50390654, -0.51235699,
       -0.69539091]), 'targetState': array([-25.99966766,  -0.13145887,  54.        ]), 'previousTarget': array([-1.71110241,  0.890144  , 73.04217601])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271463028551738
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 70, 'trapConfig': [], 'currentTarget': array([-25.99966766,  -0.13145887,  54.        ]), 'distance': 15.249763882507198, 'localFrame': array([[-0.32647637, -0.37426777, -0.86794978],
       [ 0.75358156, -0.65735442,  0.        ],
       [-0.57055062, -0.65407095,  0.49665198]]), 'currentState': array([-18.92732222,  -3.79415986,  67.00468569,  -0.32647637,
        -0.37426777,  -0.86794978]), 'targetState': array([-25.99966766,  -0.13145887,  54.        ]), 'previousTarget': array([-25.99966766,  -0.13145887,  54.        ])}
episode index:19902
target thresh 87.01653153605471
target distance 35.15353690001663
model initialize at round 19902
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([17.2531558 , -7.46535618, 64.87092464]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.74866397, -0.39296147, -0.53393216],
       [ 0.46475313,  0.8854403 ,  0.        ],
       [ 0.47276505, -0.24814664,  0.84552732]]), 'currentState': array([ 4.13455834, 15.34416408, 72.86330767,  0.74866397, -0.39296147,
       -0.53393216]), 'targetState': array([ 23.60684306, -18.51261626,  61.        ]), 'previousTarget': array([16.48805882, -6.08167003, 65.59704244])}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.62715466681944
{'scaleFactor': 20, 'timeStep': 24, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 23.60684306, -18.51261626,  61.        ]), 'distance': 3.15326601724141, 'localFrame': array([[ 0.44662067, -0.70277242, -0.55375167],
       [ 0.84398669,  0.53636411,  0.        ],
       [ 0.29701252, -0.46735904,  0.83268187]]), 'currentState': array([ 21.64469437, -16.04756702,  60.87119263,   0.44662067,
        -0.70277242,  -0.55375167]), 'targetState': array([ 23.60684306, -18.51261626,  61.        ]), 'previousTarget': array([ 23.60684306, -18.51261626,  61.        ])}
episode index:19903
target thresh 87.01782981798591
target distance 65.0
model initialize at round 19903
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.14550101,  -8.56280024,  72.01495508]), 'distance': 27.5, 'localFrame': array([[-0.99771942,  0.04001647,  0.05435654],
       [-0.04007572, -0.99919665,  0.        ],
       [ 0.05431287, -0.00217838,  0.99852159]]), 'currentState': array([-1.42762091e+01, -2.79489833e+01,  9.15000837e+01, -9.97719424e-01,
        4.00164700e-02,  5.43565384e-02]), 'targetState': array([-17.19837061,  37.21849068,  26.        ]), 'previousTarget': array([-14.01665447,  -7.86723808,  71.49166765])}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.6271465642324399
{'scaleFactor': 20, 'timeStep': 77, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([-17.19837061,  37.21849068,  26.        ]), 'distance': 1.8278014141049523, 'localFrame': array([[ 0.48845999, -0.53527441, -0.68912128],
       [ 0.73867019,  0.67406703,  0.        ],
       [ 0.46451393, -0.50903334,  0.72464603]]), 'currentState': array([-16.71354173,  36.73804602,  27.69557421,   0.48845999,
        -0.53527441,  -0.68912128]), 'targetState': array([-17.19837061,  37.21849068,  26.        ]), 'previousTarget': array([-17.19837061,  37.21849068,  26.        ])}
episode index:19904
target thresh 87.01912797009544
target distance 64.77840013606516
model initialize at round 19904
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -7.90863155, -16.0859017 ,  39.1239278 ]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.22422356, -0.97085911, -0.08459547],
       [ 0.97435179,  0.2250302 ,  0.        ],
       [ 0.01903653, -0.08242574,  0.99641538]]), 'currentState': array([-30.93549333, -20.73436467,  53.42097045,   0.22422356,
        -0.97085911,  -0.08459547]), 'targetState': array([34.16664858, -7.59210938, 13.        ]), 'previousTarget': array([ -7.46906192, -14.88920211,  38.70962569])}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.627136438312277
{'scaleFactor': 20, 'timeStep': 86, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([34.16664858, -7.59210938, 13.        ]), 'distance': 2.4651018515872534, 'localFrame': array([[ 0.88164823,  0.43789438,  0.17591163],
       [-0.44483109,  0.89561448,  0.        ],
       [-0.157549  , -0.07825096,  0.98440596]]), 'currentState': array([33.41334761, -8.21724309, 10.73759583,  0.88164823,  0.43789438,
        0.17591163]), 'targetState': array([34.16664858, -7.59210938, 13.        ]), 'previousTarget': array([34.16664858, -7.59210938, 13.        ])}
episode index:19905
target thresh 87.02042599239623
target distance 20.335430238635613
model initialize at round 19905
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.45616159,  -6.85706876,   1.        ]), 'distance': 27.135192409548427, 'localFrame': array([[ 0.20491416, -0.28773377, -0.93553165],
       [ 0.81454938,  0.58009423,  0.        ],
       [ 0.54269651, -0.76203672,  0.35324289]]), 'currentState': array([ 5.3231139 , -9.8701509 , 19.33085553,  0.20491416, -0.28773377,
       -0.93553165]), 'targetState': array([-14.45616159,  -6.85706876,   1.        ]), 'previousTarget': array([-13.65585163,  -6.95309331,   1.78710896])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6271481394032209
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-14.45616159,  -6.85706876,   1.        ]), 'distance': 3.4290201015040114, 'localFrame': array([[-0.64851576,  0.67662466, -0.34872107],
       [-0.72194354, -0.69195197,  0.        ],
       [-0.24129823,  0.25175692,  0.93722656]]), 'currentState': array([-11.55054164,  -5.51219205,   2.22754151,  -0.64851576,
         0.67662466,  -0.34872107]), 'targetState': array([-14.45616159,  -6.85706876,   1.        ]), 'previousTarget': array([-14.45616159,  -6.85706876,   1.        ])}
episode index:19906
target thresh 87.02172388490128
target distance 41.37080650748318
model initialize at round 19906
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  8.26413376, -13.80549521,  87.91762505]), 'distance': 27.5, 'localFrame': array([[ 0.51954514, -0.50938777,  0.68600069],
       [ 0.70009228,  0.71405238,  0.        ],
       [-0.48984042,  0.48026379,  0.72760089]]), 'currentState': array([ 5.36938073, 10.85677009, 99.73469215,  0.51954514, -0.50938777,
        0.68600069]), 'targetState': array([ 10.20366485, -30.32960968,  80.        ]), 'previousTarget': array([  8.03002874, -13.98986179,  87.10925135])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6271549307115518
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.20366485, -30.32960968,  80.        ]), 'distance': 2.2740923195317517, 'localFrame': array([[ 0.05435908, -0.45529945, -0.88867739],
       [ 0.99294808,  0.11854999,  0.        ],
       [ 0.1053527 , -0.88241051,  0.45853298]]), 'currentState': array([ 1.01776944e+01, -2.80571155e+01,  7.99188106e+01,  5.43590824e-02,
       -4.55299447e-01, -8.88677390e-01]), 'targetState': array([ 10.20366485, -30.32960968,  80.        ]), 'previousTarget': array([ 10.20366485, -30.32960968,  80.        ])}
episode index:19907
target thresh 87.02302164762357
target distance 22.597740434587525
model initialize at round 19907
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 17.43899893, -17.11300233,  23.41104094]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.42808601,  0.56959701,  0.70164209],
       [-0.79940122,  0.60079754,  0.        ],
       [-0.42154484, -0.56089355,  0.71252956]]), 'currentState': array([-1.87355048,  0.25740315, 14.38084445,  0.42808601,  0.56959701,
        0.70164209]), 'targetState': array([ 20.83724728, -20.16950981,  25.        ]), 'previousTarget': array([ 17.65863786, -17.46594798,  23.31207403])}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.627164512387606
{'scaleFactor': 20, 'timeStep': 21, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 20.83724728, -20.16950981,  25.        ]), 'distance': 3.1402978206027896, 'localFrame': array([[ 0.78819461, -0.58315823,  0.19666145],
       [ 0.59477329,  0.80389348,  0.        ],
       [-0.15809486,  0.11696898,  0.98047145]]), 'currentState': array([ 18.39692076, -18.28969127,  24.38962377,   0.78819461,
        -0.58315823,   0.19666145]), 'targetState': array([ 20.83724728, -20.16950981,  25.        ]), 'previousTarget': array([ 20.83724728, -20.16950981,  25.        ])}
episode index:19908
target thresh 87.02431928057608
target distance 72.0
model initialize at round 19908
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-11.05619029,  -2.439739  ,  54.21877435]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.75282361,  0.2588147 ,  0.60520374],
       [-0.32511524,  0.9456744 ,  0.        ],
       [-0.57232568, -0.19676096,  0.79607062]]), 'currentState': array([-18.19506558,  -3.85296379,  27.69917301,   0.75282361,
         0.2588147 ,   0.60520374]), 'targetState': array([ 9.98575579e-01, -5.33555327e-02,  9.90000000e+01]), 'previousTarget': array([-11.87692716,  -3.26273592,  53.40454239])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6271652882723665
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.98575579e-01, -5.33555327e-02,  9.90000000e+01]), 'distance': 3.2105496166070386, 'localFrame': array([[-0.66760426, -0.22827819,  0.70865621],
       [ 0.32354462, -0.94621291,  0.        ],
       [ 0.67053965,  0.2292819 ,  0.70555395]]), 'currentState': array([ 0.79981971, -2.07573634, 96.51442143, -0.66760426, -0.22827819,
        0.70865621]), 'targetState': array([ 9.98575579e-01, -5.33555327e-02,  9.90000000e+01]), 'previousTarget': array([ 9.98575579e-01, -5.33555327e-02,  9.90000000e+01])}
episode index:19909
target thresh 87.02561678377178
target distance 69.0
model initialize at round 19909
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.77078122, 29.49554429, 65.14587582]), 'distance': 27.5, 'localFrame': array([[-0.92467758, -0.02888611, -0.37965374],
       [ 0.03122389, -0.99951242,  0.        ],
       [-0.37946862, -0.01185427,  0.92512866]]), 'currentState': array([ 1.09222973e+01,  4.40554469e+01,  8.66053490e+01, -9.24677583e-01,
       -2.88861136e-02, -3.79653737e-01]), 'targetState': array([-17.90839369,  -1.81368005,  19.        ]), 'previousTarget': array([ 2.73801748, 29.99550986, 66.50762182])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271337882578877
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 18, 'trapConfig': [], 'currentTarget': array([-17.90839369,  -1.81368005,  19.        ]), 'distance': 12.663377097948873, 'localFrame': array([[-0.21983023,  0.3499286 , -0.91061773],
       [-0.84677269, -0.53195489,  0.        ],
       [-0.48440756,  0.77108622,  0.41324975]]), 'currentState': array([-21.87238258,  -0.88859519,  30.99133561,  -0.21983023,
         0.3499286 ,  -0.91061773]), 'targetState': array([-17.90839369,  -1.81368005,  19.        ]), 'previousTarget': array([-17.90839369,  -1.81368005,  19.        ])}
episode index:19910
target thresh 87.02691415722366
target distance 17.0
model initialize at round 19910
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 13.]), 'distance': 17.737023160455056, 'localFrame': array([[-0.13381306,  0.69164069, -0.70973743],
       [-0.98179385, -0.18994955,  0.        ],
       [-0.13481431,  0.69681584,  0.70446631]]), 'currentState': array([ 4.72992926, -7.00312919, 28.5944202 , -0.13381306,  0.69164069,
       -0.70973743]), 'targetState': array([ 0.,  0., 13.]), 'previousTarget': array([ 0.,  0., 13.])}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6271481714359916
{'scaleFactor': 20, 'timeStep': 10, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 13.]), 'distance': 4.6506798152739135, 'localFrame': array([[ 0.466477  ,  0.65267171, -0.59700824],
       [-0.81356664,  0.58147169,  0.        ],
       [ 0.34714339,  0.48570599,  0.8022351 ]]), 'currentState': array([ 2.94310063, -2.77169703, 15.29884254,  0.466477  ,  0.65267171,
       -0.59700824]), 'targetState': array([ 0.,  0., 13.]), 'previousTarget': array([ 0.,  0., 13.])}
episode index:19911
target thresh 87.02821140094467
target distance 45.62088232599024
model initialize at round 19911
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-24.60625996,  13.45345674,  59.89363193]), 'distance': 27.5, 'localFrame': array([[ 0.6728183 , -0.10473419, -0.73235666],
       [ 0.15381249,  0.98810006,  0.        ],
       [ 0.72364166, -0.1126456 ,  0.68092123]]), 'currentState': array([-32.60502239,  -5.70056358,  77.93229333,   0.6728183 ,
        -0.10473419,  -0.73235666]), 'targetState': array([-13.12442074,  40.94813281,  34.        ]), 'previousTarget': array([-25.278506  ,  13.97000752,  60.61096358])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.627145862187432
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-13.12442074,  40.94813281,  34.        ]), 'distance': 2.087579406965717, 'localFrame': array([[-0.23958244, -0.25621624, -0.93645795],
       [ 0.73041892, -0.68299942,  0.        ],
       [-0.63960023, -0.6840066 ,  0.35077986]]), 'currentState': array([-13.98111528,  40.29732225,  35.78899633,  -0.23958244,
        -0.25621624,  -0.93645795]), 'targetState': array([-13.12442074,  40.94813281,  34.        ]), 'previousTarget': array([-13.12442074,  40.94813281,  34.        ])}
episode index:19912
target thresh 87.02950851494779
target distance 84.0
model initialize at round 19912
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.65875843,  5.30011931, 58.06223746]), 'distance': 27.5, 'localFrame': array([[-0.1640524 ,  0.62235832, -0.76534759],
       [-0.96696972, -0.25489128,  0.        ],
       [-0.19508042,  0.74006794,  0.64361718]]), 'currentState': array([ 7.66351587, -4.56887546, 83.23772842, -0.1640524 ,  0.62235832,
       -0.76534759]), 'targetState': array([-8.68491935, 27.66897497,  1.        ]), 'previousTarget': array([ 2.52916437,  4.6592086 , 59.78647931])}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.6271372981280515
{'scaleFactor': 20, 'timeStep': 79, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-8.68491935, 27.66897497,  1.        ]), 'distance': 3.1207074168470137, 'localFrame': array([[ 0.65603864, -0.32331457, -0.68196846],
       [ 0.44206004,  0.89698546,  0.        ],
       [ 0.6117158 , -0.30147101,  0.73138158]]), 'currentState': array([-9.09801029, 26.54665419,  3.88245845,  0.65603864, -0.32331457,
       -0.68196846]), 'targetState': array([-8.68491935, 27.66897497,  1.        ]), 'previousTarget': array([-8.68491935, 27.66897497,  1.        ])}
episode index:19913
target thresh 87.030805499246
target distance 45.0
model initialize at round 19913
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.97423248, 24.01869142, 51.46168729]), 'distance': 27.499999999999996, 'localFrame': array([[-0.75826884,  0.08862096, -0.64589062],
       [-0.11608263, -0.99323956,  0.        ],
       [-0.64152411,  0.07497668,  0.76342996]]), 'currentState': array([33.60569356, 32.77746939, 71.53476157, -0.75826884,  0.08862096,
       -0.64589062]), 'targetState': array([-2.46484963, 13.7813104 , 28.        ]), 'previousTarget': array([17.98740395, 24.57722893, 52.86387907])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6271384011375042
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-2.46484963, 13.7813104 , 28.        ]), 'distance': 2.9724328713351813, 'localFrame': array([[-0.37367097, -0.71331221, -0.59292132],
       [ 0.88581559, -0.46403744,  0.        ],
       [-0.27513769, -0.52521895,  0.80526039]]), 'currentState': array([ 0.23501669, 12.80995568, 28.77624033, -0.37367097, -0.71331221,
       -0.59292132]), 'targetState': array([-2.46484963, 13.7813104 , 28.        ]), 'previousTarget': array([-2.46484963, 13.7813104 , 28.        ])}
episode index:19914
target thresh 87.03210235385225
target distance 45.0
model initialize at round 19914
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.98721541, -18.55618393,  20.58243775]), 'distance': 27.500000000000004, 'localFrame': array([[-0.76201131,  0.18463005, -0.62068551],
       [-0.23547961, -0.97187929,  0.        ],
       [-0.6032314 ,  0.14615878,  0.78405962]]), 'currentState': array([ -0.43085683, -32.31617979,  44.35009614,  -0.76201131,
         0.18463005,  -0.62068551]), 'targetState': array([ 2.21524482, -6.64023271,  0.        ]), 'previousTarget': array([  1.75991666, -18.48674935,  21.04120757])}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.627142589680117
{'scaleFactor': 20, 'timeStep': 35, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.21524482, -6.64023271,  0.        ]), 'distance': 2.090665503835118, 'localFrame': array([[ 0.39050967, -0.29260936, -0.8728585 ],
       [ 0.59964203,  0.80026835,  0.        ],
       [ 0.69852104, -0.52340264,  0.4879734 ]]), 'currentState': array([ 2.12074915, -7.79536588,  1.7400058 ,  0.39050967, -0.29260936,
       -0.8728585 ]), 'targetState': array([ 2.21524482, -6.64023271,  0.        ]), 'previousTarget': array([ 2.21524482, -6.64023271,  0.        ])}
episode index:19915
target thresh 87.03339907877955
target distance 22.0
model initialize at round 19915
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.16925444, -3.01850914, 21.13868331]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.30721669,  0.76280465, -0.56898768],
       [-0.92759569,  0.37358566,  0.        ],
       [ 0.21256564,  0.52779051,  0.82234605]]), 'currentState': array([ 15.94870678, -19.15080993,  39.37832336,   0.30721669,
         0.76280465,  -0.56898768]), 'targetState': array([ 0.97016204, -0.24245747, 18.        ]), 'previousTarget': array([ 3.15666445, -3.44643372, 21.51991447])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6271517574486721
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.97016204, -0.24245747, 18.        ]), 'distance': 3.025900882827999, 'localFrame': array([[-0.77695423,  0.11232189, -0.61945615],
       [-0.14307952, -0.9897112 ,  0.        ],
       [-0.61308269,  0.08863149,  0.78503126]]), 'currentState': array([ 3.50632617, -1.31573975, 16.74620067, -0.77695423,  0.11232189,
       -0.61945615]), 'targetState': array([ 0.97016204, -0.24245747, 18.        ]), 'previousTarget': array([ 0.97016204, -0.24245747, 18.        ])}
episode index:19916
target thresh 87.03469567404083
target distance 58.80188966523479
model initialize at round 19916
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.08421053, -7.09348944, 65.17069401]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.61605472,  0.44063231, -0.65293166],
       [-0.5817567 ,  0.81336286,  0.        ],
       [ 0.53107036,  0.37984736,  0.75741683]]), 'currentState': array([-16.62514122,  -3.17625381,  78.54125513,   0.61605472,
         0.44063231,  -0.65293166]), 'targetState': array([ 41.07864977, -12.71001705,  46.        ]), 'previousTarget': array([ 5.85093827, -6.9792921 , 66.36911054])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6271541964350462
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 41.07864977, -12.71001705,  46.        ]), 'distance': 2.6045188546651197, 'localFrame': array([[ 0.72596838,  0.38459425, -0.57013786],
       [-0.46813304,  0.883658  ,  0.        ],
       [ 0.50380688,  0.26690037,  0.82154904]]), 'currentState': array([ 38.83624365, -13.6245276 ,  45.04145751,   0.72596838,
         0.38459425,  -0.57013786]), 'targetState': array([ 41.07864977, -12.71001705,  46.        ]), 'previousTarget': array([ 41.07864977, -12.71001705,  46.        ])}
episode index:19917
target thresh 87.03599213964907
target distance 64.43896011725539
model initialize at round 19917
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 9.36771946, 15.71555854, 55.45383666]), 'distance': 27.499999999999996, 'localFrame': array([[-0.08533316, -0.71777382, -0.69102749],
       [ 0.99300712, -0.1180545 ,  0.        ],
       [-0.08157891, -0.68619522,  0.72282847]]), 'currentState': array([25.57498774, 37.93075557, 55.69771315, -0.08533316, -0.71777382,
       -0.69102749]), 'targetState': array([-20.79284523, -25.62533097,  55.        ]), 'previousTarget': array([10.23787373, 16.65138094, 56.31214755])}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6271490976445597
{'scaleFactor': 20, 'timeStep': 65, 'trapCount': 12, 'trapConfig': [], 'currentTarget': array([-20.79284523, -25.62533097,  55.        ]), 'distance': 2.845074959418616, 'localFrame': array([[-0.75851215, -0.48096858,  0.43969143],
       [ 0.53551096, -0.84452828,  0.        ],
       [ 0.37133184,  0.23545958,  0.8981489 ]]), 'currentState': array([-19.10773315, -23.42017536,  55.62620886,  -0.75851215,
        -0.48096858,   0.43969143]), 'targetState': array([-20.79284523, -25.62533097,  55.        ]), 'previousTarget': array([-20.79284523, -25.62533097,  55.        ])}
episode index:19918
target thresh 87.03728847561723
target distance 82.32123540111792
model initialize at round 19918
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([12.30098617, -8.06817631, 45.02592094]), 'distance': 27.5, 'localFrame': array([[ 0.06987113, -0.37165024, -0.92573977],
       [ 0.98278264,  0.18476549,  0.        ],
       [ 0.17104476, -0.90980098,  0.37816117]]), 'currentState': array([ 38.38901871, -15.66392364,  49.26492796,   0.06987113,
        -0.37165024,  -0.92573977]), 'targetState': array([-43.24705098,   8.105096  ,  36.        ]), 'previousTarget': array([13.02788287, -7.96409118, 46.25402502])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6271467891610251
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-43.24705098,   8.105096  ,  36.        ]), 'distance': 2.752130455836177, 'localFrame': array([[-0.55012731,  0.75027879,  0.36666289],
       [-0.80644453, -0.59130975,  0.        ],
       [ 0.21681134, -0.29569328,  0.93035387]]), 'currentState': array([-40.93445365,   6.63987548,  36.28150388,  -0.55012731,
         0.75027879,   0.36666289]), 'targetState': array([-43.24705098,   8.105096  ,  36.        ]), 'previousTarget': array([-43.24705098,   8.105096  ,  36.        ])}
episode index:19919
target thresh 87.03858468195827
target distance 18.0
model initialize at round 19919
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.273685  , -15.42720864,  74.        ]), 'distance': 24.9048524787388, 'localFrame': array([[-0.54374294,  0.70790771,  0.45078853],
       [-0.79305769, -0.60914653,  0.        ],
       [ 0.27459627, -0.35750131,  0.89263078]]), 'currentState': array([ 4.93123054,  1.27482564, 56.04347425, -0.54374294,  0.70790771,
        0.45078853]), 'targetState': array([  9.273685  , -15.42720864,  74.        ]), 'previousTarget': array([  9.273685  , -15.42720864,  74.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6271584815086898
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  9.273685  , -15.42720864,  74.        ]), 'distance': 2.928521833549388, 'localFrame': array([[ 0.43005005,  0.04223697,  0.90181649],
       [-0.09774379,  0.99521161,  0.        ],
       [-0.89749825, -0.08814696,  0.43211921]]), 'currentState': array([ 1.11138477e+01, -1.66223995e+01,  7.20605258e+01,  4.30050054e-01,
        4.22369704e-02,  9.01816494e-01]), 'targetState': array([  9.273685  , -15.42720864,  74.        ]), 'previousTarget': array([  9.273685  , -15.42720864,  74.        ])}
episode index:19920
target thresh 87.03988075868516
target distance 63.0
model initialize at round 19920
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.45045065, -25.16667103,  56.83986246]), 'distance': 27.499999999999996, 'localFrame': array([[-0.91537329, -0.10495336, -0.38868565],
       [ 0.11391005, -0.99349107,  0.        ],
       [-0.38615572, -0.0442752 ,  0.92137043]]), 'currentState': array([ 31.71881885, -34.68896882,  77.63536143,  -0.91537329,
        -0.10495336,  -0.38868565]), 'targetState': array([-13.5347908,  -6.4660218,  16.       ]), 'previousTarget': array([ 17.70172664, -25.43181988,  58.20929646])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6271576762658818
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.5347908,  -6.4660218,  16.       ]), 'distance': 3.8244862449816073, 'localFrame': array([[-0.91030924,  0.29366416,  0.29171639],
       [-0.30701795, -0.95170372,  0.        ],
       [ 0.27762757, -0.08956217,  0.95650486]]), 'currentState': array([-11.2487041 ,  -9.37920529,  15.04403737,  -0.91030924,
         0.29366416,   0.29171639]), 'targetState': array([-13.5347908,  -6.4660218,  16.       ]), 'previousTarget': array([-13.5347908,  -6.4660218,  16.       ])}
episode index:19921
target thresh 87.04117670581084
target distance 35.0
model initialize at round 19921
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -8.12219189, -11.30353084,  57.74776997]), 'distance': 27.5, 'localFrame': array([[ 0.32068489, -0.92696107,  0.19469046],
       [ 0.94504475,  0.32694099,  0.        ],
       [-0.06365229,  0.1839912 ,  0.98086473]]), 'currentState': array([-10.56537858,   7.16606246,  37.52019072,   0.32068489,
        -0.92696107,   0.19469046]), 'targetState': array([ -6.27995239, -25.23018426,  73.        ]), 'previousTarget': array([ -8.26303445, -10.200615  ,  57.59855533])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6271640793183333
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -6.27995239, -25.23018426,  73.        ]), 'distance': 1.8641769396510863, 'localFrame': array([[-0.42948721, -0.28529375,  0.85682449],
       [ 0.55331503, -0.83297207,  0.        ],
       [ 0.71371088,  0.47409387,  0.51560817]]), 'currentState': array([ -5.62526383, -24.38220834,  71.47439344,  -0.42948721,
        -0.28529375,   0.85682449]), 'targetState': array([ -6.27995239, -25.23018426,  73.        ]), 'previousTarget': array([ -6.27995239, -25.23018426,  73.        ])}
episode index:19922
target thresh 87.04247252334831
target distance 28.0
model initialize at round 19922
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.43501227, -7.7364391 , 52.28897581]), 'distance': 27.5, 'localFrame': array([[-0.71737185, -0.07129517,  0.69303292],
       [ 0.09889664, -0.99509771,  0.        ],
       [ 0.68963547,  0.06853862,  0.72090594]]), 'currentState': array([ 7.54391469,  2.65247882, 28.109292  , -0.71737185, -0.07129517,
        0.69303292]), 'targetState': array([-1.32960892, -8.90124374, 55.        ]), 'previousTarget': array([ 5.42910504e-03, -7.47507927e+00,  5.13076121e+01])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6271749099720534
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.32960892, -8.90124374, 55.        ]), 'distance': 3.1374400555692405, 'localFrame': array([[-0.96421895,  0.04138229, -0.26185744],
       [-0.04287847, -0.9990803 ,  0.        ],
       [-0.26161661,  0.01122805,  0.96510656]]), 'currentState': array([-2.47067777e+00, -8.35152608e+00,  5.21295815e+01, -9.64218952e-01,
        4.13822913e-02, -2.61857440e-01]), 'targetState': array([-1.32960892, -8.90124374, 55.        ]), 'previousTarget': array([-1.32960892, -8.90124374, 55.        ])}
episode index:19923
target thresh 87.04376821131049
target distance 30.683315243609016
model initialize at round 19923
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([36.50914142,  2.15967984, 55.35002486]), 'distance': 27.5, 'localFrame': array([[-0.64596383, -0.7610075 ,  0.059986  ],
       [ 0.76238038, -0.64712916,  0.        ],
       [ 0.03881869,  0.04573215,  0.99819922]]), 'currentState': array([ 2.61968228e+01,  2.22998346e+01,  7.09794923e+01, -6.45963826e-01,
       -7.61007501e-01,  5.99859979e-02]), 'targetState': array([41.35868591, -7.311573  , 48.        ]), 'previousTarget': array([36.92898658,  2.41288468, 54.97245611])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271434316087744
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 82, 'trapConfig': [], 'currentTarget': array([41.35868591, -7.311573  , 48.        ]), 'distance': 12.148571360700366, 'localFrame': array([[ 0.91183539, -0.26869136,  0.31042097],
       [ 0.28265474,  0.95922172,  0.        ],
       [-0.29776254,  0.08774196,  0.95059919]]), 'currentState': array([33.0139778 ,  0.61571294, 51.88738607,  0.91183539, -0.26869136,
        0.31042097]), 'targetState': array([41.35868591, -7.311573  , 48.        ]), 'previousTarget': array([41.35868591, -7.311573  , 48.        ])}
episode index:19924
target thresh 87.04506376971037
target distance 58.0
model initialize at round 19924
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.66852551, -14.23722962,  61.79158789]), 'distance': 27.5, 'localFrame': array([[ 0.34373576,  0.70181051, -0.6239453 ],
       [-0.89806683,  0.43985903,  0.        ],
       [ 0.27444797,  0.56034458,  0.78146802]]), 'currentState': array([-42.3993647 , -19.57952738,  83.70620751,   0.34373576,
         0.70181051,  -0.6239453 ]), 'targetState': array([-1.69428987, -5.75581287, 27.        ]), 'previousTarget': array([-27.7854457 , -14.72527896,  63.14127604])}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.627140832427764
{'scaleFactor': 20, 'timeStep': 56, 'trapCount': 6, 'trapConfig': [], 'currentTarget': array([-1.69428987, -5.75581287, 27.        ]), 'distance': 2.989181097377592, 'localFrame': array([[-0.55503192, -0.05257623,  0.83016583],
       [ 0.09430435, -0.99554341,  0.        ],
       [ 0.82646613,  0.07828825,  0.55751654]]), 'currentState': array([-1.73336724, -6.72548454, 24.17273746, -0.55503192, -0.05257623,
        0.83016583]), 'targetState': array([-1.69428987, -5.75581287, 27.        ]), 'previousTarget': array([-1.69428987, -5.75581287, 27.        ])}
episode index:19925
target thresh 87.04635919856088
target distance 31.537753010598266
model initialize at round 19925
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.27595745, 26.06341874, 43.50475602]), 'distance': 27.5, 'localFrame': array([[ 0.61321628,  0.77445709, -0.15550566],
       [-0.78399438,  0.62076792,  0.        ],
       [ 0.09653293,  0.12191557,  0.987835  ]]), 'currentState': array([-16.66315614,   3.16938745,  48.51692572,   0.61321628,
         0.77445709,  -0.15550566]), 'targetState': array([ 2.04337432, 32.93667593, 42.        ]), 'previousTarget': array([-3.00876603, 24.54441252, 43.86271495])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6271487887315068
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 2.04337432, 32.93667593, 42.        ]), 'distance': 1.97569170578227, 'localFrame': array([[ 0.08912567, -0.46463076, -0.88100787],
       [ 0.98209508,  0.1883859 ,  0.        ],
       [ 0.16596946, -0.8652335 ,  0.4731016 ]]), 'currentState': array([ 0.44947127, 32.82389992, 43.16194334,  0.08912567, -0.46463076,
       -0.88100787]), 'targetState': array([ 2.04337432, 32.93667593, 42.        ]), 'previousTarget': array([ 2.04337432, 32.93667593, 42.        ])}
episode index:19926
target thresh 87.04765449787497
target distance 71.0
model initialize at round 19926
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([20.23529913,  9.76042188, 51.49167033]), 'distance': 27.5, 'localFrame': array([[ 0.28897863, -0.93278333, -0.21542194],
       [ 0.95521066,  0.29592667,  0.        ],
       [ 0.0637491 , -0.20577333,  0.97652106]]), 'currentState': array([17.731892  , 27.40333926, 72.43708556,  0.28897863, -0.93278333,
       -0.21542194]), 'targetState': array([ 26.27009035, -32.77014424,   1.        ]), 'previousTarget': array([20.03853215, 11.16075119, 51.36536647])}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.6271374240436071
{'scaleFactor': 20, 'timeStep': 92, 'trapCount': 29, 'trapConfig': [], 'currentTarget': array([ 26.27009035, -32.77014424,   1.        ]), 'distance': 2.885199426680498, 'localFrame': array([[-0.01604196, -0.82904381,  0.5589535 ],
       [ 0.99981284, -0.01934633,  0.        ],
       [ 0.0108137 ,  0.55884889,  0.829199  ]]), 'currentState': array([ 2.62625449e+01, -2.99426598e+01,  4.25848129e-01, -1.60419590e-02,
       -8.29043810e-01,  5.58953502e-01]), 'targetState': array([ 26.27009035, -32.77014424,   1.        ]), 'previousTarget': array([ 26.27009035, -32.77014424,   1.        ])}
episode index:19927
target thresh 87.04894966766561
target distance 71.0
model initialize at round 19927
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.21252172, -8.12970124, 71.9340093 ]), 'distance': 27.500000000000004, 'localFrame': array([[-0.68760158,  0.72496255,  0.04041479],
       [-0.72555534, -0.68816382,  0.        ],
       [ 0.027812  , -0.02932317,  0.99918299]]), 'currentState': array([-6.87339084e+00, -1.72143816e+01,  9.78555315e+01, -6.87601584e-01,
        7.24962555e-01,  4.04147926e-02]), 'targetState': array([-10.53385605,   7.61825943,  27.        ]), 'previousTarget': array([-7.76357602, -9.37401199, 72.29434862])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6271369299005369
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.53385605,   7.61825943,  27.        ]), 'distance': 3.5333201013353803, 'localFrame': array([[-0.79080834,  0.35108249, -0.5013614 ],
       [-0.40576408, -0.91397785,  0.        ],
       [-0.45823321,  0.20343445,  0.86523797]]), 'currentState': array([-11.60651293,   5.74285227,  29.7958194 ,  -0.79080834,
         0.35108249,  -0.5013614 ]), 'targetState': array([-10.53385605,   7.61825943,  27.        ]), 'previousTarget': array([-10.53385605,   7.61825943,  27.        ])}
episode index:19928
target thresh 87.05024470794575
target distance 50.0
model initialize at round 19928
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.51527335,  33.98461481,  53.72342684]), 'distance': 27.5, 'localFrame': array([[-0.06918756,  0.45365276,  0.88848875],
       [-0.98856905, -0.1507688 ,  0.        ],
       [ 0.13395638, -0.87833248,  0.45889841]]), 'currentState': array([-10.96263753,  34.6234337 ,  26.23448769,  -0.06918756,
         0.45365276,   0.88848875]), 'targetState': array([-10.16901129,  33.49016586,  75.        ]), 'previousTarget': array([-10.1183057 ,  33.50546946,  52.49992381])}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.6271266031050257
{'scaleFactor': 20, 'timeStep': 87, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-10.16901129,  33.49016586,  75.        ]), 'distance': 2.2571567413966496, 'localFrame': array([[-0.4797828 , -0.63024354,  0.61041096],
       [ 0.79567678, -0.60572144,  0.        ],
       [ 0.36973901,  0.48568983,  0.79208488]]), 'currentState': array([-10.96851203,  33.27257671,  72.90042621,  -0.4797828 ,
        -0.63024354,   0.61041096]), 'targetState': array([-10.16901129,  33.49016586,  75.        ]), 'previousTarget': array([-10.16901129,  33.49016586,  75.        ])}
episode index:19929
target thresh 87.05153961872834
target distance 44.0
model initialize at round 19929
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-17.26516935,  12.95605896,  36.44687934]), 'distance': 27.499999999999996, 'localFrame': array([[-0.70766413, -0.18309255, -0.6824138 ],
       [ 0.25048024, -0.96812171,  0.        ],
       [-0.66065962, -0.17093117,  0.73096608]]), 'currentState': array([-12.71119907,   1.31859865,  60.94342751,  -0.70766413,
        -0.18309255,  -0.6824138 ]), 'targetState': array([-20.69449089,  21.71953146,  18.        ]), 'previousTarget': array([-16.78263858,  13.29849158,  37.23462896])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6271333876565179
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.69449089,  21.71953146,  18.        ]), 'distance': 3.152822000058282, 'localFrame': array([[-0.9069336 , -0.03283896, -0.41999173],
       [ 0.03618507, -0.99934511,  0.        ],
       [-0.41971668, -0.01519743,  0.90752793]]), 'currentState': array([-18.44888909,  20.20373183,  19.61242383,  -0.9069336 ,
        -0.03283896,  -0.41999173]), 'targetState': array([-20.69449089,  21.71953146,  18.        ]), 'previousTarget': array([-20.69449089,  21.71953146,  18.        ])}
episode index:19930
target thresh 87.05283440002633
target distance 54.0
model initialize at round 19930
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.76542234,   3.04655443,  34.55106871]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83596599, -0.35457775, -0.4188502 ],
       [ 0.39048033, -0.92061127,  0.        ],
       [-0.38559821, -0.16355276,  0.90805535]]), 'currentState': array([-0.90963926, -0.7020994 , 58.5703617 , -0.83596599, -0.35457775,
       -0.4188502 ]), 'targetState': array([-29.04673582,   7.50247546,   6.        ]), 'previousTarget': array([-12.93081343,   3.33989716,  35.96067491])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271019224321108
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 40, 'trapConfig': [], 'currentTarget': array([-29.04673582,   7.50247546,   6.        ]), 'distance': 8.109268511172672, 'localFrame': array([[ 0.45222352,  0.62169984, -0.63951794],
       [-0.8086877 ,  0.58823821,  0.        ],
       [ 0.37618889,  0.51717029,  0.76877618]]), 'currentState': array([-30.02343424,   9.48753698,  13.80165538,   0.45222352,
         0.62169984,  -0.63951794]), 'targetState': array([-29.04673582,   7.50247546,   6.        ]), 'previousTarget': array([-29.04673582,   7.50247546,   6.        ])}
episode index:19931
target thresh 87.05412905185266
target distance 23.0
model initialize at round 19931
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.65920931, 12.95141731, 12.        ]), 'distance': 21.58363926082262, 'localFrame': array([[ 0.14102474, -0.81560965, -0.56115321],
       [ 0.98537861,  0.170379  ,  0.        ],
       [ 0.09560872, -0.55294837,  0.82771195]]), 'currentState': array([33.10843422, 12.22853773, 33.51571578,  0.14102474, -0.81560965,
       -0.56115321]), 'targetState': array([34.65920931, 12.95141731, 12.        ]), 'previousTarget': array([34.65920931, 12.95141731, 12.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6271153800044481
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([34.65920931, 12.95141731, 12.        ]), 'distance': 3.1208063355498648, 'localFrame': array([[ 0.03268672, -0.36288356, -0.93126103],
       [ 0.99596777,  0.08971174,  0.        ],
       [ 0.08354505, -0.92750597,  0.36435272]]), 'currentState': array([ 3.52580841e+01,  1.19795504e+01,  1.49045233e+01,  3.26867161e-02,
       -3.62883562e-01, -9.31261026e-01]), 'targetState': array([34.65920931, 12.95141731, 12.        ]), 'previousTarget': array([34.65920931, 12.95141731, 12.        ])}
episode index:19932
target thresh 87.05542357422027
target distance 34.419447610133815
model initialize at round 19932
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([15.30062915, 20.33242128, 37.34440343]), 'distance': 27.500000000000004, 'localFrame': array([[-0.86859357,  0.13693598,  0.47622868],
       [-0.15572914, -0.9877998 ,  0.        ],
       [ 0.47041859, -0.07416268,  0.87932147]]), 'currentState': array([11.80221767, -0.14974905, 19.33074706, -0.86859357,  0.13693598,
        0.47622868]), 'targetState': array([17.758458  , 34.72228635, 50.        ]), 'previousTarget': array([15.76873585, 20.34065178, 36.62929424])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6271178188580605
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([17.758458  , 34.72228635, 50.        ]), 'distance': 2.6018538027945515, 'localFrame': array([[-0.46411199,  0.81861082, -0.33834358],
       [-0.86991618, -0.4931996 ,  0.        ],
       [-0.16687092,  0.29433056,  0.94102265]]), 'currentState': array([18.19581272, 32.77417039, 48.33170498, -0.46411199,  0.81861082,
       -0.33834358]), 'targetState': array([17.758458  , 34.72228635, 50.        ]), 'previousTarget': array([17.758458  , 34.72228635, 50.        ])}
episode index:19933
target thresh 87.05671796714212
target distance 67.0
model initialize at round 19933
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.26163636,  7.9216617 , 58.26014734]), 'distance': 27.500000000000004, 'localFrame': array([[-0.03040643,  0.42381907,  0.90523635],
       [-0.9974363 , -0.07155997,  0.        ],
       [ 0.06477868, -0.90291559,  0.42490841]]), 'currentState': array([ 1.09217586e+01,  1.16985116e+01,  3.18474955e+01, -3.04064313e-02,
        4.23819072e-01,  9.05236346e-01]), 'targetState': array([-5.50686995,  2.38209642, 97.        ]), 'previousTarget': array([ 4.3477579 ,  8.08961572, 56.47454432])}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6271182737425396
{'scaleFactor': 20, 'timeStep': 46, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-5.50686995,  2.38209642, 97.        ]), 'distance': 2.3436485242569316, 'localFrame': array([[-0.33947666, -0.68675727,  0.64274415],
       [ 0.89645528, -0.44313421,  0.        ],
       [ 0.28482193,  0.57619139,  0.7660809 ]]), 'currentState': array([-6.30688962,  1.70657364, 94.90325826, -0.33947666, -0.68675727,
        0.64274415]), 'targetState': array([-5.50686995,  2.38209642, 97.        ]), 'previousTarget': array([-5.50686995,  2.38209642, 97.        ])}
episode index:19934
target thresh 87.05801223063115
target distance 40.419858140510314
model initialize at round 19934
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.59857448, 13.8154414 , 78.34437743]), 'distance': 27.5, 'localFrame': array([[ 0.47710415, -0.86664462,  0.14594085],
       [ 0.87602393,  0.48226764,  0.        ],
       [-0.07038255,  0.12784768,  0.98929332]]), 'currentState': array([ 5.20629074, 34.83358089, 95.46911869,  0.47710415, -0.86664462,
        0.14594085]), 'targetState': array([-3.26103543, -3.79020421, 64.        ]), 'previousTarget': array([ 0.70449989, 15.10489395, 78.49159076])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6271207122063127
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 3, 'trapConfig': [], 'currentTarget': array([-3.26103543, -3.79020421, 64.        ]), 'distance': 4.0074745199642665, 'localFrame': array([[ 0.01098666,  0.07007008, -0.99748157],
       [-0.98792973,  0.15490273,  0.        ],
       [ 0.15451262,  0.98544169,  0.07092618]]), 'currentState': array([-2.14630454e+00, -1.34883958e+00,  6.69760655e+01,  1.09866585e-02,
        7.00700786e-02, -9.97481567e-01]), 'targetState': array([-3.26103543, -3.79020421, 64.        ]), 'previousTarget': array([-3.26103543, -3.79020421, 64.        ])}
episode index:19935
target thresh 87.0593063647003
target distance 18.0
model initialize at round 19935
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.93467057, -0.50700076, 22.        ]), 'distance': 18.877245609222765, 'localFrame': array([[-0.99606144, -0.06438438,  0.06096105],
       [ 0.06450435, -0.99791743,  0.        ],
       [ 0.0608341 ,  0.00393225,  0.99814015]]), 'currentState': array([ 3.20590414,  0.9051817 ,  3.21862338, -0.99606144, -0.06438438,
        0.06096105]), 'targetState': array([ 1.93467057, -0.50700076, 22.        ]), 'previousTarget': array([ 1.93467057, -0.50700076, 22.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6271346197786845
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.93467057, -0.50700076, 22.        ]), 'distance': 2.5646981800140534, 'localFrame': array([[ 0.76399834, -0.07824936,  0.64045575],
       [ 0.10188784,  0.99479589,  0.        ],
       [-0.63712275,  0.06525465,  0.76799507]]), 'currentState': array([ 2.06405404,  0.10241833, 19.51212037,  0.76399834, -0.07824936,
        0.64045575]), 'targetState': array([ 1.93467057, -0.50700076, 22.        ]), 'previousTarget': array([ 1.93467057, -0.50700076, 22.        ])}
episode index:19936
target thresh 87.06060036936253
target distance 10.0
model initialize at round 19936
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.92460443, -4.26622135, 19.        ]), 'distance': 15.230917334409984, 'localFrame': array([[-0.85131055, -0.02785816,  0.52392201],
       [ 0.03270635, -0.999465  ,  0.        ],
       [ 0.52364171,  0.01713557,  0.85176624]]), 'currentState': array([-1.61401526,  2.45086474,  9.20835398, -0.85131055, -0.02785816,
        0.52392201]), 'targetState': array([ 7.92460443, -4.26622135, 19.        ]), 'previousTarget': array([ 7.92460443, -4.26622135, 19.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6271480723359638
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 7.92460443, -4.26622135, 19.        ]), 'distance': 1.7365265684121851, 'localFrame': array([[-0.55180971,  0.44531086,  0.70512714],
       [-0.62801136, -0.77820417,  0.        ],
       [ 0.54873288, -0.44282786,  0.70908089]]), 'currentState': array([ 6.91564814, -4.0614547 , 17.60157147, -0.55180971,  0.44531086,
        0.70512714]), 'targetState': array([ 7.92460443, -4.26622135, 19.        ]), 'previousTarget': array([ 7.92460443, -4.26622135, 19.        ])}
episode index:19937
target thresh 87.06189424463075
target distance 30.006017766946073
model initialize at round 19937
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-20.87941354,  28.52508752,  76.80698055]), 'distance': 27.5, 'localFrame': array([[-0.50291731,  0.4415145 ,  0.74306065],
       [-0.6597409 , -0.75149314,  0.        ],
       [ 0.55840498, -0.4902275 ,  0.66922408]]), 'currentState': array([ 1.06406734, 18.82834554, 63.36425606, -0.50291731,  0.4415145 ,
        0.74306065]), 'targetState': array([-27.72396668,  31.54966991,  81.        ]), 'previousTarget': array([-19.59765092,  28.11368209,  75.85436553])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6271402199219429
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 53, 'trapConfig': [], 'currentTarget': array([-27.72396668,  31.54966991,  81.        ]), 'distance': 2.975983342907663, 'localFrame': array([[-0.78533036,  0.61821419, -0.0326717 ],
       [-0.61854441, -0.78574984,  0.        ],
       [-0.02567178,  0.0202089 ,  0.99946614]]), 'currentState': array([-2.68240300e+01,  2.88204584e+01,  8.17733017e+01, -7.85330358e-01,
        6.18214193e-01, -3.26716995e-02]), 'targetState': array([-27.72396668,  31.54966991,  81.        ]), 'previousTarget': array([-27.72396668,  31.54966991,  81.        ])}
episode index:19938
target thresh 87.0631879905179
target distance 14.94206967094241
model initialize at round 19938
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.7813672 ,  15.72850009,  13.        ]), 'distance': 17.192388641986074, 'localFrame': array([[-0.17876523,  0.67935012, -0.71170669],
       [-0.96707843, -0.25447849,  0.        ],
       [-0.18111405,  0.68827619,  0.70247675]]), 'currentState': array([-25.4441622 ,   2.06307799,  18.81295525,  -0.17876523,
         0.67935012,  -0.71170669]), 'targetState': array([-16.7813672 ,  15.72850009,  13.        ]), 'previousTarget': array([-16.7813672 ,  15.72850009,  13.        ])}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6271541244234268
{'scaleFactor': 20, 'timeStep': 11, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.7813672 ,  15.72850009,  13.        ]), 'distance': 3.3291637288948945, 'localFrame': array([[-0.34297138,  0.89705589,  0.27867788],
       [-0.93405898, -0.35711877,  0.        ],
       [ 0.0995211 , -0.26030157,  0.96038463]]), 'currentState': array([-18.20852379,  13.52178274,  10.9562403 ,  -0.34297138,
         0.89705589,   0.27867788]), 'targetState': array([-16.7813672 ,  15.72850009,  13.        ]), 'previousTarget': array([-16.7813672 ,  15.72850009,  13.        ])}
episode index:19939
target thresh 87.06448160703695
target distance 42.0
model initialize at round 19939
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([29.6155668 ,  9.57949756, 55.77590234]), 'distance': 27.5, 'localFrame': array([[-0.92472656, -0.12419243, -0.35980137],
       [ 0.13310673, -0.99110171,  0.        ],
       [-0.35659976, -0.04789198,  0.93302892]]), 'currentState': array([13.22583382, 20.22609098, 75.12213456, -0.92472656, -0.12419243,
       -0.35980137]), 'targetState': array([48.9108463 , -2.95450747, 33.        ]), 'previousTarget': array([30.93750069,  9.20972477, 55.29471945])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271226723610184
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 72, 'trapConfig': [], 'currentTarget': array([48.9108463 , -2.95450747, 33.        ]), 'distance': 15.605048172338819, 'localFrame': array([[-0.15760086, -0.81285267, -0.56074281],
       [ 0.98171794, -0.1903415 ,  0.        ],
       [-0.10673263, -0.55049127,  0.82799004]]), 'currentState': array([36.64076862,  5.37528463, 37.85564474, -0.15760086, -0.81285267,
       -0.56074281]), 'targetState': array([48.9108463 , -2.95450747, 33.        ]), 'previousTarget': array([48.9108463 , -2.95450747, 33.        ])}
episode index:19940
target thresh 87.06577509420082
target distance 81.0
model initialize at round 19940
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-41.95072042,  13.95118915,  64.12084101]), 'distance': 27.5, 'localFrame': array([[ 0.04710702, -0.77268182, -0.63304323],
       [ 0.99814676,  0.06085263,  0.        ],
       [ 0.03852235, -0.63187005,  0.77411644]]), 'currentState': array([-4.24938045e+01,  1.27807780e+01,  9.15905552e+01,  4.71070242e-02,
       -7.72681819e-01, -6.33043233e-01]), 'targetState': array([-40.92027658,  16.171919  ,  12.        ]), 'previousTarget': array([-41.49758926,  14.57639625,  65.51381283])}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6271173174565647
{'scaleFactor': 20, 'timeStep': 66, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-40.92027658,  16.171919  ,  12.        ]), 'distance': 3.258708503246269, 'localFrame': array([[ 0.12534511,  0.33163961, -0.93504212],
       [-0.93541704,  0.35354628,  0.        ],
       [ 0.33058066,  0.87465433,  0.35453664]]), 'currentState': array([-39.82365301,  14.68190775,  14.68262266,   0.12534511,
         0.33163961,  -0.93504212]), 'targetState': array([-40.92027658,  16.171919  ,  12.        ]), 'previousTarget': array([-40.92027658,  16.171919  ,  12.        ])}
episode index:19941
target thresh 87.06706845202244
target distance 31.567970351759612
model initialize at round 19941
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 6.84377202, -1.61035852,  3.90168797]), 'distance': 27.499999999999996, 'localFrame': array([[-0.41840389,  0.36365818,  0.83228055],
       [-0.65600275, -0.7547585 ,  0.        ],
       [ 0.62817082, -0.54597833,  0.55435466]]), 'currentState': array([ 15.75282249, -27.62029806,   3.30013577,  -0.41840389,
         0.36365818,   0.83228055]), 'targetState': array([5.38776071, 2.64046104, 4.        ]), 'previousTarget': array([ 7.22220413, -2.87841324,  3.65034975])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6271272989682676
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([5.38776071, 2.64046104, 4.        ]), 'distance': 2.777379107077132, 'localFrame': array([[-0.982959  , -0.04755657,  0.17756685],
       [ 0.0483245 , -0.99883169,  0.        ],
       [ 0.1773594 ,  0.00858083,  0.98410874]]), 'currentState': array([ 5.09384333,  0.23805611,  5.36231341, -0.982959  , -0.04755657,
        0.17756685]), 'targetState': array([5.38776071, 2.64046104, 4.        ]), 'previousTarget': array([5.38776071, 2.64046104, 4.        ])}
episode index:19942
target thresh 87.06836168051473
target distance 39.31467145557498
model initialize at round 19942
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.19737627, -16.80611359,  46.24278985]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.15561705, -0.95240058,  0.26213827],
       [ 0.98691259,  0.16125613,  0.        ],
       [-0.0422714 ,  0.25870756,  0.96503032]]), 'currentState': array([-32.6261449 ,  -1.18818555,  49.28920767,   0.15561705,
        -0.95240058,   0.26213827]), 'targetState': array([  6.31480878, -28.30411967,  44.        ]), 'previousTarget': array([-10.78291389, -15.95367284,  45.73957681])}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6271336968015041
{'scaleFactor': 20, 'timeStep': 29, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  6.31480878, -28.30411967,  44.        ]), 'distance': 3.389558381186528, 'localFrame': array([[ 0.71627584, -0.48661653, -0.50015325],
       [ 0.56195379,  0.82716863,  0.        ],
       [ 0.41371108, -0.28106302,  0.86593691]]), 'currentState': array([  3.73800489, -26.47461964,  42.77439103,   0.71627584,
        -0.48661653,  -0.50015325]), 'targetState': array([  6.31480878, -28.30411967,  44.        ]), 'previousTarget': array([  6.31480878, -28.30411967,  44.        ])}
episode index:19943
target thresh 87.06965477969064
target distance 41.50402984845918
model initialize at round 19943
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.03347924, -1.56900846, 41.5346572 ]), 'distance': 27.5, 'localFrame': array([[-0.2800963 ,  0.9158781 , -0.28759933],
       [-0.95628015, -0.29245216,  0.        ],
       [-0.08410905,  0.27502553,  0.95775082]]), 'currentState': array([ 13.32499626, -22.56077486,  54.36097206,  -0.2800963 ,
         0.9158781 ,  -0.28759933]), 'targetState': array([-10.02023625,  17.3088089 ,  30.        ]), 'previousTarget': array([ 1.73622186, -2.92720324, 41.70161773])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6271375232153727
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.02023625,  17.3088089 ,  30.        ]), 'distance': 3.438615963177705, 'localFrame': array([[-0.65565338,  0.2897465 , -0.69725577],
       [-0.4042096 , -0.91466639,  0.        ],
       [-0.63775642,  0.28183747,  0.71682243]]), 'currentState': array([-9.59027658, 14.91685943, 32.43265126, -0.65565338,  0.2897465 ,
       -0.69725577]), 'targetState': array([-10.02023625,  17.3088089 ,  30.        ]), 'previousTarget': array([-10.02023625,  17.3088089 ,  30.        ])}
episode index:19944
target thresh 87.0709477495631
target distance 53.0
model initialize at round 19944
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 22.38991794, -16.02237993,  36.23559065]), 'distance': 27.5, 'localFrame': array([[-0.59106915,  0.55908954,  0.58142595],
       [-0.68718044, -0.72648678,  0.        ],
       [ 0.42239827, -0.39954454,  0.81359932]]), 'currentState': array([ 35.89323295, -25.86642949,  14.39514258,  -0.59106915,
         0.55908954,   0.58142595]), 'targetState': array([ 3.36917675, -2.15607236, 67.        ]), 'previousTarget': array([ 23.63301727, -16.66751345,  35.55202675])}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6271406473849718
{'scaleFactor': 20, 'timeStep': 38, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.36917675, -2.15607236, 67.        ]), 'distance': 3.253495528450485, 'localFrame': array([[-0.64960005, -0.31559752,  0.69167766],
       [ 0.43699078, -0.89946599,  0.        ],
       [ 0.62214053,  0.30225676,  0.72220635]]), 'currentState': array([ 6.0442198 , -2.79595902, 65.26220746, -0.64960005, -0.31559752,
        0.69167766]), 'targetState': array([ 3.36917675, -2.15607236, 67.        ]), 'previousTarget': array([ 3.36917675, -2.15607236, 67.        ])}
episode index:19945
target thresh 87.07224059014503
target distance 74.8330734838124
model initialize at round 19945
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.82545303, -4.53554987, 55.46303777]), 'distance': 27.499999999999996, 'localFrame': array([[-1.58576326e-02,  9.99655727e-01, -2.09036759e-02],
       [-9.99874205e-01, -1.58610983e-02,  0.00000000e+00],
       [-3.31555258e-04,  2.09010463e-02,  9.99781494e-01]]), 'currentState': array([ 4.55707634e+00, -3.11586093e+01,  5.28695799e+01, -1.58576326e-02,
        9.99655727e-01, -2.09036759e-02]), 'targetState': array([-12.99096858,  42.03849112,  60.        ]), 'previousTarget': array([-2.27738334, -6.06349765, 55.5004665 ])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6271417484569153
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.99096858,  42.03849112,  60.        ]), 'distance': 3.4653787884364875, 'localFrame': array([[-0.63397354,  0.17393863,  0.75354024],
       [-0.26458498, -0.96436237,  0.        ],
       [ 0.72668586, -0.19937543,  0.65740178]]), 'currentState': array([-11.44309949,  39.21762066,  61.28671726,  -0.63397354,
         0.17393863,   0.75354024]), 'targetState': array([-12.99096858,  42.03849112,  60.        ]), 'previousTarget': array([-12.99096858,  42.03849112,  60.        ])}
episode index:19946
target thresh 87.07353330144937
target distance 54.81068437109507
model initialize at round 19946
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-13.11342735,  12.7122927 ,  98.86590019]), 'distance': 27.5, 'localFrame': array([[-0.31233306,  0.71558139,  0.62481304],
       [-0.91650212, -0.40002984,  0.        ],
       [ 0.24994386, -0.57264247,  0.7807744 ]]), 'currentState': array([-13.53242172, -14.78415686,  98.72552304,  -0.31233306,
         0.71558139,   0.62481304]), 'targetState': array([-12.71316943,  38.97916524,  99.        ]), 'previousTarget': array([-12.46809075,  11.66279893,  98.50162333])}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6271412545677333
{'scaleFactor': 20, 'timeStep': 49, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([-12.71316943,  38.97916524,  99.        ]), 'distance': 3.3499415468206686, 'localFrame': array([[ 0.72022989,  0.54050223,  0.43488648],
       [-0.60023438,  0.79982416,  0.        ],
       [-0.34783272, -0.26103382,  0.90048528]]), 'currentState': array([-14.73739574,  36.36590811,  99.54360222,   0.72022989,
         0.54050223,   0.43488648]), 'targetState': array([-12.71316943,  38.97916524,  99.        ]), 'previousTarget': array([-12.71316943,  38.97916524,  99.        ])}
episode index:19947
target thresh 87.07482588348905
target distance 64.0
model initialize at round 19947
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([11.88391682, -3.98895599, 29.21414122]), 'distance': 27.5, 'localFrame': array([[ 0.29819702, -0.93844565,  0.17435107],
       [ 0.95304288,  0.30283538,  0.        ],
       [-0.05279967,  0.16616405,  0.98468356]]), 'currentState': array([ 15.71605249, -17.94188589,   5.82866531,   0.29819702,
        -0.93844565,   0.17435107]), 'targetState': array([ 5.20041186, 20.34590171, 70.        ]), 'previousTarget': array([11.03964298, -2.94461121, 29.6345901 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271098157641155
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 95, 'trapConfig': [], 'currentTarget': array([11.5711544 , -4.15598878, 33.70339634]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.65152384,  0.45283617,  0.60865104],
       [-0.57072658,  0.82114017,  0.        ],
       [-0.49978782, -0.34737333,  0.79343803]]), 'currentState': array([ 15.53005961, -19.38194737,  11.14797007,   0.65152384,
         0.45283617,   0.60865104]), 'targetState': array([ 5.20041186, 20.34590171, 70.        ]), 'previousTarget': array([11.5711544 , -4.15598878, 33.70339634])}
episode index:19948
target thresh 87.07611833627699
target distance 17.908767637544358
model initialize at round 19948
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.34531794,  8.45148366, 28.        ]), 'distance': 26.106180894873926, 'localFrame': array([[ 0.00640244, -0.45174392, -0.89212468],
       [ 0.99989958,  0.0141713 ,  0.        ],
       [ 0.01264257, -0.89203509,  0.45178929]]), 'currentState': array([ 1.13075377e+01, -9.43718158e+00,  3.71766408e+01,  6.40244374e-03,
       -4.51743924e-01, -8.92124675e-01]), 'targetState': array([-5.34531794,  8.45148366, 28.        ]), 'previousTarget': array([-5.34531794,  8.45148366, 28.        ])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.627111246883711
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-5.34531794,  8.45148366, 28.        ]), 'distance': 3.1685501590118004, 'localFrame': array([[-0.7568273 , -0.05977873, -0.65087552],
       [ 0.07874072, -0.99689513,  0.        ],
       [-0.64885464, -0.05125041,  0.75918447]]), 'currentState': array([-2.9045731 ,  9.93407698, 29.37273151, -0.7568273 , -0.05977873,
       -0.65087552]), 'targetState': array([-5.34531794,  8.45148366, 28.        ]), 'previousTarget': array([-5.34531794,  8.45148366, 28.        ])}
episode index:19949
target thresh 87.0774106598261
target distance 35.32817534648762
model initialize at round 19949
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-8.21129286,  5.83818549, 64.32642928]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.62832434,  0.44731539,  0.63648838],
       [-0.57996025,  0.81464478,  0.        ],
       [-0.51851194, -0.36913796,  0.77128629]]), 'currentState': array([-35.60746002,   5.30176067,  62.        ,   0.62832434,
         0.44731539,   0.63648838]), 'targetState': array([-0.27928468,  5.99349648, 65.        ]), 'previousTarget': array([-8.21129286,  5.83818549, 64.32642928])}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6271075161684367
{'scaleFactor': 20, 'timeStep': 60, 'trapCount': 33, 'trapConfig': [], 'currentTarget': array([-0.27928468,  5.99349648, 65.        ]), 'distance': 3.101609634957862, 'localFrame': array([[ 0.53554699,  0.83544831,  0.12335134],
       [-0.84187767,  0.5396684 ,  0.        ],
       [-0.06656882, -0.10384674,  0.99236306]]), 'currentState': array([-3.07346684,  5.70491944, 63.68498986,  0.53554699,  0.83544831,
        0.12335134]), 'targetState': array([-0.27928468,  5.99349648, 65.        ]), 'previousTarget': array([-0.27928468,  5.99349648, 65.        ])}
episode index:19950
target thresh 87.07870285414934
target distance 59.0
model initialize at round 19950
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 13.23928901, -16.03883465,  49.70000192]), 'distance': 27.5, 'localFrame': array([[ 0.51904485, -0.72618642, -0.45082782],
       [ 0.81355313,  0.58149059,  0.        ],
       [ 0.26215213, -0.36677238,  0.89261093]]), 'currentState': array([ 4.0959309 , -2.51260855, 27.57105685,  0.51904485, -0.72618642,
       -0.45082782]), 'targetState': array([ 29.06429091, -39.44954998,  88.        ]), 'previousTarget': array([ 13.06928907, -15.44497876,  50.74425623])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6270996709038092
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 15, 'trapConfig': [], 'currentTarget': array([ 29.06429091, -39.44954998,  88.        ]), 'distance': 2.741861325185562, 'localFrame': array([[ 0.40298982,  0.56114686,  0.72298922],
       [-0.81224476,  0.58331676,  0.        ],
       [-0.42173173, -0.58724421,  0.69085931]]), 'currentState': array([ 29.11167099, -38.81262381,  85.33356348,   0.40298982,
         0.56114686,   0.72298922]), 'targetState': array([ 29.06429091, -39.44954998,  88.        ]), 'previousTarget': array([ 29.06429091, -39.44954998,  88.        ])}
episode index:19951
target thresh 87.07999491925959
target distance 28.97996130439447
model initialize at round 19951
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-1.62117287, 29.76148232, 30.87743279]), 'distance': 27.500000000000004, 'localFrame': array([[-0.90646256,  0.17373295,  0.38489284],
       [-0.18823427, -0.98212416,  0.        ],
       [ 0.37801255, -0.07245002,  0.92296127]]), 'currentState': array([ 2.83056231,  6.82858666, 45.38654545, -0.90646256,  0.17373295,
        0.38489284]), 'targetState': array([-2.81085996, 35.89009705, 27.        ]), 'previousTarget': array([-1.45638702, 30.15308766, 30.36539993])}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6271088242817823
{'scaleFactor': 20, 'timeStep': 22, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.81085996, 35.89009705, 27.        ]), 'distance': 2.479269900556126, 'localFrame': array([[-0.47900301,  0.86791638,  0.13144303],
       [-0.87551257, -0.48319534,  0.        ],
       [ 0.06351266, -0.11508003,  0.99132373]]), 'currentState': array([-3.71847711, 33.7470228 , 26.14545738, -0.47900301,  0.86791638,
        0.13144303]), 'targetState': array([-2.81085996, 35.89009705, 27.        ]), 'previousTarget': array([-2.81085996, 35.89009705, 27.        ])}
episode index:19952
target thresh 87.08128685516978
target distance 74.0
model initialize at round 19952
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.62866104, -2.60990438, 46.31619786]), 'distance': 27.499999999999996, 'localFrame': array([[-0.70346349, -0.33687656,  0.6258221 ],
       [ 0.43191194, -0.90191578,  0.        ],
       [ 0.56443883,  0.27030004,  0.77996583]]), 'currentState': array([  7.66703771, -10.88207336,  20.57836615,  -0.70346349,
        -0.33687656,   0.6258221 ]), 'targetState': array([-6.51004895, 12.39432381, 93.        ]), 'previousTarget': array([ 3.04755301, -2.24596089, 44.8487435 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.627077394981713
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 56, 'trapConfig': [], 'currentTarget': array([-6.51004895, 12.39432381, 93.        ]), 'distance': 5.201680665977352, 'localFrame': array([[ 0.53721591,  0.67068953,  0.51144367],
       [-0.78049157,  0.6251663 ,  0.        ],
       [-0.31973735, -0.39917748,  0.8593168 ]]), 'currentState': array([-3.53633482,  9.59408401, 89.77926047,  0.53721591,  0.67068953,
        0.51144367]), 'targetState': array([-6.51004895, 12.39432381, 93.        ]), 'previousTarget': array([-6.51004895, 12.39432381, 93.        ])}
episode index:19953
target thresh 87.08257866189287
target distance 23.0
model initialize at round 19953
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.92976234,   1.54374455,  46.        ]), 'distance': 24.859432408472475, 'localFrame': array([[-0.86886078,  0.04842266, -0.49268265],
       [-0.05564486, -0.99845062,  0.        ],
       [-0.4919193 ,  0.02741526,  0.87020906]]), 'currentState': array([-5.91264980e+00,  7.24485921e+00,  6.75432565e+01, -8.68860779e-01,
        4.84226611e-02, -4.92682649e-01]), 'targetState': array([-16.92976234,   1.54374455,  46.        ]), 'previousTarget': array([-16.92976234,   1.54374455,  46.        ])}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6270899460305261
{'scaleFactor': 20, 'timeStep': 14, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-16.92976234,   1.54374455,  46.        ]), 'distance': 2.0516694390569743, 'localFrame': array([[-0.30584052, -0.80113759, -0.51443185],
       [ 0.93423719, -0.35665233,  0.        ],
       [-0.18347332, -0.48060137,  0.85753126]]), 'currentState': array([-17.12398272,   2.46173359,  47.82453338,  -0.30584052,
        -0.80113759,  -0.51443185]), 'targetState': array([-16.92976234,   1.54374455,  46.        ]), 'previousTarget': array([-16.92976234,   1.54374455,  46.        ])}
episode index:19954
target thresh 87.08387033944172
target distance 41.39360519783767
model initialize at round 19954
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.94292084, 11.97984847, 66.31097306]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.99835312, -0.02098631, -0.05339119],
       [ 0.02101629,  0.99977913,  0.        ],
       [ 0.0533794 , -0.00112208,  0.99857367]]), 'currentState': array([-2.52552973e+01,  2.81150916e+00,  7.95151168e+01,  9.98353122e-01,
       -2.09863134e-02, -5.33911911e-02]), 'targetState': array([14.48057267, 19.13930551, 56.        ]), 'previousTarget': array([-4.28437268, 11.44450658, 66.42658017])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6270585208265156
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 75, 'trapConfig': [], 'currentTarget': array([14.48057267, 19.13930551, 56.        ]), 'distance': 12.425786812743645, 'localFrame': array([[ 0.84644658,  0.235352  , -0.47763755],
       [-0.26788473,  0.96345097,  0.        ],
       [ 0.46018036,  0.12795181,  0.87855698]]), 'currentState': array([ 5.86716812, 11.36746454, 60.45060984,  0.84644658,  0.235352  ,
       -0.47763755]), 'targetState': array([14.48057267, 19.13930551, 56.        ]), 'previousTarget': array([14.48057267, 19.13930551, 56.        ])}
episode index:19955
target thresh 87.08516188782926
target distance 18.315901276044663
model initialize at round 19955
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.76400845, -13.30082655,  57.        ]), 'distance': 22.07373490503871, 'localFrame': array([[ 0.81026795,  0.2434413 ,  0.53310616],
       [-0.28773922,  0.9577088 ,  0.        ],
       [-0.51056046, -0.15339555,  0.84604836]]), 'currentState': array([11.36050405,  5.97676335, 49.20176796,  0.81026795,  0.2434413 ,
        0.53310616]), 'targetState': array([ 18.76400845, -13.30082655,  57.        ]), 'previousTarget': array([ 18.76400845, -13.30082655,  57.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.627071515732854
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 18.76400845, -13.30082655,  57.        ]), 'distance': 2.4415606269614063, 'localFrame': array([[ 0.45035747, -0.88200122, -0.13875155],
       [ 0.89061595,  0.45475623,  0.        ],
       [ 0.06309813, -0.12357434,  0.99032722]]), 'currentState': array([ 18.28916249, -11.11797652,  56.01465469,   0.45035747,
        -0.88200122,  -0.13875155]), 'targetState': array([ 18.76400845, -13.30082655,  57.        ]), 'previousTarget': array([ 18.76400845, -13.30082655,  57.        ])}
episode index:19956
target thresh 87.08645330706845
target distance 13.020366051396465
model initialize at round 19956
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.32008935, -0.94738736, 44.        ]), 'distance': 14.399935264292662, 'localFrame': array([[ 0.39966548,  0.91267845, -0.08535545],
       [-0.91602141,  0.40112938,  0.        ],
       [ 0.03423858,  0.07818742,  0.99635056]]), 'currentState': array([ -7.97145475, -12.13415064,  47.66943583,   0.39966548,
         0.91267845,  -0.08535545]), 'targetState': array([ 0.32008935, -0.94738736, 44.        ]), 'previousTarget': array([ 0.32008935, -0.94738736, 44.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6270867982819432
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.32008935, -0.94738736, 44.        ]), 'distance': 3.2044593627475906, 'localFrame': array([[ 0.57156268,  0.37958265, -0.7274841 ],
       [-0.55322704,  0.83303052,  0.        ],
       [ 0.60601646,  0.40246388,  0.68612454]]), 'currentState': array([-1.79341252, -2.08154708, 46.12493562,  0.57156268,  0.37958265,
       -0.7274841 ]), 'targetState': array([ 0.32008935, -0.94738736, 44.        ]), 'previousTarget': array([ 0.32008935, -0.94738736, 44.        ])}
episode index:19957
target thresh 87.08774459717216
target distance 64.0
model initialize at round 19957
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.05578713,  0.51187723, 60.61820529]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.35242054, -0.46382366,  0.81281448],
       [ 0.79623285,  0.60499029,  0.        ],
       [-0.49174487,  0.64718959,  0.58252264]]), 'currentState': array([-12.21590464,   7.83946308,  35.745555  ,   0.35242054,
        -0.46382366,   0.81281448]), 'targetState': array([ 10.71120705, -10.50095441,  98.        ]), 'previousTarget': array([-3.65092598,  0.66900221, 58.92943218])}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6270833500075804
{'scaleFactor': 20, 'timeStep': 59, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 10.71120705, -10.50095441,  98.        ]), 'distance': 3.2680288586503483, 'localFrame': array([[ 0.28628837, -0.90728197,  0.30802335],
       [ 0.95364956,  0.30091944,  0.        ],
       [-0.09269021,  0.29374634,  0.9513788 ]]), 'currentState': array([13.03113564, -9.50442707, 95.9251802 ,  0.28628837, -0.90728197,
        0.30802335]), 'targetState': array([ 10.71120705, -10.50095441,  98.        ]), 'previousTarget': array([ 10.71120705, -10.50095441,  98.        ])}
episode index:19958
target thresh 87.08903575815332
target distance 44.731478084801545
model initialize at round 19958
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 0.89219934, 21.05230206, 11.82657168]), 'distance': 27.5, 'localFrame': array([[ 0.56384759, -0.704819  ,  0.43047192],
       [ 0.78087292,  0.62468991,  0.        ],
       [-0.26891146,  0.33614386,  0.90260397]]), 'currentState': array([-1.92731548, 47.60330828,  5.2429249 ,  0.56384759, -0.704819  ,
        0.43047192]), 'targetState': array([ 2.67951317,  4.22139896, 16.        ]), 'previousTarget': array([ 0.70287478, 22.53518628, 11.0870068 ])}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6270825500621686
{'scaleFactor': 20, 'timeStep': 50, 'trapCount': 19, 'trapConfig': [], 'currentTarget': array([ 2.67951317,  4.22139896, 16.        ]), 'distance': 3.732474425280519, 'localFrame': array([[-0.79210482, -0.00535548,  0.61036159],
       [ 0.00676092, -0.99997714,  0.        ],
       [ 0.61034764,  0.0041266 ,  0.79212292]]), 'currentState': array([ 5.46228301e+00,  5.27938330e+00,  1.37487278e+01, -7.92104820e-01,
       -5.35547823e-03,  6.10361592e-01]), 'targetState': array([ 2.67951317,  4.22139896, 16.        ]), 'previousTarget': array([ 2.67951317,  4.22139896, 16.        ])}
episode index:19959
target thresh 87.09032679002485
target distance 31.501266693949876
model initialize at round 19959
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 15.84234341, -18.35657059,  19.98543891]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.03942293, -0.54889844,  0.83495889],
       [ 0.99743074,  0.07163737,  0.        ],
       [-0.05981426,  0.83281366,  0.55031233]]), 'currentState': array([-1.55121696,  2.56782508, 16.        ,  0.03942293, -0.54889844,
        0.83495889]), 'targetState': array([ 24.63444654, -28.93344161,  22.        ]), 'previousTarget': array([ 15.84234341, -18.35657059,  19.98543891])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6270849872615172
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 24.63444654, -28.93344161,  22.        ]), 'distance': 2.632889727501907, 'localFrame': array([[ 0.77395766,  0.01477714,  0.63306491],
       [-0.01908947,  0.99981778,  0.        ],
       [-0.63294955, -0.01208487,  0.77409871]]), 'currentState': array([ 2.23448508e+01, -2.76360173e+01,  2.19190695e+01,  7.73957656e-01,
        1.47771355e-02,  6.33064912e-01]), 'targetState': array([ 24.63444654, -28.93344161,  22.        ]), 'previousTarget': array([ 24.63444654, -28.93344161,  22.        ])}
episode index:19960
target thresh 87.09161769279962
target distance 42.0
model initialize at round 19960
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 6.61714672,  7.45311263, 36.30796106]), 'distance': 27.499999999999996, 'localFrame': array([[-0.64831746, -0.40610429, -0.64402157],
       [ 0.53085019, -0.84746568,  0.        ],
       [-0.54578618, -0.34187897,  0.76500733]]), 'currentState': array([ 7.81703806, 13.96044064, 63.        , -0.64831746, -0.40610429,
       -0.64402157]), 'targetState': array([ 5.92900559,  3.72114131, 21.        ]), 'previousTarget': array([ 6.61714672,  7.45311263, 36.30796106])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6270910033482399
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 5.92900559,  3.72114131, 21.        ]), 'distance': 2.5887988668441984, 'localFrame': array([[-0.02413622, -0.17629569, -0.9840413 ],
       [ 0.99075788, -0.1356423 ,  0.        ],
       [-0.13347762, -0.97494666,  0.17794023]]), 'currentState': array([ 6.10929751,  4.3660631 , 23.50068996, -0.02413622, -0.17629569,
       -0.9840413 ]), 'targetState': array([ 5.92900559,  3.72114131, 21.        ]), 'previousTarget': array([ 5.92900559,  3.72114131, 21.        ])}
episode index:19961
target thresh 87.09290846649058
target distance 63.0
model initialize at round 19961
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.16526504,   7.99353232,  28.98186182]), 'distance': 27.500000000000004, 'localFrame': array([[-0.45505537,  0.53488782,  0.71191265],
       [-0.76165767, -0.64797963,  0.        ],
       [ 0.46130489, -0.54223373,  0.70226802]]), 'currentState': array([ 1.9466115 ,  1.64254516,  6.24928822, -0.45505537,  0.53488782,
        0.71191265]), 'targetState': array([-36.38685818,  18.89435238,  68.        ]), 'previousTarget': array([-11.89346472,   7.02936211,  27.77628899])}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6270878380435161
{'scaleFactor': 20, 'timeStep': 58, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-36.38685818,  18.89435238,  68.        ]), 'distance': 2.652568812318761, 'localFrame': array([[-0.64739404, -0.30653389,  0.69779505],
       [ 0.42794217, -0.90380612,  0.        ],
       [ 0.63067144,  0.29861593,  0.71629747]]), 'currentState': array([-36.32693761,  17.55281362,  65.71246758,  -0.64739404,
        -0.30653389,   0.69779505]), 'targetState': array([-36.38685818,  18.89435238,  68.        ]), 'previousTarget': array([-36.38685818,  18.89435238,  68.        ])}
episode index:19962
target thresh 87.09419911111063
target distance 68.0
model initialize at round 19962
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.64994159, 18.56564129, 18.75072847]), 'distance': 27.500000000000004, 'localFrame': array([[-0.48347894, -0.71673906,  0.50252685],
       [ 0.82902023, -0.55921861,  0.        ],
       [ 0.28102237,  0.41660493,  0.8645616 ]]), 'currentState': array([27.19874142, 34.90096088,  1.26252225, -0.48347894, -0.71673906,
        0.50252685]), 'targetState': array([-25.28014153, -28.37101415,  69.        ]), 'previousTarget': array([14.908781  , 19.62017575, 18.3070218 ])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6270822301328504
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-25.28014153, -28.37101415,  69.        ]), 'distance': 3.2313737776543103, 'localFrame': array([[ 0.69273263,  0.03712852,  0.72023814],
       [-0.05352037,  0.99856676,  0.        ],
       [-0.71920587, -0.03854741,  0.69372691]]), 'currentState': array([-2.36824281e+01, -2.82509696e+01,  6.61938144e+01,  6.92732627e-01,
        3.71285179e-02,  7.20238142e-01]), 'targetState': array([-25.28014153, -28.37101415,  69.        ]), 'previousTarget': array([-25.28014153, -28.37101415,  69.        ])}
episode index:19963
target thresh 87.09548962667266
target distance 12.567883161269371
model initialize at round 19963
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.67485975, 33.52306619, 44.        ]), 'distance': 14.262146266529122, 'localFrame': array([[-0.02446596,  0.70108147,  0.71266134],
       [-0.99939164, -0.03487623,  0.        ],
       [ 0.02485494, -0.71222779,  0.70150824]]), 'currentState': array([ 2.17869444e+00,  2.21935398e+01,  3.60736214e+01, -2.44659592e-02,
        7.01081468e-01,  7.12661345e-01]), 'targetState': array([ 5.67485975, 33.52306619, 44.        ]), 'previousTarget': array([ 5.67485975, 33.52306619, 44.        ])}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6270975067867159
{'scaleFactor': 20, 'timeStep': 8, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 5.67485975, 33.52306619, 44.        ]), 'distance': 2.97928575733687, 'localFrame': array([[ 0.53219954,  0.82258453,  0.20029563],
       [-0.83959857,  0.54320736,  0.        ],
       [-0.10880206, -0.16816792,  0.97973551]]), 'currentState': array([ 5.8105476 , 30.61651374, 43.35993338,  0.53219954,  0.82258453,
        0.20029563]), 'targetState': array([ 5.67485975, 33.52306619, 44.        ]), 'previousTarget': array([ 5.67485975, 33.52306619, 44.        ])}
episode index:19964
target thresh 87.09678001318959
target distance 17.0
model initialize at round 19964
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([1.77635684e-15, 0.00000000e+00, 6.50000000e+01]), 'distance': 21.40093433039119, 'localFrame': array([[-0.70064244,  0.33833769, -0.62819406],
       [-0.43484966, -0.90050307,  0.        ],
       [-0.56569068,  0.27316997,  0.77805669]]), 'currentState': array([12.05416584, -4.86796427, 82.        , -0.70064244,  0.33833769,
       -0.62819406]), 'targetState': array([-0., -0., 65.]), 'previousTarget': array([1.77635684e-15, 0.00000000e+00, 6.50000000e+01])}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6271042809017954
{'scaleFactor': 20, 'timeStep': 28, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 0.,  0., 65.]), 'distance': 2.773395433512739, 'localFrame': array([[ 0.76696189, -0.09216916, -0.63503882],
       [ 0.11931588,  0.99285634,  0.        ],
       [ 0.63050232, -0.07577022,  0.77248022]]), 'currentState': array([-2.57849923, -0.80837461, 64.375825  ,  0.76696189, -0.09216916,
       -0.63503882]), 'targetState': array([-0., -0., 65.]), 'previousTarget': array([ 0.,  0., 65.])}
episode index:19965
target thresh 87.09807027067433
target distance 42.585321815887255
model initialize at round 19965
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([16.35752171, -4.09974454, 43.41388014]), 'distance': 27.499999999999996, 'localFrame': array([[-0.85511478,  0.11265897,  0.50605006],
       [-0.13061848, -0.99143271,  0.        ],
       [ 0.50171458, -0.06609949,  0.86250411]]), 'currentState': array([ 42.04539693, -13.86845962,  42.43649975,  -0.85511478,
         0.11265897,   0.50605006]), 'targetState': array([ 0.95290145,  1.75840235, 44.        ]), 'previousTarget': array([17.94155828, -4.86588804, 43.20213557])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6271102945156104
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([ 0.95290145,  1.75840235, 44.        ]), 'distance': 1.8389898095094983, 'localFrame': array([[-0.89336197,  0.26742125, -0.36109593],
       [-0.28677   , -0.95799946,  0.        ],
       [-0.34592971,  0.10355148,  0.93252867]]), 'currentState': array([ 0.33479199,  1.45048585, 45.70440947, -0.89336197,  0.26742125,
       -0.36109593]), 'targetState': array([ 0.95290145,  1.75840235, 44.        ]), 'previousTarget': array([ 0.95290145,  1.75840235, 44.        ])}
episode index:19966
target thresh 87.09936039913975
target distance 43.82823785360019
model initialize at round 19966
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-15.38526423, -26.66698972,  29.28298443]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.64419664, -0.53996629, -0.54170757],
       [ 0.64238339,  0.76638344,  0.        ],
       [ 0.41515571, -0.34798394,  0.84056702]]), 'currentState': array([-26.9199547 ,  -4.44941123,  40.66631091,   0.64419664,
        -0.53996629,  -0.54170757]), 'targetState': array([ -4.96554706, -46.73695906,  19.        ]), 'previousTarget': array([-16.22013284, -25.18080264,  29.82031732])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6271024553984205
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 42, 'trapConfig': [], 'currentTarget': array([ -4.96554706, -46.73695906,  19.        ]), 'distance': 2.9742858629903997, 'localFrame': array([[ 0.48942159, -0.82583942,  0.28009956],
       [ 0.86027542,  0.50982958,  0.        ],
       [-0.14280304,  0.24096277,  0.95997095]]), 'currentState': array([ -7.85107397, -46.26777045,  19.54769783,   0.48942159,
        -0.82583942,   0.28009956]), 'targetState': array([ -4.96554706, -46.73695906,  19.        ]), 'previousTarget': array([ -4.96554706, -46.73695906,  19.        ])}
episode index:19967
target thresh 87.1006503985988
target distance 43.24547733830988
model initialize at round 19967
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ -9.51768591, -10.06537806,  72.43722776]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.70701576, -0.65176336,  0.27446903],
       [ 0.67779337,  0.73525243,  0.        ],
       [-0.20180402,  0.18603329,  0.96159594]]), 'currentState': array([-0.46712951, 13.21610824, 60.93462698,  0.70701576, -0.65176336,
        0.27446903]), 'targetState': array([-17.04193292, -29.4206139 ,  82.        ]), 'previousTarget': array([-10.31595743,  -9.8129603 ,  72.47852513])}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6271045522184912
{'scaleFactor': 20, 'timeStep': 41, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([-17.04193292, -29.4206139 ,  82.        ]), 'distance': 2.3941541460896087, 'localFrame': array([[-0.44170765,  0.15147804,  0.88427867],
       [-0.32439219, -0.94592267,  0.        ],
       [ 0.83645924, -0.2868531 ,  0.46695957]]), 'currentState': array([-16.6596958 , -29.42747989,  79.63656569,  -0.44170765,
         0.15147804,   0.88427867]), 'targetState': array([-17.04193292, -29.4206139 ,  82.        ]), 'previousTarget': array([-17.04193292, -29.4206139 ,  82.        ])}
episode index:19968
target thresh 87.10194026906434
target distance 20.557839194357985
model initialize at round 19968
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.29533883,  5.01405445, 60.        ]), 'distance': 22.58043577335996, 'localFrame': array([[-0.7051707 ,  0.11697996, -0.69932108],
       [-0.16365236, -0.98651807,  0.        ],
       [-0.68989288,  0.11444554,  0.71480769]]), 'currentState': array([23.26263181,  3.35112605, 70.41239454, -0.7051707 ,  0.11697996,
       -0.69932108]), 'targetState': array([ 3.29533883,  5.01405445, 60.        ]), 'previousTarget': array([ 3.29533883,  5.01405445, 60.        ])}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6271175363598852
{'scaleFactor': 20, 'timeStep': 13, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 3.29533883,  5.01405445, 60.        ]), 'distance': 2.676327422327189, 'localFrame': array([[-0.1308329 , -0.9657539 ,  0.22405838],
       [ 0.99094804, -0.13424601,  0.        ],
       [ 0.03007894,  0.22203021,  0.97457573]]), 'currentState': array([ 5.67393375,  5.6690918 , 61.03727563, -0.1308329 , -0.9657539 ,
        0.22405838]), 'targetState': array([ 3.29533883,  5.01405445, 60.        ]), 'previousTarget': array([ 3.29533883,  5.01405445, 60.        ])}
episode index:19969
target thresh 87.10323001054928
target distance 11.212655862602901
model initialize at round 19969
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-26.0038956 ,  29.06539891,  84.        ]), 'distance': 14.240488845551473, 'localFrame': array([[-0.49205647,  0.87054936, -0.00492426],
       [-0.87055991, -0.49206243,  0.        ],
       [-0.00242304,  0.00428686,  0.99998788]]), 'currentState': array([-1.53112552e+01,  2.29625702e+01,  7.68435731e+01, -4.92056469e-01,
        8.70549357e-01, -4.92426168e-03]), 'targetState': array([-26.0038956 ,  29.06539891,  84.        ]), 'previousTarget': array([-26.0038956 ,  29.06539891,  84.        ])}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6271275038655174
{'scaleFactor': 20, 'timeStep': 20, 'trapCount': 10, 'trapConfig': [], 'currentTarget': array([-26.0038956 ,  29.06539891,  84.        ]), 'distance': 2.790940661217993, 'localFrame': array([[ 0.31677216,  0.93949162,  0.13042584],
       [-0.94758583,  0.31950132,  0.        ],
       [-0.04167123, -0.12358968,  0.99145807]]), 'currentState': array([-26.69701956,  28.73727401,  81.31648309,   0.31677216,
         0.93949162,   0.13042584]), 'targetState': array([-26.0038956 ,  29.06539891,  84.        ]), 'previousTarget': array([-26.0038956 ,  29.06539891,  84.        ])}
episode index:19970
target thresh 87.10451962306652
target distance 52.0
model initialize at round 19970
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.90383763, 10.52083834, 66.81031777]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.40227464, -0.87692412, -0.26301938],
       [ 0.90892694,  0.41695541,  0.        ],
       [ 0.10966735, -0.2390654 ,  0.96479055]]), 'currentState': array([-11.7598466 ,  20.07376935,  91.3719674 ,   0.40227464,
        -0.87692412,  -0.26301938]), 'targetState': array([ 4.99125326, -0.29561948, 39.        ]), 'previousTarget': array([-4.21093838, 11.51599457, 66.79035188])}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6271320405952121
{'scaleFactor': 20, 'timeStep': 34, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.99125326, -0.29561948, 39.        ]), 'distance': 2.96501700743177, 'localFrame': array([[-0.39275369, -0.53734617, -0.74632676],
       [ 0.80733555, -0.59009263,  0.        ],
       [-0.44040192, -0.60253612,  0.66557973]]), 'currentState': array([ 5.90874879, -1.75397718, 41.41303142, -0.39275369, -0.53734617,
       -0.74632676]), 'targetState': array([ 4.99125326, -0.29561948, 39.        ]), 'previousTarget': array([ 4.99125326, -0.29561948, 39.        ])}
episode index:19971
target thresh 87.10580910662897
target distance 74.0
model initialize at round 19971
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  5.54791082, -13.28390888,  28.6678261 ]), 'distance': 27.499999999999996, 'localFrame': array([[-0.18401351,  0.7934367 ,  0.58017   ],
       [-0.97414509, -0.22592333,  0.        ],
       [ 0.13107394, -0.56516976,  0.81449541]]), 'currentState': array([23.56713543, -9.22479721,  8.2942855 , -0.18401351,  0.7934367 ,
        0.58017   ]), 'targetState': array([-41.62132889, -23.90951655,  82.        ]), 'previousTarget': array([  5.51740838, -14.60173563,  28.46566495])}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6271242023517207
{'scaleFactor': 20, 'timeStep': 76, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-41.62132889, -23.90951655,  82.        ]), 'distance': 2.4820108032162143, 'localFrame': array([[-0.67865496,  0.45777134,  0.57434558],
       [-0.55920365, -0.82903033,  0.        ],
       [ 0.4761499 , -0.32117614,  0.81861294]]), 'currentState': array([-40.88838256, -24.24283562,  79.65222111,  -0.67865496,
         0.45777134,   0.57434558]), 'targetState': array([-41.62132889, -23.90951655,  82.        ]), 'previousTarget': array([-41.62132889, -23.90951655,  82.        ])}
episode index:19972
target thresh 87.10709846124949
target distance 53.0127696172623
model initialize at round 19972
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  0.91899633, -16.60653032,  76.92624497]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.63189122, -0.77449985,  0.02938492],
       [ 0.77483445,  0.6321642 ,  0.        ],
       [-0.01857609,  0.02276845,  0.99956817]]), 'currentState': array([-1.49530199e+00,  3.13673470e+00,  9.59163648e+01,  6.31891217e-01,
       -7.74499849e-01,  2.93849195e-02]), 'targetState': array([  4.85078729, -48.7593054 ,  46.        ]), 'previousTarget': array([  0.20561098, -15.83374225,  76.43328251])}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6271253027585708
{'scaleFactor': 20, 'timeStep': 44, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([  4.85078729, -48.7593054 ,  46.        ]), 'distance': 2.799396157953071, 'localFrame': array([[-0.41361596, -0.90848528, -0.05980238],
       [ 0.91011417, -0.41435757,  0.        ],
       [-0.02477957, -0.054427  ,  0.99821024]]), 'currentState': array([  4.00647049, -46.13705096,  46.49752354,  -0.41361596,
        -0.90848528,  -0.05980238]), 'targetState': array([  4.85078729, -48.7593054 ,  46.        ]), 'previousTarget': array([  4.85078729, -48.7593054 ,  46.        ])}
episode index:19973
target thresh 87.10838768694101
target distance 39.0
model initialize at round 19973
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 1.10343269,  6.35447999, 31.29182799]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.20167285, -0.83935289, -0.50479183],
       [ 0.97232729,  0.23362285,  0.        ],
       [ 0.11793091, -0.49082287,  0.86324111]]), 'currentState': array([ 3.23954685, 17.93965105,  6.44286769,  0.20167285, -0.83935289,
       -0.50479183]), 'targetState': array([-0.24690342, -0.96904009, 47.        ]), 'previousTarget': array([ 0.85840051,  6.39711323, 32.4713341 ])}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6271196960605967
{'scaleFactor': 20, 'timeStep': 67, 'trapCount': 20, 'trapConfig': [], 'currentTarget': array([-0.24690342, -0.96904009, 47.        ]), 'distance': 2.6515009080753043, 'localFrame': array([[ 0.44546652, -0.79559797,  0.41058916],
       [ 0.87253797,  0.48854631,  0.        ],
       [-0.20059182,  0.35825463,  0.91182046]]), 'currentState': array([-2.49539113, -2.35615199, 46.77487626,  0.44546652, -0.79559797,
        0.41058916]), 'targetState': array([-0.24690342, -0.96904009, 47.        ]), 'previousTarget': array([-0.24690342, -0.96904009, 47.        ])}
episode index:19974
target thresh 87.10967678371641
target distance 37.0
model initialize at round 19974
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.31576942,  8.71358133, 22.06061405]), 'distance': 27.5, 'localFrame': array([[ 0.89911921,  0.26252309, -0.35023744],
       [-0.28027547,  0.95991961,  0.        ],
       [ 0.33619979,  0.09816296,  0.93666095]]), 'currentState': array([-2.76514684, 14.91491163, 43.48951124,  0.89911921,  0.26252309,
       -0.35023744]), 'targetState': array([24.61772636,  4.35517495,  7.        ]), 'previousTarget': array([12.0660378 ,  9.09887392, 22.95403114])}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.627125706193176
{'scaleFactor': 20, 'timeStep': 30, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([24.61772636,  4.35517495,  7.        ]), 'distance': 3.00889103935761, 'localFrame': array([[ 0.59184331, -0.69407215, -0.40986015],
       [ 0.76092023,  0.64884544,  0.        ],
       [ 0.26593589, -0.31187088,  0.91214837]]), 'currentState': array([23.04450965,  6.54508561,  8.33518007,  0.59184331, -0.69407215,
       -0.40986015]), 'targetState': array([24.61772636,  4.35517495,  7.        ]), 'previousTarget': array([24.61772636,  4.35517495,  7.        ])}
episode index:19975
target thresh 87.11096575158857
target distance 51.082582569007144
model initialize at round 19975
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.126005  , 11.66242165, 27.65581817]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.89502906, -0.2891614 ,  0.33957129],
       [ 0.30742872,  0.95157111,  0.        ],
       [-0.32312623,  0.10439397,  0.94058032]]), 'currentState': array([-26.33938858,   6.43455739,  34.11940513,   0.89502906,
        -0.2891614 ,   0.33957129]), 'targetState': array([22.81143389, 16.23694811, 22.        ]), 'previousTarget': array([-1.95277301, 11.49759823, 27.81745221])}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6271295268774374
{'scaleFactor': 20, 'timeStep': 36, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([22.81143389, 16.23694811, 22.        ]), 'distance': 3.6809582920578263, 'localFrame': array([[ 0.9116875 ,  0.2714555 ,  0.30844418],
       [-0.28536941,  0.9584176 ,  0.        ],
       [-0.29561833, -0.08802054,  0.95124244]]), 'currentState': array([20.34877153, 13.74166565, 20.87825427,  0.9116875 ,  0.2714555 ,
        0.30844418]), 'targetState': array([22.81143389, 16.23694811, 22.        ]), 'previousTarget': array([22.81143389, 16.23694811, 22.        ])}
episode index:19976
target thresh 87.11225459057039
target distance 21.339297952497404
model initialize at round 19976
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 11.27480423, -19.73133012,  76.47810913]), 'distance': 27.499999999999996, 'localFrame': array([[-0.00241554,  0.68440035, -0.72910241],
       [-0.99999377, -0.00352941,  0.        ],
       [-0.0025733 ,  0.72909787,  0.68440461]]), 'currentState': array([ 3.26835169e+01, -3.08021568e+01,  8.97202320e+01, -2.41554156e-03,
        6.84400350e-01, -7.29102411e-01]), 'targetState': array([ 10.50183898, -19.33161602,  76.        ]), 'previousTarget': array([ 11.54259653, -19.93964727,  76.73157811])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6271403299843356
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 10.50183898, -19.33161602,  76.        ]), 'distance': 2.946616753968718, 'localFrame': array([[-0.82818342,  0.41236514,  0.37956186],
       [-0.44571999, -0.89517244,  0.        ],
       [ 0.33977332, -0.16917831,  0.92516636]]), 'currentState': array([ 11.31958195, -21.86079162,  74.72834064,  -0.82818342,
         0.41236514,   0.37956186]), 'targetState': array([ 10.50183898, -19.33161602,  76.        ]), 'previousTarget': array([ 10.50183898, -19.33161602,  76.        ])}
episode index:19977
target thresh 87.11354330067475
target distance 64.5989770831373
model initialize at round 19977
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([14.56190804, -2.96939087, 46.71270253]), 'distance': 27.500000000000004, 'localFrame': array([[-0.56437973, -0.79915229,  0.20695686],
       [ 0.81683675, -0.5768689 ,  0.        ],
       [ 0.11938697,  0.16904997,  0.97835007]]), 'currentState': array([35.6217772 , 11.76964098, 56.4847469 , -0.56437973, -0.79915229,
        0.20695686]), 'targetState': array([-27.92121219, -32.70177227,  27.        ]), 'previousTarget': array([15.72031331, -1.65328745, 47.26728323])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6271089384371344
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 49, 'trapConfig': [], 'currentTarget': array([-27.92121219, -32.70177227,  27.        ]), 'distance': 12.528078797202335, 'localFrame': array([[-0.55093829, -0.78806544,  0.27462678],
       [ 0.81957745, -0.57296841,  0.        ],
       [ 0.15735247,  0.22507792,  0.9615509 ]]), 'currentState': array([-16.50000643, -30.61470744,  31.70669498,  -0.55093829,
        -0.78806544,   0.27462678]), 'targetState': array([-27.92121219, -32.70177227,  27.        ]), 'previousTarget': array([-27.92121219, -32.70177227,  27.        ])}
episode index:19978
target thresh 87.11483188191454
target distance 31.0
model initialize at round 19978
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-6.67176267, 10.2230692 , 44.66402574]), 'distance': 27.500000000000004, 'localFrame': array([[-0.69836695, -0.47759461,  0.53309192],
       [ 0.56449438, -0.82543691,  0.        ],
       [ 0.44003375,  0.3009274 ,  0.84605733]]), 'currentState': array([-19.66003924,   9.79045929,  20.42835946,  -0.69836695,
        -0.47759461,   0.53309192]), 'targetState': array([-3.81212974, 10.31831706, 50.        ]), 'previousTarget': array([-6.95746542, 10.42180537, 43.64432957])}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6271060600280068
{'scaleFactor': 20, 'timeStep': 57, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-3.81212974, 10.31831706, 50.        ]), 'distance': 1.9221026815072435, 'localFrame': array([[ 0.18659748,  0.72168243,  0.66660022],
       [-0.96816139,  0.25032684,  0.        ],
       [-0.16686792, -0.6453766 ,  0.74541542]]), 'currentState': array([-3.99066828,  8.45049288, 49.58306407,  0.18659748,  0.72168243,
        0.66660022]), 'targetState': array([-3.81212974, 10.31831706, 50.        ]), 'previousTarget': array([-3.81212974, 10.31831706, 50.        ])}
episode index:19979
target thresh 87.11612033430266
target distance 34.55007700848115
model initialize at round 19979
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-19.15921542,  14.24195862,  17.88826955]), 'distance': 27.500000000000004, 'localFrame': array([[-0.85316783,  0.19860158,  0.48235056],
       [-0.22671976, -0.97396004,  0.        ],
       [ 0.46979016, -0.1093584 ,  0.87597828]]), 'currentState': array([ 0.72121038, 10.42951527, 36.50234878, -0.85316783,  0.19860158,
        0.48235056]), 'targetState': array([-32.92429523,  16.88167004,   5.        ]), 'previousTarget': array([-18.89903742,  14.0341174 ,  17.17819962])}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6271074891151213
{'scaleFactor': 20, 'timeStep': 43, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-32.92429523,  16.88167004,   5.        ]), 'distance': 4.594226524791251, 'localFrame': array([[-0.79823532,  0.23278278,  0.55554707],
       [-0.27996026, -0.96001159,  0.        ],
       [ 0.53333163, -0.1555311 ,  0.83148509]]), 'currentState': array([-30.24517148,  14.3795663 ,   7.76924   ,  -0.79823532,
         0.23278278,   0.55554707]), 'targetState': array([-32.92429523,  16.88167004,   5.        ]), 'previousTarget': array([-32.92429523,  16.88167004,   5.        ])}
episode index:19980
target thresh 87.11740865785198
target distance 20.0
model initialize at round 19980
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 14.43913341, -15.05374352,  91.20256143]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.51137903, -0.47524315, -0.71598564],
       [ 0.68075194,  0.73251402,  0.        ],
       [ 0.52446952, -0.48740861,  0.69811501]]), 'currentState': array([-0.46329822, -0.91195171, 72.92204568,  0.51137903, -0.47524315,
       -0.71598564]), 'targetState': array([ 16.71962909, -17.21783968,  94.        ]), 'previousTarget': array([ 14.25291694, -14.70900916,  91.18399519])}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6271146426938555
{'scaleFactor': 20, 'timeStep': 27, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 16.71962909, -17.21783968,  94.        ]), 'distance': 3.9944470253434736, 'localFrame': array([[ 0.22762494, -0.76680248,  0.60016735],
       [ 0.95865353,  0.28457583,  0.        ],
       [-0.17079312,  0.57535255,  0.79987446]]), 'currentState': array([ 14.50818834, -15.40260939,  91.21251442,   0.22762494,
        -0.76680248,   0.60016735]), 'targetState': array([ 16.71962909, -17.21783968,  94.        ]), 'previousTarget': array([ 16.71962909, -17.21783968,  94.        ])}
episode index:19981
target thresh 87.11869685257538
target distance 32.582756326181666
model initialize at round 19981
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 26.82278058, -15.61202663,   2.36261521]), 'distance': 27.499999999999996, 'localFrame': array([[-0.47654492, -0.7224507 , -0.50096898],
       [ 0.83475415, -0.55062283,  0.        ],
       [-0.27584496, -0.41818594,  0.86546524]]), 'currentState': array([20.94739378, 11.14742803,  4.74198833, -0.47654492, -0.7224507 ,
       -0.50096898]), 'targetState': array([ 27.71818643, -19.69015341,   2.        ]), 'previousTarget': array([ 26.61772419, -13.99782277,   2.52411133])}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6271254438424237
{'scaleFactor': 20, 'timeStep': 18, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 27.71818643, -19.69015341,   2.        ]), 'distance': 3.0581822555110834, 'localFrame': array([[-0.39542837, -0.75475453, -0.52343291],
       [ 0.88579257, -0.46408137,  0.        ],
       [-0.24291546, -0.46365298,  0.85206689]]), 'currentState': array([ 27.681158  , -16.70742872,   2.67413726,  -0.39542837,
        -0.75475453,  -0.52343291]), 'targetState': array([ 27.71818643, -19.69015341,   2.        ]), 'previousTarget': array([ 27.71818643, -19.69015341,   2.        ])}
episode index:19982
target thresh 87.11998491848576
target distance 39.0
model initialize at round 19982
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.07378344, -1.69784426, 35.56074884]), 'distance': 27.500000000000004, 'localFrame': array([[-0.56187034,  0.36472861,  0.7424788 ],
       [-0.54447706, -0.83877573,  0.        ],
       [ 0.6227732 , -0.40426267,  0.66986957]]), 'currentState': array([ 1.58920505, -9.79450174,  9.3323557 , -0.56187034,  0.36472861,
        0.7424788 ]), 'targetState': array([-0.79907923,  1.83343186, 47.        ]), 'previousTarget': array([ 0.38290266, -1.89120474, 34.28636431])}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6271282176577252
{'scaleFactor': 20, 'timeStep': 39, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-0.79907923,  1.83343186, 47.        ]), 'distance': 2.72309575011834, 'localFrame': array([[ 0.95003748,  0.31200495,  0.00903893],
       [-0.31201769,  0.95007629,  0.        ],
       [-0.00858767, -0.00282031,  0.99995915]]), 'currentState': array([-3.40945192e+00,  1.56779705e+00,  4.77284525e+01,  9.50037478e-01,
        3.12004948e-01,  9.03892802e-03]), 'targetState': array([-0.79907923,  1.83343186, 47.        ]), 'previousTarget': array([-0.79907923,  1.83343186, 47.        ])}
episode index:19983
target thresh 87.12127285559598
target distance 83.50507830389404
model initialize at round 19983
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.8719226 , -16.42849337,  50.51267257]), 'distance': 27.5, 'localFrame': array([[ 0.79237956,  0.22957898,  0.56517974],
       [-0.27828841,  0.96049756,  0.        ],
       [-0.54285376, -0.15728297,  0.8249678 ]]), 'currentState': array([-36.99155987, -22.25986571,  44.18735473,   0.79237956,
         0.22957898,   0.56517974]), 'targetState': array([44.82236956, -3.99439441, 64.        ]), 'previousTarget': array([-12.57054089, -17.05165686,  50.2540311 ])}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6271271106646047
{'scaleFactor': 20, 'timeStep': 51, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([44.82236956, -3.99439441, 64.        ]), 'distance': 2.5881787719911533, 'localFrame': array([[ 0.36563977,  0.917127  ,  0.1586998 ],
       [-0.92889904,  0.37033305,  0.        ],
       [-0.05877178, -0.14741609,  0.98732688]]), 'currentState': array([43.0234197 , -4.7380254 , 62.29428558,  0.36563977,  0.917127  ,
        0.1586998 ]), 'targetState': array([44.82236956, -3.99439441, 64.        ]), 'previousTarget': array([44.82236956, -3.99439441, 64.        ])}
episode index:19984
target thresh 87.12256066391892
target distance 55.0
model initialize at round 19984
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.41785886, -2.38195716, 69.44466148]), 'distance': 27.499999999999996, 'localFrame': array([[-0.15543921,  0.79279258, -0.5893374 ],
       [-0.98131616, -0.19240217,  0.        ],
       [-0.11338979,  0.57832631,  0.80788702]]), 'currentState': array([ 1.85256026, -4.8287977 , 96.71519897, -0.15543921,  0.79279258,
       -0.5893374 ]), 'targetState': array([ 6.99953703,  0.0805066 , 42.        ]), 'previousTarget': array([ 4.7625471 , -3.24894562, 69.78221652])}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.627114231131304
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 16, 'trapConfig': [], 'currentTarget': array([ 6.99953703,  0.0805066 , 42.        ]), 'distance': 3.3907704204821796, 'localFrame': array([[-0.71579952,  0.30291078, -0.62918686],
       [-0.38971913, -0.92093377,  0.        ],
       [-0.57943943,  0.24520616,  0.77725407]]), 'currentState': array([ 9.02709302,  0.53317903, 44.67981873, -0.71579952,  0.30291078,
       -0.62918686]), 'targetState': array([ 6.99953703,  0.0805066 , 42.        ]), 'previousTarget': array([ 6.99953703,  0.0805066 , 42.        ])}
episode index:19985
target thresh 87.12384834346749
target distance 33.50997764438992
model initialize at round 19985
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-12.92022233, -29.80429535,  41.08223676]), 'distance': 27.500000000000004, 'localFrame': array([[ 0.29906987, -0.66290706,  0.68637558],
       [ 0.91152914,  0.41123548,  0.        ],
       [-0.28226199,  0.62565135,  0.72724725]]), 'currentState': array([  7.65753662, -26.04820486,  23.23013266,   0.29906987,
        -0.66290706,   0.68637558]), 'targetState': array([-25.50490696, -32.10139749,  52.        ]), 'previousTarget': array([-12.21453043, -29.18060479,  40.10171406])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6270828534553743
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 77, 'trapConfig': [], 'currentTarget': array([-25.50490696, -32.10139749,  52.        ]), 'distance': 10.907097591409341, 'localFrame': array([[-0.47838216, -0.70901221,  0.51812372],
       [ 0.82895768, -0.55931134,  0.        ],
       [ 0.28979247,  0.42950264,  0.85530568]]), 'currentState': array([-19.43241601, -30.2435394 ,  43.13219335,  -0.47838216,
        -0.70901221,   0.51812372]), 'targetState': array([-25.50490696, -32.10139749,  52.        ]), 'previousTarget': array([-25.50490696, -32.10139749,  52.        ])}
episode index:19986
target thresh 87.12513589425453
target distance 38.0
model initialize at round 19986
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([18.87539543, 17.54571729, 55.83183919]), 'distance': 27.5, 'localFrame': array([[-0.32122529,  0.87869833,  0.3531339 ],
       [-0.93920894, -0.34334613,  0.        ],
       [ 0.12124716, -0.33166652,  0.93557279]]), 'currentState': array([ 5.41904833, 14.84836884, 32.00116497, -0.32122529,  0.87869833,
        0.3531339 ]), 'targetState': array([26.87565962, 19.14938433, 70.        ]), 'previousTarget': array([19.44529691, 17.06180331, 56.05903134])}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6270783097705668
{'scaleFactor': 20, 'timeStep': 63, 'trapCount': 8, 'trapConfig': [], 'currentTarget': array([26.87565962, 19.14938433, 70.        ]), 'distance': 2.413512998948129, 'localFrame': array([[-7.87161882e-03,  9.98288766e-01,  5.79446032e-02],
       [-9.99968914e-01, -7.88486697e-03,  0.00000000e+00],
       [ 4.56885488e-04, -5.79428019e-02,  9.98319800e-01]]), 'currentState': array([ 2.55429770e+01,  1.72728788e+01,  6.92735503e+01, -7.87161882e-03,
        9.98288766e-01,  5.79446032e-02]), 'targetState': array([26.87565962, 19.14938433, 70.        ]), 'previousTarget': array([26.87565962, 19.14938433, 70.        ])}
episode index:19987
target thresh 87.12642331629293
target distance 46.30595952482441
model initialize at round 19987
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([13.87402101, -2.32341302, 82.71318008]), 'distance': 27.5, 'localFrame': array([[ 0.53799723,  0.36975425,  0.75752279],
       [-0.5664052 ,  0.8241269 ,  0.        ],
       [-0.62429491, -0.42906484,  0.65280872]]), 'currentState': array([-2.08463923, 20.06335681, 83.34855399,  0.53799723,  0.36975425,
        0.75752279]), 'targetState': array([ 31.78693594, -27.45160657,  82.        ]), 'previousTarget': array([13.96950742, -3.28130135, 82.        ])}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6270790869014611
{'scaleFactor': 20, 'timeStep': 45, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([ 31.78693594, -27.45160657,  82.        ]), 'distance': 2.4894850590747453, 'localFrame': array([[-0.04930083, -0.83050116,  0.55483084],
       [ 0.99824268, -0.05925843,  0.        ],
       [ 0.03287841,  0.55385582,  0.83196319]]), 'currentState': array([ 3.14910449e+01, -2.53667616e+01,  8.06720671e+01, -4.93008341e-02,
       -8.30501156e-01,  5.54830837e-01]), 'targetState': array([ 31.78693594, -27.45160657,  82.        ]), 'previousTarget': array([ 31.78693594, -27.45160657,  82.        ])}
episode index:19988
target thresh 87.12771060959557
target distance 36.762879084331246
model initialize at round 19988
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 2.63524384, -3.27746794, 34.48661254]), 'distance': 27.499999999999996, 'localFrame': array([[-0.26789697,  0.7590997 , -0.59329491],
       [-0.94299835, -0.33279739,  0.        ],
       [-0.197447  ,  0.55947613,  0.80498518]]), 'currentState': array([ 20.8410532 , -23.34196497,  29.77339022,  -0.26789697,
         0.7590997 ,  -0.59329491]), 'targetState': array([-10.93595184,  11.67925329,  38.        ]), 'previousTarget': array([ 3.5047614 , -4.71653951, 34.4320982 ])}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6270815207381791
{'scaleFactor': 20, 'timeStep': 40, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-10.93595184,  11.67925329,  38.        ]), 'distance': 2.381370582511495, 'localFrame': array([[-0.89830973, -0.21317047,  0.38418483],
       [ 0.23088983, -0.9729799 ,  0.        ],
       [ 0.37380412,  0.08870437,  0.9232562 ]]), 'currentState': array([-9.32417876, 11.00056677, 39.61632238, -0.89830973, -0.21317047,
        0.38418483]), 'targetState': array([-10.93595184,  11.67925329,  38.        ]), 'previousTarget': array([-10.93595184,  11.67925329,  38.        ])}
episode index:19989
target thresh 87.1289977741753
target distance 16.0
model initialize at round 19989
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.07250232, -8.87889105, 32.        ]), 'distance': 18.576031602369657, 'localFrame': array([[ 0.53684553, -0.54141725, -0.64704269],
       [ 0.71009846,  0.70410239,  0.        ],
       [ 0.45558431, -0.45946402,  0.76245377]]), 'currentState': array([-1.52564372, -2.95605004, 46.76023364,  0.53684553, -0.54141725,
       -0.64704269]), 'targetState': array([ 8.07250232, -8.87889105, 32.        ]), 'previousTarget': array([ 8.07250232, -8.87889105, 32.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6270949402846283
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 8.07250232, -8.87889105, 32.        ]), 'distance': 2.448224617862183, 'localFrame': array([[-0.05999577, -0.38058559, -0.92279744],
       [ 0.9878016 , -0.15571772,  0.        ],
       [-0.14369591, -0.91154078,  0.38528546]]), 'currentState': array([ 8.22134675, -7.70847148, 34.14517299, -0.05999577, -0.38058559,
       -0.92279744]), 'targetState': array([ 8.07250232, -8.87889105, 32.        ]), 'previousTarget': array([ 8.07250232, -8.87889105, 32.        ])}
episode index:19990
target thresh 87.13028481004503
target distance 13.180921914023871
model initialize at round 19990
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.37104356,  2.15315241, 29.        ]), 'distance': 11.82050723608531, 'localFrame': array([[-0.46939778, -0.39857347, -0.78791174],
       [ 0.64725742, -0.7622715 ,  0.        ],
       [-0.60060266, -0.50998172,  0.61578819]]), 'currentState': array([ 8.25123249,  1.43038997, 31.030937  , -0.46939778, -0.39857347,
       -0.78791174]), 'targetState': array([-3.37104356,  2.15315241, 29.        ]), 'previousTarget': array([-3.37104356,  2.15315241, 29.        ])}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6271065936993828
{'scaleFactor': 20, 'timeStep': 16, 'trapCount': 2, 'trapConfig': [], 'currentTarget': array([-3.37104356,  2.15315241, 29.        ]), 'distance': 3.2278942118418232, 'localFrame': array([[-0.76612753,  0.18915371, -0.61422266],
       [-0.23969821, -0.97084745,  0.        ],
       [-0.5963165 ,  0.14722807,  0.78913277]]), 'currentState': array([-1.41096809,  2.45158572, 26.45277745, -0.76612753,  0.18915371,
       -0.61422266]), 'targetState': array([-3.37104356,  2.15315241, 29.        ]), 'previousTarget': array([-3.37104356,  2.15315241, 29.        ])}
episode index:19991
target thresh 87.13157171721758
target distance 61.267904955665244
model initialize at round 19991
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-8.77599607,  5.45464522, 74.74159794]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.8223349 , -0.11596688, -0.55706104],
       [ 0.1396398 ,  0.99020237,  0.        ],
       [ 0.55160316, -0.07778789,  0.83047155]]), 'currentState': array([-25.96553443,  19.39346952,  91.06578086,   0.8223349 ,
        -0.11596688,  -0.55706104]), 'targetState': array([ 34.12534018, -29.33361821,  34.        ]), 'previousTarget': array([-9.94537499,  6.49527475, 75.72007321])}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.627100480218745
{'scaleFactor': 20, 'timeStep': 69, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 34.12534018, -29.33361821,  34.        ]), 'distance': 2.4972522762504967, 'localFrame': array([[ 0.16803643, -0.71150156, -0.68229707],
       [ 0.97322644,  0.22984841,  0.        ],
       [ 0.15682489, -0.66402955,  0.73107503]]), 'currentState': array([ 33.04319222, -27.12928965,  33.54595118,   0.16803643,
        -0.71150156,  -0.68229707]), 'targetState': array([ 34.12534018, -29.33361821,  34.        ]), 'previousTarget': array([ 34.12534018, -29.33361821,  34.        ])}
episode index:19992
target thresh 87.13285849570586
target distance 31.0
model initialize at round 19992
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-9.15388437, 18.72516753, 42.90136807]), 'distance': 27.500000000000004, 'localFrame': array([[-0.19257075, -0.44825152,  0.87291871],
       [ 0.9188011 , -0.39472084,  0.        ],
       [ 0.34455921,  0.80203867,  0.48786568]]), 'currentState': array([ 4.97878956,  4.76017999, 23.88830281, -0.19257075, -0.44825152,
        0.87291871]), 'targetState': array([-16.66033691,  26.14255485,  53.        ]), 'previousTarget': array([-8.63895941, 18.27206233, 41.59149518])}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6271061121845914
{'scaleFactor': 20, 'timeStep': 31, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([-16.66033691,  26.14255485,  53.        ]), 'distance': 3.170452950212827, 'localFrame': array([[-0.20124921,  0.46819841,  0.86040049],
       [-0.91872337, -0.39490172,  0.        ],
       [ 0.33977363, -0.79047004,  0.50961848]]), 'currentState': array([-16.06166507,  23.35380705,  51.6157133 ,  -0.20124921,
         0.46819841,   0.86040049]), 'targetState': array([-16.66033691,  26.14255485,  53.        ]), 'previousTarget': array([-16.66033691,  26.14255485,  53.        ])}
episode index:19993
target thresh 87.13414514552274
target distance 31.561278652723377
model initialize at round 19993
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 16.08027117, -19.13788091,  12.05095315]), 'distance': 27.500000000000004, 'localFrame': array([[-0.6239723 ,  0.46552822, -0.62764802],
       [-0.59798314, -0.80150868,  0.        ],
       [-0.50306534,  0.37532293,  0.77849724]]), 'currentState': array([37.35022371, -3.87227316,  3.63665942, -0.6239723 ,  0.46552822,
       -0.62764802]), 'targetState': array([  6.09772937, -26.30242758,  16.        ]), 'previousTarget': array([ 15.73709016, -19.82051443,  12.6404096 ])}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6270747474695677
{'scaleFactor': 20, 'timeStep': 100, 'trapCount': 74, 'trapConfig': [], 'currentTarget': array([  6.09772937, -26.30242758,  16.        ]), 'distance': 9.36249399190212, 'localFrame': array([[-0.57117608, -0.4645852 ,  0.67669674],
       [ 0.63100539, -0.77577844,  0.        ],
       [ 0.52496674,  0.42699929,  0.73626186]]), 'currentState': array([ 11.59695843, -27.85010343,   8.58248876,  -0.57117608,
        -0.4645852 ,   0.67669674]), 'targetState': array([  6.09772937, -26.30242758,  16.        ]), 'previousTarget': array([  6.09772937, -26.30242758,  16.        ])}
episode index:19994
target thresh 87.13543166668106
target distance 32.81982155999033
model initialize at round 19994
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 1, 'trapConfig': [], 'currentTarget': array([12.46337633, -0.99547702, 53.        ]), 'distance': 27.5, 'localFrame': array([[-0.35031025, -0.93098237, -0.10273532],
       [ 0.93593466, -0.3521737 ,  0.        ],
       [-0.03618068, -0.09615355,  0.99470873]]), 'currentState': array([39.80777359, -3.9167676 , 53.        , -0.35031025, -0.93098237,
       -0.10273532]), 'targetState': array([ 6.98795203, -0.4105197 , 53.        ]), 'previousTarget': array([12.46337633, -0.99547702, 53.        ])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6270826796222727
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 5, 'trapConfig': [], 'currentTarget': array([ 6.98795203, -0.4105197 , 53.        ]), 'distance': 3.1593048893723137, 'localFrame': array([[-0.79045514, -0.19404221,  0.58097186],
       [ 0.23840345, -0.9711662 ,  0.        ],
       [ 0.56422023,  0.1385057 ,  0.81392365]]), 'currentState': array([ 8.81804117, -2.74341663, 54.0906755 , -0.79045514, -0.19404221,
        0.58097186]), 'targetState': array([ 6.98795203, -0.4105197 , 53.        ]), 'previousTarget': array([ 6.98795203, -0.4105197 , 53.        ])}
episode index:19995
target thresh 87.13671805919368
target distance 29.0
model initialize at round 19995
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-5.36112233,  5.53928984, 41.39356709]), 'distance': 27.500000000000004, 'localFrame': array([[-0.83201036, -0.06532488, -0.55090055],
       [ 0.07827362, -0.99693191,  0.        ],
       [-0.54921034, -0.04312098,  0.8345709 ]]), 'currentState': array([-16.08745154,  13.97478116,  17.51809123,  -0.83201036,
        -0.06532488,  -0.55090055]), 'targetState': array([-2.39310717,  3.20515804, 48.        ]), 'previousTarget': array([-4.72830064,  5.27318114, 42.70000831])}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6270906109816038
{'scaleFactor': 20, 'timeStep': 25, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-2.39310717,  3.20515804, 48.        ]), 'distance': 3.3822286425980943, 'localFrame': array([[ 0.69978254,  0.24275994,  0.67184225],
       [-0.32774648,  0.94476571,  0.        ],
       [-0.63473352, -0.22019393,  0.74069426]]), 'currentState': array([-4.26558194,  0.50537001, 47.19722144,  0.69978254,  0.24275994,
        0.67184225]), 'targetState': array([-2.39310717,  3.20515804, 48.        ]), 'previousTarget': array([-2.39310717,  3.20515804, 48.        ])}
episode index:19996
target thresh 87.1380043230735
target distance 56.44730556728186
model initialize at round 19996
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([ 4.66559563, 19.10033669, 54.27320986]), 'distance': 27.5, 'localFrame': array([[-0.82966899, -0.44859884, -0.33227768],
       [ 0.47562297, -0.87964924,  0.        ],
       [-0.2922878 , -0.1580389 ,  0.94318161]]), 'currentState': array([31.21666541, 13.94447224, 59.24389944, -0.82966899, -0.44859884,
       -0.33227768]), 'targetState': array([-23.50139416,  24.5699913 ,  49.        ]), 'previousTarget': array([ 6.27503163, 19.26375036, 54.27508364])}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6270883144272775
{'scaleFactor': 20, 'timeStep': 55, 'trapCount': 11, 'trapConfig': [], 'currentTarget': array([-23.50139416,  24.5699913 ,  49.        ]), 'distance': 2.582775221783624, 'localFrame': array([[ 0.64860889, -0.1345263 , -0.74913896],
       [ 0.20308528,  0.97916105,  0.        ],
       [ 0.7335277 , -0.15213909,  0.66241287]]), 'currentState': array([-23.9489774 ,  22.87528598,  50.89693726,   0.64860889,
        -0.1345263 ,  -0.74913896]), 'targetState': array([-23.50139416,  24.5699913 ,  49.        ]), 'previousTarget': array([-23.50139416,  24.5699913 ,  49.        ])}
episode index:19997
target thresh 87.13929045833335
target distance 16.382293140842773
model initialize at round 19997
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.21730267,   7.03420314,  34.        ]), 'distance': 20.838542591207187, 'localFrame': array([[ 0.70929612, -0.52121659, -0.47458643],
       [ 0.59215101,  0.80582702,  0.        ],
       [ 0.38243457, -0.28102683,  0.88020891]]), 'currentState': array([-41.29057713,  23.17983605,  42.4907324 ,   0.70929612,
        -0.52121659,  -0.47458643]), 'targetState': array([-31.21730267,   7.03420314,  34.        ]), 'previousTarget': array([-31.21730267,   7.03420314,  34.        ])}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6271017282656529
{'scaleFactor': 20, 'timeStep': 12, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-31.21730267,   7.03420314,  34.        ]), 'distance': 3.383160231116095, 'localFrame': array([[ 0.45581881, -0.80030372,  0.38954225],
       [ 0.86894273,  0.49491265,  0.        ],
       [-0.19278939,  0.33848991,  0.9210086 ]]), 'currentState': array([-33.55402295,   8.72413072,  35.76908345,   0.45581881,
        -0.80030372,   0.38954225]), 'targetState': array([-31.21730267,   7.03420314,  34.        ]), 'previousTarget': array([-31.21730267,   7.03420314,  34.        ])}
episode index:19998
target thresh 87.14057646498613
target distance 41.0
model initialize at round 19998
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([-3.33161197, -1.74844387, 68.86607766]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.54022223,  0.64645827, -0.53875009],
       [-0.76734081,  0.64123948,  0.        ],
       [ 0.34546783,  0.41340493,  0.84246563]]), 'currentState': array([-8.15604051, -6.19964682, 95.57116485,  0.54022223,  0.64645827,
       -0.53875009]), 'targetState': array([-0.8266248 ,  0.56275344, 55.        ]), 'previousTarget': array([-3.99008005, -2.12646031, 69.55495806])}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.627103487369244
{'scaleFactor': 20, 'timeStep': 42, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([-0.8266248 ,  0.56275344, 55.        ]), 'distance': 3.858373219164003, 'localFrame': array([[ 0.25277062,  0.81144121, -0.52694418],
       [-0.95474916,  0.29741223,  0.        ],
       [ 0.15671964,  0.50309952,  0.84989989]]), 'currentState': array([-0.44598454, -2.07621172, 57.78891014,  0.25277062,  0.81144121,
       -0.52694418]), 'targetState': array([-0.8266248 ,  0.56275344, 55.        ]), 'previousTarget': array([-0.8266248 ,  0.56275344, 55.        ])}
episode index:19999
target thresh 87.14186234304464
target distance 76.0
model initialize at round 19999
at step 0:
{'scaleFactor': 20, 'timeStep': 1, 'trapCount': 0, 'trapConfig': [], 'currentTarget': array([  2.94252962, -11.56416097,  68.14500042]), 'distance': 27.499999999999996, 'localFrame': array([[ 0.5500984 ,  0.40101053, -0.73251779],
       [-0.58907348,  0.80807947,  0.        ],
       [ 0.59193259,  0.4315068 ,  0.68074789]]), 'currentState': array([  0.65316413, -15.90869867,  95.20297148,   0.5500984 ,
         0.40101053,  -0.73251779]), 'targetState': array([ 6.93145166, -3.99436827, 21.        ]), 'previousTarget': array([  2.55666244, -11.72530491,  69.94205678])}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6271014840339725
{'scaleFactor': 20, 'timeStep': 54, 'trapCount': 4, 'trapConfig': [], 'currentTarget': array([ 6.93145166, -3.99436827, 21.        ]), 'distance': 2.2445939206393835, 'localFrame': array([[ 0.64970634, -0.3840466 , -0.65604106],
       [ 0.50885621,  0.86085153,  0.        ],
       [ 0.56475395, -0.33383057,  0.7547252 ]]), 'currentState': array([ 7.9826017 , -4.650353  , 22.87162216,  0.64970634, -0.3840466 ,
       -0.65604106]), 'targetState': array([ 6.93145166, -3.99436827, 21.        ]), 'previousTarget': array([ 6.93145166, -3.99436827, 21.        ])}

Process finished with exit code 0